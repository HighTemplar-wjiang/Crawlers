@article{10.1145/3397317,
	title        = {Leveraging Polarization of WiFi Signals to Simultaneously Track Multiple People},
	author       = {Venkatnarayan, Raghav H. and Shahzad, Muhammad and Yun, Sangki and Vlachou, Christina and Kim, Kyu-Han},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397317},
	url          = {https://doi.org/10.1145/3397317},
	issue_date   = {June 2020},
	abstract     = {This paper presents WiPolar, an approach that simultaneously tracks multiple people using commodity WiFi devices. While two recent papers have also demonstrated multi-person tracking using commodity devices, they either require the people to continuously keep moving without stopping, and/or require the number of people to be input manually, and/or keep the WiFi devices from performing their primary function of data communication. Motivated by the increasing availability of polarized antennas on modern WiFi devices, WiPolar leverages signal polarization to perform accurate multi-person tracking using commodity devices while addressing the three limitations of prior work mentioned above. The key insight that WiPolar is based on is that different people expose different instantaneous horizontal and vertical radar cross-sections to WiFi transmitters due to differences in their physiques and orientations with respect to the transmitter. This enables WiPolar to accurately separate the multipaths reflected from different people, which, in turn, allows it to track them simultaneously. To the best of our knowledge, this is the first work that leverages polarization of WiFi signals to localize and track people. We implement WiPolar using commodity WiFi devices and extensively evaluate it for tracking up to five people in three different environments. Our results show that WiPolar achieved a median tracking error of just 56cm across all experiments. It also accurately tracks people even when they were not moving. WiPolar achieved a median tracking error of 74cm for people that were either stationary or just taking a small pause.},
	articleno    = 45,
	numpages     = 24,
	keywords     = {WiFi, Polarimetery, CSI}
}
@article{10.1145/3397314,
	title        = {Relacks: Reliable Backscatter Communication in Indoor Environments},
	author       = {Katanbaf, Mohamad and Jain, Vivek and Smith, Joshua R.},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397314},
	url          = {https://doi.org/10.1145/3397314},
	issue_date   = {June 2020},
	abstract     = {The increasing number of embedded, plugged-in radios around homes and offices in everyday objects such as appliances, TVs, and smart speakers provides an excellent opportunity to bring ultra-low-power backscatter connectivity to billions of devices. However, backscatter links suffer from high loss due to two consecutive propagations and have to operate on narrow link-budget margins, which makes them more susceptible to multipath losses in indoor environments. To address this, we propose a closed-loop backscatter system that exploits diversity sources such as communication frequency and transceivers antennas based on the channel metrics to deliver reliable coverage over an entire area. We prototype a backscatter system with Bluetooth Low Energy (BLE) transceivers and BLE compatible tags and deploy it in several multipath rich indoor environments. Our evaluations show that we can successfully communicate with a backscatter tag in a 50m2 indoor area. The proposed algorithm for selecting communication parameters achieves an average 2.7 x success rate compared to the random selection while satisfying FCC output power requirements for frequency hopping transceivers.},
	articleno    = 48,
	numpages     = 24,
	keywords     = {Internet of Things, Diversity, Reliability, Backscatter communication, Indoor, Bluetooth}
}
@article{10.1145/3397312,
	title        = {SenCAPTCHA: A Mobile-First CAPTCHA Using Orientation Sensors},
	author       = {Feng, Yunhe and Cao, Qing and Qi, Hairong and Ruoti, Scott},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397312},
	url          = {https://doi.org/10.1145/3397312},
	issue_date   = {June 2020},
	abstract     = {CAPTCHAs are used to distinguish between human- and computer-generated (i.e., bot) online traffic. As there is an ever-increasing amount of online traffic from mobile devices, it is necessary to design CAPTCHAs that work well on mobile devices. In this paper, we present SenCAPTCHA, a mobile-first CAPTCHA that leverages the device's orientation sensors. SenCAPTCHA works by showing users an image of an animal and asking them to tilt their device to guide a red ball into the center of that animal's eye. SenCAPTCHA is especially useful for devices with small screen sizes (e.g., smartphones, smartwatches). In this paper, we describe the design of SenCAPTCHA and demonstrate that it is resilient to various machine learning based attacks. We also report on two usability studies of SenCAPTCHA involving a total of 472 participants; our results show that SenCAPTCHA is viewed as an "enjoyable" CAPTCHA and that it is preferred by over half of the participants to other existing CAPTCHA systems.},
	articleno    = 43,
	numpages     = 26,
	keywords     = {orientation sensor, user study, mobile, CAPTCHA}
}
@article{10.1145/3381007,
	title        = {Personalized HeartSteps: A Reinforcement Learning Algorithm for Optimizing Physical Activity},
	author       = {Liao, Peng and Greenewald, Kristjan and Klasnja, Predrag and Murphy, Susan},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381007},
	url          = {https://doi.org/10.1145/3381007},
	issue_date   = {March 2020},
	abstract     = {With the recent proliferation of mobile health technologies, health scientists are increasingly interested in developing just-in-time adaptive interventions (JITAIs), typically delivered via notifications on mobile devices and designed to help users prevent negative health outcomes and to promote the adoption and maintenance of healthy behaviors. A JITAI involves a sequence of decision rules (i.e., treatment policies) that take the user's current context as input and specify whether and what type of intervention should be provided at the moment. In this work, we describe a reinforcement learning (RL) algorithm that continuously learns and improves the treatment policy embedded in the JITAI as data is being collected from the user. This work is motivated by our collaboration on designing an RL algorithm for HeartSteps V2 based on data collected HeartSteps V1. HeartSteps is a physical activity mobile health application. The RL algorithm developed in this work is being used in HeartSteps V2 to decide, five times per day, whether to deliver a context-tailored activity suggestion.},
	articleno    = 18,
	numpages     = 22,
	keywords     = {Just-in-Time Adaptive Intervention, Reinforcement Learning, Mobile Health}
}
@article{10.1145/3380998,
	title        = {Calm Commute: Guided Slow Breathing for Daily Stress Management in Drivers},
	author       = {Balters, Stephanie and Mauriello, Matthew L. and Park, So Yeon and Landay, James A. and Paredes, Pablo E.},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380998},
	url          = {https://doi.org/10.1145/3380998},
	issue_date   = {March 2020},
	abstract     = {Commutes provide an opportune time and space for interventions that mitigate stress-particularly stress accumulated during the workday. In this study, we test the efficacy and safety of haptic guided slow breathing interventions of short duration while driving. We also present design and experimental implications for evolving these interventions from prior simulator to moving vehicle scenarios. We ran a controlled study (N=24) testing a haptic guided breathing system in a closed circuit under normal and stressful driving conditions. Results show the intervention to be successful in both user adoption and system effectiveness with an 82% rate of engagement in intervention and clear reduction of breathing rate and physiological arousal, with no effect on driving safety and minimal effect on performance. The haptic intervention received positive acceptance from the participants: all indicated a willingness to engage with the intervention in the future and all rated the intervention as safe for traffic applications. The results of this study encourage further investigations exploring the use of the intervention on public roads and monitoring for longitudinal health benefits.},
	articleno    = 38,
	numpages     = 19,
	keywords     = {Commute, Deep Breathing, Slow Breathing, Stress Management, Guided Slow Breathing, Intervention}
}
@article{10.1145/3380988,
	title        = {Real-Time Travel Time Estimation with Sparse Reliable Surveillance Information},
	author       = {Zhang, Wen and Wang, Yang and Xie, Xike and Ge, Chuancai and Liu, Hengchang},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380988},
	url          = {https://doi.org/10.1145/3380988},
	issue_date   = {March 2020},
	abstract     = {Origin-destination (OD) travel time estimation is of paramount importance for applications such as intelligent transportation. In this work, we propose a new solution for OD travel time estimation, with road surveillance camera data. The surveillance information supports accurate and reliable observations at camera-equipped intersections, but is associated with missing and incomplete surveillance records at the camera-free intersections. To overcome this, we propose a modified version of multi-layer graph convolutional networks. The camera surveillance data is used to extract the traffic flow of each intersection, the extracted information serves as the input of the multi-layer GCN based model, based on which the real-time traffic status can be predicted. To enhance the estimation accuracy, we address the effects of various features for the travel time estimation with encoder-decoder networks and embedding techniques. We further improve the generalization of our model by using multi-task learning. Extensive experiments on real datasets are done to verify the effectiveness of our proposals.},
	articleno    = 36,
	numpages     = 23,
	keywords     = {encoder-decoder networks, embedding, multi-layer graph convolutional networks, travel time}
}
@article{10.1145/3380983,
	title        = {HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays},
	author       = {Yan, Yukang and Shi, Yingtian and Yu, Chun and Shi, Yuanchun},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380983},
	url          = {https://doi.org/10.1145/3380983},
	issue_date   = {March 2020},
	abstract     = {We propose HeadCross, a head-based interaction method to select targets on VR and AR head-mounted displays (HMD). Using HeadCross, users control the pointer with head movements and to select a target, users move the pointer into the target and then back across the target boundary. In this way, users can select targets without using their hands, which is helpful when users' hands are occupied by other tasks, e.g., while holding the handrails. However, a major challenge for head-based methods is the false positive problems: unintentional head movements may be incorrectly recognized as HeadCross gestures and trigger the selections. To address this issue, we first conduct a user study (Study 1) to observe user behavior while performing HeadCross and identify the behavior differences between HeadCross and other types of head movements. Based on the results, we discuss design implications, extract useful features, and develop the recognition algorithm for HeadCross. To evaluate HeadCross, we conduct two user studies. In Study 2, we compared HeadCross to the dwell-based selection method, button-press method, and mid-air gesture-based method. Two typical target selection tasks (text entry and menu selection) are tested on both VR and AR interfaces. Results showed that compared to the dwell-based method, HeadCross improved the sense of control; and compared to two hand-based methods, HeadCross improved the interaction efficiency and reduced fatigue. In Study 3, we compared HeadCross to three alternative designs of head-only selection methods. Results show that HeadCross was perceived to be significantly faster than the alternatives. We conclude with the discussion on the interaction potential and limitations of HeadCross.},
	articleno    = 35,
	numpages     = 22,
	keywords     = {hands-free selection, crossing selection, head-based interaction}
}
@article{10.1145/3351269,
	title        = {Designing Drones: Factors and Characteristics Influencing the Perception of Flying Robots},
	author       = {Wojciechowska, Anna and Frey, Jeremy and Mandelblum, Esther and Amichai-Hamburger, Yair and Cauchard, Jessica R.},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351269},
	url          = {https://doi.org/10.1145/3351269},
	issue_date   = {September 2019},
	abstract     = {The last few years have seen a revolution in aerial robotics where personal drones are becoming pervasive to our environments and can be bought by anyone anywhere, including at local supermarkets. As they become ubiquitous to our lives, it is crucial to understand how they are perceived and understood by people. The robotics community has extensively theorized and quantified how robotic agents are perceived as social creatures and how this affects users and passersby. However, drones present different form factors that are yet to be systematically explored. This work aims to fill this gap by understanding people's perceptions of drones and how drones physical features correlate to a series of dimensions. We explored the quadcopters available on the 2018 market and built a dataset of 63 images that were evaluated in a user study (N=307). Using the study results, we present a model of how people understand drones based on their design and which physical features are better suited for people wanting to interact with drones. Our findings highlight that safety features have a negative effect on several dimensions including trust. Our work contributes a set of design guidelines for future personal drones and concludes on the implications for ubiquitous computing.},
	articleno    = 111,
	numpages     = 19,
	keywords     = {Design, UAV, Uncanny Valley, Zoomorphism, Amazon Mechanical Turk, Human-Drone Interaction, Anthropomorphism, Human-Robot Interaction}
}
@article{10.1145/3351251,
	title        = {VoltKey: Continuous Secret Key Generation Based on Power Line Noise for Zero-Involvement Pairing and Authentication},
	author       = {Lee, Kyuin and Klingensmith, Neil and Banerjee, Suman and Kim, Younghyun},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351251},
	url          = {https://doi.org/10.1145/3351251},
	issue_date   = {September 2019},
	abstract     = {The explosive proliferation of Internet-of-Things (IoT) ecosystem fuels the needs for a mechanism for the user to easily and securely interconnect multiple heterogeneous devices with minimal involvement. However, the current paradigm of context-unaware pairing and authentication methods (e.g., using a preset or user-defined password) poses severe challenges in the usability and security aspects due to the limited and siloed user interface that requires substantial effort on establishing or maintaining a secure network. In this paper, we present VoltKey, a method that transparently and continuously generates secret keys for colocated devices, leveraging spatiotemporally unique noise contexts observed in commercial power line infrastructure. We introduce a novel scheme to extract randomness from power line noise and securely convert it into the same key by a pair of devices. The unique noise pattern observed only by trusted devices connected to a local power line prevents malicious devices without physical access from obtaining unauthorized access to the network. VoltKey can be implemented on top of standard USB power supplies as a platform-agnostic bolt-on addition to any IoT devices or wireless access points that are constantly connected to the power outlet. Through extensive experiments under various realistic deployment environments, we demonstrate that VoltKey can successfully establish a secret key among colocated devices with over 90% success rate, while effectively rejecting malicious devices that do not have access to the local power line (but may have access to a spatially nearby line).},
	articleno    = 93,
	numpages     = 26,
	keywords     = {key generation, power line noise, device authentication, device pairing}
}
@article{10.1145/3328934,
	title        = {ScratchThat: Supporting Command-Agnostic Speech Repair in Voice-Driven Assistants},
	author       = {Wu, Jason and Ahuja, Karan and Li, Richard and Chen, Victor and Bigham, Jeffrey},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328934},
	url          = {https://doi.org/10.1145/3328934},
	issue_date   = {June 2019},
	abstract     = {Speech interfaces have become an increasingly popular input method for smartphone-based virtual assistants, smart speakers, and Internet of Things (IoT) devices. While they facilitate rapid and natural interaction in the form of voice commands, current speech interfaces lack natural methods for command correction. We present ScratchThat, a method for supporting command-agnostic speech repair in voice-driven assistants, suitable for enabling corrective functionality within third-party commands. Unlike existing speech repair methods, ScratchThat is able to automatically infer query parameters and intelligently select entities in a correction clause for editing. We conducted three evaluations to (1) elicit natural forms of speech repair in voice commands, (2) compare the interaction speed and NASA TLX score of the system to existing voice-based correction methods, and (3) assess the accuracy of the ScratchThat algorithm. Our results show that (1) speech repair for voice commands differ from previous models for conversational speech repair, (2) methods for command correction based on speech repair are significantly faster than other voice-based methods, and (3) the ScratchThat algorithm facilitates accurate command repair as rated by humans (77% accuracy) and machines (0.94 BLEU score). Finally, we present several ScratchThat use cases, which collectively demonstrate its utility across many applications.},
	articleno    = 63,
	numpages     = 17,
	keywords     = {Dialog Interaction, Speech Interfaces, Voice User Interfaces, Conversational Agents, Error Correction, Speech Repair}
}
@article{10.1145/3328925,
	title        = {The PARK Framework for Automated Analysis of Parkinson's Disease Characteristics},
	author       = {Langevin, Raina and Ali, Mohammad Rafayet and Sen, Taylan and Snyder, Christopher and Myers, Taylor and Dorsey, E. Ray and Hoque, Mohammed Ehsan},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328925},
	url          = {https://doi.org/10.1145/3328925},
	issue_date   = {June 2019},
	abstract     = {There are about 900,000 people with Parkinson's disease (PD) in the United States. Even though there are benefits of early treatment, unfortunately, over 40% of individuals with PD over 65 years old do not see a neurologist. It is often very difficult for these individuals to get to a physician's office for diagnosis and subsequent monitoring. To address this problem, we present PARK, Parkinson's Analysis with Remote Kinetic-tasks. PARK instructs and guides users through six motor tasks and one audio task selected from the standardized MDS-UPDRS rating scale and records their performance via webcam. An initial experiment was conducted with 127 participants with PD and 127 age-matched controls, in which a total of 1,778 video recordings were collected. 90.6% of the PD participants agreed that PARK was easy to use, and 93.7% mentioned that they would use the system in the future. We explored objective differences between those with and without PD. A novel motion feature based on the Fast Fourier Transform (FFT) of optical flow in a region of interest was designed to quantify these differences in the collected video recordings. Additionally, we found that facial action unit AU4 (brow lowerer) was expressed significantly more often, while AU12 (lip corner puller) was expressed less often in various tasks for participants with PD.},
	articleno    = 54,
	numpages     = 22,
	keywords     = {Parkinson's disease, Accessible, Remote, MDS-UPDRS}
}
@article{10.1145/3328924,
	title        = {Toccata: Supporting Classroom Orchestration with Activity Based Computing},
	author       = {Lachand, Valentin and Michel, Christine and Tabard, Aur\'{e}lien},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328924},
	url          = {https://doi.org/10.1145/3328924},
	issue_date   = {June 2019},
	abstract     = {We present Toccata, a system that facilitates the management of rich multi-device pedagogical activities. Through interviews with high school teachers, we identified a set of barriers to conducting digital activities in schools: set-up time, network problems, difficulties in following and changing plans as activities unfold. We designed and developed Toccata to support the planning of pedagogical activities (scripting), seamless sharing of content and collaboration across people and devices, live management of activities in the classroom, roaming for situations outside classrooms, resumption across sessions, and resilience to unstable network conditions. We deployed Toccata in three classes, over seven teaching sessions, involving a total of 69 students. Together, these deployments show that Toccata is a generic solution for managing multi-device activities in schools. We reflect on how Activity Based Computing principles support Orchestration in Toccata, and discuss the design opportunities it creates such as better awareness of learners' activity, micro-orchestration techniques for enabling teachers to better control devices in classrooms, or supporting reflective practices of teachers.},
	articleno    = 53,
	numpages     = 24
}
@article{10.1145/3328922,
	title        = {VPS Tactile Display: Tactile Information Transfer of Vibration, Pressure, and Shear},
	author       = {Kim, Lawrence H. and Castillo, Pablo and Follmer, Sean and Israr, Ali},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328922},
	url          = {https://doi.org/10.1145/3328922},
	issue_date   = {June 2019},
	abstract     = {One of the challenges in the field of haptics is to provide meaningful and realistic sensations to users. While most real world tactile sensations are composed of multiple dimensions, most commercial product only include vibration as it is the most cost effective solution. To improve on this, we introduce VPS (Vibration, Pressure, Shear) display, a multi-dimensional tactile array that increases information transfer by combining Vibration, Pressure, and Shear similar to how RGB LED combines red, blue, and green to create new colors. We characterize the device performance and dynamics for each tactile dimension in terms of its force and displacement profiles, and evaluate information transfer of the VPS display through a stimulus identification task. Our results indicate that the information transfer through a single taxel increases from 0.56 bits to 2.15 bits when pressure and shear are added to vibrations with a slight decrease in identification accuracy. We also explored the pleasantness and continuity of VPS and the study results reveal that tactile strokes in shear mode alone are rated highest on perceived pleasantness and continuity.},
	articleno    = 51,
	numpages     = 17,
	keywords     = {VPS Tactile Display, Haptics, Pleasant Touch, Multi-dimensional Haptics}
}
@article{10.1145/3314411,
	title        = {Measuring the Effects of Stress on Mobile Interaction},
	author       = {Sarsenbayeva, Zhanna and van Berkel, Niels and Hettiachchi, Danula and Jiang, Weiwei and Dingler, Tilman and Velloso, Eduardo and Kostakos, Vassilis and Goncalves, Jorge},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314411},
	url          = {https://doi.org/10.1145/3314411},
	issue_date   = {March 2019},
	abstract     = {Research shows that environmental factors such as ambient noise and cold ambience can render users situationally impaired, adversely affecting interaction with mobile devices. However, an internal factor which is known to negatively impact cognitive abilities -- stress -- has not been systematically investigated in terms of its impact on mobile interaction. In this paper, we report a study where we use the Trier Social Stress Test to induce stress on participants, and investigate its effect on three aspects of mobile interaction: target acquisition, visual search, and text entry. We find that stress reduces completion time and accuracy during target acquisition tasks, as well as completion time during visual search tasks. Finally, we are able to directly contrast the magnitude of these effects to previously published effects of environmentally-caused impairments. Our work contributes to the growing body of literature on situational impairments.},
	articleno    = 24,
	numpages     = 18,
	keywords     = {Trier, performance, mobile interaction, situational impairments, stress, Social Stress Test, Smartphones}
}
@article{10.1145/3314406,
	title        = {Reconstructing Human Joint Motion with Computational Fabrics},
	author       = {Liu, Ruibo and Shao, Qijia and Wang, Siqi and Ru, Christina and Balkcom, Devin and Zhou, Xia},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314406},
	url          = {https://doi.org/10.1145/3314406},
	issue_date   = {March 2019},
	abstract     = {Accurate and continuous monitoring of joint rotational motion is crucial for a wide range of applications such as physical rehabilitation [6, 85] and motion training [22, 54, 68]. Existing motion capture systems, however, either need instrumentation of the environment, or fail to track arbitrary joint motion, or impose wearing discomfort by requiring rigid electrical sensors right around the joint area. This work studies the use of everyday fabrics as a flexible and soft sensing medium to monitor joint angular motion accurately and reliably. Specifically we focus on the primary use of conductive stretchable fabrics to sense the skin deformation during joint motion and infer the joint rotational angle. We tackle challenges of fabric sensing originated by the inherent properties of elastic materials by leveraging two types of sensing fabric and characterizing their properties based on models in material science. We apply models from bio-mechanics to infer joint angles and propose the use of dual strain sensing to enhance sensing robustness against user diversity and fabric position offsets. We fabricate prototypes using off-the-shelf fabrics and micro-controller. Experiments with ten participants show 9.69° median angular error in tracking joint angle and its sensing robustness across various users and activities.},
	articleno    = 19,
	numpages     = 26,
	keywords     = {smart fabric/textile, joint motion sensing, fabric/textile sensing}
}
@article{10.1145/3314398,
	title        = {Privacy-Preserving Cross-Domain Location Recommendation},
	author       = {Gao, Chen and Huang, Chao and Yu, Yue and Wang, Huandong and Li, Yong and Jin, Depeng},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314398},
	url          = {https://doi.org/10.1145/3314398},
	issue_date   = {March 2019},
	abstract     = {Cross-domain recommendation is a typical solution for data sparsity and cold start issue in the field of location recommendation. Specifically, data of an auxiliary domain is leveraged to improve the recommendation of the target domain. There is a typical scenario that two interaction domains (location based check-in service, for example) combine data to perform the cross-domain location recommendation task. Existing approaches are based on the assumption that the interaction data from the auxiliary domain can be directly shared across domains. However, such an assumption is not reasonable, since in the real world those domains may be operated by different companies. Therefore, directly sharing raw data may violate business privacy policy and increase the risk of privacy leakage since the user-location interaction records are very sensitive.In this paper, we propose a framework named privacy-preserving cross-domain location recommendation which works in two stages. First, for the interaction data from the auxiliary domain, we adopt a differential privacy based protection mechanism to hide the real locations of each user to meet the criterion of differential privacy. Then we share the protected user-location interaction to the target domain. Second, we develop a new method of Confidence-aware Collective Matrix Factorization (CCMF) to effectively exploit the transferred interaction data. To verify its efficacy, we collect two real-world datasets suitable for the task. Extensive experiments demonstrate that our proposed framework achieves the best performance compared with the state-of-the-art baseline methods. We further demonstrate that our method can alleviate the data sparsity issue significantly while protecting users' location privacy.},
	articleno    = 11,
	numpages     = 21,
	keywords     = {Differential Privacy, Cross-domain Location Recommendation, Matrix Factorization}
}
@article{10.1145/3314396,
	title        = {Evaluating the Impact of Technology Assisted Hotspot Policing on Situational Awareness and Task-Load},
	author       = {Engelbrecht, Hendrik and Lukosch, Stephan G. and Datcu, Dragos},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314396},
	url          = {https://doi.org/10.1145/3314396},
	issue_date   = {March 2019},
	abstract     = {Everyday field work of a police officer requires the perception, filtering and understanding of large amounts of information in highly dynamic situations. This presents opportunities for ICT to alleviate strain on officers by providing adequate information provisioning. We evaluate the usage of a mobile location-based hotspot policing system, comprised of a smartphone, smartwatch and a web-application, during real field work with officers in high and low hotspot density locations. We use a repeated measures design to compare possible effects with our baseline measure, i.e. field work without using the system. Usability, task-load and situational awareness (SA), as well as possible mediators, are evaluated to gain insight into the differences between modes of transportation and the overall viability of the system itself. No significant difference was found between the two locations. Officers using the system scored high on usability measures and interview feedback was largely positive. Measures on SA remained stable throughout baseline and experimental shifts. Task-load was significantly higher with the use of the system. The contradiction in these findings can be explained by showing the differences in the nature of field work with and without the system.},
	articleno    = 9,
	numpages     = 18,
	keywords     = {Situation Awareness, Mobile Computing, Location-Based System, Experiment, Police, Notification Systems}
}
@article{10.1145/3314395,
	title        = {Modeling Biobehavioral Rhythms with Passive Sensing in the Wild: A Case Study to Predict Readmission Risk after Pancreatic Surgery},
	author       = {Doryab, Afsaneh and Dey, Anind K. and Kao, Grace and Low, Carissa},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314395},
	url          = {https://doi.org/10.1145/3314395},
	issue_date   = {March 2019},
	abstract     = {Biobehavioral rhythms are associated with numerous health and life outcomes. We study the feasibility of detecting rhythms in data that is passively collected from Fitbit devices and using the obtained model parameters to predict readmission risk after pancreatic surgery. We analyze data from 49 patients who were tracked before surgery, in hospital, and after discharge. Our analysis produces a model of individual patients' rhythms for each stage of treatment that is predictive of readmission. All of the rhythm-based models outperform the traditional approaches to readmission risk stratification that uses administrative data.},
	articleno    = 8,
	numpages     = 21,
	keywords     = {Feature Extraction, Data Processing, Patient Readmission, Mobile and Wearable Sensing, Machine Learning, Circadian Rhythm, Cancer}
}
@article{10.1145/3287077,
	title        = {NALoc: Nonlinear Ambient-Light-Sensor-Based Localization System},
	author       = {Yang, Lin and Wang, Zeyu and Wang, Wei and Zhang, Qian},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287077},
	url          = {https://doi.org/10.1145/3287077},
	issue_date   = {December 2018},
	abstract     = {Visible light position (VLP) is a revolutionary technique which enables many promising applications. As the human eye is sensitive to low-rate changes, VLP systems often convey location information through light flickering over 1 KHz, which induces a heavy burden on the VLP receiver. Existing solutions either rely on the high-resolution cameras or a dedicated photodiode to capture the location information, but the high power consumption and extraction deployment cost hinder their wide adoption. In this paper, we present a light-weight VLP system, NALoc, which leverages the ambient light sensor (ALS) readily on many mobile devices to sense high-frequency-modulation location information. To overcome the insufficient sampling ability of ALS, we exploit the nonlinearity of ALS to sense the leaked energy from high frequency(≥ 1 KHz) at a low sampling rate(100 Hz). Extensive evaluations demonstrate that our system can achieve a decimeter-level localization accuracy with about 1 mW power consumption, which is 2000 times less than existing camera-based VLP solutions.},
	articleno    = 199,
	numpages     = 22,
	keywords     = {Indoor Localization, Visible Light, Ambient Light Sensor, Nonlinearity, Signal Processing}
}
@article{10.1145/3287073,
	title        = {Large-Scale Automatic Depression Screening Using Meta-Data from WiFi Infrastructure},
	author       = {Ware, Shweta and Yue, Chaoqun and Morillo, Reynaldo and Lu, Jin and Shang, Chao and Kamath, Jayesh and Bamis, Athanasios and Bi, Jinbo and Russell, Alexander and Wang, Bing},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287073},
	url          = {https://doi.org/10.1145/3287073},
	issue_date   = {December 2018},
	abstract     = {Depression is a serious public health problem. Current diagnosis techniques rely on physician-administered or patient self-administered interview tools, which are burdensome and suffer from recall bias. Recent studies have proposed new approaches that use sensing data collected on smartphones to serve as "human sensors" for automatic depression screening. These approaches, however, require running an app on the phones for continuous data collection. We explore a novel approach that uses data collected from WiFi infrastructure for large-scale automatic depression screening. Specifically, when smartphones connect to a WiFi network, their locations (and hence the locations of the users) can be determined by the access points that they associate with; the location information over time provides important insights into the behavior of the users, which can be used for depression screening. To investigate the feasibility of this approach, we have analyzed two datasets, each collected over several months, involving tens of participants recruited from a university. Our results demonstrate that WiFi meta-data is effective for passive depression screening: the F1 scores are as high as 0.85 for predicting depression, comparable to those obtained by using sensing data collected directly from smartphones.},
	articleno    = 195,
	numpages     = 27,
	keywords     = {Depression assessment, Sensor data analysis, Prediction}
}
@article{10.1145/3287068,
	title        = {A Cuttable Wireless Power Transfer Sheet},
	author       = {Takahashi, Ryo and Sasatani, Takuya and Okuya, Fuminori and Narusue, Yoshiaki and Kawahara, Yoshihiro},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287068},
	url          = {https://doi.org/10.1145/3287068},
	issue_date   = {December 2018},
	abstract     = {We propose a cuttable wireless power transfer sheet which allows users to modify its size and shape. This intuitive manipulation allows users to easily add wireless power transmission capabilities to everyday objects. The properties of the sheet such as thinness, flexibility, and lightness make our sheet highly compatible with various configurations. We contribute a set of technical principles for the design of circuitry, which integrates H-tree wiring and time division power supply techniques. H-tree wiring allows the sheet to remain functional even when cut from the outside of the sheet, whereas time division power supply avoids the reduction in power transfer efficiency caused by the magnetic interference between adjacent transmitter coils. Through the evaluations, we found that our time division power supply scheme mitigates the degradation of power transfer efficiency and successfully improves the average efficiency. Furthermore, we present four applications which integrates our sheet into daily objects: wireless charging furniture, bag, jacket, and craft; these applications confirmed the feasibility of our prototype.},
	articleno    = 190,
	numpages     = 25,
	keywords     = {thin film coil array, cuttable electronics, Wireless power transfer sheet}
}
@article{10.1145/3287063,
	title        = {GymCam: Detecting, Recognizing and Tracking Simultaneous Exercises in Unconstrained Scenes},
	author       = {Khurana, Rushil and Ahuja, Karan and Yu, Zac and Mankoff, Jennifer and Harrison, Chris and Goel, Mayank},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287063},
	url          = {https://doi.org/10.1145/3287063},
	issue_date   = {December 2018},
	abstract     = {Worn sensors are popular for automatically tracking exercises. However, a wearable is usually attached to one part of the body, tracks only that location, and thus is inadequate for capturing a wide range of exercises, especially when other limbs are involved. Cameras, on the other hand, can fully track a user's body, but suffer from noise and occlusion. We present GymCam, a camera-based system for automatically detecting, recognizing and tracking multiple people and exercises simultaneously in unconstrained environments without any user intervention. We collected data in a varsity gym, correctly segmenting exercises from other activities with an accuracy of 84.6%, recognizing the type of exercise at 93.6% accuracy, and counting the number of repetitions to within ± 1.7 on average. GymCam advances the field of real-time exercise tracking by filling some crucial gaps, such as tracking whole body motion, handling occlusion, and enabling single-point sensing for a multitude of users.},
	articleno    = 185,
	numpages     = 17,
	keywords     = {computer vision, single-point sensing, exercise tracking, health sensing}
}
@article{10.1145/3287060,
	title        = {Modeling and Forecasting the Popularity Evolution of Mobile Apps: A Multivariate Hawkes Process Approach},
	author       = {Ouyang, Yi and Guo, Bin and Guo, Tong and Cao, Longbing and Yu, Zhiwen},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287060},
	url          = {https://doi.org/10.1145/3287060},
	issue_date   = {December 2018},
	abstract     = {In recent years, with the rapid development of mobile app ecosystem, the number and categories of mobile apps have grown tremendously. However, the global prevalence of mobile apps also leads to fierce competition. As a result, many apps will disappear. To thrive in this competitive app market, it is vital for app developers to understand the popularity evolution of their mobile apps, and inform strategic decision-making for better mobile app development. Therefore, it is significant and necessary to model and forecast the future popularity evolution of mobile apps. The popularity evolution of mobile apps is usually a long-term process, affected by various complex factors. However, existing works lack the capabilities to model such complex factors. To better understand the popularity evolution, in this paper, we aim to forecast the popularity evolution of mobile apps by incorporating complex factors, i.e., exogenous stimulis and endogenous excitations. Specifically, we propose a model based on the Multivariate Hawkes Process (MHP), which is an exogenous stimulis-driven self-exciting point process, to model the exogenous stimulis and endogenous excitations simultaneously. Extensive experimental studies on a real-world dataset from app store demonstrate that MHP outperforms the state-of-the-art methods regarding popularity evolution forecasting.},
	articleno    = 182,
	numpages     = 23,
	keywords     = {Endogenous Excitations, Popularity Evolution, Mobile Apps, Hawkes Process, Exogenous Stimulis}
}
@article{10.1145/3287058,
	title        = {V-Speech: Noise-Robust Speech Capturing Glasses Using Vibration Sensors},
	author       = {Maruri, H\'{e}ctor A. Cordourier and Lopez-Meyer, Paulo and Huang, Jonathan and Beltman, Willem Marco and Nachman, Lama and Lu, Hong},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287058},
	url          = {https://doi.org/10.1145/3287058},
	issue_date   = {December 2018},
	abstract     = {Smart glasses are often used in public environments or industrial scenarios that are relatively noisy. Background noise and sound from competing speakers deteriorate voice communication or performance of automatic speech recognition (ASR). Typically, signal processing techniques are used to reduce noise and enhance voice quality, but they have limitations in performance, hardware and/or computing resources. Voice capturing techniques using bone conducting on the head have been proposed in some experimental and commercial devices, with good robustness against environmental noise, but limited by signal distortions inherent to the capturing method. We present V-Speech, a novel sensing and signal processing solution that enables speech recognition and human-to-human communication in very noisy environments. It captures the voice signal with a vibration sensor located in the nasal pads of smart glasses and performs a transformation to the sensor signal in order to mimic that of a regular microphone in low noise conditions. The signal transformation is key, as it eliminates the "nasal distortion" that is introduced for nasal phonemes in the speech induced vibrations of the nasal bone. The output of V-Speech has low noise, sounds natural, and can be used in voice communication or as input to an off-the-shelf ASR service. We evaluated V-Speech in noise-free and noisy conditions with 30 volunteer speakers uttering 145 phrases and validated its performance on ASR engines and with assessments of voice quality using the Perceptual Evaluation of Speech Quality (PESQ) metric. The results show in extreme noise conditions a mean improvement of 50% for Word Error Rate (WER), and 1.0 on a scale of 5.0 for PESQ. In addition, real life recordings were made under various representative noise conditions, some with sound pressure levels of 93 dBA, which require hearing protection. Subjective listening tests were conducted according to a modified ITU P.835 approach to determine intelligibility, naturalness and overall quality. Under these extreme conditions, where V-Speech achieved 30 dB SNR, subjective results show the speech is intelligible, and the naturalness of the speech is rated as fair to good. This enables clear voice communication in challenging work environments, for example in places with industrial, factory, mining and construction noise. With our proposed smart switching technique between a regular microphone signal and V-Speech, the optimal quality can be maintained from low to high noise conditions.},
	articleno    = 180,
	numpages     = 23,
	keywords     = {Smart glasses, Vibration sensing, Voice capturing, Head worn devices, Accelerometer}
}
@article{10.1145/3287052,
	title        = {CueAuth: Comparing Touch, Mid-Air Gestures, and Gaze for Cue-Based Authentication on Situated Displays},
	author       = {Khamis, Mohamed and Trotter, Ludwig and M\"{a}kel\"{a}, Ville and Zezschwitz, Emanuel von and Le, Jens and Bulling, Andreas and Alt, Florian},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287052},
	url          = {https://doi.org/10.1145/3287052},
	issue_date   = {December 2018},
	abstract     = {Secure authentication on situated displays (e.g., to access sensitive information or to make purchases) is becoming increasingly important. A promising approach to resist shoulder surfing attacks is to employ cues that users respond to while authenticating; this overwhelms observers by requiring them to observe both the cue itself as well as users' response to the cue. Although previous work proposed a variety of modalities, such as gaze and mid-air gestures, to further improve security, an understanding of how they compare with regard to usability and security is still missing as of today. In this paper, we rigorously compare modalities for cue-based authentication on situated displays. In particular, we provide the first comparison between touch, mid-air gestures, and calibration-free gaze using a state-of-the-art authentication concept. In two in-depth user studies (N=20, N=17) we found that the choice of touch or gaze presents a clear tradeoff between usability and security. For example, while gaze input is more secure, it is also more demanding and requires longer authentication times. Mid-air gestures are slightly slower and more secure than touch but users hesitate to use them in public. We conclude with three significant design implications for authentication using touch, mid-air gestures, and gaze and discuss how the choice of modality creates opportunities and challenges for improved authentication in public.},
	articleno    = 174,
	numpages     = 22,
	keywords     = {Pursuits, Privacy, Public Displays, SwiPIN, Eye Tracking}
}
@article{10.1145/3287051,
	title        = {Why Are They Collecting My Data? Inferring the Purposes of Network Traffic in Mobile Apps},
	author       = {Jin, Haojian and Liu, Minyi and Dodhia, Kevan and Li, Yuanchun and Srivastava, Gaurav and Fredrikson, Matthew and Agarwal, Yuvraj and Hong, Jason I.},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287051},
	url          = {https://doi.org/10.1145/3287051},
	issue_date   = {December 2018},
	abstract     = {Many smartphone apps collect potentially sensitive personal data and send it to cloud servers. However, most mobile users have a poor understanding of why their data is being collected. We present MobiPurpose, a novel technique that can take a network request made by an Android app and then classify the data collection purposes, as one step towards making it possible to explain to non-experts the data disclosure contexts. Our purpose inference works by leveraging two observations: 1) developer naming conventions (e.g., URL paths) of ten offer hints as to data collection purposes, and 2) external knowledge, such as app metadata and information about the domain name, are meaningful cues that can be used to infer the behavior of different traffic requests. MobiPurpose parses each traffic request body into key-value pairs, and infers the data type and data collection purpose of each key-value pair using a combination of supervised learning and text pattern bootstrapping. We evaluated MobiPurpose's effectiveness using a dataset cross-labeled by ten human experts. Our results show that MobiPurpose can predict the data collection purpose with an average precision of 84% (among 19 unique categories).},
	articleno    = 173,
	numpages     = 27,
	keywords     = {Mobile Privacy, Purposes of Data Collection, Contextual Integrity, Privacy in Context}
}
@article{10.1145/3287039,
	title        = {What is That in Your Hand? Recognizing Grasped Objects via Forearm Electromyography Sensing},
	author       = {Fan, Junjun and Fan, Xiangmin and Tian, Feng and Li, Yang and Liu, Zitao and Sun, Wei and Wang, Hongan},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287039},
	url          = {https://doi.org/10.1145/3287039},
	issue_date   = {December 2018},
	abstract     = {Knowing the object in hand can offer essential contextual information revealing a user's fine-grained activities. In this paper, we investigate the feasibility, accuracy, and robustness of recognizing the uninstrumented object in a user's hand by sensing and decoding her forearm muscular activities via off-the-shelf electromyography (EMG) sensors. We present results from three studies to advance our fundamental understanding of the opportunities that EMG brings in object interaction recognition. In the first study, we investigated the influence of physical properties of objects such as shape, size, and weight on EMG signals. We also conducted a thorough exploration of the feature spaces and sensor positions which can provide a solid base to rely on for future designers and practitioners for such interactive technique. In the second study, we assessed the feasibility and accuracy of inferring the types of grasped objects via using forearm muscular activity as a cue. Our results indicate that the types of objects can be recognized with up to 94.2% accuracy by employing user-dependent training. In the third study, we investigated the robustness of this approach in a realistic office setting where users were allowed to interact with objects as they would naturally. Our approach achieved up to 82.5% accuracy in discriminating 15 types of objects, even when training and testing phrases were purposefully performed on different days to incorporate changes in EMG patterns over time. Overall, this work contributes a set of fundamental findings and guidelines on using EMG technologies for object-based activity tracking.},
	articleno    = 161,
	numpages     = 24,
	keywords     = {Grasped Object Detection, Activity Tracking, Electromyography (EMG), Context-Aware Applications}
}
@article{10.1145/3264950,
	title        = {Unlock with Your Heart: Heartbeat-Based Authentication on Commercial Mobile Phones},
	author       = {Wang, Lei and Huang, Kang and Sun, Ke and Wang, Wei and Tian, Chen and Xie, Lei and Gu, Qing},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264950},
	url          = {https://doi.org/10.1145/3264950},
	issue_date   = {September 2018},
	abstract     = {In this paper, we propose to use the vibration of the chest in response to the heartbeat as a biometric feature to authenticate the user on mobile devices. We use the built-in accelerometer to capture the heartbeat signals on commercial mobile phones. The user only needs to press the phone on his/her chest, and the system can identify the user within a few heartbeats. To reliably extract heartbeat features, we design a two-step alignment scheme that can handle the natural variability in human heart rates. We further use an adaptive template selection scheme to authenticate the user under different body postures and body states. Based on heartbeat signals collected on twenty users, the experimental results show that our method can achieve an authentication accuracy of 96.49% and the heartbeat features are stable over a period of three months.},
	articleno    = 140,
	numpages     = 22,
	keywords     = {Biometrics-based Authentication, Mobile System}
}
@article{10.1145/3264944,
	title        = {Augmenting User Identification with WiFi Based Gesture Recognition},
	author       = {Shahzad, Muhammad and Zhang, Shaohu},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264944},
	url          = {https://doi.org/10.1145/3264944},
	issue_date   = {September 2018},
	abstract     = {Over the last few years, researchers have proposed several WiFi based gesture recognition systems that can recognize predefined gestures performed by users at runtime. As most environments are inhabited by multiple users, the true potential of WiFi based gesture recognition can be unleashed only when each user can independently define the actions that the system should take when the user performs a certain predefined gesture. To enable this, a gesture recognition system should not only be able to recognize any given predefined gesture, but should also be able to identify the user that performed it. Unfortunately, none of the prior WiFi based gesture recognition systems can identify the user performing the gesture. In this paper, we propose WiID, a WiFi and gesture based user identification system that can identify the user as soon as he/she performs a predefined gesture at runtime. WiID integrates with the WiFi based gesture recognition systems as an add-on module whose sole objective is to identify the users that perform the predefined gestures. The design of WiID is based on our novel result which states that the timeseries of the frequencies that appear in WiFi channel's frequency response while performing a given gesture are different in the samples of that gesture performed by different users, and are similar in the samples of that gesture performed by the same user. We implemented and extensively evaluated WiID in a variety of environments using a comprehensive data set comprising over 25,000 gesture samples.},
	articleno    = 134,
	numpages     = 27,
	keywords     = {User identification, Gesture recognition, WiFi}
}
@article{10.1145/3264940,
	title        = {Estimating the Physical Distance between Two Locations with Wi-Fi Received Signal Strength Information Using Obstacle-Aware Approach},
	author       = {Nakatani, Tomoya and Maekawa, Takuya and Shirakawa, Masumi and Hara, Takahiro},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264940},
	url          = {https://doi.org/10.1145/3264940},
	issue_date   = {September 2018},
	abstract     = {This study presents a new method for estimating the physical distance between two locations using Wi-Fi signals from APs observed by Wi-Fi signal receivers such as smartphones. We assume that a Wi-Fi signal strength vector is observed at location A and another Wi-Fi signal strength vector is observed at location B. With these two Wi-Fi signal strength vectors, we attempt to estimate the physical distance between locations A and B. In this study, we estimate the physical distance based on supervised machine learning and do not use labeled training data collected in an environment of interest. Note that, because signal propagation is greatly affected by obstacles such as walls, precisely estimating the distance between locations A and B is difficult when there is a wall between locations A and B. Our method first estimates whether or not there is a wall between locations A and B focusing on differences in signal propagation properties between 2.4 GHz and 5 GHz signals, and then estimates the physical distance using a neural network depending on the presence of walls. Because our approach is based on Wi-Fi signal strengths and does not require a site survey in an environment of interest, we believe that various context-aware applications can be easily implemented based on the distance estimation technique such as low-cost indoor navigation, the analysis and discovery of communities and groups, and Wi-Fi geo-fencing. Our experiment revealed that the proposed method achieved an MAE of about 3-4 meters and the performance is almost identical to an environment-dependent method, which is trained on labeled data collected in the same environment.},
	articleno    = 130,
	numpages     = 26,
	keywords     = {distance estimation, Context recognition, Wi-Fi signal, deep neural network, pattern recognition, handheld device, context awareness}
}
@article{10.1145/3264939,
	title        = {A Tale of Two Interactions: Inferring Performance in Hospitality Encounters from Cross-Situation Social Sensing},
	author       = {Muralidhar, Skanda and Mast, Marianne Schmid and Gatica-Perez, Daniel},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264939},
	url          = {https://doi.org/10.1145/3264939},
	issue_date   = {September 2018},
	abstract     = {People behave differently in different situations. With the advances in ubiquitous sensing technologies, it is now easier to capture human behavior across multiple situations automatically and unobtrusively. We investigate human behavior across two situations that are ubiquitous in hospitality (job interview and reception desk) with the objective of inferring performance on the job. Utilizing a dataset of 338 dyadic interactions, played by students from a hospitality management school, we first study the connections between automatically extracted nonverbal cues, linguistic content, and various perceived variables of soft skills and performance in these two situations. A correlation analysis reveals connection between perceived variables and nonverbal cues displayed during job interviews, and perceived performance on the job. We then propose a computational framework, with nonverbal cues and linguistic style from the two interactions as features, to infer the perceived performance and soft skills in the reception desk situation as a regression task. The best inference performance, with R2 = 0.40, is achieved using a combination of nonverbal cues extracted from the reception desk setting and the human-rated interview scores. We observe that some behavioral cues (greater speaking turn duration and head nods) are positively correlated to higher ratings for all perceived variables across both situations. The best performance using verbal content is achieved by fusion of LIWC and Doc2Vec features with R2 = 0.25 for perceived performance. Our work has implications for the creation of behavioral training systems with focus on specific behaviors for hospitality students.},
	articleno    = 129,
	numpages     = 24,
	keywords     = {first impressions, hospitality, job performance, Social computing, nonverbal behavior, hirability, multimodal interaction, reception desk}
}
@article{10.1145/3264937,
	title        = {Using Autoencoders to Automatically Extract Mobility Features for Predicting Depressive States},
	author       = {Mehrotra, Abhinav and Musolesi, Mirco},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264937},
	url          = {https://doi.org/10.1145/3264937},
	issue_date   = {September 2018},
	abstract     = {Recent studies have shown the potential of exploiting GPS data for passively inferring people's mental health conditions. However, feature extraction for characterizing human mobility remains a heuristic process that relies on the domain knowledge of the condition under consideration. Moreover, we do not have guarantees that these "hand-crafted" metrics are able to effectively capture mobility behavior of users. Indeed, informative emerging patterns in the data might not be characterized by them. This is also a complex and often time-consuming task, since it usually consists of a lengthy trial-and-error process.In this paper, we investigate the potential of using autoencoders for automatically extracting features from the raw input data. Through a series of experiments we show the effectiveness of autoencoder-based features for predicting depressive states of individuals compared to "hand-crafted" ones. Our results show that automatically extracted features lead to an improvement of the performance of the prediction models, while, at the same time, reducing the complexity of the feature design task. Moreover, through an extensive experimental performance analysis, we demonstrate the optimal configuration of the key parameters at the basis of the proposed approach.},
	articleno    = 127,
	numpages     = 20,
	keywords     = {Mobile Sensing, Notifications, Application Usage, Context-aware Computing}
}
@article{10.1145/3264930,
	title        = {SweepLoc: Automatic Video-Based Indoor Localization by Camera Sweeping},
	author       = {Li, Mingkuan and Liu, Ning and Niu, Qun and Liu, Chang and Chan, S.-H. Gary and Gao, Chengying},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264930},
	url          = {https://doi.org/10.1145/3264930},
	issue_date   = {September 2018},
	abstract     = {Indoor localization based on visual landmarks has received much attention in commercial sites with rich features (e.g., shopping malls, museums) recently because landmarks are relatively stable over a long time. Prior arts often require a user to take multiple independent images around his/her location, and manually confirm shortlisted landmarks. The process is sophisticated, inconvenient, slow, unnatural and error-prone. To overcome these limitations, we propose SweepLoc, a novel, efficient and automatic video-based indoor localization system. SweepLoc mimics our natural scanning around to identify nearby landmarks in an unfamiliar site to localize.In SweepLoc, a user simply takes a short video clip (about 6 to 8 seconds) of his/her surroundings by sweeping the camera. Using correlation and scene continuity between successive video frames, it automatically and efficiently selects key frames (where potential landmarks are centered) and subsequently reduces the decision error on landmarks. With identified landmarks, SweepLoc formulates an optimization problem to locate the user, taking compass noise and floor map constraint into account. We have implemented SweepLoc in Android platform. Our extensive experimental results in a food plaza and a premium mall demonstrate that SweepLoc is fast (less than 1 second to localize), and achieves substantially better accuracy as compared with the state-of-the-art approaches (reducing the localization error by 30%).},
	articleno    = 120,
	numpages     = 25,
	keywords     = {scene-continuity constraint, key frame selection, Video-based indoor localization}
}
@article{10.1145/3264914,
	title        = {HyRise: A Robust and Ubiquitous Multi-Sensor Fusion-Based Floor Localization System},
	author       = {Elbakly, Rizanne and Elhamshary, Moustafa and Youssef, Moustafa},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264914},
	url          = {https://doi.org/10.1145/3264914},
	issue_date   = {September 2018},
	abstract     = {Floor localization is an integral part of indoor localization systems that are deployed in any typical high-rise building. Nevertheless, while many efforts have been made to detect floor change events leveraging phone-embedded sensors, there are still a number of pitfalls that need to be overcome to provide robust and accurate localization in the 3D space.In this paper, we present HyRise: a robust and ubiquitous probabilistic crowdsourcing-based floor determination system. HyRise is a hybrid system that combines the barometer sensor and the ubiquitous Wi-Fi access points installed in the building into a probabilistic framework to identify the user's floor. In particular, HyRise incorporates a discrete Markov localization algorithm where the motion model is based on the vertical transitions detected from the sampled pressure readings and the observation model is based on the overheard Wi-Fi access points (APs) to find the most probable floor of the user. HyRise also has provisions to handle practical deployment issues including handling the inherent drift in the barometer readings, the noisy wireless environment, heterogeneous devices, among others.HyRise is implemented on Android phones and evaluated using three different testbeds: a campus building, a shopping mall, and a residential building with different floorplan layouts and APs densities. The results show that HyRise can identify the exact user's floor correctly in 93%, 92% and 77% of the cases for the campus building, the shopping mall, and the more challenging residential building; respectively. In addition, it can identify the floor with at most 1-floor error in 100% of the cases for all three testbeds. Moreover, the floor localization accuracy outperforms that achieved by other state-of-the-art techniques by at least 79% and up to 278%. This accuracy is achieved with no training overhead, is robust to the different user devices, and is consistent in buildings with different structures and APs densities.},
	articleno    = 104,
	numpages     = 23,
	keywords     = {Sensor-based floor estimation, crowdsourcing, 3D indoor localization}
}
@article{10.1145/3264912,
	title        = {Epidermal Robots: Wearable Sensors That Climb on the Skin},
	author       = {Dementyev, Artem and Hernandez, Javier and Choi, Inrak and Follmer, Sean and Paradiso, Joseph},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264912},
	url          = {https://doi.org/10.1145/3264912},
	issue_date   = {September 2018},
	abstract     = {Epidermal sensing has enabled significant advancements towards the measurement and understanding of health. Most of the existing medical instruments require direct expert manipulation of a doctor, measure a single parameter, and/or have limited sensing coverage. In contrast, this work demonstrates the first epidermal robot with the ability to move over the surface of the skin and capture a large range of body parameters. In particular, we developed SkinBot, a 2x4x2 centimeter-size robot that moves over the skin surface with a two-legged suction-based locomotion. We demonstrate three of the potential medical sensing applications which include the measurement of body biopotentials (e.g., electrodermal activity, electrocardiography) through modified suction cups that serve as electrodes, skin imaging through a skin-facing camera that can capture skin anomalies, and inertial body motions through a 6-axis accelerometer and gyroscope that can capture changes of body posture and subtle cardiorespiratory vibrations.},
	articleno    = 102,
	numpages     = 22,
	keywords     = {Wearable, sensors, robotics, skin, health, epidermal robots}
}
@article{10.1145/3214287,
	title        = {Duet: Estimating User Position and Identity in Smart Homes Using Intermittent and Incomplete RF-Data},
	author       = {Vasisht, Deepak and Jain, Anubhav and Hsu, Chen-Yu and Kabelac, Zachary and Katabi, Dina},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214287},
	url          = {https://doi.org/10.1145/3214287},
	issue_date   = {June 2018},
	abstract     = {Although past work on RF-based indoor localization has delivered important advances, it typically makes assumptions that hinder its adoption in smart home applications. Most localization systems assume that users carry their phones on them at home, an assumption that has been proven highly inaccurate in past measurements. The few localization systems that do not require the user to carry a device on her, cannot tell the identity of the person; yet identification is essential to most smart home applications. This paper focuses on addressing these issues so that smart homes can benefit from recent advances in indoor localization.We introduce Duet, a multi-modal system that takes as input measurements from both device-based and device-free localization. Duet introduces a new framework that combines probabilistic inference with first order logic to reason about the users' most likely locations and identities in light of the measurements. We implement Duet and compare it with a baseline that uses state-of-art WiFi-based localization. The results of two weeks of monitoring in two smart environments show that Duet accurately localizes and identifies the users for 94% and 96% of the time in the two places. In contrast, the baseline is accurate 17% and 42% respectively.},
	articleno    = 84,
	numpages     = 21,
	keywords     = {Multi-modal Sensor System, RF-based Indoor Positioning}
}
@article{10.1145/3214284,
	title        = {A Weakly Supervised Learning Framework for Detecting Social Anxiety and Depression},
	author       = {Salekin, Asif and Eberle, Jeremy W. and Glenn, Jeffrey J. and Teachman, Bethany A. and Stankovic, John A.},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214284},
	url          = {https://doi.org/10.1145/3214284},
	issue_date   = {June 2018},
	abstract     = {Although social anxiety and depression are common, they are often underdiagnosed and undertreated, in part due to difficulties identifying and accessing individuals in need of services. Current assessments rely on client self-report and clinician judgment, which are vulnerable to social desirability and other subjective biases. Identifying objective, nonburdensome markers of these mental health problems, such as features of speech, could help advance assessment, prevention, and treatment approaches. Prior research examining speech detection methods has focused on fully supervised learning approaches employing strongly labeled data. However, strong labeling of individuals high in symptoms or state affect in speech audio data is impractical, in part because it is not possible to identify with high confidence which regions of a long speech indicate the person's symptoms or affective state. We propose a weakly supervised learning framework for detecting social anxiety and depression from long audio clips. Specifically, we present a novel feature modeling technique named NN2Vec that identifies and exploits the inherent relationship between speakers' vocal states and symptoms/affective states. Detecting speakers high in social anxiety or depression symptoms using NN2Vec features achieves F-1 scores 17% and 13% higher than those of the best available baselines. In addition, we present a new multiple instance learning adaptation of a BLSTM classifier, named BLSTM-MIL. Our novel framework of using NN2Vec features with the BLSTM-MIL classifier achieves F-1 scores of 90.1% and 85.44% in detecting speakers high in social anxiety and depression symptoms.},
	articleno    = 81,
	numpages     = 26,
	keywords     = {weakly supervised learning, weakly labeled, speech, embedding, multiple instance learning, depression, audio word, mental disorder, feature modeling, assessment, anxiety, Social anxiety}
}
@article{10.1145/3191762,
	title        = {Visualizing Location Uncertainty on Mobile Devices: Cross-Cultural Differences in Perceptions and Preferences},
	author       = {Ranasinghe, Champika and Krukar, Jakub and Kray, Christian},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191762},
	url          = {https://doi.org/10.1145/3191762},
	issue_date   = {March 2018},
	abstract     = {Location uncertainty is often ignored but a key context parameter for location-based services. The standard way of visualizing location uncertainty on mobile devices is using a concentric circle. However, the impact of different visual variables (shape, size, boundary, middle dot, color) of this standard visualization on users is not well understood. There is a potential for misinterpretation, particularly across cultures. We ran a study that was previously conducted in Germany (N=32) in Sri Lanka (N=20) to investigate how users perceive different visualizations of location uncertainty on mobile devices. In particular, we investigated the impact of the four graphic dimensions, shape, boundary, middle dot and size. We identified consistencies and inconsistencies concerning perceptions of users regarding visualizations of location uncertainty across cultures. We also quantified the impact of different visualizations on the perception of users. Based on the consistencies between different visualizations and between the two cultures, we derived guidelines for visualizing location uncertainty that help developers in aligning location uncertainty with the perceptions of users. We also highlight the need for further research on cultural differences (and similarities) regarding how visualizations of location uncertainty impact the perceptions of users.},
	articleno    = 30,
	numpages     = 22,
	keywords     = {cross-cultural differences, location uncertainty, mobile geovisualization}
}
@article{10.1145/3191753,
	title        = {Joint Modeling of Heterogeneous Sensing Data for Depression Assessment via Multi-Task Learning},
	author       = {Lu, Jin and Shang, Chao and Yue, Chaoqun and Morillo, Reynaldo and Ware, Shweta and Kamath, Jayesh and Bamis, Athanasios and Russell, Alexander and Wang, Bing and Bi, Jinbo},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191753},
	url          = {https://doi.org/10.1145/3191753},
	issue_date   = {March 2018},
	abstract     = {Depression is a common mood disorder that causes severe medical problems and interferes negatively with daily life. Identifying human behavior patterns that are predictive or indicative of depressive disorder is important. Clinical diagnosis of depression relies on costly clinician assessment using survey instruments which may not objectively reflect the fluctuation of daily behavior. Self-administered surveys, such as the Quick Inventory of Depressive Symptomatology (QIDS) commonly used to monitor depression, may show disparities from clinical decision. Smartphones provide easy access to many behavioral parameters, and Fitbit wrist bands are becoming another important tool to assess variables such as heart rates and sleep efficiency that are complementary to smartphone sensors. However, data used to identify depression indicators have been limited to a single platform either iPhone, or Android, or Fitbit alone due to the variation in their methods of data collection. The present work represents a large-scale effort to collect and integrate data from mobile phones, wearable devices, and self reports in depression analysis by designing a new machine learning approach. This approach constructs sparse mappings from sensing variables collected by various tools to two separate targets: self-reported QIDS scores and clinical assessment of depression severity. We propose a so-called heterogeneous multi-task feature learning method that jointly builds inference models for related tasks but of different types including classification and regression tasks. The proposed method was evaluated using data collected from 103 college students and could predict the QIDS score with an R2 reaching 0.44 and depression severity with an F1-score as high as 0.77. By imposing appropriate regularizers, our approach identified strong depression indicators such as time staying at home and total time asleep.},
	articleno    = 21,
	numpages     = 21,
	keywords     = {Depression assessment, Multi-task learning, Prediction, Sensor data analysis}
}
@article{10.1145/3191745,
	title        = {EngageMon: Multi-Modal Engagement Sensing for Mobile Games},
	author       = {Huynh, Sinh and Kim, Seungmin and Ko, JeongGil and Balan, Rajesh Krishna and Lee, Youngki},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191745},
	url          = {https://doi.org/10.1145/3191745},
	issue_date   = {March 2018},
	abstract     = {Understanding the engagement levels players have with a game is a useful proxy for evaluating the game design and user experience. This is particularly important for mobile games as an alternative game is always just an easy download away. However, engagement is a subjective concept and usually requires fine-grained highly disruptive interviews or surveys to determine accurately. In this paper, we present EngageMon, a first-of-its-kind system that uses a combination of sensors from the smartphone (touch events), a wristband (photoplethysmography and electrodermal activity sensor readings), and an external depth camera (skeletal motion information) to accurately determine the engagement level of a mobile game player. Our design was guided by feedback obtained from interviewing 22 mobile game developers, testers, and designers. We evaluated EngageMon using data collected from 64 participants (54 in a lab-setting study and another 10 in a more natural setting study) playing six games from three different categories including endless runner, 3D motorcycle racing, and casual puzzle. Using all three sets of sensors, EngageMon was able to achieve an average accuracy of 85% and 77% under cross-sample and cross-subject evaluations respectively. Overall, EngageMon can accurately determine the engagement level of mobiles users while they are actively playing a game.},
	articleno    = 13,
	numpages     = 27,
	keywords     = {games, emotion recognition, mobile sensing, user experience, engagement}
}
@article{10.1145/3191739,
	title        = {TagFree Activity Identification with RFIDs},
	author       = {Fan, Xiaoyi and Gong, Wei and Liu, Jiangchuan},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191739},
	url          = {https://doi.org/10.1145/3191739},
	issue_date   = {March 2018},
	abstract     = {Human activity identification plays a critical role in many Internet-of-Things applications, which is typically achieved through attaching tracking devices, e.g., RFID tags, to human bodies. The attachment can be inconvenient and considered intrusive. A tag-free solution instead deploys stationary tags as references, and analyzes the backscattered signals that could be affected by human activities in close proximity. The information offered by today's RFID tags however are quite limited, and the typical raw data (RSSI and phase angles) are not necessarily good indicators of human activities (being either insensitive or unreliable as revealed by our realworld experiments). As such, existing tag-based activity identification solutions are far from being satisfactory, not to mention tag-free. It is also well known that the accuracy of the readings can be noticeably affected by multipath, which unfortunately is inevitable in an indoor environment and is complicated with multiple reference tags.In this paper, we however argue that multipath indeed brings rich information that can be explored to identify fine-grained human activities. Our experiments suggest that both the backscattered signal power and angle are correlated with human activities, impacting multiple paths with different levels. We present TagFree, the first RFID-based device-free activity identification system by analyzing the multipath signals. Different from conventional solutions that directly rely on the unreliable raw data, TagFree gathers massive angle information as spectrum frames from multiple tags, and preprocesses them to extract key features. It then analyzes their patterns through a deep learning framework. Our TagFree is readily deployable using off-the-shelf RFID devices and a prototype has been implemented using a commercial Impinj reader. Our extensive experiments demonstrate the superiority of our TagFree on activity identification in multipath-rich environments.},
	articleno    = 7,
	numpages     = 23,
	keywords     = {Activity Identification, Deep Learning, Backscatter, RFID}
}
@article{10.1145/3161417,
	title        = {Money Drives: Can Monetary Incentives Based on Real-Time Monitoring Improve Driving Behavior?},
	author       = {Cohen, Yehoshua Shuki and Shmueli, Erez},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161417},
	url          = {https://doi.org/10.1145/3161417},
	issue_date   = {December 2017},
	abstract     = {This paper examines the effectiveness of monetary incentives based on real-time monitoring as means to improve driving behavior of company car drivers. We conducted a 5-months 60-drivers field study with one of the largest public transportation companies in Israel. Driving behavior was measured continuously using In-Vehicle Data Recorders (IVDR) that were pre-installed in the vehicles, enabling naturalistic, objective and concise measurements. The driving behavior measurements were then used to examine two different monetary incentive schemes: (1) a simple individual incentive scheme where each driver was rewarded based on his own improvement in driving behavior, and (2) a peer-reward scheme where each driver was rewarded based on the improvement of his peers. Drivers were also provided with daily feedback about their improvement and the reward they gained using text messages and a dedicated smartphone app. We find that the two incentive schemes presented an average improvement of 25% in driving behavior, whereas the control group (that did not use any monetary incentive) presented no improvement at all. Surprisingly and in contrast to the reported superiority of the peer-reward scheme in previous studies, we find the individual scheme to perform better in our setting (31% vs. 15% improvement). Finally, we find that the monetary incentive schemes were able to reduce fuel consumption significantly, suggesting that such incentives can serve as a sustainable mechanism for improving driving behavior in real-world applications.},
	articleno    = 131,
	numpages     = 22,
	keywords     = {Peer-Pressure, Driving Behavior, Real-Time Monitoring, Monetary Incentives}
}
@article{10.1145/3161201,
	title        = {An LSTM Based System for Prediction of Human Activities with Durations},
	author       = {Krishna, Kundan and Jain, Deepali and Mehta, Sanket V. and Choudhary, Sunav},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161201},
	url          = {https://doi.org/10.1145/3161201},
	issue_date   = {December 2017},
	abstract     = {Human activity prediction is an interesting problem with a wide variety of applications like intelligent virtual assistants, contextual marketing, etc. One formulation of this problem is jointly predicting human activities (viz. eating, commuting, etc.) with associated durations. Herein a deep learning system is proposed for this problem. Given a sequence of past activities and durations, the system estimates the probabilities for future activities and their durations. Two distinct Long-Short Term Memory (LSTM) networks are developed that cater to different assumptions about the data and achieve different modeling complexities and prediction accuracies. The networks are trained and tested with two real-world datasets, one being publicly available while the other collected from a field experiment. Modeling on the segment level public dataset mitigates the cold-start problem. Experiments indicate that compared to traditional approaches based on sequence mining or hidden Markov modeling, LSTM networks perform significantly better. The ability of LSTM networks to detect long term correlations in activity data is also demonstrated. The trained models are each less than 500KB in size and can be deployed to run in real-time on a mobile device without any dependencies on the cloud. This can help applications like mobile personal assistants by providing predictive context.},
	articleno    = 147,
	numpages     = 31,
	keywords     = {Long-Short Term Memory Network, Daily Routine Generation, Human Activity Prediction}
}
@article{10.1145/3161198,
	title        = {Privacy-Preserving Image Processing with Binocular Thermal Cameras},
	author       = {Griffiths, Erin and Assana, Salah and Whitehouse, Kamin},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161198},
	url          = {https://doi.org/10.1145/3161198},
	issue_date   = {December 2017},
	abstract     = {Today, cameras and digital image processing are transforming industries and the human environment with rich, informative sensing. However, image processing is not utilized nearly as much in homes where concerns about image privacy dominate. In a preliminary study with 200 participants, we found 21% would reject a camera based system even if the system was designed to not report images as they could still be collected if the camera system was hacked. In this paper, we demonstrate a hardware-based approach for privacy-preserving image processing: the ability to automatically extract information from imaging sensors without the risk of compromising image privacy, even if the system is hacked. The basic idea is to limit both the memory available on board the camera and the data rate of camera communication to prevent a full image from ever being extracted. As a proof of concept, we prototype a system, called Lethe, that tracks and identifies individuals by height with a thermal camera as they move from room to room. Our results show that Lethe can detect the presence of individuals with 96.9% accuracy and determine their direction of travel with 99.7% accuracy. Additionally, Lethe can identify individuals 96.0% of the time with a 5cm (~2in) or greater difference in walking height and 92.9% with a 2.5cm (~1in) or greater difference. Finally, Lethe performs this processing with only 33 bytes of memory (or 0.69% of the full thermal image).},
	articleno    = 133,
	numpages     = 25
}
@article{10.1145/3161197,
	title        = {Early Destination Prediction with Spatio-Temporal User Behavior Patterns},
	author       = {Imai, Ryo and Tsubouchi, Kota and Konishi, Tatsuya and Shimosaka, Masamichi},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161197},
	url          = {https://doi.org/10.1145/3161197},
	issue_date   = {December 2017},
	abstract     = {Predicting user behavior makes it possible to provide personalized services. Destination prediction (e.g. predicting a future location) can be applied to various practical applications. An example of destination prediction is personalized GIS services, which are expected to provide alternate routes to enable users to avoid congested roads. However, the destination prediction problem requires critical trade-offs between timing and accuracy. In this paper, we focus on early destination prediction as the central issue, as early recognition in destination prediction has not been fully explored. As an alternative to the traditional two basic approaches with trajectory tracking that narrow down the candidates with respect to the trip progress, and Next Place Prediction (NPP) that infers the future location of a user from user habits, we propose a new probabilistic model based on both conventional models. The advantage of our model is that it drastically narrows down the destination candidates efficiently at the early stage of a trip, owing to the staying information derived from the NPP approach. In other words, our approach achieves high prediction accuracy by considering both approaches at the same time. To implement our model, we employ SubSynE for state-of-the-art prediction based on trajectory tracking as well as a multi-class logistic regression based on user contexts. Despite the simplicity of our model, the proposed method provides improved performance compared to conventional approaches based on the experimental results using the GPS logs of 1,646 actual users from the commercial services.},
	articleno    = 142,
	numpages     = 19,
	keywords     = {Sequence modeling, Destination prediction, Next place prediction, User location, Early recognition}
}
@article{10.1145/3161195,
	title        = {Exploring the Communication of Progress in Home-Based Falls Rehabilitation Using Exergame Technologies},
	author       = {Uzor, Stephen and Baillie, Lynne},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161195},
	url          = {https://doi.org/10.1145/3161195},
	issue_date   = {December 2017},
	abstract     = {Little is known on how to effectively represent rehabilitation progress, over a period of time, using exercise game (exergame) technologies. Progress in falls rehabilitation, which consists of improved performance in balance and muscle strength, is essential to assuring seniors of a reduced risk of falling. In this paper, we build on our previous research into exergames for falls, and we investigate how an exergame system can be used to communicate long-term progress to seniors. Using a multiphase user-centered requirements gathering process, we first investigated stakeholder perspectives regarding progress in self-managed rehabilitation. Following this we describe the home-based evaluation of our prototype exergame system, which highlights rehabilitation progress, with seniors, over a period of 2 months. Progress, in our system is communicated using charts of exercise performance and frequency, as well as medals awarded for achieving longer-term rehabilitation milestones. We report on seniors' opinions and preferences regarding the potential of our exergame system to communicate this rehabilitation progress in a meaningful way. Finally we discuss implications for design, based on our studies, to inform the development of more effective exergame systems for long-term unassisted rehabilitation in the home.},
	articleno    = 167,
	numpages     = 20,
	keywords     = {HCI, Progress, Exercise progression, User centred design, Quality of movement, Exergames, Rehabilitation, Falls, Medals, Design, UCD}
}
@article{10.1145/3161191,
	title        = {RF-Copybook: A Millimeter Level Calligraphy Copybook Based on Commodity RFID},
	author       = {Chang, Liqiong and Xiong, Jie and Wang, Ju and Chen, Xiaojiang and Wang, Yu and Tang, Zhanyong and Fang, Dingyi},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161191},
	url          = {https://doi.org/10.1145/3161191},
	issue_date   = {December 2017},
	abstract     = {As one of the best ways to learn and appreciate the Chinese culture, Chinese calligraphy is widely practiced and learned all over the world. Traditional calligraphy learners spend a great amount of time imitating the image templates of reputed calligraphers. In this paper, we propose an RF-based Chinese calligraphy template, named RF-Copybook, to precisely monitor the writing process of the learner and provide detail instructions to improve the learner's imitating behavior. With two RFID tags attached on the brush pen and three antennas equipped at the commercial RFID reader side, RF-Copybook tracks the pen's 3-dimensional movements precisely. The key intuition behind RF-Copybook's idea is that: (i) when there is only direct path signal between the tag and the antenna, the phase measured at the reader changes linearly with the distance, (ii) the reader offers very fine-grained phase readings, thus a millimeter level accuracy of antenna-tag distance can be obtained, (iii) by combing multiple antenna-tag distances, we can quantify the writing process with stroke based feature models. Extensive experiments show that RF-Copybook is robust against the environmental noise and achieves high accuracies across different environments in the estimation of the brush pen's elevation angle, nib's moving speed and position.},
	articleno    = 128,
	numpages     = 19,
	keywords     = {Antenna-tag Distance, Chinese Calligraphy, Phase, RFID}
}
@article{10.1145/3161181,
	title        = {RDeepSense: Reliable Deep Mobile Computing Models with Uncertainty Estimations},
	author       = {Yao, Shuochao and Zhao, Yiran and Shao, Huajie and Zhang, Aston and Zhang, Chao and Li, Shen and Abdelzaher, Tarek},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161181},
	url          = {https://doi.org/10.1145/3161181},
	issue_date   = {December 2017},
	abstract     = {Recent advances in deep learning have led various applications to unprecedented achievements, which could potentially bring higher intelligence to a broad spectrum of mobile and ubiquitous applications. Although existing studies have demonstrated the effectiveness and feasibility of running deep neural network inference operations on mobile and embedded devices, they overlooked the reliability of mobile computing models. Reliability measurements such as predictive uncertainty estimations are key factors for improving the decision accuracy and user experience. In this work, we propose RDeepSense, the first deep learning model that provides well-calibrated uncertainty estimations for resource-constrained mobile and embedded devices. RDeepSense enables the predictive uncertainty by adopting a tunable proper scoring rule as the training criterion and dropout as the implicit Bayesian approximation, which theoretically proves its correctness. To reduce the computational complexity, RDeepSense employs efficient dropout and predictive distribution estimation instead of the model ensemble or sampling-based method for inference operations. We evaluate RDeepSense with four mobile sensing applications using Intel Edison devices. Results show that RDeepSense can reduce around 90% of the energy consumption while producing superior uncertainty estimations and preserving at least the same model accuracy compared with other state-of-the-art methods.},
	articleno    = 173,
	numpages     = 26,
	keywords     = {Reliability, Mobile Computing, Deep Learning, Uncertainty Estimation, Internet-of-Things}
}
@article{10.1145/3161170,
	title        = {Flower-Pop: Facilitating Casual Group Conversations With Multiple Mobile Devices},
	author       = {Lee, Moon-Hwan and Row, Yea-Kyung and Son, Oosung and Lee, Uichin and Kim, Jaejeung and Jeong, Jungi and Maeng, Seungryoul and Nam, Tek-Jin},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161170},
	url          = {https://doi.org/10.1145/3161170},
	issue_date   = {December 2017},
	abstract     = {We explore the potential use of mobile devices as a collaborative sensing system that can proactively mediate casual group conversations. In this study, we aim to investigate (i) the impacts of a mobile system's passive and active conversation facilitation and (ii) the ways in which sociocultural aspects that affect casual group conversation should be considered in the design of proactive mobile systems. Toward this goal, we developed Flower-Pop, a mobile system that monitors group conversations and visualizes interaction patterns using metaphorical expressions based on blossoms. This system provides passive facilitation as well as active facilitation modes such as proactive conversation visualization and photo sharing. The active modes can encourage inactive participants to share photos and select random people to speak. Focusing on Korea, our field study showed that Flower-Pop's mediation created smooth topic/speaker transitions and encouraged less-active speakers to better engage in group conversation. We also found that the sociocultural aspects of casual group conversation, such as the location's characteristics, social relations, and the group's interests, affected participants' use of the Flower-Pop system. Based on our findings, we discuss methods for designing mobile systems for conversation facilitation and outline how opportune sociocultural factors could be identified based on mobile devices.},
	articleno    = 150,
	numpages     = 24,
	keywords     = {Mobile devices, collocated social interaction, group conversation}
}
@article{10.1145/3131895,
	title        = {Low-Resource Multi-Task Audio Sensing for Mobile and Embedded Devices via Shared Deep Neural Network Representations},
	author       = {Georgiev, Petko and Bhattacharya, Sourav and Lane, Nicholas D. and Mascolo, Cecilia},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131895},
	url          = {https://doi.org/10.1145/3131895},
	issue_date   = {September 2017},
	abstract     = {Continuous audio analysis from embedded and mobile devices is an increasingly important application domain. More and more, appliances like the Amazon Echo, along with smartphones and watches, and even research prototypes seek to perform multiple discriminative tasks simultaneously from ambient audio; for example, monitoring background sound classes (e.g., music or conversation), recognizing certain keywords (‘Hey Siri' or ‘Alexa'), or identifying the user and her emotion from speech. The use of deep learning algorithms typically provides state-of-the-art model performances for such general audio tasks. However, the large computational demands of deep learning models are at odds with the limited processing, energy and memory resources of mobile, embedded and IoT devices.In this paper, we propose and evaluate a novel deep learning modeling and optimization framework that specifically targets this category of embedded audio sensing tasks. Although the supported tasks are simpler than the task of speech recognition, this framework aims at maintaining accuracies in predictions while minimizing the overall processor resource footprint. The proposed model is grounded in multi-task learning principles to train shared deep layers and exploits, as input layer, only statistical summaries of audio filter banks to further lower computations.We find that for embedded audio sensing tasks our framework is able to maintain similar accuracies, which are observed in comparable deep architectures that use single-task learning and typically more complex input layers. Most importantly, on an average, this approach provides almost a 2.1\texttimes{} reduction in runtime, energy, and memory for four separate audio sensing tasks, assuming a variety of task combinations.},
	articleno    = 50,
	numpages     = 19,
	keywords     = {deep learning, shared representation, Audio sensing, multi-task learning}
}
@article{10.1145/3448075,
	title        = {Orbuculum - Predicting When Users Intend to Leave Large Public Displays},
	author       = {Alt, Florian and Buschek, Daniel and Heuss, David and M\"{u}ller, J\"{o}rg},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448075},
	url          = {https://doi.org/10.1145/3448075},
	issue_date   = {March 2021},
	abstract     = {We present a system, predicting the point in time when users of a public display are about to leave. The ability to react to users' intention to leave is valuable for researchers and practitioners alike: users can be presented additional content with the goal to maximize interaction times; they can be offered a discount coupon for redemption in a nearby store hence enabling new business models; or feedback can be collected from users right after they have finished interaction without interrupting their task. Our research consists of multiple steps: (1) We identified features that hint at users' intention to leave from observations and video logs. (2) We implemented a system capable of detecting such features from Microsoft Kinect's skeleton data and subsequently make a prediction. (3) We trained and deployed a prediction system with a Quiz game which reacts when users are about to leave (N=249), achieving an accuracy of 78%. The majority of users indeed reacted to the presented intervention.},
	articleno    = 46,
	numpages     = 16,
	keywords     = {prediction, public display, deployment-based research, audience behavior}
}
@article{10.1145/3432701,
	title        = {MM-Fit: Multimodal Deep Learning for Automatic Exercise Logging across Sensing Devices},
	author       = {Str\"{o}mb\"{a}ck, David and Huang, Sangxia and Radu, Valentin},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432701},
	url          = {https://doi.org/10.1145/3432701},
	issue_date   = {December 2020},
	abstract     = {Fitness tracking devices have risen in popularity in recent years, but limitations in terms of their accuracy and failure to track many common exercises presents a need for improved fitness tracking solutions. This work proposes a multimodal deep learning approach to leverage multiple data sources for robust and accurate activity segmentation, exercise recognition and repetition counting. For this, we introduce the MM-Fit dataset; a substantial collection of inertial sensor data from smartphones, smartwatches and earbuds worn by participants while performing full-body workouts, and time-synchronised multi-viewpoint RGB-D video, with 2D and 3D pose estimates. We establish a strong baseline for activity segmentation and exercise recognition on the MM-Fit dataset, and demonstrate the effectiveness of our CNN-based architecture at extracting modality-specific spatial temporal features from inertial sensor and skeleton sequence data. We compare the performance of unimodal and multimodal models for activity recognition across a number of sensing devices and modalities. Furthermore, we demonstrate the effectiveness of multimodal deep learning at learning cross-modal representations for activity recognition, which achieves 96% accuracy across all sensing modalities on unseen subjects in the MM-Fit dataset; 94% using data from the smartwatch only; 85% from the smartphone only; and 82% on data from the earbud device. We strengthen single-device performance by using the zeroing-out training strategy, which phases out the other sensing modalities. Finally, we implement and evaluate a strong repetition counting baseline on our MM-Fit dataset. Collectively, these tasks contribute to recognising, segmenting and timing exercise and non-exercise activities for automatic exercise logging.},
	articleno    = 168,
	numpages     = 22,
	keywords     = {multimodal learning, activity recognition, smartwatch, earbud, repetition counting, wearable, exercise recognition, smartphone, deep learning}
}
@article{10.1145/3432235,
	title        = {Real-Time Arm Gesture Recognition in Smart Home Scenarios via Millimeter Wave Sensing},
	author       = {Liu, Haipeng and Wang, Yuheng and Zhou, Anfu and He, Hanyue and Wang, Wei and Wang, Kunpeng and Pan, Peilin and Lu, Yixuan and Liu, Liang and Ma, Huadong},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432235},
	url          = {https://doi.org/10.1145/3432235},
	issue_date   = {December 2020},
	abstract     = {"In air" gesture recognition using millimeter wave (mmWave) radar and its applications in natural human-computer-interaction for smart home has shown its potential. However, the state-of-the-art works still fall short in terms of limited gesture space, vulnerable to surrounding interference, and off-line recognition. In this paper, we propose mHomeGes, a real-time mmWave arm gesture recognition system for practical smart home-usage. To this end, we first distill arm gesture's position and dynamic variation, and then custom-design a lightweight convolution neural network to recognize fine-grained gestures. Next, we propose a user discovery method to focus on the target human gesture, thus eliminating the adverse impact of surrounding interference. Finally, we design a hidden Markov model-based voting mechanism to handle continuous gesture signals at run-time, which leads to continuous gesture recognition in real-time. We implement mHomeGes on a commodity mmWave radar and also perform a user study, which demonstrates that mHomeGes achieves high recognition accuracy above 95.30% in real-time across various smart home scenarios, regardless of the impact of surrounding movements, concurrent gestures, human physiological conditions, and outer packing materials. Moreover, we have also publicly archived a mmWave gesture data-set collected during developing mHomeGes, which consists of about 22,000 instances from 25 persons and may have an independent value of facilitating future research.},
	articleno    = 140,
	numpages     = 28,
	keywords     = {Human-Computer Interaction, Millimeter Wave Sensing, Smart Home, Gesture Recognition}
}
@article{10.1145/3432219,
	title        = {Wearable Computing Technology for Assessment of Cognitive Functioning of Bipolar Patients and Healthy Controls},
	author       = {Hafiz, Pegah and Miskowiak, Kamilla Woznica and Maxhuni, Alban and Kessing, Lars Vedel and Bardram, Jakob Eyvind},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432219},
	url          = {https://doi.org/10.1145/3432219},
	issue_date   = {December 2020},
	abstract     = {Mobile cognitive tests have been emerged to first, bring the assessments outside the clinics and second, frequently measure individuals' cognitive performance in their free-living environment. Patients with Bipolar Disorder (BD) suffer from cognitive impairments and poor sleep quality negatively affects their cognitive performance. Wearables are capable of unobtrusively collecting multivariate data including activity and sleep features. In this study, we analyzed daily attention, working memory, and executive functions of patients with BD and healthy controls by using a smartwatch-based tool called UbiCAT to 1) investigate its concurrent validity and feasibility, 2) identify digital phenotypes of mental health using cognitive and mobile sensor data, and 3) classify patients and healthy controls on the basis of their daily cognitive and mobile data. Our findings demonstrated that UbiCAT is feasible with valid measures for in-the-wild cognitive assessments. The analysis showed that the patients responded more slowly during the attention task than the healthy controls, which could indicate a lower alertness of this group. Furthermore, sleep duration correlated positively with participants' working memory performance the next day. Statistical analysis showed that features including cognitive measures of attention and executive functions, sleep duration, time in bed, awakening frequency and duration, and step counts are the digital phenotypes of mental health diagnosis. Supervised learning models was used to classify individuals' mental health diagnosis using their daily observations. Overall, we achieved accuracy of approximately 74% using K-Nearest Neighbour (KNN) method.},
	articleno    = 129,
	numpages     = 22,
	keywords     = {mental health, mobile sensing, bipolar disorder, wearable technology, cognition, digital phenotype}
}
@article{10.1145/3432197,
	title        = {Recognizing Running Movement Changes with Quaternions on a Sports Watch},
	author       = {Seuter, Matthias and Pollock, Alexandra and Bauer, Gernot and Kray, Christian},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432197},
	url          = {https://doi.org/10.1145/3432197},
	issue_date   = {December 2020},
	abstract     = {Sports watches are popular amongst runners but limited in terms of sensor locations (i.e., one location at the wrist). Thus, apps on the watch cannot directly sense movement changes in arbitrary body locations. This, in turn, severely limits training support services, contextual awareness and seamless interaction mechanisms. Our approach addresses this gap and connects the watch with two strategically placed inertial measurement units (IMUs) via bluetooth low energy (BLE). Our prototypical app for Wear OS receives orientation (quaternion) data and matches the sensors to the arm or leg segments using a flexible and simple procedure. We collected data from eight runners and used support vector machines (SVMs) to recognize different movements. Findings from our evaluation with different parameters indicate feasible recognition and low false-positive rates for two to three different movements, for both placements. Our approach can thus help to improve applications that support training and thus contributes to developing motion capture for personal use; it also enables movement-based interaction while running.},
	articleno    = 151,
	numpages     = 18,
	keywords     = {IMUs, Interaction Techniques, Motion Capture, Running, Movement, Quaternions}
}
@article{10.1145/3432191,
	title        = {BFree: Enabling Battery-Free Sensor Prototyping with Python},
	author       = {Kortbeek, Vito and Bakar, Abu and Cruz, Stefany and Yildirim, Kasim Sinan and Pawe\l{}czak, Przemys\l{}aw and Hester, Josiah},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432191},
	url          = {https://doi.org/10.1145/3432191},
	issue_date   = {December 2020},
	abstract     = {Building and programming tiny battery-free energy harvesting embedded computer systems is hard for the average maker because of the lack of tools, hard to comprehend programming models, and frequent power failures. With the high ecologic cost of equipping the next trillion embedded devices with batteries, it is critical to equip the makers, hobbyists, and novice embedded systems programmers with easy-to-use tools supporting battery-free energy harvesting application development. This way, makers can create untethered embedded systems that are not plugged into the wall, the desktop, or even a battery, providing numerous new applications and allowing for a more sustainable vision of ubiquitous computing. In this paper, we present BFree, a system that makes it possible for makers, hobbyists, and novice embedded programmers to develop battery-free applications using Python programming language and widely available hobbyist maker platforms. BFree provides energy harvesting hardware and a power failure resilient version of Python, with durable libraries that enable common coding practice and off the shelf sensors. We develop demonstration applications, benchmark BFree against battery-powered approaches, and evaluate our system in a user study. This work enables makers to engage with a future of ubiquitous computing that is useful, long-term, and environmentally responsible.},
	articleno    = 135,
	numpages     = 39,
	keywords     = {Making, Intermittent Computing, Energy Harvesting}
}
@article{10.1145/3369839,
	title        = {Facilitating Temporal Synchronous Target Selection through User Behavior Modeling},
	author       = {Zhang, Tengxiang and Yi, Xin and Wang, Ruolin and Gao, Jiayuan and Wang, Yuntao and Yu, Chun and Li, Simin and Shi, Yuanchun},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369839},
	url          = {https://doi.org/10.1145/3369839},
	issue_date   = {December 2019},
	abstract     = {Temporal synchronous target selection is an association-free selection technique: users select a target by generating signals (e.g., finger taps and hand claps) in sync with its unique temporal pattern. However, classical pattern set design and input recognition algorithm of such techniques did not leverage users' behavioral information, which limits their robustness to imprecise inputs. In this paper, we improve these two key components by modeling users' interaction behavior. In the first user study, we asked users to tap a finger in sync with blinking patterns with various period and delay, and modeled their finger tapping ability using Gaussian distribution. Based on the results, we generated pattern sets for up to 22 targets that minimized the possibility of confusion due to imprecise inputs. In the second user study, we validated that the optimized pattern sets could reduce error rate from 23% to 7% for the classical Correlation recognizer. We also tested a novel Bayesian, which achieved higher selection accuracy than the Correlation recognizer when the input sequence is short. The informal evaluation results show that the selection technique can be effectively scaled to different modalities and sensing techniques.},
	articleno    = 159,
	numpages     = 24,
	keywords     = {Bayesian prediction, target selection, pattern generation, synchronous tapping}
}
@article{10.1145/3369828,
	title        = {A Multisensor Person-Centered Approach to Understand the Role of Daily Activities in Job Performance with Organizational Personas},
	author       = {Das Swain, Vedant and Saha, Koustuv and Rajvanshy, Hemang and Sirigiri, Anusha and Gregg, Julie M. and Lin, Suwen and Martinez, Gonzalo J. and Mattingly, Stephen M. and Mirjafari, Shayan and Mulukutla, Raghu and Nepal, Subigya and Nies, Kari and Reddy, Manikanta D. and Robles-Granda, Pablo and Campbell, Andrew T. and Chawla, Nitesh V. and D'Mello, Sidney and Dey, Anind K. and Jiang, Kaifeng and Liu, Qiang and Mark, Gloria and Moskal, Edward and Striegel, Aaron and Tay, Louis and Abowd, Gregory D. and De Choudhury, Munmun},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369828},
	url          = {https://doi.org/10.1145/3369828},
	issue_date   = {December 2019},
	abstract     = {Several psychologists posit that performance is not only a function of personality but also of situational contexts, such as day-level activities. Yet in practice, since only personality assessments are used to infer job performance, they provide a limited perspective by ignoring activity. However, multi-modal sensing has the potential to characterize these daily activities. This paper illustrates how empirically measured activity data complements traditional effects of personality to explain a worker's performance. We leverage sensors in commodity devices to quantify the activity context of 603 information workers. By applying classical clustering methods on this multisensor data, we take a person-centered approach to describe workers in terms of both personality and activity. We encapsulate both these facets into an analytical framework that we call organizational personas. On interpreting these organizational personas we find empirical evidence to support that, independent of a worker's personality, their activity is associated with job performance. While the effects of personality are consistent with the literature, we find that the activity is equally effective in explaining organizational citizenship behavior and is less but significantly effective for task proficiency and deviant behaviors. Specifically, personas that exhibit a daily-activity pattern with fewer location visits, batched phone-use, shorter desk-sessions and longer sleep duration, tend to perform better on all three performance metrics. Organizational personas are a descriptive framework to identify the testable hypotheses that can disentangle the role of malleable aspects like activity in determining the performance of a worker population.},
	articleno    = 130,
	numpages     = 27,
	keywords     = {media access control, Wireless sensor networks, radio interference, multi-channel}
}
@article{10.1145/3369823,
	title        = {MAAT: Mobile Apps As Things in the IoT},
	author       = {Lindquist, Wyatt and Helal, Sumi and Khaled, Ahmed and Kotonya, Gerald and Lee, Jaejoon},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369823},
	url          = {https://doi.org/10.1145/3369823},
	issue_date   = {December 2019},
	abstract     = {As the Internet of Things (IoT) proliferates, the potential for its opportunistic interaction with traditional mobile apps becomes apparent. We argue that to fully take advantage of this potential, mobile apps must become things themselves, and interact in a smart space like their hardware counterparts. We present an extension to our Atlas thing architecture on smartphones, allowing mobile apps to behave as things and provide powerful services and functionalities. To this end, we also consider the role of the mobile app developer, and introduce actionable keywords (AKWs)---a dynamically programmable description---to enable potential thing to thing interactions. The AKWs empower the mobile app to dynamically react to services provided by other things, without being known a priori by the original app developer. In this paper, we present the mobile-apps-as-things (MAAT) concept along with its AKW concept and programming construct. For MAAT to be adopted by developers, changes to the existing development environments (IDE) should remain minimal to stay acceptable and practically usable, thus we also propose an IDE plugin to simplify the addition of this dynamic behavior. We present details of MAAT, along with the implementation of the IDE plugin, and give a detailed benchmarking evaluation to assess the responsiveness of our implementation to impromptu interactions and dynamic app behavioral changes. We also investigate another study, targeting Android developers, which evaluates the acceptability and usability of the MAAT IDE plugin.},
	articleno    = 143,
	numpages     = 22,
	keywords     = {actionable keywords, mobile apps, thing architecture, Internet of Things}
}
@article{10.1145/3414117,
	title        = {Finger Gesture Tracking for Interactive Applications: A Pilot Study with Sign Languages},
	author       = {Liu, Yilin and Jiang, Fengyang and Gowda, Mahanth},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3414117},
	url          = {https://doi.org/10.1145/3414117},
	issue_date   = {September 2020},
	abstract     = {This paper presents FinGTrAC, a system that shows the feasibility of fine grained finger gesture tracking using low intrusive wearable sensor platform (smart-ring worn on the index finger and a smart-watch worn on the wrist). The key contribution is in scaling up gesture recognition to hundreds of gestures while using only a sparse wearable sensor set where prior works have been able to only detect tens of hand gestures. Such sparse sensors are convenient to wear but cannot track all fingers and hence provide under-constrained information. However application specific context can fill the gap in sparse sensing and improve the accuracy of gesture classification. Rich context exists in a number of applications such as user-interfaces, sports analytics, medical rehabilitation, sign language translation etc. This paper shows the feasibility of exploiting such context in an application of American Sign Language (ASL) translation. Noisy sensor data, variations in gesture performance across users and the inability to capture data from all fingers introduce non-trivial challenges. FinGTrAC exploits a number of opportunities in data preprocessing, filtering, pattern matching, context of an ASL sentence to systematically fuse the available sensory information into a Bayesian filtering framework. Culminating into the design of a Hidden Markov Model, a Viterbi decoding scheme is designed to detect finger gestures and the corresponding ASL sentences in real time. Extensive evaluation on 10 users shows a recognition accuracy of 94.2% for 100 most frequently used ASL finger gestures over different sentences. When the size of the dictionary is extended to 200 words, the accuracy is degrades gracefully to 90% thus indicating the robustness and scalability of the multi-stage optimization framework.},
	articleno    = 112,
	numpages     = 21,
	keywords     = {Bayesian Inference, Gesture, Wearable, IoT}
}
@article{10.1145/3411829,
	title        = {Designing Interactions Beyond Conscious Control: A New Model for Wearable Interfaces},
	author       = {Jain, Abhinandan and Horowitz, Adam Haar and Schoeller, Felix and Leigh, Sang-won and Maes, Pattie and Sra, Misha},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411829},
	url          = {https://doi.org/10.1145/3411829},
	issue_date   = {September 2020},
	abstract     = {Recent research in psychology distinguishes levels of consciousness into a tripartite model - conscious, unconscious and metaconscious. HCI technologies largely focus on the conscious pathway for computer-to-human interaction, requiring explicit user attention and action. In contrast, the other two pathways provide opportunities to create new interfaces that can alter emotion, cognition and behavior without demands on attentional resources. These direct interfaces connect to cognitive processes that are in our perception but outside our conscious control. In this work, we feature two sub-categories, namely preconscious and metasomatic within the tripartite model of consciousness. Our goal is to provide a finer categorization of cognitive processes that can better help classify HCI research related to activating non-conscious cognitive pathways. We present the design of two wearable devices, MoveU and Frisson. From lessons learned during the iterative design process and the user studies, we present four design considerations that can be used to aid HCI researchers of future devices that influence the mind. With this work we aim to highlight that awareness of consciousness levels can be a valuable design element that can help to expand the range of computer-to-human interface devices we build.},
	articleno    = 108,
	numpages     = 23,
	keywords     = {wearable computing, design methods, interaction design}
}
@article{10.1145/3411827,
	title        = {Extending Coverage of Stationary Sensing Systems with Mobile Sensing Systems for Human Mobility Modeling},
	author       = {Yang, Yu and Fang, Zhihan and Xie, Xiaoyang and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411827},
	url          = {https://doi.org/10.1145/3411827},
	issue_date   = {September 2020},
	abstract     = {Human mobility modeling has many applications in location-based services, mobile networking, city management, and epidemiology. Previous sensing approaches for human mobility are mainly categorized into two types: stationary sensing systems (e.g., surveillance cameras and toll booths) and mobile sensing systems (e.g., smartphone apps and vehicle tracking devices). However, stationary sensing systems only provide mobility information of human in limited coverage (e.g., camera-equipped roads) and mobile sensing systems only capture a limited number of people (e.g., people using a particular smartphone app). In this work, we design a novel system Mohen to model human mobility with a heterogeneous sensing system. The key novelty of Mohen is to fundamentally extend the sensing coverage of a large-scale stationary sensing system with a small-scale sensing system. Based on the evaluation on data from real-world urban sensing systems, our system outperforms them by 35% and achieves a competitive result to an Oracle method.},
	articleno    = 100,
	numpages     = 21,
	keywords     = {Heterogeneous sensing systems, Human mobility, Sensor networks}
}
@article{10.1145/3411824,
	title        = {SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors},
	author       = {Zhang, Yun C. and Zhang, Shibo and Liu, Miao and Daly, Elyse and Battalio, Samuel and Kumar, Santosh and Spring, Bonnie and Rehg, James M. and Alshurafa, Nabil},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411824},
	url          = {https://doi.org/10.1145/3411824},
	issue_date   = {September 2020},
	abstract     = {The development and validation of computational models to detect daily human behaviors (e.g., eating, smoking, brushing) using wearable devices requires labeled data collected from the natural field environment, with tight time synchronization of the micro-behaviors (e.g., start/end times of hand-to-mouth gestures during a smoking puff or an eating gesture) and the associated labels. Video data is increasingly being used for such label collection. Unfortunately, wearable devices and video cameras with independent (and drifting) clocks make tight time synchronization challenging. To address this issue, we present the Window Induced Shift Estimation method for Synchronization (SyncWISE) approach. We demonstrate the feasibility and effectiveness of our method by synchronizing the timestamps of a wearable camera and wearable accelerometer from 163 videos representing 45.2 hours of data from 21 participants enrolled in a real-world smoking cessation study. Our approach shows significant improvement over the state-of-the-art, even in the presence of high data loss, achieving 90% synchronization accuracy given a synchronization tolerance of 700 milliseconds. Our method also achieves state-of-the-art synchronization performance on the CMU-MMAC dataset.},
	articleno    = 107,
	numpages     = 26,
	keywords     = {Accelerometry, Wearable Sensor, Wearable Camera, Video, Time Synchronization, Temporal Drift, Automatic Synchronization}
}
@article{10.1145/3411823,
	title        = {Predicting Subjective Measures of Social Anxiety from Sparsely Collected Mobile Sensor Data},
	author       = {Rashid, Haroon and Mendu, Sanjana and Daniel, Katharine E. and Beltzer, Miranda L. and Teachman, Bethany A. and Boukhechba, Mehdi and Barnes, Laura E.},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411823},
	url          = {https://doi.org/10.1145/3411823},
	issue_date   = {September 2020},
	abstract     = {Exploiting the capabilities of smartphones for monitoring social anxiety shows promise for advancing our ability to both identify indicators of and treat social anxiety in natural settings. Smart devices allow researchers to collect passive data unobtrusively through built-in sensors and active data using subjective, self-report measures with Ecological Momentary Assessment (EMA) studies. Prior work has established the potential to predict subjective measures from passive data. However, the majority of the past work on social anxiety has focused on a limited subset of self-reported measures. Furthermore, the data collected in real-world studies often results in numerous missing values in one or more data streams, which ultimately reduces the usable data for analysis and limits the potential of machine learning algorithms. We explore several approaches for addressing these problems in a smartphone based monitoring and intervention study of eighty socially anxious participants over a five week period. Our work complements and extends prior work in two directions: (i) we show the predictability of seven different self-reported dimensions of social anxiety, and (ii) we explore four imputation methods to handle missing data and evaluate their effectiveness in the prediction of subjective measures from the passive data. Our evaluation shows imputation of missing data reduces prediction error by as much as 22%. We discuss the implications of these results for future research.},
	articleno    = 109,
	numpages     = 24,
	keywords     = {Social anxiety, mobile sensing, data imputation, mental health, ecological momentary assessment}
}
@article{10.1145/3411820,
	title        = {CarOSense: Car Occupancy Sensing with the Ultra-Wideband Keyless Infrastructure},
	author       = {Ma, Yongsen and Zeng, Yunze and Jain, Vivek},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411820},
	url          = {https://doi.org/10.1145/3411820},
	issue_date   = {September 2020},
	abstract     = {Ultra-Wideband (UWB) is a popular technology to provide high accuracy localization, asset tracking and access control applications. Due to the accurate ranging feature and robustness to relay attacks, car manufacturers are upgrading the keyless entry infrastructure to UWB. As car occupancy monitoring is an essential step to support regulatory requirements and provide customized user experience, we build CarOSense to explore the possibility of reusing UWB keyless infrastructure as an orthogonal sensing modality to detect per-seat car occupancy. CarOSense uses a novel deep learning model, MaskMIMO, to learn spatial/time features by 2D convolutions and per-seat attentions by a multi-task mask. We collect UWB data from 10 car locations with up to 16 occupancy states in each location. We implement CarOSense as a cross-platform demo and evaluate it in 15 different scenarios, including leave-one-out test of unknown car locations and stress test of unseen scenarios. Results show that the average accuracy is 94.6% for leave-one-out test and 87.0% for stress test. CarOSense is robust in a large set of untrained scenarios with the model trained on a small set of training data. We also benchmark the computation cost and demonstrate that CarOSense is lightweight and can run smoothly in real-time on embedded devices.},
	articleno    = 91,
	numpages     = 28,
	keywords     = {Wireless Sensing, Smart Automobiles, Ultra-Wideband (UWB), Wireless Systems}
}
@article{10.1145/3397334,
	title        = {Learning to Recognize Handwriting Input with Acoustic Features},
	author       = {Yin, Huanpu and Zhou, Anfu and Su, Guangyuan and Chen, Bo and Liu, Liang and Ma, Huadong},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397334},
	url          = {https://doi.org/10.1145/3397334},
	issue_date   = {June 2020},
	abstract     = {For mobile or wearable devices with a small touchscreen, handwriting input (instead of typing on the touchscreen) is highly desirable for efficient human-computer interaction. Previous passive acoustic-based handwriting solutions mainly focus on print-style capital input, which is inconsistent with people's daily habits and thus causes inconvenience. In this paper, we propose WritingRecorder, a novel universal text entry system that enables free-style lowercase handwriting recognition. WritingRecorder leverages the built-in microphone of the smartphones to record the handwritten sound, and then designs an adaptive segmentation method to detect letter fragments in real-time from the recorded sound. Then we design a neural network named Inception-LSTM to extract the hidden and unique acoustic pattern associated with the writing trajectory of each letter and thus classify each letter. Moreover, we adopt a word selection method based on language model, so as to recognize legislate words from all possible letter combinations. We implement WritingRecorder as an APP on mobile phones and conduct the extensive experimental evaluation. The results demonstrate that WritingRecorder works in real-time and can achieve 93.2% accuracy even for new users without collecting and training on their handwriting samples, under a series of practical scenarios.},
	articleno    = 64,
	numpages     = 26,
	keywords     = {Acoustic sensing, Handwriting recognition}
}
@article{10.1145/3397326,
	title        = {Exploring LoRa for Long-Range Through-Wall Sensing},
	author       = {Zhang, Fusang and Chang, Zhaoxin and Niu, Kai and Xiong, Jie and Jin, Beihong and Lv, Qin and Zhang, Daqing},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397326},
	url          = {https://doi.org/10.1145/3397326},
	issue_date   = {June 2020},
	abstract     = {Wireless signals have been extensively utilized for contactless sensing in the past few years. Due to the intrinsic nature of employing the weak target-reflected signal for sensing, the sensing range is limited. For instance, WiFi and RFID can achieve 3-6 meter sensing range while acoustic-based sensing is limited to less than one meter. In this work, we identify exciting sensing opportunities with LoRa, which is the new long-range communication technology designed for IoT communication. We explore the sensing capability of LoRa, both theoretically and experimentally. We develop the sensing model to characterize the relationship between target movement and signal variation, and propose novel techniques to increase LoRa sensing range to over 25 meters for human respiration sensing. We further build a prototype system which is capable of sensing both coarse-grained and fine-grained human activities. Experimental results show that (1) human respiration can still be sensed when the target is 25 meters away from the LoRa devices, and 15 meters away with a wall in between; and (2) human walking (both displacement and direction) can be tracked accurately even when the target is 30 meters away from the LoRa transceiver pair.},
	articleno    = 68,
	numpages     = 27,
	keywords     = {LoRa signal, Wireless sensing, Long range sensing, Modeling}
}
@article{10.1145/3397325,
	title        = {Making Sense of Sleep: Multimodal Sleep Stage Classification in a Large, Diverse Population Using Movement and Cardiac Sensing},
	author       = {Zhai, Bing and Perez-Pozuelo, Ignacio and Clifton, Emma A. D. and Palotti, Joao and Guan, Yu},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397325},
	url          = {https://doi.org/10.1145/3397325},
	issue_date   = {June 2020},
	abstract     = {Traditionally, sleep monitoring has been performed in hospital or clinic environments, requiring complex and expensive equipment set-up and expert scoring. Wearable devices increasingly provide a viable alternative for sleep monitoring and are able to collect movement and heart rate (HR) data. In this work, we present a set of algorithms for sleep-wake and sleep-stage classification based upon actigraphy and cardiac sensing amongst 1,743 participants. We devise movement and cardiac features that could be extracted from research-grade wearable sensors and derive models and evaluate their performance in the largest open-access dataset for human sleep science. Our results demonstrated that neural network models outperform traditional machine learning methods and heuristic models for both sleep-wake and sleep-stage classification. Convolutional neural networks (CNNs) and long-short term memory (LSTM) networks were the best performers for sleep-wake and sleep-stage classification, respectively. Using SHAP (SHapley Additive exPlanation) with Random Forest we identified that frequency features from cardiac sensors are critical to sleep-stage classification. Finally, we introduced an ensemble-based approach to sleep-stage classification, which outperformed all other baselines, achieving an accuracy of 78.2% and F1 score of 69.8% on the classification task for three sleep stages. Together, this work represents the first systematic multimodal evaluation of sleep-wake and sleep-stage classification in a large, diverse population. Alongside the presentation of an accurate sleep-stage classification approach, the results highlight multimodal wearable sensing approaches as scalable methods for accurate sleep-classification, providing guidance on optimal algorithm deployment for automated sleep assessment. The code used in this study can be found online at: https://github.com/bzhai/multimodal_sleep_stage_benchmark.git},
	articleno    = 67,
	numpages     = 33,
	keywords     = {Heart Rate, Neural Networks, Heart Rate Variability, Sleep, Multistage Classification, sleep Stage, Actigraphy, Multimodal Sensing}
}
@article{10.1145/3397319,
	title        = {How Subtle Can It Get? A Trimodal Study of Ring-Sized Interfaces for One-Handed Drone Control},
	author       = {Yau, Yui-Pan and Lee, Lik Hang and Li, Zheng and Braud, Tristan and Ho, Yi-Hsuan and Hui, Pan},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397319},
	url          = {https://doi.org/10.1145/3397319},
	issue_date   = {June 2020},
	abstract     = {Flying drones have become common objects in our daily lives, serving a multitude of purposes. Many of these purposes involve outdoor scenarios where the user combines drone control with another activity. Traditional interaction methods rely on physical or virtual joysticks that occupy both hands, thus restricting drone usability. In this paper, we investigate one-handed human-to-drone-interaction by leveraging three modalities: force, touch, and IMU. After prototyping three different combinations of these modalities on a smartphone, we evaluate them against the current commercial standard through two user experiments. These experiments help us to find the combination of modalities that strikes a compromise between user performance, perceived task load, wrist rotation, and interaction area size. Accordingly, we select a method that achieves faster task completion times than the two-handed commercial baseline by 16.54% with the merits of subtle user behaviours inside a small-size ring-form device and implements this method within the ring-form device. The last experiment involving 12 participants shows that thanks to its small size and weight, the ring device displays better performance than the same method implemented on a mobile phone. Furthermore, users unanimously found the device useful for controlling a drone in mobile scenarios (AVG = 3.92/5), easy to use (AVG = 3.58/5) and easy to learn (AVG = 3.58/5). Our findings give significant design clues in search of subtle and effective interaction through finger augmentation devices with drone control. The users with our prototypical system and a multi-modal on-finger device can control a drone with subtle wrist rotation (pitch gestures: 43.24° amplitude and roll gestures: 46.35° amplitude) and unnoticeable thumb presses within a miniature-sized area of (1.08 * 0.61 cm2).},
	articleno    = 63,
	numpages     = 29,
	keywords     = {multi-modal inputs, On-finger interaction, Human-drone interaction}
}
@article{10.1145/3381010,
	title        = {MmASL: Environment-Independent ASL Gesture Recognition Using 60 GHz Millimeter-Wave Signals},
	author       = {Santhalingam, Panneer Selvam and Hosain, Al Amin and Zhang, Ding and Pathak, Parth and Rangwala, Huzefa and Kushalnagar, Raja},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381010},
	url          = {https://doi.org/10.1145/3381010},
	issue_date   = {March 2020},
	abstract     = {Home assistant devices such as Amazon Echo and Google Home have become tremendously popular in the last couple of years. However, due to their voice-controlled functionality, these devices are not accessible to Deaf and Hard-of-Hearing (DHH) people. Given that over half a million people in the United States communicate using American Sign Language (ASL), there is a need of a home assistant system that can recognize ASL. The objective of this work is to design a home assistant system for DHH users (referred to as mmASL) that can perform ASL recognition using 60 GHz millimeter-wave wireless signals. mmASL has two important components. First, it can perform reliable wake-word detection using spatial spectrograms. Second, using a scalable and extensible multi-task deep learning model, mmASL can learn the phonological properties of ASL signs and use them to accurately recognize the ASL signs. We implement mmASL on 60 GHz software radio platform with phased array, and evaluate it using a large-scale data collection from 15 signers, 50 ASL signs and over 12K sign instances. We show that mmASL is tolerant to the presence of other interfering users and their activities, change of environment and different user positions. We compare mmASL with a well-studied Kinect and RGB camera based ASL recognition systems, and find that it can achieve a comparable performance (87% average accuracy of sign recognition), validating the feasibility of using 60 GHz mmWave system for ASL sign recognition.},
	articleno    = 26,
	numpages     = 30,
	keywords     = {accessible computing, sign language recognition, gesture recognition, personal digital assistants, 60 GHz milli-meter wave wireless}
}
@article{10.1145/3380999,
	title        = {GIobalFusion: A Global Attentional Deep Learning Framework for Multisensor Information Fusion},
	author       = {Liu, Shengzhong and Yao, Shuochao and Li, Jinyang and Liu, Dongxin and Wang, Tianshi and Shao, Huajie and Abdelzaher, Tarek},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380999},
	url          = {https://doi.org/10.1145/3380999},
	issue_date   = {March 2020},
	abstract     = {The paper enhances deep-neural-network-based inference in sensing applications by introducing a lightweight attention mechanism called the global attention module for multi-sensor information fusion. This mechanism is capable of utilizing information collected from higher layers of the neural network to selectively amplify the influence of informative features and suppress unrelated noise at the fusion layer. We successfully integrate this mechanism into a new end-to-end learning framework, called GIobalFusion, where two global attention modules are deployed for spatial fusion and sensing modality fusion, respectively. Through an extensive evaluation on four public human activity recognition (HAR) datasets, we successfully demonstrate the effectiveness of GlobalFusion at improving information fusion quality. The new approach outperforms the state-of-the-art algorithms on all four datasets with a clear margin. We also show that the learned attention weights agree well with human intuition. We then validate the efficiency of GlobalFusion by testing its inference time and energy consumption on commodity IoT devices. Only a negligible overhead is induced by the global attention modules.},
	articleno    = 19,
	numpages     = 27,
	keywords     = {Internet of Things (IoT), neural networks, multisensor information fusion}
}
@article{10.1145/3380993,
	title        = {Assumptions Checked: How Families Learn About and Use the Echo Dot},
	author       = {Beneteau, Erin and Guan, Yini and Richards, Olivia K. and Zhang, Mingrui Ray and Kientz, Julie A. and Yip, Jason and Hiniker, Alexis},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380993},
	url          = {https://doi.org/10.1145/3380993},
	issue_date   = {March 2020},
	abstract     = {Users of voice assistants often report that they fall into patterns of using their device for a limited set of interactions, like checking the weather and setting alarms. However, it's not clear if limited use is, in part, due to lack of learning about the device's functionality. We recruited 10 diverse families to participate in a one-month deployment study of the Echo Dot, enabling us to investigate: 1) which features families are aware of and engage with, and 2) how families explore, discover, and learn to use the Echo Dot. Through audio recordings of families' interactions with the device and pre- and post-deployment interviews, we find that families' breadth of use decreases steadily over time and that families learn about functionality through trial and error, asking the Echo Dot about itself, and through outside influencers such as friends and family. Formal outside learning influencers, such as manufacturer emails, are less influential. Drawing from diffusion of innovation theory, we describe how a home-based voice interface might be positioned as a near-peer to the user, and that by describing its own functionality using just-in-time learning, the home-based voice interface becomes a trustworthy learning influencer from which users can discover new functionalities.},
	articleno    = 3,
	numpages     = 23,
	keywords     = {Smart Speakers, Learning, Voice interfaces, Digital Home Assistants}
}
@article{10.1145/3380990,
	title        = {PrivateBus: Privacy Identification and Protection in Large-Scale Bus WiFi Systems},
	author       = {Fang, Zhihan and Fu, Boyang and Qin, Zhou and Zhang, Fan and Zhang, Desheng},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380990},
	url          = {https://doi.org/10.1145/3380990},
	issue_date   = {March 2020},
	abstract     = {Recently, the ubiquity of mobile devices leads to an increasing demand of public network services, e.g., WiFi hot spots. As a part of this trend, modern transportation systems are equipped with public WiFi devices to provide Internet access for passengers as people spend a large amount of time on public transportation in their daily life. However, one of the key issues in public WiFi spots is the privacy concern due to its open access nature. Existing works either studied location privacy risk in human traces or privacy leakage in private networks such as cellular networks based on the data from cellular carriers. To the best of our knowledge, none of these work has been focused on bus WiFi privacy based on large-scale real-world data. In this paper, to explore the privacy risk in bus WiFi systems, we focus on two key questions how likely bus WiFi users can be uniquely re-identified if partial usage information is leaked and how we can protect users from the leaked information. To understand the above questions, we conduct a case study in a large-scale bus WiFi system, which contains 20 million connection records and 78 million location records from 770 thousand bus WiFi users during a two-month period. Technically, we design two models for our uniqueness analyses and protection, i.e., a PB-FIND model to identify the probability a user can be uniquely re-identified from leaked information; a PB-HIDE model to protect users from potentially leaked information. Specifically, we systematically measure the user uniqueness on users' finger traces (i.e., connection URL and domain), foot traces (i.e., locations), and hybrid traces (i.e., both finger and foot traces). Our measurement results reveal (i) 97.8% users can be uniquely re-identified by 4 random domain records of their finger traces and 96.2% users can be uniquely re-identified by 5 random locations on buses; (ii) 98.1% users can be uniquely re-identified by only 2 random records if both their connection records and locations are leaked to attackers. Moreover, the evaluation results show our PB-HIDE algorithm protects more than 95% users from the potentially leaked information by inserting only 1.5% synthetic records in the original dataset to preserve their data utility.},
	articleno    = 9,
	numpages     = 23,
	keywords     = {uniqueness, hybrid traces, foot traces, finger traces, bus WiFi}
}
@article{10.1145/3380989,
	title        = {UbiquiTouch: Self Sustaining Ubiquitous Touch Interfaces},
	author       = {Waghmare, Anandghan and Xue, Qiuyue and Zhang, Dingtian and Zhao, Yuhui and Mittal, Shivan and Arora, Nivedita and Byrne, Ceara and Starner, Thad and Abowd, Gregory D},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380989},
	url          = {https://doi.org/10.1145/3380989},
	issue_date   = {March 2020},
	abstract     = {We present UbiquiTouch, an ultra low power wireless touch interface. With an average power consumption of 30.91μW, UbiquiTouch can run on energy harvested from ambient light. It achieves this performance through low power touch sensing and passive communication to a nearby smartphone using ambient FM backscatter. This approach allows UbiquiTouch to be deployed in mobile situations both in indoor and outdoor locations, without the need for any additional infrastructure for operation. To demonstrate the potential of this technology, we evaluate it in several different and realistic scenarios. Finally, we address the future application space for this technology.},
	articleno    = 27,
	numpages     = 22,
	keywords     = {Power harvesting, Backscatter communication, Touch interaction, Low power computing}
}
@article{10.1145/3380987,
	title        = {SmokingOpp: Detecting the Smoking 'Opportunity' Context Using Mobile Sensors},
	author       = {Chatterjee, Soujanya and Moreno, Alexander and Lizotte, Steven Lloyd and Akther, Sayma and Ertin, Emre and Fagundes, Christopher P and Lam, Cho and Rehg, James M. and Wan, Neng and Wetter, David W. and Kumar, Santosh},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380987},
	url          = {https://doi.org/10.1145/3380987},
	issue_date   = {March 2020},
	abstract     = {Context plays a key role in impulsive adverse behaviors such as fights, suicide attempts, binge-drinking, and smoking lapse. Several contexts dissuade such behaviors, but some may trigger adverse impulsive behaviors. We define these latter contexts as 'opportunity' contexts, as their passive detection from sensors can be used to deliver context-sensitive interventions.In this paper, we define the general concept of 'opportunity' contexts and apply it to the case of smoking cessation. We operationalize the smoking 'opportunity' context, using self-reported smoking allowance and cigarette availability. We show its clinical utility by establishing its association with smoking occurrences using Granger causality. Next, we mine several informative features from GPS traces, including the novel location context of smoking spots, to develop the SmokingOpp model for automatically detecting the smoking 'opportunity' context. Finally, we train and evaluate the SmokingOpp model using 15 million GPS points and 3,432 self-reports from 90 newly abstinent smokers in a smoking cessation study.},
	articleno    = 4,
	numpages     = 26,
	keywords     = {Context, Smoking Cessation, Mobile Health, GPS traces, Intervention}
}
@article{10.1145/3351278,
	title        = {ShoesHacker: Indoor Corridor Map and User Location Leakage through Force Sensors in Smart Shoes},
	author       = {Yu, Tuo and Nahrstedt, Klara},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351278},
	url          = {https://doi.org/10.1145/3351278},
	issue_date   = {September 2019},
	abstract     = {The past few years have witnessed the rise of smart shoes, the wearable devices that measure foot force or track foot motion. However, people are not aware of the possible privacy leakage from in-shoe force sensors. In this paper, we explore the possibility of locating an indoor victim based on the force signals leaked from smart shoes. We present ShoesHacker, an attack scheme that reconstructs the corridor map of the building that the victim walks in based on force data only. The corridor map enables the attacker to recognize the building, and thus locate the victim on a global map. To handle the lack of training data, we design the stair landing detection algorithm, based on which we extract training data when victims are walking in stairwells. We estimate the trajectory of each walk, and propose the path merging algorithm to merge the trajectories. Moreover, we design a metric to quantify the similarity between corridor maps, which makes building recognition possible. Our experimental results show that, the building recognition accuracy reaches 77.5% in a 40-building dataset, and the victim can be located with an average error lower than 6 m, which reveals the danger of privacy leakage through smart shoes. CCS Concepts: • Information systems~Mobile information processing systems; Location based services; • Human-centered computing~Mobile devices; Ubiquitous and mobile computing systems and tools; • Security and privacy~Domain-specific security and privacy architectures.},
	articleno    = 120,
	numpages     = 29,
	keywords     = {smart shoes, force sensors, Corridor map reconstruction}
}
@article{10.1145/3351268,
	title        = {News From the Background to the Foreground: How People Use Technology To Manage Media Transitions: A Study of Technology-Mediated News Behaviors in a Hyper-Connected World},
	author       = {Wirfs-Brock, Jordan and Quehl, Katie},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351268},
	url          = {https://doi.org/10.1145/3351268},
	issue_date   = {September 2019},
	abstract     = {People are the designers and curators of their own news and information ecosystems, due to the disruption of the news industry and developments in media technology. To understand how people use technology to manage their news consumption, we conducted a two-week diary study with 14 participants, focusing on how people transition between news content and behaviors via different media, sources, platforms and devices. We used an inductive, qualitative analysis of the diary study data to analyze the news behaviors and their underlying motivations and found that people frequently shift their focus between ambient background news streams and active foreground news behaviors. Although people often passively consume news content as a background activity, they also actively manage background news habits to increase the chances of relevant foreground experiences. People manage news consumption by developing routines that are often supported by technology use and social interactions. We encourage product designers to treat backgrounding as an essential part of news consumption behavior and suggest new design directions that employ ubiquitous computing technologies---such as context sensing and routine modeling---to more effectively attend to background-to-foreground behaviors and transitions.},
	articleno    = 110,
	numpages     = 19,
	keywords     = {transmodal, cross-device, media consumption, multitasking, news, diary study, passive media, cross-media, information seeking, cross-platform}
}
@article{10.1145/3351262,
	title        = {Recruit Until It Fails: Exploring Performance Limits for Identification Systems},
	author       = {Sugrim, Shridatt and Liu, Can and Lindqvist, Janne},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351262},
	url          = {https://doi.org/10.1145/3351262},
	issue_date   = {September 2019},
	abstract     = {Distinguishing identities is useful for several applications such as automated grocery or personalized recommendations. Unfortunately, several recent proposals for identification systems are evaluated using poor recruitment practices. We discovered that 23 out of 30 surveyed systems used datasets with 20 participants or less. Those studies achieved an average classification accuracy of 93%. We show that the classifier performance is misleading when the participant count is small. This is because the finite precision of measurements creates upper limits on the number of users that can be distinguished.To demonstrate why classifier performance is misleading, we used publicly available datasets. The data was collected from human subjects. We created five systems with at least 20 participants each. In three cases we achieved accuracies greater than 90% by merely applying readily available machine learning software packages, often with default parameters. For datasets where we had sufficient participants, we evaluated how the performance degrades as the number of participants increases. One of the systems built suffered a drop in accuracy that was over 35% as the participant count increased from 20 to 250. We argue that data from small participant count datasets do not adequately explore variations. Systems trained on such limited data are likely to incorrectly identify users when the user base increases beyond what was tested. We conclude by explaining generalizable reasons for this issue and provide insights on how to conduct more robust system analysis and design.},
	articleno    = 104,
	numpages     = 26,
	keywords     = {identification, sample size, machine learning, quantitative methods}
}
@article{10.1145/3351260,
	title        = {Using Deep Learning and Mobile Offloading to Control a 3D-Printed Prosthetic Hand},
	author       = {Shatilov, Kirill A. and Chatzopoulos, Dimitris and Hang, Alex Wong Tat and Hui, Pan},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351260},
	url          = {https://doi.org/10.1145/3351260},
	issue_date   = {September 2019},
	abstract     = {Although many children are born with congenital limb malformation, contemporary functional artificial hands are costly and are not meant to be adapted to growing hand. In this work, we develop a low cost, adaptable and personalizable system of an artificial prosthetic hand accompanied with hardware and software modules. Our solution consists of (i) a consumer grade electromyography (EMG) recording hardware, (ii) a mobile companion device empowered by deep learning classification algorithms, (iii) an cloud component for offloading computations, and (iv) mechanical 3D printed arm operated by the embedded hardware. We focus on the flexibility of the designed system making it more affordable than the alternatives. We use 3D printed materials and open-source software thus enabling the community to contribute and improve the system. In this paper, we describe the proposed system and its components and present the experiments we conducted in order to show the feasibility and applicability of our approach. Extended experimentation shows that our proposal is energy efficient and has high accuracy.},
	articleno    = 102,
	numpages     = 19,
	keywords     = {EMG, Deep learning, Electromyography, prosthesis}
}
@article{10.1145/3351240,
	title        = {Knocker: Vibroacoustic-Based Object Recognition with Smartphones},
	author       = {Gong, Taesik and Cho, Hyunsung and Lee, Bowon and Lee, Sung-Ju},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351240},
	url          = {https://doi.org/10.1145/3351240},
	issue_date   = {September 2019},
	abstract     = {While smartphones have enriched our lives with diverse applications and functionalities, the user experience still often involves manual cumbersome inputs. To purchase a bottle of water for instance, a user must locate an e-commerce app, type the keyword for a search, select the right item from the list, and finally place an order. This process could be greatly simplified if the smartphone identifies the object of interest and automatically executes the user preferred actions for the object. We present Knocker that identifies the object when a user simply knocks on an object with a smartphone. The basic principle of Knocker is leveraging a unique set of responses generated from the knock. Knocker takes a multimodal sensing approach that utilizes microphones, accelerometers, and gyroscopes to capture the knock responses, and exploits machine learning to accurately identify objects. We also present 15 applications enabled by Knocker that showcase the novel interaction method between users and objects. Knocker uses only the built-in smartphone sensors and thus is fully deployable without specialized hardware or tags on either the objects or the smartphone. Our experiments with 23 objects show that Knocker achieves an accuracy of 98% in a controlled lab and 83% in the wild.},
	articleno    = 82,
	numpages     = 21,
	keywords     = {Object interaction, Smartphone sensing, Machine learning, Object recognition, Multimodal sensing}
}
@article{10.1145/3351234,
	title        = {The Positive Impact of Push vs Pull Progress Feedback: A 6-Week Activity Tracking Study in the Wild},
	author       = {Cauchard, Jessica R. and Frey, Jeremy and Zahrt, Octavia and Johnson, Krister and Crum, Alia and Landay, James A.},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351234},
	url          = {https://doi.org/10.1145/3351234},
	issue_date   = {September 2019},
	abstract     = {Lack of physical activity has been shown to increase disease and reduce life expectancy. In response, mobile devices are increasingly being used to support people's health and fitness by tracking physical activity. Prior work shows that the type of feedback, either ambient or via notification, affects users' behavior towards their physical activity. Yet, these phone- and watch-based interactions and notifications have primarily been visual in nature. Inspired by prior research, we explored the impact of feedback modality (visual, tactile, and hybrid: visual/tactile) on 44 participants' behavior and exercise mindset in a 6-week field study. We present the differences between modalities and the notion of push vs. pull for interface feedback and notifications. Across 1,662 days of study data, we found statistically significant impacts of feedback modality and, in particular, the positive effects of push feedback on participants' mindset about the process of exercise. Our results also highlight design guidelines for wearables and multimodal notification systems.},
	articleno    = 76,
	numpages     = 23,
	keywords     = {Health, Step counter, Quantified self-tracking devices, Fitness, Behavior change, Vibrotactile display, Personal informatics, Multimodal, Activity tracker, Wearables, Vibrations}
}
@article{10.1145/3351232,
	title        = {Back to Real Pictures: A Cross-Generational Understanding of Users' Mental Models of Photo Cloud Storage},
	author       = {Axtell, Benett and Munteanu, Cosmin},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351232},
	url          = {https://doi.org/10.1145/3351232},
	issue_date   = {September 2019},
	abstract     = {Personal pictures storage is currently split between a myriad of physical and digital tools. Cloud photo storage and social networks are seeing increasing adoption, and are being recommended to families (especially older generations) as digital pictures solutions. The ubiquity of these platforms raises the question of whether the design of their photo-based operations consider the mental models of their cross-generational users. Understanding mental models is a key factor for the usability (and adoption) of these technologies. Previous works have observed that perceptions of digital storage limit adoption, especially for older users. However, we do not yet understand users' mental models of these applications. This impedes efforts to design applications better matching diverse user needs. We present here a cross-generational investigation of users' mental models of ubiquitous picture technologies, including cloud storage and social sharing. We find that mental models are split (both between generations and domains), contributing to lower adoption by older adults. Our analysis reveals that digital tools need to understand their roots in physical pictures and bridge this divide by including physical concepts as an aspect of use, if we are to support cross-generational interactions with personal and family pictures.},
	articleno    = 74,
	numpages     = 24,
	keywords     = {digital picture management, Exposing mental models, technology and older adults}
}
@article{10.1145/3351229,
	title        = {EduSense: Practical Classroom Sensing at Scale},
	author       = {Ahuja, Karan and Kim, Dohyun and Xhakaj, Franceska and Varga, Virag and Xie, Anne and Zhang, Stanley and Townsend, Jay Eric and Harrison, Chris and Ogan, Amy and Agarwal, Yuvraj},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351229},
	url          = {https://doi.org/10.1145/3351229},
	issue_date   = {September 2019},
	abstract     = {Providing university teachers with high-quality opportunities for professional development cannot happen without data about the classroom environment. Currently, the most effective mechanism is for an expert to observe one or more lectures and provide personalized formative feedback to the instructor. Of course, this is expensive and unscalable, and perhaps most critically, precludes a continuous learning feedback loop for the instructor. In this paper, we present the culmination of two years of research and development on EduSense, a comprehensive sensing system that produces a plethora of theoretically-motivated visual and audio features correlated with effective instruction, which could feed professional development tools in much the same way as a Fitbit sensor reports step count to an end user app. Although previous systems have demonstrated some of our features in isolation, EduSense is the first to unify them into a cohesive, real-time, in-the-wild evaluated, and practically-deployable system. Our two studies quantify where contemporary machine learning techniques are robust, and where they fall short, illuminating where future work remains to bring the vision of automated classroom analytics to reality.},
	articleno    = 71,
	numpages     = 26,
	keywords     = {Sensing, Classroom, Speech Detection, Machine Learning, Pedagogy, Teacher, Computer Vision, Audio, Instructor}
}
@article{10.1145/3328910,
	title        = {Multi-Stage Receptivity Model for Mobile Just-In-Time Health Intervention},
	author       = {Choi, Woohyeok and Park, Sangkeun and Kim, Duyeon and Lim, Youn-kyung and Lee, Uichin},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328910},
	url          = {https://doi.org/10.1145/3328910},
	issue_date   = {June 2019},
	abstract     = {A critical aspect of mobile just-in-time (JIT) health intervention is proper delivery timing, which correlates with successfully promoting target behaviors. Despite extensive prior studies on interruptibility, however, our understanding of the receptivity of mobile JIT health intervention is limited. This work extends prior interruptibility models to capture the JIT intervention process by including multiple stages of conscious and subconscious decisions. We built BeActive, a mobile intervention system for preventing prolonged sedentary behaviors, and we collected users' responses to a given JIT support and relevant contextual factors and cognitive/physical states for three weeks. Using a multi-stage model, we systematically analyzed the responses to deepen our understanding of receptivity using a mixed methodology. Herein, we identify the key factors relevant to each stage outcome and show that the receptivity of JIT intervention is nuanced and context-dependent. We propose several practical design implications for mobile JIT health intervention and context-aware computing.},
	articleno    = 39,
	numpages     = 26,
	keywords     = {prolonged sedentariness, interruptibility, receptivity, Just-in-time intervention}
}
@article{10.1145/3328907,
	title        = {An Optimized Recurrent Unit for Ultra-Low-Power Keyword Spotting},
	author       = {Amoh, Justice and Odame, Kofi M.},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328907},
	url          = {https://doi.org/10.1145/3328907},
	issue_date   = {June 2019},
	abstract     = {There is growing interest in being able to run neural networks on sensors, wearables and internet-of-things (IoT) devices. However, the computational demands of neural networks make them difficult to deploy on resource-constrained edge devices.To meet this need, our work introduces a new recurrent unit architecture that is specifically adapted for on-device low power acoustic event detection (AED). The proposed architecture is based on the gated recurrent unit ('GRU' -- introduced by Cho et al. [9]) but features optimizations that make it implementable on ultra-low power micro-controllers such as the Arm Cortex M0+.Our new architecture, the Embedded Gated Recurrent Unit (eGRU) is demonstrated to be highly efficient and suitable for short-duration AED and keyword spotting tasks. A single eGRU cell is 60\texttimes{} faster and 10\texttimes{} smaller than a GRU cell. Despite its optimizations, eGRU compares well with GRU across tasks of varying complexities.The practicality of eGRU is investigated in a wearable acoustic event detection application. An eGRU model is implemented and tested on the Arm Cortex M0-based Atmel ATSAMD21E18 processor. The Arm M0+ implementation of the eGRU model compares favorably with a full precision GRU that is running on a workstation. The embedded eGRU model achieves a classification accuracy 95.3%, which is only 2% less than the full precision GRU.},
	articleno    = 36,
	numpages     = 17,
	keywords     = {acoustic event detection, Wearable devices, edge machine learning, deep learning, keyword-spotting, recurrent neural networks}
}
@article{10.1145/3314416,
	title        = {RF-Focus: Computer Vision-Assisted Region-of-Interest RFID Tag Recognition and Localization in Multipath-Prevalent Environments},
	author       = {Wang, Zhongqin and Xu, Min and Ye, Ning and Wang, Ruchuan and Huang, Haiping},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314416},
	url          = {https://doi.org/10.1145/3314416},
	issue_date   = {March 2019},
	abstract     = {Capturing RFID tags in the region of interest (ROI) is challenging. Many issues, such as multipath interference, frequency-dependent hardware characteristics and phase periodicity, make RF phase difficult to accurately indicate the tag-to-antenna distance for RFID tag localization. In this paper, we propose a comprehensive solution, called RF-Focus, which fuses RFID and computer vision (CV) techniques to recognize and locate moving RFID-tagged objects within ROI. Firstly, we build a multipath propagation model and propose a dual-antenna solution to minimize the impact of multipath interference on RF phase. Secondly, by extending the multipath model, we estimate phase shifts due to hardware characteristics at different operating frequencies. Thirdly, to minimize the tag position uncertainty due to RF phase periodicity, we leverage CV to extract image regions of being likely to contain ROI RFID-tagged objects, and then associate them with the processed RF phase after the removal of the phase shifts due to multipath interference and hardware characteristics for recognition and localization. Our experiments demonstrate the effectiveness of multipath modelling and hardware-related phase shift estimation. When five RFID-tagged objects are moving in the ROI, RF-Focus achieves the average recognition accuracy of 91.67% and localization accuracy of 94.26% given a false positive rate of 10%.},
	articleno    = 29,
	numpages     = 30,
	keywords     = {RFID, Computer Vision, Localization, Multipath Interference, False Positive Reading, Frequency Hopping}
}
@article{10.1145/3314413,
	title        = {From Fingerprint to Footprint: Cold-Start Location Recommendation by Learning User Interest from App Data},
	author       = {Tu, Zhen and Fan, Yali and Li, Yong and Chen, Xiang and Su, Li and Jin, Depeng},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314413},
	url          = {https://doi.org/10.1145/3314413},
	issue_date   = {March 2019},
	abstract     = {With increasing diversity of user interest and preference, personalized location recommendation is essential and beneficial to our daily life. To achieve this, the most critical challenge is the cold-start recommendation problem, for we cannot learn preference from cold-start users without any historical records. In this paper, we demonstrate that it is feasible to make personalized location recommendation by learning user interest and location features from app usage data. By proposing a novel generative model to transfer user interests from app usage behavior to location preference, we achieve personalized location recommendation via learning the interest's correlation between locations and apps. Based on two real-world datasets, we evaluate our method's performance with a variety of scenarios and parameters. The results demonstrate that our method outperforms the state-of-the-art solutions in solving cold-start problem, i.e., when there are 60% cold-start users, we can still achieve a 77.0% hitrate in recommending the top five locations, which is at least 9.6% higher than the baselines. Our study is the first step forward for transferring user interests learning from online fingerprints to offline footprints, which paves the way for better personalized location recommendation services.},
	articleno    = 26,
	numpages     = 22,
	keywords     = {cold-start problem, generative model, Location recommendation}
}
@article{10.1145/3314408,
	title        = {Using Built-In Sensors to Predict and Utilize User Satisfaction for CPU Settings on Smartphones},
	author       = {Poyraz, Emirhan and Memik, Gokhan},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314408},
	url          = {https://doi.org/10.1145/3314408},
	issue_date   = {March 2019},
	abstract     = {Understanding user experience/satisfaction with mobile systems in order to manage computational resources has become a popular approach in recent years. One of the key issues in this area is to gauge user satisfaction. In this paper, we propose and evaluate a system to save energy by altering CPU core count and frequency while keeping users satisfied. Specifically, the system uses the sensor data collected from two popular personal devices: a smartphone and a smartwatch. In the proposed architecture, we first develop prediction models by collecting sensor data along with user performance satisfaction inputs. Then, our system predicts users' current satisfaction and sets CPU core/frequency based on these predictions in real-time. We observe that sensor data gathered from these two devices are highly correlated with users' instantaneous satisfaction of the phone.We evaluate the proposed system by developing and comparing two different models. First, we develop a user-independent (user-oblivious) model by using data gathered from 10 users. Second, we develop user-dependent (personal) models for 20 different users. We demonstrate that both models can predict satisfaction with over 97% accuracy on average when a binary satisfaction model is used (i.e., users indicating satisfied versus unsatisfied). The prediction accuracy is over 91% on average if a 3-level satisfaction model is used. Our results also show that when compared to default scheme, the user-independent and user-dependent models save 8.96% and 10.12% of the total system energy on average, respectively, without impacting user satisfaction.},
	articleno    = 21,
	numpages     = 25,
	keywords     = {Smartwatches, user experience, multicore architecture, sensors, Smartphones, power optimization}
}
@article{10.1145/3314391,
	title        = {CAP: Context-Aware App Usage Prediction with Heterogeneous Graph Embedding},
	author       = {Chen, Xinlei and Wang, Yu and He, Jiayou and Pan, Shijia and Li, Yong and Zhang, Pei},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314391},
	url          = {https://doi.org/10.1145/3314391},
	issue_date   = {March 2019},
	abstract     = {Context-aware mobile application (App) usage prediction benefits a variety of applications such as precise bandwidth allocation, App launch acceleration, etc. Prior works have explored this topic through individual data profiles and contextual information. However, it is still a challenging problem because of the following three aspects: i. App usage behavior is usually influenced by multiple factors, especially temporal and spatial factors. ii. It is difficult to describe individuals' preferences, which are usually time-variant. iii. A single user's data is sparse on the spatial domain and only covers a limited number of locations. Prediction becomes more difficult when the user appears at a new location.This paper presents CAP, a context-aware App usage prediction algorithm that takes both contextual information (location &amp; time) and attribution (App with type information) into consideration. We find that the relationships between App-location, App-time, and App-App type are essential to prediction and propose a heterogeneous graph embedding algorithm to map them into the common comparable latent space. In addition, we create a user profile for each user with App usage and trajectory history to describe the individual dynamic preference for personalized prediction. We evaluate the performance of our proposed CAP with two large-scale real-world datasets. Extensive evaluations demonstrate that CAP achieves 30% higher accuracy than a state-of-the-art method Personalized Ranking Metric Embedding (PRME) in terms of Accuracy@5. In terms of mean reciprocal rank (MRR), CAP achieves 1.5\texttimes{} higher than the straightforward baseline Sta and 2\texttimes{} higher than PRME. Our investigation enables a range of applications to benefit from such timely predictions, including network operators, service providers, and etc.},
	articleno    = 4,
	numpages     = 25,
	keywords     = {Graph Embedding, Behaviour Modeling, Context Aware, Application Usage}
}
@article{10.1145/3287076,
	title        = {HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices},
	author       = {Yan, Yukang and Yu, Chun and Yi, Xin and Shi, Yuanchun},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287076},
	url          = {https://doi.org/10.1145/3287076},
	issue_date   = {December 2018},
	abstract     = {We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.},
	articleno    = 198,
	numpages     = 23,
	keywords     = {Virtual Reality, Gesture, Head Movement Interaction}
}
@article{10.1145/3287074,
	title        = {CoSense: Collaborative Urban-Scale Vehicle Sensing Based on Heterogeneous Fleets},
	author       = {Xie, Xiaoyang and Yang, Yu and Fang, Zhihan and Wang, Guang and Zhang, Fan and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287074},
	url          = {https://doi.org/10.1145/3287074},
	issue_date   = {December 2018},
	abstract     = {The real-time vehicle sensing at urban scale is essential to various urban services. To date, most existing approaches rely on static infrastructures (e.g., traffic cameras) or mobile services (e.g., smartphone apps). However, these approaches are often inadequate for urban scale vehicle sensing at the individual level because of their static natures or low penetration rates. In this paper, we design a sensing system called coSense to utilize commercial vehicular fleets (e.g., taxis, buses, and trucks) for real-time vehicle sensing at urban scale, given (i) the availability of well-equipped commercial fleets sensing other vehicles by onboard cameras or peer-to-peer communication, and (ii) an increasing trend of connected vehicles and autonomous vehicles with periodical status broadcasts for safety applications. Compared to existing solutions based on cameras and smartphones, the key features of coSense are in its high penetration rates and transparent sensing for participating drivers. The key technical challenge we addressed is how to recover spatiotemporal sensing gaps by considering various mobility patterns of commercial vehicles with deep learning. We evaluate coSense with a preliminary road test and a large-scale trace-driven evaluation based on vehicular fleets in the Chinese city Shenzhen, including 14 thousand taxis, 13 thousand buses, 13 thousand trucks, and 10 thousand regular vehicles. We compare coSense to infrastructure and cellphone-based approaches, and the results show that we increase the sensing accuracy by 10.1% and 16.6% on average.},
	articleno    = 196,
	numpages     = 25,
	keywords     = {Mobility Patterns, Heterogeneous Fleets, Vehicle Sensing}
}
@article{10.1145/3287072,
	title        = {ObstacleWatch: Acoustic-Based Obstacle Collision Detection for Pedestrian Using Smartphone},
	author       = {Wang, Zi and Tan, Sheng and Zhang, Linghan and Yang, Jie},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287072},
	url          = {https://doi.org/10.1145/3287072},
	issue_date   = {December 2018},
	abstract     = {Walking while using a smartphone is becoming a major pedestrian safety concern as people may unknowingly bump into various obstacles that could lead to severe injuries. In this paper, we propose ObstacleWatch, an acoustic-based obstacle collision detection system to improve the safety of pedestrians who are engaged in smartphone usage while walking. ObstacleWatch leverages the advanced audio hardware of the smartphone to sense the surrounding obstacles and infers fine-grained information about the frontal obstacle for collision detection. In particular, our system emits well-designed inaudible beep signals from the smartphone built-in speaker and listens to the reflections with the stereo recording of the smartphone. By analyzing the reflected signals received at two microphones, ObstacleWatch is able to extract fine-grained information of the frontal obstacle including the distance, angle and size for detecting the possible collisions and to alert users. Our experimental evaluation under two real-world environments with different types of phones and obstacles shows that ObstacleWatch achieves over 92% accuracy in predicting obstacle collisions with distance estimation errors at about 2 cm. Results also show that ObstacleWatch is robust to different sizes of objects and is compatible to different phone models with low energy consumption.},
	articleno    = 194,
	numpages     = 22,
	keywords     = {Mobile Sensing, Acoustic Localization, Pedestrian Safety, Acoustic Ranging, Smartphone}
}
@article{10.1145/3287071,
	title        = {Modeling RFID Signal Reflection for Contact-Free Activity Recognition},
	author       = {Wang, Yanwen and Zheng, Yuanqing},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287071},
	url          = {https://doi.org/10.1145/3287071},
	issue_date   = {December 2018},
	abstract     = {Wireless sensing techniques for tracking human activities have been vigorously developed in recent years. Yet current RFID based human activity recognition techniques need either direct contact to human body (e.g., attaching RFIDs to users) or specialized hardware (e.g., software defined radios, antenna array). How to wirelessly track human activities using commodity RFID systems without attaching tags to users (i.e., a contact-free scenario) still faces lots of technical challenges. In this paper, we quantify the correlation between RF phase values and human activities by modeling intrinsic characteristics of signal reflection in contact-free scenarios. Based on the signal reflection model, we introduce TACT that can recognize human activities using commodity RFIDs without attaching any RFID tags to users. TACT first reliably detects the presence of human activities and segments phase values. Then, candidate phase segments are classified according to their coarse-grained features (e.g., moving speed, moving distance, activity duration) as well as their fine-grained feature of phase waveform. We deploy and leverage multiple tags to increase the coverage and enhance the robustness of the system. We implement TACT with commodity RFID systems. We invite 12 participants to evaluate our system in various scenarios. The experiment results show that TACT can recognize eight types of human activities with 93.5% precision under different and challenging experiment settings.},
	articleno    = 193,
	numpages     = 22,
	keywords     = {backscatter communication, contact-free, Activity recognition, RFID systems}
}
@article{10.1145/3287056,
	title        = {Coconut: An IDE Plugin for Developing Privacy-Friendly Apps},
	author       = {Li, Tianshi and Agarwal, Yuvraj and Hong, Jason I.},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287056},
	url          = {https://doi.org/10.1145/3287056},
	issue_date   = {December 2018},
	abstract     = {Although app developers are responsible for protecting users' privacy, this task can be very challenging. In this paper, we present Coconut, an Android Studio plugin that helps developers handle privacy requirements by engaging developers to think about privacy during the development process and providing real-time feedback on potential privacy issues. We start by presenting new findings based on a series of semi-structured interviews with Android developers, probing into the difficulties with privacy that developers face when building apps. Based on these findings, we implemented a proof-of-concept prototype of Coconut and evaluated it in a controlled lab study with 18 Android developers (including eight professional developers). Our study results suggest that apps developed with Coconut handled privacy concerns better, and the developers that used Coconut had a better understanding of their code's behavior and wrote a better privacy policy for their app. We also found that requiring developers to do a small amount of annotating work regarding their apps' personal data practices during the development process may result in a significant improvement in app privacy.},
	articleno    = 178,
	numpages     = 35,
	keywords     = {human-centered methods, privacy, Android development, programming environment}
}
@article{10.1145/3287047,
	title        = {Passerby Crowdsourcing: Workers' Behavior and Data Quality Management},
	author       = {Iwamoto, Eiichi and Matsubara, Masaki and Ota, Chihiro and Nakamura, Satoshi and Terada, Tsutomu and Kitagawa, Hiroyuki and Morishima, Atsuyuki},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287047},
	url          = {https://doi.org/10.1145/3287047},
	issue_date   = {December 2018},
	abstract     = {Worker recruitment is one of the important problems in crowdsourcing, and many proposals have been presented for placing equipment in physical spaces for recruiting workers. One of the essential challenges of the approach is how to keep people attracted because those who perform tasks at first gradually lose interest and do not access the equipment. This study uses a different approach to the worker recruitment problem. In our approach, we dive into people's personal spaces by projecting task images on the floor, thereby allowing the passersby to effortlessly access tasks while walking. The problem then changes from how to keep people engaged to how to manage data quality because many passersby unconsciously or intentionally walk through the task screen on the floor without doing the task, which produces unintended results. We explore a machine-learning approach to select only the intended results and manage the data quality. The system assesses the workers' intention from their behavior. We identify the features for classifiers based on our observations of the passersby. We then conduct extensive evaluations with real data. The results show that the features are effective in practice, and the classifiers improve the data quality.},
	articleno    = 169,
	numpages     = 20,
	keywords     = {Crowdsourcing, Long-term practical use, Worker recruitment}
}
@article{10.1145/3287034,
	title        = {Revisitation in Urban Space vs. Online: A Comparison across POIs, Websites, and Smartphone Apps},
	author       = {Cao, Hancheng and Chen, Zhilong and Xu, Fengli and Li, Yong and Kostakos, Vassilis},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287034},
	url          = {https://doi.org/10.1145/3287034},
	issue_date   = {December 2018},
	abstract     = {We present the first large-scale analysis of POI revisitation patterns, which aims to model the periodic behavior in human mobility. We apply the revisitation analysis technique, which has previously been used to understand website revisitation, and smartphone app revisitations. We analyze a 1.5-year-long Foursquare check-in dataset with 266,909 users in 415 cities around the globe, as well as a Chinese social networking dataset on continuous localization of 15,000 users in Beijing. Our analysis identifies four major POI revisitation patterns and four user revisitation patterns of distinct characteristics, and demonstrates the role of POI functions and geographic constraints in shaping these patterns. We compare our results to previous analysis on website and app revisitation, and highlight the similarities and differences between physical and cyber revisitation activities. These point to fundamental characteristics of human behavior.},
	articleno    = 156,
	numpages     = 24,
	keywords     = {POI, urban space, Revisitation, human mobility}
}
@article{10.1145/3264923,
	title        = {FluidMeter: Gauging the Human Daily Fluid Intake Using Smartwatches},
	author       = {Hamatani, Takashi and Elhamshary, Moustafa and Uchiyama, Akira and Higashino, Teruo},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264923},
	url          = {https://doi.org/10.1145/3264923},
	issue_date   = {September 2018},
	abstract     = {Water is the most vital nutrient in the human body accounting for about 60% of the body weight. To maintain optimal health, it is important for humans to consume a sufficient amount of fluids daily. Therefore, tracking the amount of human daily fluid intake has a myriad of health applications like dehydration prevention.In this paper, we present FluidMeter: a ubiquitous and unobtrusive system to track the amount of fluid intake leveraging the inertial sensors embedded in smartwatches. To achieve this, FluidMeter first separates the drinking activities from other human activities (playing, running, eating, etc.). Thereafter, it analyzes the sampled sensors data during the extracted drinking episodes to recognize the sequence of micro-activities (lift the bottle, sip, release the bottle) that constitute the drinking activity. Finally, it applies some machine learning algorithms on some features extracted from sampled sensor data during the sipping period to gauge the amount of fluid intake in the designated drinking episode.FluidMeter is evaluated by collecting more than 260 hours of different human activities by 70 different participants using different smartwatch models. The results show that FluidMeter can recognize the drinking activity and its micro-activities accurately which is comparable to that achieved by the state-of-the-art techniques. Finally, FluidMeter can estimate the overall amount of fluid intake in grams accurately with a estimation error limited to 15%, highlighting its promise as a ubiquitous health service.},
	articleno    = 113,
	numpages     = 25,
	keywords     = {in-the-wild evaluation, Wearable computing, activity recognition, gesture recognition, automatic fluid intake monitoring}
}
@article{10.1145/3264922,
	title        = {A Simple but Quantifiable Approach to Dynamic Price Prediction in Ride-on-Demand Services Leveraging Multi-Source Urban Data},
	author       = {Guo, Suiming and Chen, Chao and Wang, Jingyuan and Liu, Yaxiao and Xu, Ke and Zhang, Daqing and Chiu, Dah Ming},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264922},
	url          = {https://doi.org/10.1145/3264922},
	issue_date   = {September 2018},
	abstract     = {Ride-on-demand (RoD) services such as Uber and Didi are becoming increasingly popular, and in these services dynamic prices play an important role in balancing the supply and demand to benefit both drivers and passengers. However, dynamic prices also create concerns. For passengers, the "unpredictable" prices sometimes prevent them from making quick decisions: one may wonder if it is possible to get a lower price if s/he chooses to wait a while. It is necessary to provide more information to them, and predicting the dynamic prices is a possible solution. For the transportation industry and policy makers, there are also concerns about the relationship between RoD services and their more traditional counterparts such as metro, bus, and taxi: whether they affect each other and how.In this paper we tackle these two concerns by predicting the dynamic prices using multi-source urban data. Price prediction could help passengers understand whether they could get a lower price in neighboring locations or within a short time, thus alleviating their concerns. The prediction is based on urban data from multiple sources, including the RoD service itself, taxi service, public transportation, weather, the map of a city, etc. We train a simple linear regression model with high-dimensional composite features to perform the prediction. By combining simple basic features into composite features, we compensate for the loss of expressiveness in a linear model due to the lack of non-linearity. Additionally, the use of multi-source data and a linear model enables us to quantify and explain the relationship between multiple means of transportation by examining the weights of different features in the model. Our hope is that the study not only serves as an accurate prediction to make passengers more satisfied, but also sheds light on the concern about the relationship between different means of transportation for either the industry or policy makers.},
	articleno    = 112,
	numpages     = 24,
	keywords     = {Prediction, Urban Transportation, Ride-on-demand service, Dynamic pricing}
}
@article{10.1145/3264918,
	title        = {Watching and Safeguarding Your 3D Printer: Online Process Monitoring Against Cyber-Physical Attacks},
	author       = {Gao, Yang and Li, Borui and Wang, Wei and Xu, Wenyao and Zhou, Chi and Jin, Zhanpeng},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264918},
	url          = {https://doi.org/10.1145/3264918},
	issue_date   = {September 2018},
	abstract     = {The increasing adoption of 3D printing in many safety and mission critical applications exposes 3D printers to a variety of cyber attacks that may result in catastrophic consequences if the printing process is compromised. For example, the mechanical properties (e.g., physical strength, thermal resistance, dimensional stability) of 3D printed objects could be significantly affected and degraded if a simple printing setting is maliciously changed. To address this challenge, this study proposes a model-free real-time online process monitoring approach that is capable of detecting and defending against the cyber-physical attacks on the firmwares of 3D printers. Specifically, we explore the potential attacks and consequences of four key printing attributes (including infill path, printing speed, layer thickness, and fan speed) and then formulate the attack models. Based on the intrinsic relation between the printing attributes and the physical observations, our defense model is established by systematically analyzing the multi-faceted, real-time measurement collected from the accelerometer, magnetometer and camera. The Kalman filter and Canny filter are used to map and estimate three aforementioned critical toolpath information that might affect the printing quality. Mel-frequency Cepstrum Coefficients are used to extract features for fan speed estimation. Experimental results show that, for a complex 3D printed design, our method can achieve 4% Hausdorff distance compared with the model dimension for infill path estimate, 6.07% Mean Absolute Percentage Error (MAPE) for speed estimate, 9.57% MAPE for layer thickness estimate, and 96.8% accuracy for fan speed identification. Our study demonstrates that, this new approach can effectively defend against the cyber-physical attacks on 3D printers and 3D printing process.},
	articleno    = 108,
	numpages     = 27,
	keywords     = {3D Printing, Sensor Fusion, Cyber-Physical Security, Online Process Monitoring}
}
@article{10.1145/3264917,
	title        = {People Like Me: Designing for Reflection on Aggregate Cohort Data in Personal Informatics Systems},
	author       = {Feustel, Clayton and Aggarwal, Shyamak and Lee, Bongshin and Wilcox, Lauren},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264917},
	url          = {https://doi.org/10.1145/3264917},
	issue_date   = {September 2018},
	abstract     = {Increases in data complexity in personal informatics systems require new ways of contextualizing personal data to facilitate meaningful reflection. An emerging approach for providing such context includes augmenting one's personal data with the data of others "like them" to help individuals make sense of their data. However, we do not yet understand how an individual's self-reflection process is affected when the data of others is made available. In this paper, we investigate how people reflect on three types of personal data when presented alongside a large set of aggregated data of multiple cohorts. We conducted personal and cohort data reviews using a subset of participants from a mobile-sensing study that collected physical activity, digital social activity, and perceived stress, from 47 students over three weeks. Participants preferred to use characteristics of the data (e.g., maxima, minima) and graphical presentation (e.g., appearance of trends) along with demographic identities (e.g., age, gender) when relating to cohorts. We further characterize how participants incorporated cohort data into their self-reflection process, and conclude with discussion of the implications for personal informatics systems that leverage the data of "people like me" to enable meaningful reflection.},
	articleno    = 107,
	numpages     = 21,
	keywords     = {Personal Informatics, Health and Wellness, Self-Reflection, Qualitative Study, Cohort Data}
}
@article{10.1145/3328917,
	title        = {On-Body Sensing of Cocaine Craving, Euphoria and Drug-Seeking Behavior Using Cardiac and Respiratory Signals},
	author       = {Gullapalli, Bhanu Teja and Natarajan, Annamalai and Angarita, Gustavo A. and Malison, Robert T. and Ganesan, Deepak and Rahman, Tauhidur},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328917},
	url          = {https://doi.org/10.1145/3328917},
	issue_date   = {June 2019},
	abstract     = {Drug addiction is a chronic brain-based disorder that affects a person's behavior and leads to an inability to control drug usage. Ubiquitous physiological sensing technologies to detect illicit drug use have been well studied and understood for different types of drugs. However, we currently lack the ability to continuously and passively measure the user state in ways that might shed light on the complex relationships between cocaine-induced subjective states (e.g., craving and euphoria) and compulsive drug-seeking behavior. More specifically, the applicability of wearable sensors to detect drug-related states is underexplored. In the current work, we take an initial step in the modeling of cocaine craving, euphoria and drug-seeking behavior using electrocardiographic (ECG) and respiratory signals unobtrusively collected from a wearable chest band. Ten experienced cocaine users were studied using a human laboratory paradigm of self-regulated (i.e., "binge") cocaine administration, during which self-reported visual analog scale (VAS) ratings of cocaine-induced subjective effects (i.e., craving and euphoria) and behavioral measures of drug-seeking behavior (i.e., button clicks for drug infusions) are collected. Our results are encouraging and show that self-reported VAS Craving scores are predicted with a normalized root-mean-squared error (NRMSE) of 17.6% and a Pearson correlation coefficient of 0.49. Similarly, for VAS Euphoria prediction, an NRMSE of 16.7% and a Pearson correlation coefficient of 0.73 were achieved. We further analyze the relative importance of different morphology-related ECG and respiratory features for craving and euphoria prediction. Demographic factor analysis reveals how one single factor (i.e., average dollar ($) per cocaine use) can help to further boost the performance of our craving and euphoria models. Lastly, we model drug-seeking behavior using cardiac and respiratory signals. Specifically, we demonstrate that the latter signals can predict participant button clicks with an F1 score of 0.80 and estimate different levels of click density with a correlation coefficient of 0.85 and an NRMSE of 17.9%.},
	articleno    = 46,
	numpages     = 31,
	keywords     = {On-body sensing, Euphoria, Drug-seeking behavior, Cardiac, and Respiratory signal, Craving}
}
@article{10.1145/3328915,
	title        = {Understanding Motivators, Constraints, and Practices of Sharing Internet of Things},
	author       = {Garg, Radhika and Moreno, Christopher},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328915},
	url          = {https://doi.org/10.1145/3328915},
	issue_date   = {June 2019},
	abstract     = {Smart devices such as mobile phones, tablets, and smart watches are designed under the assumption that they will be used by a single user. In contrast, many other devices, such as smart thermostats and smart speakers, are inherently sharable. This paper presents results from a diary study that we conducted with 20 participants to gain a nuanced understanding of the purposes, motivators, and constraints involved in the sharing of smart devices, which are cumulatively referred to as the Internet of Things. We also report on users' practices of coordinating their shared use with sharees/co-users, the impact of not understanding a smart device's behavior and the context of shared use, the differences between sharing personal and inherently sharable devices in terms of content that is available and accessible, trust between sharees, and measures taken to ensure accountable use. Finally, we discuss the implications of our findings and provide guidelines for the design of future smart devices.},
	articleno    = 44,
	numpages     = 21,
	keywords     = {Sharing of devices, Diary study, Smart speakers, Smart phones, Activity trackers, Smart watches, Smart thermostat, Internet of Things, Smart devices}
}
@article{10.1145/3328914,
	title        = {Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking},
	author       = {Feng, Xianglong and Swaminathan, Viswanathan and Wei, Sheng},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328914},
	url          = {https://doi.org/10.1145/3328914},
	issue_date   = {June 2019},
	abstract     = {360-degree video streaming has been gaining popularities recently with the rapid growth of adopting mobile head mounted display (HMD) devices in the consumer video market, especially for live broadcasts. The 360-degree video streaming introduces brand new bandwidth and latency challenges in live streaming due to the significantly increased video data. However, most of the existing bandwidth saving approaches based on viewport prediction have only focused on the video-on-demand (VOD) use cases leveraging historical user behavior data, which is not available in live broadcasts. We develop a new viewport prediction scheme for live 360-degree video streaming using video content-based motion tracking and dynamic user interest modeling. To obtain real-time performance, we implement the Gaussian mixture model (GMM) and optical flow algorithms for motion detection and feature tracking. Then, the user's future viewport of interest is generated by leveraging a dynamic user interest model that weighs all the features and motion information abstracted from the live video frames. Furthermore, we develop two enhancement techniques that take into consideration of user feedback for fast error recovery and view updates. Consequently, our predicted viewports are irregular and dynamically adjusted to cover the maximum portions of the actual user viewports and thus ensure a high prediction accuracy. We evaluate our viewport prediction approach using a public user head movement dataset, which contains the data of 48 users watching 6 360-degree videos. The experimental results show that the proposed approach supports sophisticated user head movement patterns and outperforms the existing velocity-based approach in terms of prediction accuracy. In addition, the motion tracking scheme introduces minimum latency overhead to ensure the quality of live streaming experience.},
	articleno    = 43,
	numpages     = 22
}
@article{10.1145/3314418,
	title        = {ShoesLoc: In-Shoe Force Sensor-Based Indoor Walking Path Tracking},
	author       = {Yu, Tuo and Jin, Haiming and Nahrstedt, Klara},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314418},
	url          = {https://doi.org/10.1145/3314418},
	issue_date   = {March 2019},
	abstract     = {Currently, in-shoe force sensors have been widely used for step counting and gait analysis. However, it has not been realized that in-shoe force sensors are also capable of tracking walking paths. In this paper, we present ShoesLoc, an indoor walking path tracking method based on in-shoe force sensors. We show that, based on the force signals from a user's shoes, it is possible to estimate the walking direction change and the stride length of each step with machine learning techniques. We further apply a particle filter to combine this information with the constraint of barriers in floor maps, and thus can determine the walking path and the current position of the user. To solve the problem of the low accuracy caused by cumulative walking direction errors, we improve the particle filter by designing the direction correction algorithm. Moreover, we propose the weight normalization method to handle the impact of handbags and backpacks. Our experimental results show that, after a convergence phase, ShoesLoc achieves the average location error of 0.9-1.3 m. Compared with traditional indoor tracking technologies, ShoesLoc does not require the installation of wireless anchors, and has good robustness to environment changes such as the magnetic interference.},
	articleno    = 31,
	numpages     = 23,
	keywords     = {Indoor tracking, force sensors, walking path tracking, smart shoes}
}
@article{10.1145/3314414,
	title        = {Modeling Spatio-Temporal App Usage for a Large User Population},
	author       = {Wang, Huandong and Li, Yong and Zeng, Sihan and Wang, Gang and Zhang, Pengyu and Hui, Pan and Jin, Depeng},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314414},
	url          = {https://doi.org/10.1145/3314414},
	issue_date   = {March 2019},
	abstract     = {With the wide adoption of mobile devices, it becomes increasingly important to understand how users use mobile apps. Knowing when and where certain apps are used is instrumental for app developers to improve app usability and for Internet service providers (ISPs) to optimize their network services. However, modeling spatio-temporal patterns of app usage has been a challenging problem due to the complicated usage behavior and the very limited personal data. In this paper, we propose a Bayesian mixture model to capture when, where and what apps are used and predict future app usage. To solve the challenge of data sparsity, we apply a hierarchical Dirichlet process to leverage the shared spatio-temporal patterns to accurately model users with insufficient data. We then evaluate our model using a large dataset of app usage traces involving 1.7 million users over 3503 apps. Our analysis shows a clear correlation between the user's location and the apps being used. Extensive evaluations show that our model can accurately predict users' future locations and app usage, outperforming the state-of-the-art algorithms by 11.7% and 11.1%, respectively. In addition, our model can be used to synthesize app usage traces that do not leak user privacy while preserving the key data statistical properties.},
	articleno    = 27,
	numpages     = 23,
	keywords     = {app usage, Bayesian mixture model, spatio-temporal pattern}
}
@article{10.1145/3314401,
	title        = {Beyond Control: Enabling Smart Thermostats for Leakage Detection},
	author       = {Jain, Milan and Gupta, Mridula and Singh, Amarjeet and Chandan, Vikas},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314401},
	url          = {https://doi.org/10.1145/3314401},
	issue_date   = {March 2019},
	abstract     = {Smart thermostats, with multiple sensory abilities, are becoming pervasive and ubiquitous, in both residential and commercial buildings. By analyzing occupants' behavior, adjusting set temperature automatically, and adapting to temporal and spatial changes in the atmosphere, smart thermostats can maximize both - energy savings and user comfort. In this paper, we study smart thermostats for refrigerant leakage detection. Retail outlets, such as milk-booths and quick service restaurants set up cold-rooms to store perishable items. In each room, a refrigeration unit (akin to air-conditioners) is used to maintain a suitable temperature for the stored products. Often, refrigerant leaks through the coils (or valves) of the refrigeration unit which slowly diminishes the cooling capacity of the refrigeration unit while allowing it to be functional. Such leaks waste significant energy, risk occupants' health, and impact the quality of stored perishable products. While store managers usually fail to sense the early symptoms of such leaks, current techniques to report refrigerant leakage are often not scalable. We propose Greina - to continuously monitor the readily available ambient information from the thermostat and timely report such leaks. We evaluate our approach on 74 outlets of a retail enterprise and results indicate that Greina can report the leakage a week in advance when compared to manual reporting.},
	articleno    = 14,
	numpages     = 21,
	keywords     = {Fault Detection, Ambient Sensing, Smart Thermostat, Refrigeration Unit, Refrigerant Gas Leakage}
}
@article{10.1145/3287080,
	title        = {EIS: A Wearable Device for Epidermal American Sign Language Recognition},
	author       = {Zhu, Zijie and Wang, Xuewei and Kapoor, Aakaash and Zhang, Zhichao and Pan, Tingrui and Yu, Zhou},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287080},
	url          = {https://doi.org/10.1145/3287080},
	issue_date   = {December 2018},
	abstract     = {American Sign Language (ASL) is widely used among hearing impaired individuals in English-speaking countries. Various technologies have been developed to perform ASL recognition, including optical signal sensing, electrical signal sensing, and mechanical signal sensing. However, wearable devices using those methods have bulky and complex sensing modules that lead to long-term discomfort as well as poor accuracy. In this paper, we present an epidermal-iontronic sensing (EIS)-based wearable device that wears on finger joints for 35 fingerspelling ASL recognitions (i.e., 26 alphabets from A to Z and 9 digits from one to nine). Compared to current on-market devices, such design is lighter, comfortable to wear and has better appearance according to user comments. When bending the finger, a physical contact forms between the ionic material and the epidermis of skin, leading to an electric double layer (EDL) established at the interface. Therefore, a significant capacitive change can be achieved with various finger gestures. By using Nafion as the ionic sensing material, we developed a sensing device to provide excellent flexibility and optical transparency. We used machine learning methods, such as neural networks to track and perform ASL recognition using the signals obtained from the designed device. The algorithm achieved a within-user accuracy of 99.6% and a cross-user accuracy of 76.1% when adapted the model to different users. This wearable device is low-cost and has broad potential to be integrated in future application of human-machine interactions (HMI), smart home controls, and nonverbal communications.},
	articleno    = 202,
	numpages     = 22,
	keywords     = {American Sign Language, Wearable Device, Gesture Recognition, Iontronic Capacitive Sensing, Input Techniques, User Input}
}
@article{10.1145/3287070,
	title        = {A Language for Online State Processing of Binary Sensors, Applied to Ambient Assisted Living},
	author       = {Volanschi, Nic and Serpette, Bernard and Carteron, Adrien and Consel, Charles},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287070},
	url          = {https://doi.org/10.1145/3287070},
	issue_date   = {December 2018},
	abstract     = {There is a large variety of binary sensors in use today, and useful context-aware services can be defined using such binary sensors. However, the currently available approaches for programming context-aware services do not conveniently support binary sensors. Indeed, no existing approach simultaneously supports a notion of state, central to binary sensors, offers a complete set of operators to compose states, allows to define reusable abstractions by means of such compositions, and implements efficient online processing of these operators.This paper proposes a new language for event processing specifically targeted to binary sensors. The central contributions of this language are a native notion of state and semi-causal operators for temporal state composition including: Allen's interval relations generalized for handling multiple intervals, and temporal filters for handling delays. Compared to other approaches such as CEP (complex event processing), our language provides less discontinued information, allows less restricted compositions, and supports reusable abstractions. We implemented an interpreter for our language and applied it to successfully rewrite a full set of real Ambient Assisted Living services. The performance of our prototype interpreter is shown to compete well with a commercial CEP engine when expressing the same services.},
	articleno    = 192,
	numpages     = 26,
	keywords     = {Binary sensors, Smart homes, Allen interval algebra, Ambient assisted living}
}
@article{10.1145/3287062,
	title        = {Driving with the Fishes: Towards Calming and Mindful Virtual Reality Experiences for the Car},
	author       = {Paredes, Pablo E. and Balters, Stephanie and Qian, Kyle and Murnane, Elizabeth L. and Ord\'{o}\~{n}ez, Francisco and Ju, Wendy and Landay, James A.},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287062},
	url          = {https://doi.org/10.1145/3287062},
	issue_date   = {December 2018},
	abstract     = {We present the use of in-car virtual reality (VR) as a way to create calm, mindful experiences for passengers and, someday, autonomous vehicle occupants. Specifically, we describe a series of studies aimed at exploring appropriate VR content, understanding the influence of car movement, and determining the length and other parameters of the simulation to avoid physical discomfort. Overall, our quantitative and qualitative insights suggest calm VR applications are well suited to an automotive context. Testing combinations of VR content designed to provide the participant with a static or dynamic experience versus stationary and moving vehicle modes, we find that a simulated experience of diving in the ocean while in a moving car elicited significantly lower levels of autonomic arousal as compared with a static VR plus stationary car condition. No significant motion sickness effects were subjectively reported by participants nor observable in the data, though a crossover interaction effect reveals how incongruence between the movement of the car and movement in VR could affect nausea. We conclude with recommendations for the design of calming and mindful VR experiences in moving vehicles.},
	articleno    = 184,
	numpages     = 21,
	keywords     = {Presence, Wellbeing, Kinesthetic Congruence, Autonomous Vehicles, Commute, Motion Sickness, Virtual Reality, Augmented Virtuality, Cars, Immersion, Stress Management, Mindfulness, Commuting, Calmness}
}
@article{10.1145/3287061,
	title        = {The Internet of What? Understanding Differences in Perceptions and Adoption for the Internet of Things},
	author       = {Page, Xinru and Bahirat, Paritosh and Safi, Muhammad I. and Knijnenburg, Bart P. and Wisniewski, Pamela},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287061},
	url          = {https://doi.org/10.1145/3287061},
	issue_date   = {December 2018},
	abstract     = {This study explores people's perceptions of and attitudes towards Internet of Things (IoT) devices and their resulting (non)adoption behaviors. Based on 38 interviews (19 pairs each consisting of a Millennial and their parent), we found that few had a clear understanding of IoT, even among those who had already adopted it. Rather, they relied on two distinct conceptual models of IoT that shaped their beliefs, concerns, and adoption decisions: Many approached IoT with an "user-centric" technology mentality, viewing IoT devices as tools to be controlled by the end-user, and focusing on their tangible aspects (e.g. breakability). Others drew on an "agentic" technology perspective, where IoT behaviors were device-driven and, at times, negotiated between the user, other people, and/or the IoT devices. Our study revealed that consumer-oriented IoT currently cater towards the agentic view and raise concerns for those coming from a user-centric perspective. We also found that generational differences in attitudes towards IoT were rather explained by these differing perspectives. Instead of following the trend towards greater automation and agentic modes of interaction, we advocate for a hybrid and personalized approach that supports a spectrum of agentic and user-centric perspectives and provide design recommendations to work towards this end.},
	articleno    = 183,
	numpages     = 22,
	keywords     = {Internet of Things, User-Centric, Technology Adoption, Agentic}
}
@article{10.1145/3287057,
	title        = {Just-in-Time but Not Too Much: Determining Treatment Timing in Mobile Health},
	author       = {Liao, Peng and Dempsey, Walter and Sarker, Hillol and Hossain, Syed Monowar and al'Absi, Mustafa and Klasnja, Predrag and Murphy, Susan},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287057},
	url          = {https://doi.org/10.1145/3287057},
	issue_date   = {December 2018},
	abstract     = {There is a growing scientific interest in the use and development of just-in-time adaptive interventions in mobile health. These mobile interventions typically involve treatments, such as reminders, activity suggestions and motivational messages, delivered via notifications on a smartphone or a wearable to help users make healthy decisions in the moment. To be effective in influencing health, the combination of the right treatment and right delivery time is likely critical. A variety of prediction/detection algorithms have been developed with the goal of pinpointing the best delivery times. The best delivery times might be times of greatest risk and/or times at which the user might be most receptive to the treatment notifications. In addition, to avoid over burdening users, there is of ten a constraint on the number of treatments that should be provided per time interval (e.g., day or week). Yet there may be many more times at which the user is predicted or detected to be at risk and/or receptive. The goal then is to spread treatment uniformly across all of these times. In this paper, we introduce a method that spreads the treatment uniformly across the delivery times. This method can also be used to provide data for learning whether the treatments are effective at the delivery times. This work is motivated by our work on two mobile health studies, a smoking cessation study and a physical activity study.},
	articleno    = 179,
	numpages     = 21,
	keywords     = {Just-in-Time Adaptive Intervention, Mobile Health, Budget Constraint, Treatment Timing}
}
@article{10.1145/3287055,
	title        = {PCIAS: Precise and Contactless Measurement of Instantaneous Angular Speed Using a Smartphone},
	author       = {Li, Zeshui and Dai, Haipeng and Wang, Wei and Liu, Alex X. and Chen, Guihai},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287055},
	url          = {https://doi.org/10.1145/3287055},
	issue_date   = {December 2018},
	abstract     = {Measuring Instantaneous Angular Speed (IAS) of rotating objects is ubiquitous in industry and our daily life. Engineers diagnose the operation condition of engines with IAS. Anemometers obtain instantaneous wind speed with the IAS of rotating cups. Traditional IAS measurement systems have their limitations in the aspects of installation, accuracy, and cost. In this paper, we propose PCIAS, a system that uses acoustic signals of a smartphone to measure IAS of rotating objects in a contactless manner. PCIAS covers a pretty large IAS measurement range (the numerical interval of IAS) from 10 Revolutions Per Minute (RPM) to 10000 RPM, which outperforms almost all existing Commercial-Off-The-Shelf (COTS) IAS meters. In PCIAS, we first choose an appropriate measurement range according to applications. We then use the smartphone to collect acoustic signals backscattered or generated by the object. Next, we extract acoustic features of the object to eliminate interferences from the environment. After that, we propose a robust tracking algorithm to estimate IAS by matching cycle time length of acoustic features adaptively. We build two testbeds to evaluate the accuracy and the robustness of our system in different IAS ranges. Our experiments show that PCIAS achieves a relative accuracy of more than 92% in the low IAS range, more than 94% in the middle IAS range, and more than 96% in the high IAS range. Finally, We exhibit two typical cases to demonstrate the practical use of our system.},
	articleno    = 177,
	numpages     = 24,
	keywords     = {Wireless Sensing, Instantaneous Angular Speed, Acoustic Signals}
}
@article{10.1145/3264959,
	title        = {MindID: Person Identification from Brain Waves through Attention-Based Recurrent Neural Network},
	author       = {Zhang, Xiang and Yao, Lina and Kanhere, Salil S. and Liu, Yunhao and Gu, Tao and Chen, Kaixuan},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264959},
	url          = {https://doi.org/10.1145/3264959},
	issue_date   = {September 2018},
	abstract     = {Person identification technology recognizes individuals by exploiting their unique, measurable physiological and behavioral characteristics. However, the state-of-the-art person identification systems have been shown to be vulnerable, e.g., anti-surveillance prosthetic masks can thwart face recognition, contact lenses can trick iris recognition, vocoder can compromise voice identification and fingerprint films can deceive fingerprint sensors. EEG (Electroencephalography)-based identification, which utilizes the user's brainwave signals for identification and offers a more resilient solution, has recently drawn a lot of attention. However, the state-of-the-art systems cannot achieve similar accuracy as the aforementioned methods. We propose MindID, an EEG-based biometric identification approach, with the aim of achieving high accuracy and robust performance. At first, the EEG data patterns are analyzed and the results show that the Delta pattern contains the most distinctive information for user identification. Next, the decomposed Delta signals are fed into an attention-based Encoder-Decoder RNNs (Recurrent Neural Networks) structure which assigns varying attention weights to different EEG channels based on their importance. The discriminative representations learned from the attention-based RNN are used to identify the user through a boosting classifier. The proposed approach is evaluated over 3 datasets (two local and one public). One local dataset (EID-M) is used for performance assessment and the results illustrate that our model achieves an accuracy of 0.982 and significantly outperforms the state-of-the-art and relevant baselines. The second local dataset (EID-S) and a public dataset (EEG-S) are utilized to demonstrate the robustness and adaptability, respectively. The results indicate that the proposed approach has the potential to be widely deployed in practical settings.},
	articleno    = 149,
	numpages     = 23,
	keywords     = {attention mechanism, biometric identification, EEG pattern decomposition, deep learning, EEG}
}
@article{10.1145/3264955,
	title        = {MultiSoft: Soft Sensor Enabling Real-Time Multimodal Sensing with Contact Localization and Deformation Classification},
	author       = {Yoon, Sang Ho and Paredes, Luis and Huo, Ke and Ramani, Karthik},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264955},
	url          = {https://doi.org/10.1145/3264955},
	issue_date   = {September 2018},
	abstract     = {We introduce MultiSoft, a multilayer soft sensor capable of sensing real-time contact localization, classification of deformation types, and estimation of deformation magnitudes. We propose a multimodal sensing pipeline that carries out both inverse problem solving and machine learning tasks. Specifically, we employ an electrical impedance tomography (EIT) for contact localization and a support vector machine (SVM) for classifying deformations and regressing their magnitudes. We propose a deformation-aware system which enables maintaining a persistent single-point contact localization throughout the deformation. By updating a time-varying distribution of conductivity change caused by deformations, a single-point contact localization can be maintained and restored to support interaction using both contact localization and deformations.We devise a multilayer structure to fabricate a highly stretchable and flexible soft sensor with a short sensor settlement after excitations. Through a series of experiments and evaluations, we validate both raw sensor and multimodal sensing performance with the proposed method. We further demonstrate applicability and feasibility of MultiSoft with example applications.},
	articleno    = 145,
	numpages     = 21,
	keywords     = {multimodal sensing, input device, wearables, sensing technique, Soft user interface}
}
@article{10.1145/3264951,
	title        = {Sensing Behavioral Change over Time: Using Within-Person Variability Features from Mobile Sensing to Predict Personality Traits},
	author       = {Wang, Weichen and Harari, Gabriella M. and Wang, Rui and M\"{u}ller, Sandrine R. and Mirjafari, Shayan and Masaba, Kizito and Campbell, Andrew T.},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264951},
	url          = {https://doi.org/10.1145/3264951},
	issue_date   = {September 2018},
	abstract     = {Personality traits describe individual differences in patterns of thinking, feeling, and behaving ("between-person" variability). But individuals also show changes in their own patterns over time ("within-person" variability). Existing approaches to measuring within-person variability typically rely on self-report methods that do not account for fine-grained behavior change patterns (e.g., hour-by-hour). In this paper, we use passive sensing data from mobile phones to examine the extent to which within-person variability in behavioral patterns can predict self-reported personality traits. Data were collected from 646 college students who participated in a self-tracking assignment for 14 days. To measure variability in behavior, we focused on 5 sensed behaviors (ambient audio amplitude, exposure to human voice, physical activity, phone usage, and location data) and computed 4 within-person variability features (simple standard deviation, circadian rhythm, regularity index, and flexible regularity index). We identified a number of significant correlations between the within-person variability features and the self-reported personality traits. Finally, we designed a model to predict the personality traits from the within-person variability features. Our results show that we can predict personality traits with good accuracy. The resulting predictions correlate with self-reported personality traits in the range of r = 0.32, MAE = 0.45 (for Openness in iOS users) to r = 0.69, MAE = 0.55 (for Extraversion in Android users). Our results suggest that within-person variability features from smartphone data has potential for passive personality assessment.},
	articleno    = 141,
	numpages     = 21,
	keywords     = {Personality Traits, Within-Person Variability, Mobile Sensing}
}
@article{10.1145/3264948,
	title        = {Your Apps Give You Away: Distinguishing Mobile Users by Their App Usage Fingerprints},
	author       = {Tu, Zhen and Li, Runtong and Li, Yong and Wang, Gang and Wu, Di and Hui, Pan and Su, Li and Jin, Depeng},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264948},
	url          = {https://doi.org/10.1145/3264948},
	issue_date   = {September 2018},
	abstract     = {Understanding mobile app usage has become instrumental to service providers to optimize their online services. Meanwhile, there is a growing privacy concern that users' app usage may uniquely reveal who they are. In this paper, we seek to understand how likely a user can be uniquely re-identified in the crowd by the apps she uses. We systematically quantify the uniqueness of app usage via large-scale empirical measurements. By collaborating with a major cellular network provider, we obtained a city-scale anonymized dataset on mobile app traffic (1.37 million users, 2000 apps, 9.4 billion network connection records). Through extensive analysis, we show that the set of apps that a user has installed is already highly unique. For users with more than 10 apps, 88% of them can be uniquely re-identified by 4 random apps. The uniqueness level is even higher if we consider when and where the apps are used. We also observe that user attributes (e.g., gender, social activity, and mobility patterns) all have an impact on the uniqueness of app usage. Our work takes the first step towards understanding the unique app usage patterns for a large user population, paving the way for further research to develop privacy-protection techniques and building personalized online services.},
	articleno    = 138,
	numpages     = 23,
	keywords     = {Usage Patterns, User Privacy, Mobile Apps}
}
@article{10.1145/3264946,
	title        = {Cooperative Target Tracking and Signal Propagation Learning Using Mobile Sensors},
	author       = {Tan, Jiajie and Wong, Wangkit and Zhu, Xinyu and Wu, Hang and Chan, S.-H. Gary},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264946},
	url          = {https://doi.org/10.1145/3264946},
	issue_date   = {September 2018},
	abstract     = {Target tracking refers to positioning mobile objects over time. The targets may be hospital patients, park visitors, mall shoppers, warehouse assets, etc. We consider a novel cooperative system to track targets, where a target carries low-cost RF tag which not only beacons its ID, but also receives and rebroadcasts beacons of tags within a certain hop away. Mobile sensors, equipped with localization and communication modules, are used to capture and forward the beacons to a server to track the targets. Such multi-hop approach greatly extends the sensing range of the mobile sensors, or equivalently, the beaconing range of the tags, leading to cost-effective deployment.We propose Mosent, a highly accurate multi-hop system using mobile sensors for target tracking. To account for complex signal propagation in different indoor and outdoor environment, we represent the received signal strength (RSS) matrix overcoming the assumption on propagation model. Given sensor locations, beacons detected by the sensors and RSS matrix, Mosent jointly considers temporal and spatial information to track targets using a modified particle filter. Mosent has an optional, independent and offline module to learn spatial signal propagation in terms of RSS matrix using cooperative mobile sensors equipped with beaconing transceivers. We have implemented Mosent and conducted extensive experiments. Our results show that Mosent achieves 4.37m and 9.46m tracking error in the campus and the shopping mall, respectively, which outperforms other state-of-the-art approaches with significantly lower tracking error (often by more than 30%).},
	articleno    = 136,
	numpages     = 21,
	keywords     = {fingerprint, particle filter, indoor localization, mobile sensor, Target tracking}
}
@article{10.1145/3264938,
	title        = {Managing In-Home Environments through Sensing, Annotating, and Visualizing Air Quality Data},
	author       = {Moore, Jimmy and Goffin, Pascal and Meyer, Miriah and Lundrigan, Philip and Patwari, Neal and Sward, Katherine and Wiese, Jason},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264938},
	url          = {https://doi.org/10.1145/3264938},
	issue_date   = {September 2018},
	abstract     = {Air quality is important, varies across time and space, and is largely invisible. Pioneering past work deploying air quality monitors in residential environments found that study participants improved their awareness of and engagement with air quality. However, these systems fielded a single monitor and did not support user-specified annotations, inhibiting their utility. We developed MAAV -- a system to Measure Air quality, Annotate data streams, and Visualize real-time PM2.5 levels -- to explore how participants engage with an air quality system addressing these challenges. MAAV supports collecting data from multiple air quality monitors, annotating that data through multiple modalities, and sending text message prompts when it detects a PM2.5 spike. MAAV also features an interactive tablet interface for displaying measurement data and annotations. Through six long-term field deployments (20-47 weeks, mean 37.7 weeks), participants found these system features important for understanding the air quality in and around their homes. Participants gained new insights from between-monitor comparisons, reflected on past PM2.5 spikes with the help of their annotations, and adapted their system usage as they familiarized themselves with their air quality data and MAAV. These results yield important insights for designing residential sensing systems that integrate into users' everyday lives.},
	articleno    = 128,
	numpages     = 28,
	keywords     = {user engagement, Air quality, annotation, in-home sensing, longitudinal deployment, personal informatics, thematic analysis}
}
@article{10.1145/3264936,
	title        = {Al-Light: An Alcohol-Sensing Smart Ice Cube},
	author       = {Matsui, Hidenori and Hashizume, Takahiro and Yatani, Koji},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264936},
	url          = {https://doi.org/10.1145/3264936},
	issue_date   = {September 2018},
	abstract     = {Inappropriate alcohol drinking may cause health and social problems. Although controlling the intake of alcohol is effective to solve the problem, it is laborious to track consumption manually. A system that automatically records the amount of alcohol consumption has a potential to improve behavior in drinking activities. Existing devices and systems support drinking activity detection and liquid intake estimation, but our target scenario requires the capability of determining the alcohol concentration of a beverage. We present Al-light, a smart ice cube to detect the alcohol concentration level of a beverage using an optical method. Al-light is the size of 31.9 x 38.6 x 52.6 mm and users can simply put it into a beverage for estimation. It embeds near infrared (1450 nm) and visible LEDs, and measures the magnitude of light absorption. Our device design integrates prior technology in a patent which exploits different light absorption properties between water and ethanol to determine alcohol concentration. Through our revisitation studies, we found that light at the wavelength of 1450 nm has strong distinguishability even with different types of commercially-available beverages. Our quantitative examinations on alcohol concentration estimation revealed that Al-light was able to achieve the estimation accuracy of approximately 2 % v/v with 13 commercially-available beverages. Although our current approach needs a regressor to be trained for a particular ambient light condition or the sensor to be calibrated using measurements with water, it does not require beverage-dependent models unlike prior work. We then discuss four applications our current prototype supports and future research directions.},
	articleno    = 126,
	numpages     = 20,
	keywords     = {Alcohol concentration sensing, smart ice cube, near-infrared spectroscopy}
}
@article{10.1145/3264921,
	title        = {Crowd-AI Camera Sensing in the Real World},
	author       = {Guo, Anhong and Jain, Anuraag and Ghose, Shomiron and Laput, Gierad and Harrison, Chris and Bigham, Jeffrey P.},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264921},
	url          = {https://doi.org/10.1145/3264921},
	issue_date   = {September 2018},
	abstract     = {Smart appliances with built-in cameras, such as the Nest Cam and Amazon Echo Look, are becoming pervasive. They hold the promise of bringing high fidelity, contextually rich sensing into our homes, workplaces and other environments. Despite recent and impressive advances, computer vision systems are still limited in the types of sensing questions they can answer, and more importantly, do not easily generalize across diverse human environments. In response, researchers have investigated hybrid crowd- and AI-powered methods that collect human labels to bootstrap automatic processes. However, deployments have been small and mostly confined to institutional settings, leaving open questions about the scalability and generality of the approach. In this work, we describe our iterative development of Zensors++, a full-stack crowd-AI camera-based sensing system that moves significantly beyond prior work in terms of scale, question diversity, accuracy, latency, and economic feasibility. We deployed Zensors++ in the wild, with real users, over many months and environments, generating 1.6 million answers for nearly 200 questions created by our participants, costing roughly 6/10ths of a cent per answer delivered. We share lessons learned, insights gleaned, and implications for future crowd-AI vision systems.},
	articleno    = 111,
	numpages     = 20,
	keywords     = {computer vision, sensing, machine learning, camera, human computation, deployment, Internet of things, crowdsourcing, Smart environments}
}
@article{10.1145/3264919,
	title        = {FootNotes: Geo-Referenced Audio Annotations for Nonvisual Exploration},
	author       = {Gleason, Cole and Fiannaca, Alexander J. and Kneisel, Melanie and Cutrell, Edward and Morris, Meredith Ringel},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264919},
	url          = {https://doi.org/10.1145/3264919},
	issue_date   = {September 2018},
	abstract     = {The majority of information in the physical environment is conveyed visually, meaning that people with vision impairments often lack access to the shared cultural, historical, and practical features that define a city. How can someone who is blind find out about the sleek skyscrapers that dot a modern city's skyline, historic cannons that have been remade into traffic pillars, or ancient trees that uproot a neighborhood's sidewalks? We present FootNotes, a system that embeds rich textual descriptions of objects and locations in OpenStreetMap, a popular geowiki. Both sighted and blind users can annotate the physical environment with functional, visual, historical, and social descriptions. We report on the experience of ten participants with vision impairments who used a spatialized audio application to interact with these annotations while exploring a city. By sharing rich annotations of physical objects and areas, FootNotes helps people thoroughly explore a new location or serendipitously discover previously unknown features of familiar environments.},
	articleno    = 109,
	numpages     = 24,
	keywords     = {blindness, mixed reality, navigation, assistive technology, Visual impairment, augmented reality}
}
@article{10.1145/3264903,
	title        = {Sprintz: Time Series Compression for the Internet of Things},
	author       = {Blalock, Davis and Madden, Samuel and Guttag, John},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264903},
	url          = {https://doi.org/10.1145/3264903},
	issue_date   = {September 2018},
	abstract     = {Thanks to the rapid proliferation of connected devices, sensor-generated time series constitute a large and growing portion of the world's data. Often, this data is collected from distributed, resource-constrained devices and centralized at one or more servers. A key challenge in this setup is reducing the size of the transmitted data without sacrificing its quality. Lower quality reduces the data's utility, but smaller size enables both reduced network and storage costs at the servers and reduced power consumption in sensing devices. A natural solution is to compress the data at the sensing devices. Unfortunately, existing compression algorithms either violate the memory and latency constraints common for these devices or, as we show experimentally, perform poorly on sensor-generated time series.We introduce a time series compression algorithm that achieves state-of-the-art compression ratios while requiring less than 1KB of memory and adding virtually no latency. This method is suitable not only for low-power devices collecting data, but also for servers storing and querying data; in the latter context, it can decompress at over 3GB/s in a single thread, even faster than many algorithms with much lower compression ratios. A key component of our method is a high-speed forecasting algorithm that can be trained online and significantly outperforms alternatives such as delta coding.Extensive experiments on datasets from many domains show that these results hold not only for sensor data but also across a wide array of other time series.},
	articleno    = 93,
	numpages     = 23,
	keywords     = {Data Compression, Time Series, Embedded Systems}
}
@article{10.1145/3214276,
	title        = {Does the Public Still Look at Public Displays? A Field Observation of Public Displays in the Wild},
	author       = {Parker, Callum and Tomitsch, Martin and Kay, Judy},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214276},
	url          = {https://doi.org/10.1145/3214276},
	issue_date   = {June 2018},
	abstract     = {Public displays are widely used for displaying information in public space, such as shopping centres. They are typically programmed to display advertisements or general information about the space in which they are situated. Due to recent advances in technology, public displays are becoming ubiquitous in space around cities and can potentially enable new interactions with public space. However, despite these advances, research reports that public displays are often found to be: (1) generally irrelevant to the space in which they are situated; and (2) ignored by passers-by. Although much research has focused on tackling these issues, a gap remains regarding knowledge about how public displays in the wild are currently being used at a time when people are increasingly relying on their smartphones as a main source for accessing information and for connecting with others. The study reported in this article aims to address this gap by presenting new insights about the current practices of non-research public displays and their role in a hyperconnected society. To achieve this, we provide results from a field observation study of non-research public displays and contextualise our findings within an analysis of related work. This article makes three main contributions: (1) identifying how user engagement with public displays has changed over the past 10 years; (2) understanding how the pervasiveness of smartphones and other connected devices has modified whether users notice public displays and their interactions with public displays; and (3) outlining design recommendations and opportunities towards making public displays more relevant in a hyperconnected society.},
	articleno    = 73,
	numpages     = 24,
	keywords     = {Field Observation, Public Displays, Design, Hyperconnected society}
}
@article{10.1145/3214261,
	title        = {A Survey of Attention Management Systems in Ubiquitous Computing Environments},
	author       = {Anderson, Christoph and H\"{u}bener, Isabel and Seipp, Ann-Kathrin and Ohly, Sandra and David, Klaus and Pejovic, Veljko},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214261},
	url          = {https://doi.org/10.1145/3214261},
	issue_date   = {June 2018},
	abstract     = {Today's information and communication devices provide always-on connectivity, instant access to an endless repository of information, and represent the most direct point of contact to almost any person in the world. Despite these advantages, devices such as smartphones or personal computers lead to the phenomenon of attention fragmentation, continuously interrupting individuals' activities and tasks with notifications. Attention management systems aim to provide active support in such scenarios, managing interruptions, for example, by postponing notifications to opportune moments for information delivery. In this article, we review attention management system research with a particular focus on ubiquitous computing environments. We first examine cognitive theories of attention and extract guidelines for practical attention management systems. Mathematical models of human attention are at the core of these systems, and in this article, we review sensing and machine learning techniques that make such models possible. We then discuss design challenges towards the implementation of such systems, and finally, we investigate future directions in this area, paving the way for new approaches and systems supporting users in their attention management.},
	articleno    = 58,
	numpages     = 27,
	keywords     = {Cognition, Ubiquitous Computing, Attention Management, Interruption Management}
}
@article{10.1145/3191786,
	title        = {Detecting Urban Anomalies Using Multiple Spatio-Temporal Data Sources},
	author       = {Zhang, Huichu and Zheng, Yu and Yu, Yong},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191786},
	url          = {https://doi.org/10.1145/3191786},
	issue_date   = {March 2018},
	abstract     = {Urban anomalies, such as abnormal movements of crowds and accidents, may result in loss of life or property if not handled properly. It would be of great value for governments if anomalies can be automatically alerted in their early stage. However, detecting anomalies in urban area has two main challenges. First, the criteria to determine an anomaly on different occasions (e.g. rainy days vs. sunny days, or holidays vs. workdays) and in different places (e.g. tourist attractions vs. office areas) are distinctly different, as these occasions and places have their own definitions on normal patterns. Second, urban anomalies often exhibit complex forms (e.g. road closure may cause decrease in taxi flow and increase in bike flow). We need an algorithm that not only models the anomaly degree of individual data source but also the combination of changes in multiple data sources. In this paper, we propose a two-step method to tackle those challenges. In the first step, we use a similarity-based algorithm to estimate an anomaly score for each individual data source in each region and time slot based on the values of historically similar regions. Those scores are fed into the second step, where we propose an algorithm based on one-class Support Vector Machine to capture rare patterns occurred in multiple data sources, nearby regions or time slots, and give a final, integrated anomaly score for each region. Evaluations based on both synthetic and real world datasets show the advantages of our method beyond baseline techniques such as distance-based, probability-based methods.},
	articleno    = 54,
	numpages     = 18,
	keywords     = {urban computing, multiple data sources, spatio-temporal data mining, anomaly detection}
}
@article{10.1145/3191784,
	title        = {Capturing the Shifting Shapes: Enabling Efficient Screen-Camera Communication with a Pattern-Based Dynamic Barcode},
	author       = {Zhan, Tong and Li, Wenzhong and Chen, Xu and Lu, Sanglu},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191784},
	url          = {https://doi.org/10.1145/3191784},
	issue_date   = {March 2018},
	abstract     = {With the increasing availability of LCD displays and phone cameras in today's environment, screen-camera communication using dynamic barcode has emerged as a convenient infrastructure-free form to establish impromptu communication channel among mobile devices. Due to the short wavelengths and narrow beams of visible light, screen-camera communication is highly directional, low-interference and secure, which envisions a wide range of application scenarios. Conventional screen-camera communication systems encode data bits with color in dynamic barcodes, which suffers from the frame mixture problem caused by the rolling shutter effect of CMOS camera in high capturing rate. In this paper, we propose a novel design of dynamic barcode called ShiftCode that encodes data bits with shifting shape patterns, which provide a new way to expand the barcode capacity for screen-camera communications. ShiftCode adopts a pattern-based layout design to embed multiple data bits in a symbol representation. With such layout, it exploits a decoding mechanism to solve the frame mixture problem and achieves high frame capturing rate. It further intruduces a two-level reliability technique for intra-frame error correction and inter-frame redundancy, which reduces the overhead and delay of retransmission. The proposed ShiftCode is implemented on the Android platform, and extensive experiments show that it achieves at least two-fold improvement on goodput compared with the conventional screen-camera communication systems.},
	articleno    = 52,
	numpages     = 25,
	keywords     = {reliability, Screen-camera communication, frame mixture problem, pattern-based dynamic barcode}
}
@article{10.1145/3191773,
	title        = {RF-Kinect: A Wearable RFID-Based Approach Towards 3D Body Movement Tracking},
	author       = {Wang, Chuyu and Liu, Jian and Chen, Yingying and Xie, Lei and Liu, Hong Bo and Lu, Sanclu},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191773},
	url          = {https://doi.org/10.1145/3191773},
	issue_date   = {March 2018},
	abstract     = {The rising popularity of electronic devices with gesture recognition capabilities makes the gesture-based human-computer interaction more attractive. Along this direction, tracking the body movement in 3D space is desirable to further facilitate behavior recognition in various scenarios. Existing solutions attempt to track the body movement based on computer version or wearable sensors, but they are either dependent on the light or incurring high energy consumption. This paper presents RF-Kinect, a training-free system which tracks the body movement in 3D space by analyzing the phase information of wearable RFID tags attached on the limb. Instead of locating each tag independently in 3D space to recover the body postures, RF-Kinect treats each limb as a whole, and estimates the corresponding orientations through extracting two types of phase features, Phase Difference between Tags (PDT) on the same part of a limb and Phase Difference between Antennas (PDA) of the same tag. It then reconstructs the body posture based on the determined orientation of limbs grounded on the human body geometric model, and exploits Kalman filter to smooth the body movement results, which is the temporal sequence of the body postures. The real experiments with 5 volunteers show that RF-Kinect achieves 8.7° angle error for determining the orientation of limbs and 4.4cm relative position error for the position estimation of joints compared with Kinect 2.0 testbed.},
	articleno    = 41,
	numpages     = 28,
	keywords     = {Body movement tracking, RFID}
}
@article{10.1145/3191768,
	title        = {SilentKey: A New Authentication Framework through Ultrasonic-Based Lip Reading},
	author       = {Tan, Jiayao and Wang, Xiaoliang and Nguyen, Cam-Tu and Shi, Yu},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191768},
	url          = {https://doi.org/10.1145/3191768},
	issue_date   = {March 2018},
	abstract     = {This paper presents SilentKey, a new authentication framework to identify mobile device users through ultrasonic-based lip reading. The main idea is to generate ultrasonic signals from a mobile phone and analyze the fine-grained impact of mouth motions on the reflected signal. The new framework is effective since people have unique characteristics when performing mouth motions, which represent not only what people input, but also how they input. SilentKey is robust against attacks since the input cannot be recorded or imitated. We implement a prototype and demonstrate the effectiveness of the system by fifty volunteers. Such a non-intrusive identification mechanism provides a natural user interface which can also be applied by people with speaking or viewing difficulties.},
	articleno    = 36,
	numpages     = 18,
	keywords     = {Ultrasonic Sensing, Mobile Computing, Implicit Authentication}
}
@article{10.1145/3191747,
	title        = {Robust Sensor-Orientation-Independent Feature Selection for Animal Activity Recognition on Collar Tags},
	author       = {Kamminga, Jacob W. and Le, Duc V. and Meijers, Jan Pieter and Bisby, Helena and Meratnia, Nirvana and Havinga, Paul J.M.},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191747},
	url          = {https://doi.org/10.1145/3191747},
	issue_date   = {March 2018},
	abstract     = {Fundamental challenges faced by real-time animal activity recognition include variation in motion data due to changing sensor orientations, numerous features, and energy and processing constraints of animal tags. This paper aims at finding small optimal feature sets that are lightweight and robust to the sensor's orientation. Our approach comprises four main steps. First, 3D feature vectors are selected since they are theoretically independent of orientation. Second, the least interesting features are suppressed to speed up computation and increase robustness against overfitting. Third, the features are further selected through an embedded method, which selects features through simultaneous feature selection and classification. Finally, feature sets are optimized through 10-fold cross-validation. We collected real-world data through multiple sensors around the neck of five goats. The results show that activities can be accurately recognized using only accelerometer data and a few lightweight features. Additionally, we show that the performance is robust to sensor orientation and position. A simple Naive Bayes classifier using only a single feature achieved an accuracy of 94 % with our empirical dataset. Moreover, our optimal feature set yielded an average of 94 % accuracy when applied with six other classifiers. This work supports embedded, real-time, energy-efficient, and robust activity recognition for animals.},
	articleno    = 15,
	numpages     = 27,
	keywords     = {Animal Activity Recognition, Decision Tree, Naive Bayes, Embedded Systems, Sensor Orientation, Machine Learning}
}
@article{10.1145/3191738,
	title        = {Comparing a Single-Touch Whiteboard and a Multi-Touch Tabletop for Collaboration in School Museum Visits},
	author       = {Clayphan, Andrew and Collins, Anthony and Kay, Judy and Slawitschka, Nathan and Horder, Jenny},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191738},
	url          = {https://doi.org/10.1145/3191738},
	issue_date   = {March 2018},
	abstract     = {This paper explores two important classes of large screen displays, single-touch whiteboards and multi-touch tabletops, for the context of collaborative learning by school groups at a museum. To do this, we designed MuseWork, as a worksheet activity, with two phases: first, students explore the museum, individually or in pairs, guided by our tablet worksheet app; then, in groups, they collaborate to create a poster at a large-screen display, using our device-customised MuseWork interfaces. Our goal was to gain insights about the implications for engagement and collaboration when groups use these devices; single-touch whiteboards are important as they are widely available in classrooms and multi-touch tabletops are an emerging technology. Our research questions asked: 1) whether MuseWork enabled groups to complete the collaborative task at both devices and 2) how the whiteboard and tabletop each affect key aspects of collaboration. We report a between-subjects study of 67 students, aged 10--14 years, from 2 schools, in 12 groups. Our results, based on qualitative and quantitative data, indicate the MuseWork interface for each device proved effective, with groups completing the activity and satisfied with the result and the experience (RQ1). Comparisons of groups using each device (RQ2) give new insights in terms of the products of the collaborative activity, and the strategies groups spontaneously developed for group co-ordination and device use. Our contributions are insights from the first in-the-field study of children collaborating at single-touch interactive whiteboards and multi-touch tabletops.},
	articleno    = 6,
	numpages     = 23,
	keywords     = {Small Group Collaboration, Museums, Tablets, Interactive Tabletops and Whiteboards}
}
@article{10.1145/3161413,
	title        = {Smartphone App Usage Prediction Using Points of Interest},
	author       = {Yu, Donghan and Li, Yong and Xu, Fengli and Zhang, Pengyu and Kostakos, Vassilis},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161413},
	url          = {https://doi.org/10.1145/3161413},
	issue_date   = {December 2017},
	abstract     = {In this paper we present the first population-level, city-scale analysis of application usage on smartphones. Using deep packet inspection at the network operator level, we obtained a geo-tagged dataset with more than 6 million unique devices that launched more than 10,000 unique applications across the city of Shanghai over one week. We develop a technique that leverages transfer learning to predict which applications are most popular and estimate the whole usage distribution based on the Point of Interest (POI) information of that particular location. We demonstrate that our technique has an 83.0% hitrate in successfully identifying the top five popular applications, and a 0.15 RMSE when estimating usage with just 10% sampled sparse data. It outperforms by about 25.7% over the existing state-of-the-art approaches. Our findings pave the way for predicting which apps are relevant to a user given their current location, and which applications are popular where. The implications of our findings are broad: it enables a range of systems to benefit from such timely predictions, including operating systems, network operators, appstores, advertisers, and service providers.},
	articleno    = 174,
	numpages     = 21,
	keywords     = {points of interest, Smartphone applications, behaviour modeling, usage}
}
@article{10.1145/3161410,
	title        = {Continuous Authentication Using Eye Movement Response of Implicit Visual Stimuli},
	author       = {Zhang, Yongtuo and Hu, Wen and Xu, Weitao and Chou, Chun Tung and Hu, Jiankun},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161410},
	url          = {https://doi.org/10.1145/3161410},
	issue_date   = {December 2017},
	abstract     = {Smart head-worn or head-mounted devices, including smart glasses and Virtual Reality (VR) headsets, are gaining popularity. Online shopping and in-app purchase from such headsets are presenting new e-commerce opportunities to the app developers. For convenience, users of these headsets may store account login, bank account and credit card details in order to perform quick in-app purchases. If the device is unattended, then an attacker, which can include insiders, can make use of the stored account and banking details to perform their own in-app purchases at the expense of the legitimate owner. To better protect the legitimate users of VR headsets (or head mounted displays in general) from such threats, in this paper, we propose to use eye movement to continuously authenticate the current wearer of the VR headset. We built a prototype device which allows us to apply visual stimuli to the wearer and to video the eye movements of the wearer at the same time. We use implicit visual stimuli (the contents of existing apps) which evoke eye movements from the headset wearer but without distracting them from their normal activities. This is so that we can continuously authenticate the wearer without them being aware of the authentication running in the background. We evaluated our proposed system experimentally with 30 subjects. Our results showed that the achievable authentication accuracy for implicit visual stimuli is comparable to that of using explicit visual stimuli. We also tested the time stability of our proposed method by collecting eye movement data on two different days that are two weeks apart. Our authentication method achieved an Equal Error Rate of 6.9% (resp. 9.7%) if data collected from the same day (resp. two weeks apart) were used for testing. In addition, we considered active impersonation attacks where attackers trying to imitate legitimate users' eye movements. We found that for a simple (resp. complex) eye tracking scene, a successful attack could be realised after on average 5.67 (13.50) attempts and our proposed authentication algorithm gave a false acceptance rate of 14.17% (3.61%). These results show that active impersonating attacks can be prevented using complex scenes and an appropriate limit on the number of authentication attempts. Lastly, we carried out a survey to study the user acceptability to our proposed implicit stimuli. We found that on a 5-point Likert scale, at least 60% of the respondents either agreed or strongly agreed that our proposed implicit stimuli were non-intrusive.},
	articleno    = 177,
	numpages     = 22,
	keywords     = {biometrics, Eye movement, continuous authentication, account takeover, insider threat}
}
@article{10.1145/3161193,
	title        = {Urban Impulses: Evoked Responses From Local Event Stimuli},
	author       = {Krumm, John},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161193},
	url          = {https://doi.org/10.1145/3161193},
	issue_date   = {December 2017},
	abstract     = {In modeling human behavior, we expect people to make noticeable reactions to the events they witness. For people at a scheduled event like a concert or sports game, we can measure reactions by looking at geotagged social media posts. We work from a database of known events from a commercial ticket broker and a database of geotagged tweets to show how we can derive impulse response functions of tweet counts as event responses. Tweet counts typically rise in anticipation of the event and gradually fall after the event starts. We draw an analogy between evoked responses in functional magnetic resonance imaging (fMRI) from mental stimuli and social media responses from local event stimuli. Our analysis of event and tweet data shows that our derived impulse responses are statistically significant and that we can use the functions to accurately predict reactions to some event types. We give examples of impulse response functions derived from repeated events at different venues.},
	articleno    = 148,
	numpages     = 18,
	keywords     = {Urban computing, Evoked responses, Experimentation, Algorithms, local events, Twitter, Economics}
}
@article{10.1145/3161192,
	title        = {Context Recognition In-the-Wild: Unified Model for Multi-Modal Sensors and Multi-Label Classification},
	author       = {Vaizman, Yonatan and Weibel, Nadir and Lanckriet, Gert},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161192},
	url          = {https://doi.org/10.1145/3161192},
	issue_date   = {December 2017},
	abstract     = {Automatic recognition of behavioral context (location, activities, body-posture etc.) can serve health monitoring, aging care, and many other domains. Recognizing context in-the-wild is challenging because of great variability in behavioral patterns, and it requires a complex mapping from sensor features to predicted labels. Data collected in-the-wild may be unbalanced and incomplete, with cases of missing labels or missing sensors. We propose using the multiple layer perceptron (MLP) as a multi-task model for context recognition. Based on features from multi-modal sensors, the model simultaneously predicts many diverse context labels. We analyze the advantages of the model's hidden layers, which are shared among all sensors and all labels, and provide insight to the behavioral patterns that these hidden layers may capture. We demonstrate how recognition of new labels can be improved when utilizing a model that was trained for an initial set of labels, and show how to train the model to withstand missing sensors. We evaluate context recognition on the previously published ExtraSensory Dataset, which was collected in-the-wild. Compared to previously suggested models, the MLP improves recognition, even with fewer parameters than a linear model. The ability to train a good model using data that has incomplete, unbalanced labeling and missing sensors encourages further research with uncontrolled, in-the-wild behavior.},
	articleno    = 168,
	numpages     = 22,
	keywords     = {Multi-modal sensing, Behavioral context recognition, Multi-label classification}
}
@article{10.1145/3161162,
	title        = {SynchroWatch: One-Handed Synchronous Smartwatch Gestures Using Correlation and Magnetic Sensing},
	author       = {Reyes, Gabriel and Wu, Jason and Juneja, Nikita and Goldshtein, Maxim and Edwards, W. Keith and Abowd, Gregory D. and Starner, Thad},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161162},
	url          = {https://doi.org/10.1145/3161162},
	issue_date   = {December 2017},
	abstract     = {SynchroWatch is a one-handed interaction technique for smartwatches that uses rhythmic correlation between a user's thumb movement and on-screen blinking controls. Our technique uses magnetic sensing to track the synchronous extension and reposition of the thumb, augmented with a passive magnetic ring. The system measures the relative changes in the magnetic field induced by the required thumb movement and uses a time-shifted correlation approach with a reference waveform for detection of synchrony. We evaluated the technique during three distraction tasks with varying degrees of hand and finger movement: active walking, browsing on a computer, and relaxing while watching online videos. Our initial offline results suggest that intentional synchronous gestures can be distinguished from other movement. A second evaluation using a live implementation of the system running on a smartwatch suggests that this technique is viable for gestures used to respond to notifications or issue commands. Finally, we present three demonstration applications that highlight the technique running in real-time on the smartwatch.},
	articleno    = 158,
	numpages     = 26,
	keywords     = {smartwatches, interaction techniques, synchronous gestures, magnetic sensing, wearable computing, one-handed input}
}
@article{10.1145/3161159,
	title        = {RADAR: Road Obstacle Identification for Disaster Response Leveraging Cross-Domain Urban Data},
	author       = {Chen, Longbiao and Fan, Xiaoliang and Wang, Leye and Zhang, Daqing and Yu, Zhiyong and Li, Jonathan and Nguyen, Thi-Mai-Trang and Pan, Gang and Wang, Cheng},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161159},
	url          = {https://doi.org/10.1145/3161159},
	issue_date   = {December 2017},
	abstract     = {Typhoons and hurricanes cause extensive damage to coast cities annually, demanding urban authorities to take effective actions in disaster response to reduce losses. One of the first priority in disaster response is to identify and clear road obstacles, such as fallen trees and ponding water, and restore road transportation in a timely manner for supply and rescue. Traditionally, identifying road obstacles is done by manual investigation and reporting, which is labor intensive and time consuming, hindering the timely restoration of transportation. In this work, we propose RADAR, a low-cost and real-time approach to identify road obstacles leveraging large-scale vehicle trajectory data and heterogeneous road environment sensing data. First, based on the observation that road obstacles may cause abnormal slow motion behaviors of vehicles in the surrounding road segments, we propose a cluster direct robust matrix factorization (CDRMF) approach to detect road obstacles by identifying the collective anomalies of slow motion behaviors from vehicle trajectory data. Then, we classify the detected road obstacles leveraging the correlated spatial and temporal features extracted from various road environment data, including satellite images and meteorological records. To address the challenges of heterogeneous features and sparse labels, we propose a semi-supervised approach combining co-training and active learning (CORAL). Real experiments on Xiamen City show that our approach accurately detects and classifies the road obstacles during the 2016 typhoon season with precision and recall both above 90%, and outperforms the state-of-the-art baselines.},
	articleno    = 130,
	numpages     = 23,
	keywords     = {disaster response, Mobility data mining, cross-domain data, urban computing}
}
@article{10.1145/3132031,
	title        = {Devices and Data and Agents, Oh My: How Smart Home Abstractions Prime End-User Mental Models},
	author       = {Clark, Meghan and Newman, Mark W. and Dutta, Prabal},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3132031},
	url          = {https://doi.org/10.1145/3132031},
	issue_date   = {September 2017},
	abstract     = {With the advent of DIY smart homes and the Internet of Things comes the emergence of user interfaces for domestic human-building interaction. However, the design trade-offs between the different representations of a smart home’s capabilities are still not well-understood. In this work, we examine how four different smart home abstractions affect end users’ mental models of a hypothetical system. We develop four questionnaires, each of which describes the same hypothetical smart home using a different abstraction, and then we collect responses depicting desired smart home applications from over 1,500 Mechanical Turk workers. We find that the choice of abstraction strongly primes end users’ responses. In particular, the purely device-oriented abstraction results in the most limited scenarios, suggesting that if we want users to associate smart home technologies with valuable high-level applications we should shift the UI paradigm for the Internet of Things from device-oriented control to other abstractions that inspire a greater diversity of interactions.},
	articleno    = 44,
	numpages     = 26,
	keywords     = {priming, interface design, smart homes, mental models, IoT, natural language processing}
}
@article{10.1145/3132029,
	title        = {Does This App Really Need My Location? Context-Aware Privacy Management for Smartphones},
	author       = {Chitkara, Saksham and Gothoskar, Nishad and Harish, Suhas and Hong, Jason I. and Agarwal, Yuvraj},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3132029},
	url          = {https://doi.org/10.1145/3132029},
	issue_date   = {September 2017},
	abstract     = {The enormous popularity of smartphones, their rich sensing capabilities, and the data they have about their users have lead to millions of apps being developed and used. However, these capabilities have also led to numerous privacy concerns. Platform manufacturers, as well as researchers, have proposed numerous ways of mitigating these concerns, primarily by providing fine-grained visibility and privacy controls to the user on a per-app basis. In this paper, we show that this per-app permission approach is suboptimal for many apps, primarily because most data accesses occur due to a small set of popular third-party libraries which are common across multiple apps. To address this problem, we present the design and implementation of ProtectMyPrivacy (PmP) for Android, which can detect critical contextual information at runtime when privacy-sensitive data accesses occur. In particular, PmP infers the purpose of the data access, i.e. whether the data access is by a third-party library or by the app itself for its functionality. Based on crowdsourced data, we show that there are in fact a set of 30 libraries which are responsible for more than half of private data accesses. Controlling sensitive data accessed by these libraries can therefore be an effective mechanism for managing their privacy. We deployed our PmP app to 1,321 real users, showing that the number of privacy decisions that users have to make are significantly reduced. In addition, we show that our users are better protected against data leakage when using our new library-based blocking mechanism as compared to the traditional app-level permission mechanisms.},
	articleno    = 42,
	numpages     = 22,
	keywords     = {Purpose, Permissions Model, Third Party Libraries, Privacy, Android}
}
@article{10.1145/3132024,
	title        = {Glabella: Continuously Sensing Blood Pressure Behavior Using an Unobtrusive Wearable Device},
	author       = {Holz, Christian and Wang, Edward J.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3132024},
	url          = {https://doi.org/10.1145/3132024},
	issue_date   = {September 2017},
	abstract     = {We propose Glabella, a wearable device that continuously and unobtrusively monitors heart rates at three sites on the wearer’s head. Our glasses prototype incorporates optical sensors, processing, storage, and communication components, all integrated into the frame to passively collect physiological data about the user without the need for any interaction. Glabella continuously records the stream of reflected light intensities from blood flow as well as inertial measurements of the user’s head. From the temporal differences in pulse events across the sensors, our prototype derives the wearer’s pulse transit time on a beat-to-beat basis.Numerous efforts have found a significant correlation between a person’s pulse transit time and their systolic blood pressure. In this paper, we leverage this insight to continuously observe pulse transit time as a proxy for the behavior of systolic blood pressure levels—at a substantially higher level of convenience and higher rate than traditional blood pressure monitors, such as cuff-based oscillometric devices. This enables our prototype to model the beat-to-beat fluctuations in the user’s blood pressure over the course of the day and record its short-term responses to events, such as postural changes, exercise, eating and drinking, resting, medication intake, location changes, or time of day.During our in-the-wild evaluation, four participants wore a custom-fit Glabella prototype device over the course of five days throughout their daytime job and regular activities. Participants additionally measured their radial blood pressure three times an hour using a commercial oscillometric cuff. Our analysis shows a high correlation between the pulse transit times computed on our devices with participants’ heart rates (mean r = 0.92, SE = 0.03, angular artery) and systolic blood pressure values measured using the oscillometric cuffs (mean r = 0.79, SE = 0.15, angular-superficial temporal artery, considering participants’ self-administered cuff-based measurements as ground truth). Our results indicate that Glabella has the potential to serve as a socially-acceptable capture device, requiring no user input or behavior changes during regular activities, and whose continuous measurements may prove informative to physicians as well as users’ self-tracking activities.},
	articleno    = 58,
	numpages     = 23,
	keywords     = {pulse transit time, in-the-wild user study, heart rate monitoring, cuffless sensing, continuous tracking, convenience, blood pressure monitoring, Physiological sensing, wearable device, unobtrusive wearable}
}
@article{10.1145/3130964,
	title        = {OpenKeychain: An Architecture for Cryptography with Smart Cards and NFC Rings on Android},
	author       = {Sch\"{u}rmann, Dominik and Dechand, Sergej and Wolf, Lars},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130964},
	url          = {https://doi.org/10.1145/3130964},
	issue_date   = {September 2017},
	abstract     = {While many Android apps provide end-to-end encryption, the cryptographic keys are still stored on the device itself and can thus be stolen by exploiting vulnerabilities. External cryptographic hardware solves this issue, but is currently only used for two-factor authentication and not for communication encryption.In this paper, we design, implement, and evaluate an architecture for NFC-based cryptography on Android. Our high-level API provides cryptographic operations without requiring knowledge of public-key cryptography. By developing OpenKeychain, we were able to roll out this architecture for more than 100,000 users. It provides encryption for emails, messaging, and a password manager. We provide a threat model, NFC performance measurements, and discuss their impact on our architecture design. As an alternative form factor to smart cards, we created the prototype of an NFC signet ring. To evaluate the UI components and form factors, a lab study with 40 participants at a large company has been conducted. We measured the time required by the participants to set up the system and reply to encrypted emails. These measurements and a subsequent interview indicate that our NFC-based solutions are more user friendly in comparison to traditional password-protected keys.},
	articleno    = 99,
	numpages     = 24,
	keywords     = {ring, smart card, NFC, near-field communication}
}
@article{10.1145/3130960,
	title        = {Inferring Mood Instability on Social Media by Leveraging Ecological Momentary Assessments},
	author       = {Saha, Koustuv and Chan, Larry and De Barbaro, Kaya and Abowd, Gregory D. and De Choudhury, Munmun},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130960},
	url          = {https://doi.org/10.1145/3130960},
	issue_date   = {September 2017},
	abstract     = {Active and passive sensing technologies are providing powerful mechanisms to track, model, and understand a range of health behaviors and well-being states. Despite yielding rich, dense and high fidelity data, current sensing technologies often require highly engineered study designs and persistent participant compliance, making them difficult to scale to large populations and to data acquisition tasks spanning extended time periods. This paper situates social media as a new passive, unobtrusive sensing technology. We propose a semi-supervised machine learning framework to combine small samples of data gathered through active sensing, with large-scale social media data to infer mood instability (MI) in individuals. Starting from a theoretically-grounded measure of MI obtained from mobile ecological momentary assessments (EMAs), we show that our model is able to infer MI in a large population of Twitter users with 96% accuracy and F-1 score. Additionally, we show that, our model predicts self-identifying Twitter users with bipolar and borderline personality disorder to exhibit twice the likelihood of high MI, compared to that in a suitable control. We discuss the implications and the potential for integrating complementary sensing capabilities to address complex research challenges in precision medicine.},
	articleno    = 95,
	numpages     = 27,
	keywords     = {Affective Instability, Ecological Momentary Assessments, Social media, Twitter, Mental Well-Being, EMA, Mood Instability, Affect, Health, Mood}
}
@article{10.1145/3130959,
	title        = {Label Propagation: An Unsupervised Similarity Based Method for Integrating New Sensors in Activity Recognition Systems},
	author       = {Rey, Vitor F. and Lukowicz, Paul},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130959},
	url          = {https://doi.org/10.1145/3130959},
	issue_date   = {September 2017},
	abstract     = {Current activity recognition systems mostly work with static, pre-trained sensor configuration. As a consequence they are not able to leverage new sensors appearing in their environment (e.g. the user buying a new wearable devices). In this work we present a method inspired by semi-supervised graph methods that can add new sensors to an existing system in an unsupervised manner. We have evaluated our method in two well known activity recognition datasets and found that it can take advantage of the information provided by new unknown sensor sources, improving the recognition performance in most cases.},
	articleno    = 94,
	numpages     = 24
}
@article{10.1145/3130957,
	title        = {Microinteraction Ecological Momentary Assessment Response Rates: Effect of Microinteractions or the Smartwatch?},
	author       = {Ponnada, Aditya and Haynes, Caitlin and Maniar, Dharam and Manjourides, Justin and Intille, Stephen},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130957},
	url          = {https://doi.org/10.1145/3130957},
	issue_date   = {September 2017},
	abstract     = {Mobile-based ecological-momentary-assessment (EMA) is an in-situ measurement methodology where an electronic device prompts a person to answer questions of research interest. EMA has a key limitation: interruption burden. Microinteraction-EMA(µEMA) may reduce burden without sacrificing high temporal density of measurement. In µEMA, all EMA prompts can be answered with ‘at a glance' microinteractions. In a prior 4-week pilot study comparing standard EMA delivered on a phone (phone-EMA) vs. µEMA delivered on a smartwatch (watch-µEMA), watch-µEMA demonstrated higher response rates and lower perceived burden than phone-EMA, even when the watch-µEMA interruption rate was 8 times more than phone-EMA. A new 4-week dataset was gathered on smartwatch-based EMA (i.e., watch-EMA with 6 back-to-back, multiple-choice questions on a watch) to compare whether the high response rates of watch-µEMA previously observed were a result of using microinteractions, or due to the novelty and accessibility of the smartwatch. No statistically significant differences in compliance, completion, and first-prompt response rates were observed between phone-EMA and watch-EMA. However, watch-µEMA response rates were significantly higher than watch-EMA. This pilot suggests that (1) the high compliance and low burden previously observed in watch-µEMA is likely due to the microinteraction question technique, not simply the use of the watch versus the phone, and that (2) compliance with traditional EMA (with long surveys) may not improve simply by moving survey delivery from the phone to a smartwatch.},
	articleno    = 92,
	numpages     = 16,
	keywords     = {Microinteractions, Wearable computing, Empirical Studies, Experience sampling, Ecological momentary assessment, Smartwatch, Compliance}
}
@article{10.1145/3130942,
	title        = {Your Search Path Tells Others Where to Park: Towards Fine-Grained Parking Availability Crowdsourcing Using Parking Decision Models},
	author       = {Liu, Ruilin and Yang, Yu and Kwak, Daehan and Zhang, Desheng and Iftode, Liviu and Nath, Badri},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130942},
	url          = {https://doi.org/10.1145/3130942},
	issue_date   = {September 2017},
	abstract     = {A main challenge faced by the state-of-the-art parking sensing systems is to infer the state of the spots not covered by participants’ parking/unparking events (called background availability) when the system penetration rate is limited. In this paper, we tackle this problem by exploring an empirical phenomenon that ignoring a spot along a driver’s parking search trajectory is likely due to the unavailability. However, complications caused by drivers’ preferences, e.g. ignoring the spots too far from the driver’s destination, have to be addressed based on human parking decisions. We build a model based on a dataset of more than 55,000 real parking decisions to predict the probability that a driver would take a spot, assuming the spot is available. Then, we present a crowdsourcing system, called ParkScan, which leverages the learned parking decision model in collaboration with the hidden Markov model to estimate background parking spot availability. We evaluated ParkScan with real-world data from both off-street scenarios (i.e., two public parking lots) and an on-street parking scenario (i.e., 35 urban blocks in Seattle). Both of the experiments showed that with a 5% penetration rate, ParkScan reduces over 12.9% of availability estimation errors for all the spots during parking peak hours, compared to the baseline using only the historical data. Also, even with a single participant driver, ParkScan cuts off at least 15% of the estimation errors for the spots along the driver’s parking search trajectory.},
	articleno    = 78,
	numpages     = 27,
	keywords     = {Crowdsourcing, Parking, Mobile Sensing, Human Decision Modeling}
}
@article{10.1145/3130941,
	title        = {PrivacyStreams: Enabling Transparency in Personal Data Processing for Mobile Apps},
	author       = {Li, Yuanchun and Chen, Fanglin and Li, Toby Jia-Jun and Guo, Yao and Huang, Gang and Fredrikson, Matthew and Agarwal, Yuvraj and Hong, Jason I.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130941},
	url          = {https://doi.org/10.1145/3130941},
	issue_date   = {September 2017},
	abstract     = {Smartphone app developers often access and use privacy-sensitive data to create apps with rich and meaningful interactions. However, it can be challenging for auditors and end-users to know what granularity of data is being used and how, thereby hindering assessment of potential risks. Furthermore, developers lack easy ways of offering transparency to users regarding how personal data is processed, even if their intentions are to make their apps more privacy friendly. To address these challenges, we introduce PrivacyStreams, a functional programming model for accessing and processing personal data as a stream. PrivacyStreams is designed to make it easy for developers to make use of personal data while simultaneously making it easier to analyze how that personal data is processed and what granularity of data is actually used. We present the design and implementation of PrivacyStreams, as well as several user studies and experiments to demonstrate its usability, utility, and support for privacy.},
	articleno    = 76,
	numpages     = 26,
	keywords     = {Personal data, transparency, functional programming, data granularity, mobile apps}
}
@article{10.1145/3130932,
	title        = {Technology Supported Behavior Restriction for Mitigating Self-Interruptions in Multi-Device Environments},
	author       = {Kim, Jaejeung and Cho, Chiwoo and Lee, Uichin},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130932},
	url          = {https://doi.org/10.1145/3130932},
	issue_date   = {September 2017},
	abstract     = {The interruptions people experience may be initiated from digital devices but also from oneself, an action which is termed “self-interruption.” Prior work mostly focused on understanding work-related self-interruptions and designing tools for mitigating them in work contexts. However, self-interruption to off-tasks (e.g., viewing social networking sites, and playing mobile games) has received little attention in the HCI community thus far. We conducted a formative study about self-interruptions to off-tasks and coping strategies in multi-device working environments. Off-task usage was considered a serious roadblock to productivity, and yet, the habitual usage and negative triggers made it challenging to manage off-task usage. To mitigate these concerns, we developed “PomodoLock,” a self-interruption management tool that allows users voluntarily to set a timer for a fixed period, during which it selectively blocks interruption sources across multiple devices. To understand the effect of restricting access to self-interruptive sources such as applications and websites, we conducted a three-week field trial (n=40) where participants were asked to identify disrupting apps and sites to be blocked, but the multi-device blocking feature was only provided to the experimental group. Our study results showed the perceived coercion and the stress of the experimental group were lower despite its behavioral restriction with multi-device blocking. Qualitative study results from interviews and surveys confirm that multi-device blocking significantly reduced participants’ mental effort for managing self-interruptions, thereby leading to a reduction in the overall stress level. The findings suggest that when the coerciveness of behavioral restriction is appropriately controlled, coercive design can positively assist users in achieving their goals.},
	articleno    = 64,
	numpages     = 21,
	keywords     = {Self-interruption, Behavior restriction, Technological coercion, Interruption management}
}
@article{10.1145/3130914,
	title        = {Modus Operandi of Crowd Workers: The Invisible Role of Microtask Work Environments},
	author       = {Gadiraju, Ujwal and Checco, Alessandro and Gupta, Neha and Demartini, Gianluca},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130914},
	url          = {https://doi.org/10.1145/3130914},
	issue_date   = {September 2017},
	abstract     = {The ubiquity of the Internet and the widespread proliferation of electronic devices has resulted in flourishing microtask crowdsourcing marketplaces, such as Amazon MTurk. An aspect that has remained largely invisible in microtask crowdsourcing is that of work environments; defined as the hardware and software affordances at the disposal of crowd workers which are used to complete microtasks on crowdsourcing platforms. In this paper, we reveal the significant role of work environments in the shaping of crowd work. First, through a pilot study surveying the good and bad experiences workers had with UI elements in crowd work, we revealed the typical issues workers face. Based on these findings, we then deployed over 100 distinct microtasks on CrowdFlower, addressing workers in India and USA in two identical batches. These tasks emulate the good and bad UI element designs that characterize crowdsourcing microtasks. We recorded hardware specifics such as CPU speed and device type, apart from software specifics including the browsers used to complete tasks, operating systems on the device, and other properties that define the work environments of crowd workers. Our findings indicate that crowd workers are embedded in a variety of work environments which influence the quality of work produced. To confirm and validate our data-driven findings we then carried out semi-structured interviews with a sample of Indian and American crowd workers from this platform. Depending on the design of UI elements in microtasks, we found that some work environments support crowd workers more than others. Based on our overall findings resulting from all the three studies, we introduce ModOp, a tool that helps to design crowdsourcing microtasks that are suitable for diverse crowd work environments. We empirically show that the use of ModOp results in reducing the cognitive load of workers, thereby improving their user experience without affecting the accuracy or task completion time.},
	articleno    = 49,
	numpages     = 29,
	keywords     = {Crowd Workers, Design, Work Environment, Human Factors, Microtasks, Crowdsourcing, Performance, User Interface}
}
@article{10.1145/3130910,
	title        = {Remote Control by Body Movement in Synchrony with Orbiting Widgets: An Evaluation of TraceMatch},
	author       = {Clarke, Christopher and Bellino, Alessio and Esteves, Augusto and Gellersen, Hans},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130910},
	url          = {https://doi.org/10.1145/3130910},
	issue_date   = {September 2017},
	abstract     = {In this work we consider how users can use body movement for remote control with minimal effort and maximum flexibility. TraceMatch is a novel technique where the interface displays available controls as circular widgets with orbiting targets, and where users can trigger a control by mimicking the displayed motion. The technique uses computer vision to detect circular motion as a uniform type of input, but is highly appropriable as users can produce matching motion with any part of their body. We present three studies that investigate input performance with different parts of the body, user preferences, and spontaneous choice of movements for input in realistic application scenarios. The results show that users can provide effective input with their head, hands and while holding objects, that multiple controls can be effectively distinguished by the difference in presented phase and direction of movement, and that users choose and switch modes of input seamlessly.},
	articleno    = 45,
	numpages     = 22,
	keywords     = {Computer vision, Remote control, User input, Path mimicry, Input techniques, User evaluation, Gesture input, Motion matching, Vision-based interfaces, Motion correlation, Movement correlation}
}
@article{10.1145/3090096,
	title        = {Mediated Atmospheres: A Multimodal Mediated Work Environment},
	author       = {Zhao, Nan and Azaria, Asaph and Paradiso, Joseph A.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090096},
	url          = {https://doi.org/10.1145/3090096},
	issue_date   = {June 2017},
	abstract     = {Atmosphere - the sensorial qualities of a space, shaped by the composition of light, sound, objects, people, etc. - has remarkable influence on our experiences and behavior. Manipulating it has been shown to be powerful, affecting cognitive performance, mood and even physiology, our work envisions and implements a smart office prototype, capable of digitally transforming its atmosphere - creating what we call Mediated Atmospheres (MA) - using computationally controlled lighting, video projection and sound. Additionally, we equipped this space with a modular real-time data collection infrastructure, integrating a set of biosignal sensors. Through a user study (N=29) we demonstrate MA's effects on occupants’ ability to focus and to recover from a stressful situation. Our evaluation is based on subjective measurements of perception, as well as objective measurements, extracted from recordings of heart rate variability and facial features. We compare multiple signal processing approaches for quantifying changes in occupant physiological state. Our findings show that MA significantly (p&lt;0.05) affect occupants’ perception as well as physiological response, which encouragingly correlate with occupants’ perception. Our findings is a first step towards personalized control of the ambient atmosphere to support wellbeing and productivity.},
	articleno    = 31,
	numpages     = 23,
	keywords     = {Mediated Atmospheres, Face Tracking, Restoration, Multimodal, Heart Rate Variability, Adaptive Building, Wellbeing, Smart Office, Augmented Reality, Productivity, Perception, Ubiquitous Computing}
}
@article{10.1145/3090094,
	title        = {Gain Without Pain: Accurate WiFi-Based Localization Using Fingerprint Spatial Gradient},
	author       = {Wu, Chenshu and Xu, Jingao and Yang, Zheng and Lane, Nicholas D. and Yin, Zuwei},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090094},
	url          = {https://doi.org/10.1145/3090094},
	issue_date   = {June 2017},
	abstract     = {Among numerous indoor localization systems proposed during the past decades, WiFi fingerprint-based localization has been one of the most attractive solutions, which is known to be free of extra infrastructure and specialized hardware. However, current WiFi fingerprinting suffers from a pivotal problem of RSS fluctuations caused by unpredictable environmental dynamics. The RSS variations lead to severe spatial ambiguity and temporal instability in RSS fingerprinting, both impairing the location accuracy. To overcome such drawbacks, we propose fingerprint spatial gradient (FSG), a more stable and distinctive form than RSS fingerprints, which exploits the spatial relationships among the RSS fingerprints of multiple neighbouring locations. As a spatially relative form, FSG is more resistant to RSS uncertainties. Based on the concept of FSG, we design novel algorithms to construct FSG on top of a general RSS fingerprint database and then propose effective FSG matching methods for location estimation. Unlike previous works, the resulting system, named ViVi, yields performance gain without the pains of introducing extra information or additional service restrictions or assuming impractical RSS models. Extensive experiments in different buildings demonstrate that ViVi achieves great performance, outperforming the best among four comparative start-of-the-art approaches by 29% in mean accuracy and 19% in 95th percentile accuracy and outweighing the worst one by 39% and 24% respectively. We envision FSG as a promising supplement and alternative to existing RSS fingerprinting for future WiFi localization.},
	articleno    = 29,
	numpages     = 19,
	keywords     = {Spatial Gradient, WiFi Fingerprint, Indoor Localization}
}
@article{10.1145/3090081,
	title        = {Towards Calm Displays: Matching Ambient Illumination in Bedrooms},
	author       = {Ku\v{c}era, Jan and Scott, James and Chen, Nicholas and Olivier, Patrick and Hodges, Steve},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090081},
	url          = {https://doi.org/10.1145/3090081},
	issue_date   = {June 2017},
	abstract     = {We present a system for making emissive computer displays (LCDs) look like they are reflective, i.e. not emitting light but instead reflecting ambient light, an effect that we call a “calm display”. We achieve this effect by using a light sensor and a one-time calibration process to drive an algorithm which controls the display's backlight intensity and gamma correction functionality to continually match the brightness and chromaticity of the ambient light. We present an experimental evaluation of our system, showing quantitatively that the color and brightness output by our system is perceptually close to that of a piece of paper under similar lighting conditions. We argue that calm displays can more easily fade into the background, and further that they are more suitable for environments such as bedrooms where glowing displays are often out-of-place. We validate these claims and more generally explore users’ perception of calm displays, through a field study of an LCD display deployed in participants’ bedrooms.},
	articleno    = 16,
	numpages     = 21,
	keywords     = {RGB sensors, ambient displays, negative brightness, picture frames, color perception, displays, calm technology}
}
@article{10.1145/3090078,
	title        = {A Large-Scale, Long-Term Analysis of Mobile Device Usage Characteristics},
	author       = {Hintze, Daniel and Hintze, Philipp and Findling, Rainhard D. and Mayrhofer, Ren\'{e}},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090078},
	url          = {https://doi.org/10.1145/3090078},
	issue_date   = {June 2017},
	abstract     = {Today, mobile devices like smartphones and tablets have become an indispensable part of people's lives, posing many new questions e.g., in terms of interaction methods, but also security. In this paper, we conduct a large scale, long term analysis of mobile device usage characteristics like session length, interaction frequency, and daily usage in locked and unlocked state with respect to location context and diurnal pattern. Based on detailed logs from 29,279 mobile phones and tablets representing a total of 5,811 years of usage time, we identify and analyze 52.2 million usage sessions with some participants providing data for more than four years.Our results show that context has a highly significant effect on both frequency and extent of mobile device usage, with mobile phones being used twice as much at home compared to in the office. Interestingly, devices are unlocked for only 46 % of the interactions. We found that with an average of 60 interactions per day, smartphones are used almost thrice as often as tablet devices (23), while usage sessions on tablets are three times longer, hence are used almost for an equal amount of time throughout the day. We conclude that usage session characteristics differ considerably between tablets and smartphones. These results inform future approaches to mobile interaction as well as security.},
	articleno    = 13,
	numpages     = 21,
	keywords     = {Locked usage, Session length, Usage session, Tablet, Device unlocking, Smartphone, User context, Daily interactions}
}
@article{10.1145/3090052,
	title        = {Every Byte Counts: Selective Prefetching for Mobile Applications},
	author       = {Baumann, Paul and Santini, Silvia},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090052},
	url          = {https://doi.org/10.1145/3090052},
	issue_date   = {June 2017},
	abstract     = {Quick responses to user actions are instrumental to the success of mobile applications. To ensure such responsiveness, applications often prefetch data objects before the user requests them. This way, applications can avoid the need to retrieve data through slow network connections during user interactions. However, prefetches may also harm. They increase launch delays and might cause substantial amounts of data to be downloaded through energy-hungry, cellular connections. In this paper, we propose EBC, a novel algorithm to schedule application prefetches and overcome their drawbacks. EBC computes application usage probabilities and traffic volume estimates to determine when and for which applications prefetches should be triggered. Thereby, it applies different strategies depending on whether a cellular or Wi-Fi connection is available. We evaluate the performance of EBC on two publicly available, large-scale data sets: LiveLab and Device Analyzer. Our results show that EBC can lower launch delays and ensure freshness of application content. At the same time, it reduces the amount of data downloaded through cellular connections. On the Device Analyzer data set, for instance, EBC achieves a 10% reduction in cellular traffic and a 36% better average freshness with respect to its closest competitor.},
	articleno    = 6,
	numpages     = 29
}
@article{10.1145/3053331,
	title        = {Guessing Attacks on User-Generated Gesture Passwords},
	author       = {Liu, Can and Clark, Gradeigh D. and Lindqvist, Janne},
	year         = 2017,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 1,
	doi          = {10.1145/3053331},
	url          = {https://doi.org/10.1145/3053331},
	issue_date   = {March 2017},
	abstract     = {Touchscreens, the dominant input type for mobile phones, require unique authentication solutions. Gesture passwords have been proposed as an alternative ubiquitous authentication technique. Prior security analysis has relied on inconsistent measurements such as mutual information or shoulder surfing attacks.We present the first approach for measuring the security of gestures with guessing attacks that model real-world attacker behavior. Our major contributions are: 1) a comprehensive analysis of the weak subspace for gesture passwords, 2) a method for enumerating the size of the full theoretical gesture password space, 3) a design of a novel guessing attack against user-chosen gestures using a dictionary, and 4) a brute-force attack used for benchmarking the performance of the guessing attack. Our dictionary attack, tested on newly collected user data, achieves a cracking rate of 47.71% after two weeks of computation using 109 guesses. This is a difference of 35.78 percentage points compared to the 11.93% cracking rate of the brute-force attack. In conclusion, users are not taking full advantage of the large theoretical password space and instead choose their gesture passwords from weak subspaces. We urge for further work on addressing this challenge.},
	articleno    = 3,
	numpages     = 24,
	keywords     = {dynamic time warping, free-form gesture, password space, guessing attack}
}
@article{10.1145/3534606,
	title        = {ToothSonic: Earable Authentication via Acoustic Toothprint},
	author       = {Wang, Zi and Ren, Yili and Chen, Yingying and Yang, Jie},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534606},
	url          = {https://doi.org/10.1145/3534606},
	issue_date   = {July 2022},
	abstract     = {Earables (ear wearables) are rapidly emerging as a new platform encompassing a diverse range of personal applications. The traditional authentication methods hence become less applicable and inconvenient for earables due to their limited input interface. Nevertheless, earables often feature rich around-the-head sensing capability that can be leveraged to capture new types of biometrics. In this work, we propose ToothSonic that leverages the toothprint-induced sonic effect produced by a user performing teeth gestures for earable authentication. In particular, we design representative teeth gestures that can produce effective sonic waves carrying the information of the toothprint. To reliably capture the acoustic toothprint, it leverages the occlusion effect of the ear canal and the inward-facing microphone of the earables. It then extracts multi-level acoustic features to reflect the intrinsic toothprint information for authentication. The key advantages of ToothSonic are that it is suitable for earables and is resistant to various spoofing attacks as the acoustic toothprint is captured via the user's private teeth-ear channel that modulates and encrypts the sonic waves. Our experiment studies with 25 participants show that ToothSonic achieves up to 95% accuracy with only one of the users' tooth gestures.},
	articleno    = 78,
	numpages     = 24,
	keywords     = {Acoustic Sensing, Earable Authentication, Biometrics, Wearable, Ear Canal, Tooth}
}
@article{10.1145/3534581,
	title        = {TransRisk: Mobility Privacy Risk Prediction Based on Transferred Knowledge},
	author       = {Xie, Xiaoyang and Hong, Zhiqing and Qin, Zhou and Fang, Zhihan and Tian, Yuan and Zhang, Desheng},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534581},
	url          = {https://doi.org/10.1145/3534581},
	issue_date   = {July 2022},
	abstract     = {Human mobility data may lead to privacy concerns because a resident can be re-identified from these data by malicious attacks even with anonymized user IDs. For an urban service collecting mobility data, an efficient privacy risk assessment is essential for the privacy protection of its users. The existing methods enable efficient privacy risk assessments for service operators to fast adjust the quality of sensing data to lower privacy risk by using prediction models. However, for these prediction models, most of them require massive training data, which has to be collected and stored first. Such a large-scale long-term training data collection contradicts the purpose of privacy risk prediction for new urban services, which is to ensure that the quality of high-risk human mobility data is adjusted to low privacy risk within a short time. To solve this problem, we present a privacy risk prediction model based on transfer learning, i.e., TransRisk, to predict the privacy risk for a new target urban service through (1) small-scale short-term data of its own, and (2) the knowledge learned from data from other existing urban services. We envision the application of TransRisk on the traffic camera surveillance system and evaluate it with real-world mobility datasets already collected in a Chinese city, Shenzhen, including four source datasets, i.e., (i) one call detail record dataset (CDR) with 1.2 million users; (ii) one cellphone connection data dataset (CONN) with 1.2 million users; (iii) a vehicular GPS dataset (Vehicles) with 10 thousand vehicles; (iv) an electronic toll collection transaction dataset (ETC) with 156 thousand users, and a target dataset, i.e., a camera dataset (Camera) with 248 cameras. The results show that our model outperforms the state-of-the-art methods in terms of RMSE and MAE. Our work also provides valuable insights and implications on mobility data privacy risk assessment for both current and future large-scale services.},
	articleno    = 82,
	numpages     = 19,
	keywords     = {Privacy, Heterogeneous Datasets, Mobility Patterns}
}
@article{10.1145/3534580,
	title        = {Template Matching Based Early Exit CNN for Energy-Efficient Myocardial Infarction Detection on Low-Power Wearable Devices},
	author       = {Rashid, Nafiul and Demirel, Berken Utku and Odema, Mohanad and Al Faruque, Mohammad Abdullah},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534580},
	url          = {https://doi.org/10.1145/3534580},
	issue_date   = {July 2022},
	abstract     = {Myocardial Infarction (MI), also known as heart attack, is a life-threatening form of heart disease that is a leading cause of death worldwide. Its recurrent and silent nature emphasizes the need for continuous monitoring through wearable devices. The wearable device solutions should provide adequate performance while being resource-constrained in terms of power and memory. This paper proposes an MI detection methodology using a Convolutional Neural Network (CNN) that outperforms the state-of-the-art works on wearable devices for two datasets - PTB and PTB-XL, while being energy and memory-efficient. Moreover, we also propose a novel Template Matching based Early Exit (TMEX) CNN architecture that further increases the energy efficiency compared to baseline architecture while maintaining similar performance. Our baseline and TMEX architecture achieve 99.33% and 99.24% accuracy on PTB dataset, whereas on PTB-XL dataset they achieve 84.36% and 84.24% accuracy, respectively. Both architectures are suitable for wearable devices requiring only 20 KB of RAM. Evaluation of real hardware shows that our baseline architecture is 0.6x to 53x more energy-efficient than the state-of-the-art works on wearable devices. Moreover, our TMEX architecture further improves the energy efficiency by 8.12% (PTB) and 6.36% (PTB-XL) while maintaining similar performance as the baseline architecture.},
	articleno    = 68,
	numpages     = 22,
	keywords     = {wearable devices, early exit CNN, energy-efficiency, myocardial infarction}
}
@article{10.1145/3517252,
	title        = {Learning Disentangled Behaviour Patterns for Wearable-Based Human Activity Recognition},
	author       = {Su, Jie and Wen, Zhenyu and Lin, Tao and Guan, Yu},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517252},
	url          = {https://doi.org/10.1145/3517252},
	issue_date   = {March 2022},
	abstract     = {In wearable-based human activity recognition (HAR) research, one of the major challenges is the large intra-class variability problem. The collected activity signal is often, if not always, coupled with noises or bias caused by personal, environmental, or other factors, making it difficult to learn effective features for HAR tasks, especially when with inadequate data. To address this issue, in this work, we proposed a Behaviour Pattern Disentanglement (BPD) framework, which can disentangle the behavior patterns from the irrelevant noises such as personal styles or environmental noises, etc. Based on a disentanglement network, we designed several loss functions and used an adversarial training strategy for optimization, which can disentangle activity signals from the irrelevant noises with the least dependency (between them) in the feature space. Our BPD framework is flexible, and it can be used on top of existing deep learning (DL) approaches for feature refinement. Extensive experiments were conducted on four public HAR datasets, and the promising results of our proposed BPD scheme suggest its flexibility and effectiveness. This is an open-source project, and the code can be found at http://github.com/Jie-su/BPD},
	articleno    = 28,
	numpages     = 19,
	keywords     = {Human Activity Recognition, Deep learning, Wearable Sensing, Machine Learning}
}
@article{10.1145/3495000,
	title        = {ModElec: A Design Tool for Prototyping Physical Computing Devices Using Conductive 3D Printing},
	author       = {He, Liang and Wittkopf, Jarrid A. and Jun, Ji Won and Erickson, Kris and Ballagas, Rafael Tico},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3495000},
	url          = {https://doi.org/10.1145/3495000},
	issue_date   = {Dec 2021},
	abstract     = {Integrating electronics with highly custom 3D designs for the physical fabrication of interactive prototypes is traditionally cumbersome and requires numerous iterations of manual assembly and debugging. With the new capabilities of 3D printers, combining electronic design and 3D modeling workflows can lower the barrier for achieving interactive functionality or iterating on the overall design. We present ModElec---an interactive design tool that enables the coordinated expression of electronic and physical design intent by allowing designers to integrate 3D-printable circuits with 3D forms. With ModElec, the user can arrange electronic parts in a 3D body, modify the model design with embedded circuits updated, and preview the auto-generated 3D traces that can be directly printed with a multi-material-based 3D printer. We demonstrate the potential of ModElec with four example applications, from a set of game controls to reconfigurable devices. Further, the tool was reported as easy to use through a preliminary evaluation with eight designers.},
	articleno    = 159,
	numpages     = 20,
	keywords     = {Physical computing, Conductive 3D printing, Electronic design, 3D modeling}
}
@article{10.1145/3494997,
	title        = {Mover: Generalizability Verification of Human Mobility Models via Heterogeneous Use Cases},
	author       = {Lyu, Wenjun and Wang, Guang and Yang, Yu and Zhang, Desheng},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494997},
	url          = {https://doi.org/10.1145/3494997},
	issue_date   = {Dec 2021},
	abstract     = {Human mobility models typically produce mobility data to capture human mobility patterns individually or collectively based on real-world observations or assumptions, which are essential for many use cases in research and practice, e.g., mobile networking, autonomous driving, urban planning, and epidemic control. However, most existing mobility models suffer from practical issues like unknown accuracy and uncertain parameters in new use cases because they are normally designed and verified based on a particular use case (e.g., mobile phones, taxis, or mobile payments). This causes significant challenges for researchers when they try to select a representative human mobility model with appropriate parameters for new use cases. In this paper, we introduce a MObility VERification framework called MOVER to systematically measure the performance of a set of representative mobility models including both theoretical and empirical models based on a diverse set of use cases with various measures. Based on a taxonomy built upon spatial granularity and temporal continuity, we selected four representative mobility use cases (e.g., the vehicle tracking system, the camera-based system, the mobile payment system, and the cellular network system) to verify the generalizability of the state-of-the-art human mobility models. MOVER methodically characterizes the accuracy of five different mobility models in these four use cases based on a comprehensive set of mobility measures and provide two key lessons learned: (i) For the collective level measures, the finer spatial granularity of the user cases, the better generalization of the theoretical models; (ii) For the individual-level measures, the lower periodic temporal continuity of the user cases, the theoretical models typically generalize better than the empirical models. The verification results can help the research community to select appropriate mobility models and parameters in different use cases.},
	articleno    = 171,
	numpages     = 21,
	keywords     = {Generalizability, Mobility modeling, Heterogeneous use cases}
}
@article{10.1145/3494989,
	title        = {SkinKit: Construction Kit for On-Skin Interface Prototyping},
	author       = {Ku, Pin-Sung and Molla, Md. Tahmidul Islam and Huang, Kunpeng and Kattappurath, Priya and Ranjan, Krithik and Kao, Hsin-Liu Cindy},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494989},
	url          = {https://doi.org/10.1145/3494989},
	issue_date   = {Dec 2021},
	abstract     = {The emergence of on-skin interfaces has created an opportunity for seamless, always-available on-body interactions. However, developing a new fabrication process for on-skin interfaces can be time-consuming, challenging to incorporate new features, and not available for quick form-factor preview through prototyping. We introduce SkinKit, the first construction toolkit for on-skin interfaces, which enables fast, low-fidelity prototyping with a slim form factor directly applicable to the skin. SkinKit comprises modules consisting of skin-conformable base substrates and reusable Flexible Printed Circuits Board (FPCB) blocks. They are easy to attach and remove under tangible plug-and-play construction but still offer robust conductive connections in a slim form. Further, SkinKit aims to lower the barrier to entry in building on-skin interfaces without demanding technical expertise. It leverages a variety of preprogrammed modules connected in unique sequences to achieve various function customizations. We describe our iterative design and development process of SkinKit, comparing materials, connection mechanisms, and modules reflecting on its capability. We report results from single- and multi- session workshops with 34 maker participants spanning STEM and design backgrounds. Our findings reveal how diverse maker populations engage in on-skin interface design, what types of applications they choose to build, and what challenges they faced.},
	articleno    = 165,
	numpages     = 23,
	keywords     = {soft circuitry, construction kits, On-skin interfaces, wearable computing}
}
@article{10.1145/3494984,
	title        = {Leakage or Identification: Behavior-Irrelevant User Identification Leveraging Leakage Current on Laptops},
	author       = {Ding, Dian and Yang, Lanqing and Chen, Yi-Chao and Xue, Guangtao},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494984},
	url          = {https://doi.org/10.1145/3494984},
	issue_date   = {Dec 2021},
	abstract     = {The convenience of laptops brings with it the risk of information leakage, and conventional security systems based on the password or the explicit biometric do little to alleviate this problem. Biometric identification based on anatomical features provides far stronger security; however, a lack of suitable sensors on laptops limits the applicability of this technology. In this paper, we developed a behavior-irrelevant user identification system applicable to laptops with a metal casing. The proposed scheme, referred to as LeakPrint, is based on leakage current, wherein the system uses an earphone to capture current leaking through the body and then transmits the corresponding signal to a server for identification. The user identification is achieved via denoising, dimension reduction, and feature extraction. Compared to other biometric identification methods, the proposed system is less dependent on external hardware and more robust to environmental noise. The experiments in real-world environments demonstrated that LeakPrint can verify user identity with high accuracy (93.6%), while providing effective defense against replay attacks (96.5%) and mimicry attacks (90.9%).},
	articleno    = 152,
	numpages     = 23,
	keywords     = {Leakage Current, User Identification, Laptop}
}
@article{10.1145/3494976,
	title        = {Computing Touch-Point Ambiguity on Mobile Touchscreens for Modeling Target Selection Times},
	author       = {Yamanaka, Shota and Usuba, Hiroki},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494976},
	url          = {https://doi.org/10.1145/3494976},
	issue_date   = {Dec 2021},
	abstract     = {Finger-Fitts law (FFitts law) is a model to predict touch-pointing times, modified from Fitts' law. It considers the absolute touch-point precision, or a finger tremor factor σa, to decrease the admissible target area and thus increase the task difficulty. Among choices such as running an independent task or performing parameter optimization, there is no consensus on the best methodology to measure σa. This inconsistency could be detrimental to HCI studies such as pointing technique evaluations and user group comparisons. By integrating the results of our 1D and 2D touch-pointing experiments and reanalyses of previous studies' data, we examined the advantages and disadvantages of each approach to compute σa. We found that the parameter optimization method is a suboptimal choice for predicting the performance.},
	articleno    = 186,
	numpages     = 21,
	keywords     = {mobile devices, finger input, touchscreens, Fitts' law, pointing}
}
@article{10.1145/3494973,
	title        = {Winect: 3D Human Pose Tracking for Free-Form Activity Using Commodity WiFi},
	author       = {Ren, Yili and Wang, Zi and Tan, Sheng and Chen, Yingying and Yang, Jie},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494973},
	url          = {https://doi.org/10.1145/3494973},
	issue_date   = {Dec 2021},
	abstract     = {WiFi human sensing has become increasingly attractive in enabling emerging human-computer interaction applications. The corresponding technique has gradually evolved from the classification of multiple activity types to more fine-grained tracking of 3D human poses. However, existing WiFi-based 3D human pose tracking is limited to a set of predefined activities. In this work, we present Winect, a 3D human pose tracking system for free-form activity using commodity WiFi devices. Our system tracks free-form activity by estimating a 3D skeleton pose that consists of a set of joints of the human body. In particular, we combine signal separation and joint movement modeling to achieve free-form activity tracking. Our system first identifies the moving limbs by leveraging the two-dimensional angle of arrival of the signals reflected off the human body and separates the entangled signals for each limb. Then, it tracks each limb and constructs a 3D skeleton of the body by modeling the inherent relationship between the movements of the limb and the corresponding joints. Our evaluation results show that Winect is environment-independent and achieves centimeter-level accuracy for free-form activity tracking under various challenging environments including the none-line-of-sight (NLoS) scenarios.},
	articleno    = 176,
	numpages     = 29,
	keywords     = {Free-form Activity, Human Pose Estimation, Commodity WiFi, Channel State Information (CSI), 3D Human Skeleton, WiFi Sensing}
}
@article{10.1145/3478126,
	title        = {Examining the Social Context of Alcohol Drinking in Young Adults with Smartphone Sensing},
	author       = {Meegahapola, Lakmal and Labhart, Florian and Phan, Thanh-Trung and Gatica-Perez, Daniel},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478126},
	url          = {https://doi.org/10.1145/3478126},
	issue_date   = {Sept 2021},
	abstract     = {According to prior work, the type of relationship between a person consuming alcohol and others in the surrounding (friends, family, spouse, etc.), and the number of those people (alone, with one person, with a group) are related to many aspects of alcohol consumption, such as the drinking amount, location, motives, and mood. Even though the social context is recognized as an important aspect that influences the drinking behavior of young adults in alcohol research, relatively little work has been conducted in smartphone sensing research on this topic. In this study, we analyze the weekend nightlife drinking behavior of 241 young adults in a European country, using a dataset consisting of self-reports and passive smartphone sensing data over a period of three months. Using multiple statistical analyses, we show that features from modalities such as accelerometer, location, application usage, bluetooth, and proximity could be informative about different social contexts of drinking. We define and evaluate seven social context inference tasks using smartphone sensing data, obtaining accuracies of the range 75%-86% in four two-class and three three-class inferences. Further, we discuss the possibility of identifying the sex composition of a group of friends using smartphone sensor data with accuracies over 70%. The results are encouraging towards supporting future interventions on alcohol consumption that incorporate users' social context more meaningfully and reducing the need for user self-reports when creating drink logs for self-tracking tools and public health studies.},
	articleno    = 121,
	numpages     = 26,
	keywords     = {self-reports, young adults, passive sensing, interaction sensing, continuous sensing, drinking, alcohol, social context, smartphone sensing, nightlife, mobile sensing}
}
@article{10.1145/3478105,
	title        = {VeriMask: Facilitating Decontamination of N95 Masks in the COVID-19 Pandemic: Challenges, Lessons Learned, and Safeguarding the Future},
	author       = {Long, Yan and Curtiss, Alexander and Rampazzi, Sara and Hester, Josiah and Fu, Kevin},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478105},
	url          = {https://doi.org/10.1145/3478105},
	issue_date   = {Sept 2021},
	abstract     = {The US CDC has recognized moist-heat as one of the most effective and accessible methods of decontaminating N95 masks for reuse in response to the persistent N95 mask shortages caused by the COVID-19 pandemic. However, it is challenging to reliably deploy this technique in healthcare settings due to a lack of smart technologies capable of ensuring proper decontamination conditions of hundreds of masks simultaneously. To tackle these challenges, we developed an open-source wireless sensor platform---VeriMask1 ---that facilitates per-mask verification of the moist-heat decontamination process. VeriMask is capable of monitoring hundreds of masks simultaneously in commercially available heating systems and provides a novel throughput-maximization functionality to help operators optimize the decontamination settings. We evaluate VeriMask in laboratory and real-scenario clinical settings and find that it effectively detects decontamination failures and operator errors in multiple settings and increases the mask decontamination throughput. Our easy-to-use, low-power, low-cost, scalable platform integrates with existing hospital protocols and equipment, and can be broadly deployed in under-resourced facilities to protect front-line healthcare workers by lowering their risk of infection from reused N95 masks. We also memorialize the design challenges, guidelines, and lessons learned from developing and deploying VeriMask during the COVID-19 Pandemic. Our hope is that by reflecting and reporting on this design experience, technologists and front-line health workers will be better prepared to collaborate for future pandemics, regarding mask decontamination, but also other forms of crisis tech.},
	articleno    = 119,
	numpages     = 29,
	keywords     = {Wireless Sensor, N95 Masks Decontamination, COVID-19}
}
@article{10.1145/3478096,
	title        = {Approaching the Real-World: Supporting Activity Recognition Training with Virtual IMU Data},
	author       = {Kwon, Hyeokhyen and Wang, Bingyao and Abowd, Gregory D. and Pl\"{o}tz, Thomas},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478096},
	url          = {https://doi.org/10.1145/3478096},
	issue_date   = {Sept 2021},
	abstract     = {Recently, IMUTube introduced a paradigm change for bootstrapping human activity recognition (HAR) systems for wearables. The key idea is to utilize videos of activities to support training activity recognizers based on inertial measurement units (IMUs). This system retrieves video from public repositories and subsequently generates virtual IMU data from this. The ultimate vision for such a system is to make large amounts of weakly labeled videos accessible for model training in HAR and, as such, to overcome one of the most pressing issues in the field: the lack of significant amounts of labeled sample data. In this paper we present the first in-detail exploration of IMUTube in a realistic assessment scenario: the analysis of free-weight gym exercises. We make significant progress towards a flexible, fully-functional IMUTube system by extending it such that it can handle a range of artifacts that are common in unrestricted online videos, including various forms of video noise, non-human poses, body part occlusions, and extreme camera and human motion. By overcoming these real-world challenges, we are able to generate high-quality virtual IMU data, which allows us to employ IMUTube for practical analysis tasks. We show that HAR systems trained by incorporating virtual sensor data generated by IMUTube significantly outperform baseline models trained only with real IMU data. In doing so we demonstrate the practical utility of IMUTube and the progress made towards the final vision of the new bootstrapping paradigm.},
	articleno    = 111,
	numpages     = 32,
	keywords     = {Data Collection, Activity Recognition, Machine Learning}
}
@article{10.1145/3478078,
	title        = {The Empathetic Car: Exploring Emotion Inference via Driver Behaviour and Traffic Context},
	author       = {Liu, Shu and Koch, Kevin and Zhou, Zimu and F\"{o}ll, Simon and He, Xiaoxi and Menke, Tina and Fleisch, Elgar and Wortmann, Felix},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478078},
	url          = {https://doi.org/10.1145/3478078},
	issue_date   = {Sept 2021},
	abstract     = {An empathetic car that is capable of reading the driver's emotions has been envisioned by many car manufacturers. Emotion inference enables in-vehicle applications to improve driver comfort, well-being, and safety. Available emotion inference approaches use physiological, facial, and speech-related data to infer emotions during driving trips. However, existing solutions have two major limitations: Relying on sensors that are not built into the vehicle restricts emotion inference to those people leveraging corresponding devices (e.g., smartwatches). Relying on modalities such as facial expressions and speech raises privacy concerns. By contrast, researchers in mobile health have been able to infer affective states (e.g., emotions) based on behavioral and contextual patterns decoded in available sensor streams, e.g., obtained by smartphones. We transfer this rationale to an in-vehicle setting by analyzing the feasibility of inferring driver emotions by passively interpreting the data streams of the control area network (CAN-bus) and the traffic context (inferred from the front-view camera). Therefore, our approach does not rely on particularly privacy-sensitive data streams such as the driver facial video or driver speech, but is built based on existing CAN-bus data and traffic information, which is available in current high-end or future vehicles. To assess our approach, we conducted a four-month field study on public roads covering a variety of uncontrolled daily driving activities. Hence, our results were generated beyond the confines of a laboratory environment. Ultimately, our proposed approach can accurately recognise drivers' emotions and achieve comparable performance as the medical-grade physiological sensor-based state-of-the-art baseline method.},
	articleno    = 117,
	numpages     = 34,
	keywords     = {Intelligent vehicle, Traffic contexts, Driving behaviours, Control area network (CAN), Emotion recognition}
}
@article{10.1145/3463522,
	title        = {Watching Your Phone's Back: Gesture Recognition by Sensing Acoustical Structure-Borne Propagation},
	author       = {Wang, Lei and Zhang, Xiang and Jiang, Yuanshuang and Zhang, Yong and Xu, Chenren and Gao, Ruiyang and Zhang, Daqing},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463522},
	url          = {https://doi.org/10.1145/3463522},
	issue_date   = {June 2021},
	abstract     = {Gesture recognition on the back surface of mobile phone, not limited to the touch screen, is an enabling Human-Computer Interaction (HCI) mechanism which enriches the user interaction experiences. However, there are two main limitations in the existing Back-of-Device (BoD) gesture recognition systems. They can only handle coarse-grained gesture recognition such as tap detection and cannot avoid the air-borne propagation suffering from the interference in the air. In this paper, we propose StruGesture, a fine-grained gesture recognition system using the back of mobile phones with ultrasonic signals. The key technique is to use the structure-borne sounds (i.e., sound propagation via structure of the device) to recognize sliding gestures on the back of mobile phones. StruGesture can fully extract the structure-borne component from the hybrid Channel Impulse Response (CIR) based on Peak Selection Algorithm. We develop a deep adversarial learning architecture to learn the gesture-specific representation for robust and effective recognition. Extensive experiments are designed to evaluate the robustness over nine deployment scenarios. The results show that StruGesture outperforms the competitive state-of-the-art classifiers by achieving an average recognition accuracy of 99.5% over 10 gestures.},
	articleno    = 82,
	numpages     = 26,
	keywords     = {Structure-borne Recognition, Acoustic Sensing, Mobile System}
}
@article{10.1145/3463507,
	title        = {Exploration of Person-Independent BCIs for Internal and External Attention-Detection in Augmented Reality},
	author       = {Vortmann, Lisa-Marie and Putze, Felix},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463507},
	url          = {https://doi.org/10.1145/3463507},
	issue_date   = {June 2021},
	abstract     = {Adding attention-awareness to an Augmented Reality setting by using a Brain-Computer Interface promises many interesting new applications and improved usability. The possibly complicated setup and relatively long training period of EEG-based BCIs however, reduce this positive effect immensely. In this study, we aim at finding solutions for person-independent, training-free BCI integration into AR to classify internally and externally directed attention. We assessed several different classifier settings on a dataset of 14 participants consisting of simultaneously recorded EEG and eye tracking data. For this, we compared the classification accuracies of a linear algorithm, a non-linear algorithm, and a neural net that were trained on a specifically generated feature set, as well as a shallow neural net for raw EEG data. With a real-time system in mind, we also tested different window lengths of the data aiming at the best payoff between short window length and high classification accuracy. Our results showed that the shallow neural net based on 4-second raw EEG data windows was best suited for real-time person-independent classification. The accuracy for the binary classification of internal and external attention periods reached up to 88% accuracy with a model that was trained on a set of selected participants. On average, the person-independent classification rate reached 60%. Overall, the high individual differences could be seen in the results. In the future, further datasets are necessary to compare these results before optimizing a real-time person-independent attention classifier for AR.},
	articleno    = 80,
	numpages     = 27,
	keywords     = {person-independence, eye tracking, augmented reality, EEG, attention}
}
@article{10.1145/3463505,
	title        = {ShaZam: Charge-Free Wearable Devices via Intra-Body Power Transfer from Everyday Objects},
	author       = {Mohammed, Noor and Wang, Rui and Jackson, Robert W. and Noh, Yeonsik and Gummeson, Jeremy and Lee, Sunghoon Ivan},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463505},
	url          = {https://doi.org/10.1145/3463505},
	issue_date   = {June 2021},
	abstract     = {In this work, we investigate a wireless power transfer technology that can unobtrusively charge wearable devices while users interact with everyday objects, such as an office desk, laptop, or car. We design and develop our solution, ShaZam, that exploits the human body as a medium to transfer Radio Frequency (RF) energy-carrier signals from minimally-instrumented daily objects to wearable devices. We focus on establishing the technical groundwork of the proposed technology by incorporating the capacitive coupling mechanism, in which the forward signal path is established through the human body, and the return path is established via capacitive coupling to the surrounding environment. To showcase the feasibility of our technology, we investigate three different use scenarios---i.e., interacting with a keyboard on a desk, a laptop, and the steering wheel of a car---to transfer power to a wrist-worn device. Using data obtained from ten healthy individuals within a setting where uncontrolled electromagnetic interference was relatively low, we demonstrate that we can transfer approximately 0.5 mW - 1 mW of DC power to the wrist-worn device. We also investigate several critical environmental and design parameters that could affect the power transfer and offer design guidelines that optimize performance. Our initial results suggest the potential for a new design paradigm towards completely charge-free wearable devices.},
	articleno    = 75,
	numpages     = 25,
	keywords     = {Intra-body power transfer, wireless power transfer, wearable device, energy harvester, batteryless device, capacitive power transfer}
}
@article{10.1145/3448122,
	title        = {Multimodal Joint Head Orientation Estimation in Interacting Groups via Proxemics and Interaction Dynamics},
	author       = {Tan, Stephanie and Tax, David M. J. and Hung, Hayley},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448122},
	url          = {https://doi.org/10.1145/3448122},
	issue_date   = {March 2021},
	abstract     = {Human head orientation estimation has been of interest because head orientation serves as a cue to directed social attention. Most existing approaches rely on visual and high-fidelity sensor inputs and deep learning strategies that do not consider the social context of unstructured and crowded mingling scenarios. We show that alternative inputs, like speaking status, body location, orientation, and acceleration contribute towards head orientation estimation. These are especially useful in crowded and in-the-wild settings where visual features are either uninformative due to occlusions or prohibitive to acquire due to physical space limitations and concerns of ecological validity. We argue that head orientation estimation in such social settings needs to account for the physically evolving interaction space formed by all the individuals in the group. To this end, we propose an LSTM-based head orientation estimation method that combines the hidden representations of the group members. Our framework jointly predicts head orientations of all group members and is applicable to groups of different sizes. We explain the contribution of different modalities to model performance in head orientation estimation. The proposed model outperforms baseline methods that do not explicitly consider the group context, and generalizes to an unseen dataset from a different social event.},
	articleno    = 35,
	numpages     = 22,
	keywords     = {scene understanding, interaction dynamics, head orientation estimation}
}
@article{10.1145/3448120,
	title        = {One More Bite? Inferring Food Consumption Level of College Students Using Smartphone Sensing and Self-Reports},
	author       = {Meegahapola, Lakmal and Ruiz-Correa, Salvador and Robledo-Valero, Viridiana del Carmen and Hernandez-Huerfano, Emilio Ernesto and Alvarez-Rivera, Leonardo and Chenu-Abente, Ronald and Gatica-Perez, Daniel},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448120},
	url          = {https://doi.org/10.1145/3448120},
	issue_date   = {March 2021},
	abstract     = {While the characterization of food consumption level has been extensively studied in nutrition and psychology research, advancements in passive smartphone sensing have not been fully utilized to complement mobile food diaries in characterizing food consumption levels. In this study, a new dataset regarding the holistic food consumption behavior of 84 college students in Mexico was collected using a mobile application combining passive smartphone sensing and self-reports. We show that factors such as sociability and activity types and levels have an association to food consumption levels. Finally, we define and assess a novel ubicomp task, by using machine learning techniques to infer self-perceived food consumption level (eating as usual, overeating, undereating) with an accuracy of 87.81% in a 3-class classification task by using passive smartphone sensing and self-report data. Furthermore, we show that an accuracy of 83.49% can be achieved for the same classification task by using only smartphone sensing data and time of eating, which is an encouraging step towards building context-aware mobile food diaries and making food diary based apps less tedious for users.},
	articleno    = 26,
	numpages     = 28,
	keywords     = {food consumption, health, well-being, passive mobile sensing, eating behavior, overeating}
}
@article{10.1145/3448112,
	title        = {SelfHAR: Improving Human Activity Recognition through Self-Training with Unlabeled Data},
	author       = {Tang, Chi Ian and Perez-Pozuelo, Ignacio and Spathis, Dimitris and Brage, Soren and Wareham, Nick and Mascolo, Cecilia},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448112},
	url          = {https://doi.org/10.1145/3448112},
	issue_date   = {March 2021},
	abstract     = {Machine learning and deep learning have shown great promise in mobile sensing applications, including Human Activity Recognition. However, the performance of such models in real-world settings largely depends on the availability of large datasets that captures diverse behaviors. Recently, studies in computer vision and natural language processing have shown that leveraging massive amounts of unlabeled data enables performance on par with state-of-the-art supervised models.In this work, we present SelfHAR, a semi-supervised model that effectively learns to leverage unlabeled mobile sensing datasets to complement small labeled datasets. Our approach combines teacher-student self-training, which distills the knowledge of unlabeled and labeled datasets while allowing for data augmentation, and multi-task self-supervision, which learns robust signal-level representations by predicting distorted versions of the input.We evaluated SelfHAR on various HAR datasets and showed state-of-the-art performance over supervised and previous semi-supervised approaches, with up to 12% increase in F1 score using the same number of model parameters at inference. Furthermore, SelfHAR is data-efficient, reaching similar performance using up to 10 times less labeled data compared to supervised approaches. Our work not only achieves state-of-the-art performance in a diverse set of HAR datasets, but also sheds light on how pre-training tasks may affect downstream performance.},
	articleno    = 36,
	numpages     = 30,
	keywords     = {unlabeled data, self-training, deep learning, semi-supervised training, human activity recognition, self-supervised training}
}
@article{10.1145/3448101,
	title        = {RF-Identity: Non-Intrusive Person Identification Based on Commodity RFID Devices},
	author       = {Feng, Chao and Xiong, Jie and Chang, Liqiong and Wang, Fuwei and Wang, Ju and Fang, Dingyi},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448101},
	url          = {https://doi.org/10.1145/3448101},
	issue_date   = {March 2021},
	abstract     = {Person identification plays a critical role in a large range of applications. Recently, RF based person identification becomes a hot research topic due to the contact-free nature of RF sensing that is particularly appealing in current COVID-19 pandemic. However, existing systems still have multiple limitations: i) heavily rely on the gait patterns of users for identification; ii) require a large amount of data to train the model and also extensive retraining for new users and iii) require a large frequency bandwidth which is not available on most commodity RF devices for static person identification. This paper proposes RF-Identity, an RFID-based identification system to address the above limitations and the contribution is threefold. First, by integrating walking pattern features with unique body shape features (e.g., height), RF-Identity achieves a high accuracy in person identification. Second, RF-Identity develops a data augmentation scheme to expand the size of the training data set, thus reducing the human effort in data collection. Third, RF-Identity utilizes the tag diversity in spatial domain to identify static users without a need of large frequency bandwidth. Extensive experiments show an identification accuracy of 94.2% and 95.9% for 50 dynamic and static users, respectively.},
	articleno    = 9,
	numpages     = 23,
	keywords     = {Deep learning, body feature, Person identification, RFID tag}
}
@article{10.1145/3448100,
	title        = {CrossGR: Accurate and Low-Cost Cross-Target Gesture Recognition Using Wi-Fi},
	author       = {Li, Xinyi and Chang, Liqiong and Song, Fangfang and Wang, Ju and Chen, Xiaojiang and Tang, Zhanyong and Wang, Zheng},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448100},
	url          = {https://doi.org/10.1145/3448100},
	issue_date   = {March 2021},
	abstract     = {This paper focuses on a fundamental question in Wi-Fi-based gesture recognition: "Can we use the knowledge learned from some users to perform gesture recognition for others?". This problem is also known as cross-target recognition. It arises in many practical deployments of Wi-Fi-based gesture recognition where it is prohibitively expensive to collect training data from every single user. We present CrossGR, a low-cost cross-target gesture recognition system. As a departure from existing approaches, CrossGR does not require prior knowledge (such as who is currently performing a gesture) of the target user. Instead, CrossGR employs a deep neural network to extract user-agnostic but gesture-related Wi-Fi signal characteristics to perform gesture recognition. To provide sufficient training data to build an effective deep learning model, CrossGR employs a generative adversarial network to automatically generate many synthetic training data from a small set of real-world examples collected from a small number of users. Such a strategy allows CrossGR to minimize the user involvement and the associated cost in collecting training examples for building an accurate gesture recognition system. We evaluate CrossGR by applying it to perform gesture recognition across 10 users and 15 gestures. Experimental results show that CrossGR achieves an accuracy of over 82.6% (up to 99.75%). We demonstrate that CrossGR delivers comparable recognition accuracy, but uses an order of magnitude less training samples collected from the end-users when compared to state-of-the-art recognition systems.},
	articleno    = 21,
	numpages     = 23,
	keywords     = {Deep Learning, Cross-target, Wireless Sensing, Gesture Recognition, Wi-Fi}
}
@article{10.1145/3448094,
	title        = {Full-Dimension Relative Positioning for RFID-Enabled Self-Checkout Services},
	author       = {Duan, Chunhui and Liu, Jiajun and Ding, Xuan and Li, Zhenhua and Liu, Yunhao},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448094},
	url          = {https://doi.org/10.1145/3448094},
	issue_date   = {March 2021},
	abstract     = {Self-checkout services in today's retail stores are well received as they set free the labor force of cashiers and shorten conventional checkout lines. However, existing self-checkout options either require customers to scan items one by one, which is troublesome and inefficient, or rely on deployments of massive sensors and cameras together with complex tracking algorithms. On the other hand, RFID-based item-level tagging in retail offers an extraordinary opportunity to enhance current checkout experiences. In this work, we propose Taggo, a lightweight and efficient self-checkout schema utilizing well-deployed RFIDs. Taggo attaches a few anchor tags on the four upper edges of each shopping cart, so as to figure out which cart each item belongs to, through relative positioning among the tagged items and anchor tags without knowing their absolute positions. Specifically, a full-dimension ordering technique is devised to accurately determine the order of tags in each dimension, as well as to address the negative impacts from imperfect measurements in indoor surroundings. Besides, we design a holistic classifying solution based on probabilistic modeling to map each item to the correct cart that carries it. We have implemented Taggo with commercial RFID devices and evaluated it extensively in our lab environment. On average, Taggo achieves 90% ordering accuracy in real-time, eventually producing 95% classifying accuracy.},
	articleno    = 7,
	numpages     = 23,
	keywords     = {relative positioning, RFID, self-checkout}
}
@article{10.1145/3448088,
	title        = {Modelling Memory for Individual Re-Identification in Decentralised Mobile Contact Tracing Applications},
	author       = {Bedogni, Luca and Rumi, Shakila Khan and Salim, Flora D.},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448088},
	url          = {https://doi.org/10.1145/3448088},
	issue_date   = {March 2021},
	abstract     = {In 2020 the coronavirus outbreak changed the lives of people worldwide. After an initial time period in which it was unclear how to battle the virus, social distancing has been recognised globally as an effective method to mitigate the disease spread. This called for technological tools such as Mobile Contact Tracing Applications (MCTA), which are used to digitally trace contacts among people, and in case a positive case is found, people with the application installed which had been in contact will be notified. De-centralised MCTA may suffer from a novel kind of privacy attack, based on the memory of the human beings, which upon notification of the application can identify who is the positive individual responsible for the notification. Our results show that it is indeed possible to identify positive people among the group of contacts of a human being, and this is even easier when the sociability of the positive individual is low. In practice, our simulation results show that identification can be made with an accuracy of more than 90% depending on the scenario. We also provide three mitigation strategies which can be implemented in de-centralised MCTA and analyse which of the three are more effective in limiting this novel kind of attack.},
	articleno    = 4,
	numpages     = 21,
	keywords     = {privacy preserving, datasets, digital contact tracing, mobile computing}
}
@article{10.1145/3448080,
	title        = {Fine-Grained and Context-Aware Behavioral Biometrics for Pattern Lock on Smartphones},
	author       = {Shi, Dai and Tao, Dan and Wang, Jiangtao and Yao, Muyan and Wang, Zhibo and Chen, Houjin and Helal, Sumi},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448080},
	url          = {https://doi.org/10.1145/3448080},
	issue_date   = {March 2021},
	abstract     = {Pattern lock has been widely used in smartphones as a simple and effective authentication mechanism, which however is shown to be vulnerable to various attacks. In this paper, we design a novel authentication system for more secure pattern unlocking on smartphones. The basic idea is to utilize various behavior information of the user during pattern unlocking as additional authentication fingerprints, so that even if the pattern password is leaked to an attacker, the system remains safe and protected. To accommodate a variety of user contexts by our system, a context-aware module is proposed to distinguish any of such contexts (e.g., body postures when drawing the pattern) and use it to guide the authentication. Moreover, we design a polyline weighted strategy with overlapping based on the consistency of pattern lock, which analyzes the behavior information of the user during the unlock process in a fine-grained manner and takes an overall consideration the results of different polylines. Based on 14,850 samples collected from 77 participants, we have extensively evaluated the proposed system. The results demonstrate that it outperforms state-of-the-art implicit authentication based pattern lock approaches, and that each key module in our system is effective.},
	articleno    = 33,
	numpages     = 30,
	keywords     = {context-aware, implicit authentication, pattern lock}
}
@article{10.1145/3432236,
	title        = {Understanding Physical Practices and the Role of Technology in Manual Self-Tracking},
	author       = {Abtahi, Parastoo and Ding, Victoria and Yang, Anna C. and Bruzzese, Tommy and Romanos, Alyssa B. and Murnane, Elizabeth L. and Follmer, Sean and Landay, James A.},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432236},
	url          = {https://doi.org/10.1145/3432236},
	issue_date   = {December 2020},
	abstract     = {Self-tracking practices enable users to record and analyze their personal data. In recent years, non-digital forms of manual self-tracking, such as bullet journaling, have gained popularity. We conduct a survey (N = 404) and follow-up interviews (N = 18) to better understand users' motivations for physical tracking, the challenges they face with their current practices, and their perceptions of both digital and physical tracking tools. We find that for some users, physical practices are a structured and constructive creative outlet and a form of artistic expression. While the resulting physical artifacts may not easily enable retrospective reflection over long-term data, they preserve personal traces in a unique and tangible form that is meaningful to users. Moreover, the reflective power of physical tracking stems from the interaction with the physical materiality, the slow pace of these practices, the creative exploration they facilitate, and the associated digital disconnect. We conclude with design implications for future technologies, including ways digital tools might extend current physical practices and support richly reflective self-tracking.},
	articleno    = 115,
	numpages     = 24,
	keywords     = {self-reflection, mindfulness, Self-tracking, mood tracking, quantified self, bullet journaling}
}
@article{10.1145/3432232,
	title        = {MorphingCircuit: An Integrated Design, Simulation, and Fabrication Workflow for Self-Morphing Electronics},
	author       = {Wang, Guanyun and Qin, Fang and Liu, Haolin and Tao, Ye and Zhang, Yang and Zhang, Yongjie Jessica and Yao, Lining},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432232},
	url          = {https://doi.org/10.1145/3432232},
	issue_date   = {December 2020},
	abstract     = {Manufacturing nonplanar electronics often requires the integration of functions and forms through embedding circuit boards into three-dimensional (3D) shapes. While most popular solutions rely on cavities where electronics reside in forms of rigid circuit boards, other alternative approaches leverage 3D printing or layer lamination to create 3D electronics that often require expensive manufacturing processes and materials. Furthermore, many conventional methods are incompatible with complex geometries (e.g., surfaces that twist or have local minima). In response, we introduce MorphingCircuit, an integrated design, simulation, and fabrication workflow that combines electronic functions with forms through four-dimensional (4D) printing, which effectively reduces cost, production time, and e-waste. Specifically, we start by printing a flat substrate and assembling functional electronics on top of it. The flat structure will then self-morph into a preprogrammed 3D shape when triggered by external heating. Overall, our comprehensive 3D electronics fabrication pipeline encompasses the design, simulation, fabrication, and transformation, with which we hope to inspire designers, researchers, and makers to create conformal electronics on complex substrate geometries that were previously difficult or impossible to design or manufacture.},
	articleno    = 157,
	numpages     = 26,
	keywords     = {shape-changing interfaces, 4D printing, morphing circuits, 3D electronics, 3D printing, 3D PCB, 3D circuits}
}
@article{10.1145/3432228,
	title        = {BitLight: Turning DLP Projections into an Interactive Surface through Bit-Level Light Encoding},
	author       = {Liu, Song and He, Tian},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432228},
	url          = {https://doi.org/10.1145/3432228},
	issue_date   = {December 2020},
	abstract     = {This paper presents BitLight, a novel paradigm that uses the rapid flashing of a digital light processing (DLP) projector to encode an imperceptible mask temporally that, when sensed by a photodiode, uniquely specifies where the photodiode is located on the projected image. BitLight is inspired by the psychophysical phenomenon that the human visual system (HVS) cannot resolve rapid temporal changes in optical signals, so redundant optical signals could be inserted for tracking with some or little compromise on the original human perceived visual content. BitLight is the first to devise a bit-level temporal encoding to display RGB colors while also embedding tracking signals in a digital fashion. Compared to traditional visible-light-communication (VLC) systems that use frame-level encoding techniques such as luminance changes [10, 25, 26, 32] and alpha channel [16], the bit-level encoding of BitLight makes better use of the ultra-fine temporal division capabilities of DLP projectors to embed a much higher tracking data throughput and thus achieve faster localization speed. With our current prototype hardwares of a low-end microcontroller, cheap photodiodes, and a commercial off-the-shelfDLP projector, evaluation results have demonstrated an average of only 9.5ms to localize the sensor, versus 200ms by a comparison testbed that uses simple frame-level encodings.},
	articleno    = 141,
	numpages     = 23,
	keywords     = {Visible Light Positioning, Localization, Visible Light Communication (VLC), Human-computer interface (HCI), Wearable devices}
}
@article{10.1145/3432225,
	title        = {FORTNIoT: Intelligible Predictions to Improve User Understanding of Smart Home Behavior},
	author       = {Coppers, Sven and Vanacken, Davy and Luyten, Kris},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432225},
	url          = {https://doi.org/10.1145/3432225},
	issue_date   = {December 2020},
	abstract     = {Ubiquitous environments, such as smart homes, are becoming more intelligent and autonomous. As a result, their behavior becomes harder to grasp and unintended behavior becomes more likely. Researchers have contributed tools to better understand and validate an environments' past behavior (e.g. logs, end-user debugging), and to prevent unintended behavior. There is, however, a lack of tools that help users understand the future behavior of such an environment. Information about the actions it will perform, and why it will perform them, remains concealed. In this paper, we contribute FORTNIoT, a well-defined approach that combines self-sustaining predictions (e.g. weather forecasts) and simulations of trigger-condition-action rules to deduce when these rules will trigger in the future and what state changes they will cause to connected smart home entities. We implemented a proof-of-concept of this approach, as well as a visual demonstrator that shows such predictions, including causes and effects, in an overview of a smart home's behavior. A between-subject evaluation with 42 participants indicates that FORTNIoT predictions lead to a more accurate understanding of the future behavior, more confidence in that understanding, and more appropriate trust in what the system will (not) do. We envision a wide variety of situations where predictions about the future are beneficial to inhabitants of smart homes, such as debugging unintended behavior and managing conflicts by exception, and hope to spark a new generation of intelligible tools for ubiquitous environments.},
	articleno    = 124,
	numpages     = 24,
	keywords     = {Scrutability, Smart Homes, Simulations, Internet-of-Things, Predictions, Intelligibility}
}
@article{10.1145/3432192,
	title        = {TouchID: User Authentication on Mobile Devices via Inertial-Touch Gesture Analysis},
	author       = {Zhang, Xinchen and Yin, Yafeng and Xie, Lei and Zhang, Hao and Ge, Zefan and Lu, Sanglu},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432192},
	url          = {https://doi.org/10.1145/3432192},
	issue_date   = {December 2020},
	abstract     = {Due to the widespread use of mobile devices, it is essential to authenticate users on mobile devices to prevent sensitive information leakage. In this paper, we propose TouchID, which combinedly uses the touch sensor and the inertial sensor for gesture analysis, to provide a touch gesture based user authentication scheme. Specifically, TouchID utilizes the touch sensor to analyze the on-screen gesture while using the inertial sensor to analyze the device's motion caused by the touch gesture, and then combines the unique features from the on-screen gesture and the device's motion for user authentication. To mitigate the intra-class difference and reduce the inter-class similarity, we propose a spatial alignment method for sensor data and segment the touch gesture into multiple sub-gestures in space domain, to keep the stability of the same user and enhance the discriminability of different users. To provide a uniform representation of touch gestures with different topological structures, we present a four-part based feature selection method, which classifies a touch gesture into a start node, an end node, the turning node(s), and the smooth paths, and then select effective features from these parts based on Fisher Score. In addition, considering the uncertainty of user's postures, which may change the sensor data of same touch gesture, we propose a multi-threshold kNN based model to adaptively tolerate the posture difference for user authentication. Finally, we implement TouchID on commercial smartphones and conduct extensive experiments to evaluate TouchID. The experiment results show that TouchID can achieve a good performance for user authentication, i.e., having a low equal error rate of 4.90%.},
	articleno    = 162,
	numpages     = 29,
	keywords     = {User authentication, Mobile devices, Inertial-touch gesture analysis, Touch gesture}
}
@article{10.1145/3369838,
	title        = {HeyTeddy: Conversational Test-Driven Development for Physical Computing},
	author       = {Kim, Yoonji and Choi, Youngkyung and Kang, Daye and Lee, Minkyeong and Nam, Tek-Jin and Bianchi, Andrea},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369838},
	url          = {https://doi.org/10.1145/3369838},
	issue_date   = {December 2019},
	abstract     = {Physical computing is a complex activity that consists of different but tightly coupled tasks: programming and assembling hardware for circuits. Prior work clearly shows that this coupling is the main source of mistakes that unfruitfully take a large portion of novices' debugging time. While past work presented systems that simplify prototyping or introduce novel debugging functionalities, these tools either limit what users can accomplish or are too complex for beginners. In this paper, we propose a general-purpose prototyping tool based on conversation. HeyTeddy guides users during hardware assembly by providing additional information on requests or by interactively presenting the assembly steps to build a circuit. Furthermore, the user can program and execute code in real-time on their Arduino platform without having to write any code, but instead by using commands triggered by voice or text via chat. Finally, the system also presents a set of test capabilities for enhancing debugging with custom and proactive unit tests. We codesigned the system with 10 users over 6 months and tested it with realistic physical computing tasks. With the result of two user studies, we show that conversational programming is feasible and that voice is a suitable alternative for programming simple logic and encouraging exploration. We also demonstrate that conversational programming with unit tests is effective in reducing development time and overall debugging problems while increasing users' confidence. Finally, we highlight limitations and future avenues of research.},
	articleno    = 139,
	numpages     = 21,
	keywords     = {Test-driven development, Physical computing, Conversational agent, End-user development}
}
@article{10.1145/3369833,
	title        = {Design and Evaluation of DIO Construction Toolkit for Co-Making Shared Constructions},
	author       = {Arora, Jatin and Mathur, Kartik and Goel, Manvi and Kumar, Piyush and Mishra, Abhijeet and Parnami, Aman},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369833},
	url          = {https://doi.org/10.1145/3369833},
	issue_date   = {December 2019},
	abstract     = {We present the design and implementation of DIO, a novel digital-physical construction toolkit to enable constructionist learning for children from age group 8-12 years. The toolkit comprises of dome-shaped (D) tangible modules with various attachments that allow suspension on the body of multiple children and/or in the environment to support a variety of sensing/input (I), actuation/output (O) functionalities. The modules are enabled for wireless communication and can be linked together using an Augmented Reality based programming interface running on a smartphone. The smartphone recognizes our hemispherical modules omnidirectionally through novel computer vision based 3D patterns; custom made to provide logical as well as semantic encoding. In this paper, we show how, owing to its unique form-factor, the toolkit enables multi-user constructions for the children and offers a shared learning experience. We further reflect on our learning from a one-year long iterative design process and contribute a social scaffolding based procedure to engage children with such constructionist toolkits effectively.},
	articleno    = 127,
	numpages     = 25,
	keywords     = {Augmented Reality, Tangible blocks, Co-making, Constructionist learning}
}
@article{10.1145/3369827,
	title        = {DrawingPresence: A Method for Assessing Temporal Fluctuations of Presence Status in a VR Experience},
	author       = {Mai, Christian and Thiem, Niklas and Hussmann, Heinrich},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369827},
	url          = {https://doi.org/10.1145/3369827},
	issue_date   = {December 2019},
	abstract     = {One of the main measures to evaluate a head-mounted display (HMD) based experience is the state of feeling present in virtual reality (VR). The detection of disturbances of such an experience that occur over time, namely breaks in presence (BIP), enables the evaluation and improvement of these. Existing methods do not detect BIPs, e.g., questionnaires, or are complex in their application and evaluation, e.g., physiological and behavioral measures. We propose a handy post-experience method in which users reflect on their experienced state of presence by drawing a line in a paper-based drawing template. The amplitude of the drawn line represents the state of presence of the temporal progress of the experience. We propose a descriptive model that describes temporal variations in the drawings by the definition of relevant points over time, e.g., putting on the HMD, phases of the experience, transition into VR, and parameters, e.g., the transition time. The descriptive model enables us to objectively evaluate user drawings and represent the course of the drawings by a defined set of parameters. Our exploratory user study (N = 30) showed that the drawings are very consistent between participants and the method is able to securely detect a variety of BIPs. Moreover, the results indicate that the method might be used in the future to evaluate the strength of BIPs and to reflect the temporal course of a presence experience in detail. Additional application examples and a detailed discussion pave the way for others to use our method. Further, they serve as a motivation to continue working on the method and the general understanding of temporal fluctuations of the presence experience.},
	articleno    = 146,
	numpages     = 21,
	keywords     = {usability, user experience, head-mounted displays, virtual reality, method, presence}
}
@article{10.1145/3369824,
	title        = {VocaBura: A Method for Supporting Second Language Vocabulary Learning While Walking},
	author       = {Hautasaari, Ari and Hamada, Takeo and Ishiyama, Kuntaro and Fukushima, Shogo},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369824},
	url          = {https://doi.org/10.1145/3369824},
	issue_date   = {December 2019},
	abstract     = {Learning a new language is difficult and time-consuming. Apart from dedicated classroom study, second language (L2) learners often lack opportunities to switch their attention to vocabulary learning over other daily routines. In this work, we propose a method that enables L2 learners to study new vocabulary items during their dead time, such as when commuting to school or work. We developed a smartphone application, VocaBura, which combines audio learning with location-relevant L1-L2 word pairs to allow users to discover new vocabulary items while walking past buildings, shops and other locations. Our evaluation results indicated that Japanese beginner level English learners were able to retain more vocabulary items with the proposed method compared to traditional audio-based study despite being less aware of L2 vocabulary acquisition having occurred. In our second study, we report on the level of English vocabulary coverage for L2 learning achievable with our proposed method. We discuss several design implications for educational technologies supporting second language learning.},
	articleno    = 135,
	numpages     = 23,
	keywords     = {context awareness, audio learning, walking, Second language vocabulary learning}
}
@article{10.1145/3369821,
	title        = {My Mom Was Getting This Popup: Understanding Motivations and Processes in Helping Older Relatives with Mobile Security and Privacy},
	author       = {Mendel, Tamir and Toch, Eran},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369821},
	url          = {https://doi.org/10.1145/3369821},
	issue_date   = {December 2019},
	abstract     = {Security and privacy pose a serious barrier to the use of mobile technology by older adults. While support from family and friends is known to be an effective enabler in older adults' technology adoption, we know very little about the family members' motivations for providing help, the context, and the process in which they provide it. To bridge this gap, we have conducted a mixed method study, qualitatively analyzing the helpers' assistance stories and quantitatively estimating the factors that affect helpers' willingness to offer assistance to older relatives regarding mobile security and privacy problems. Our findings point to the potential for helping older relatives, i.e., people are more willing to help and guide them than other social groups. Furthermore, we show that familiarity with an older relative's preferences is essential in providing meaningful support. We discuss our findings in the context of developing a theory of collective efficacy for security and privacy and new collaborative technologies that can reduce the barriers to social help.},
	articleno    = 147,
	numpages     = 20,
	keywords     = {Assistance, Mobile computing, Help, Security and privacy, Older adults, Smartphones, Support}
}
@article{10.1145/3369814,
	title        = {CityGuard: Citywide Fire Risk Forecasting Using A Machine Learning Approach},
	author       = {Wang, Qianru and Zhang, Junbo and Guo, Bin and Hao, Zexia and Zhou, Yifang and Sun, Junkai and Yu, Zhiwen and Zheng, Yu},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369814},
	url          = {https://doi.org/10.1145/3369814},
	issue_date   = {December 2019},
	abstract     = {Forecasting the fire risk is of great importance to fire prevention deployments in a city, which can reduce loss even deaths caused by fires. However, it is very challenging because fires are influenced by many complex factors, including spatial correlations, temporal dependencies, even the mixture of these two and external factors. Firstly, the fire risk of a region is influenced by temporal effect of internal factors (e.g., the historical fire risk records) and temporal effect of external factors (e.g., weather). Secondly, a region's fire risk is not only influenced by its inherent geospatial attributes (e.g., POIs) but also dependent on other regions in spatial. To address these challenges, we propose a machine learning approach to forecast the fire risk, entitled NeuroFire. NeuroFire can represent internal and external temporal effect then combine the temporal representation and spatial dependencies by a spatial-temporal loss function. Experimental evaluations on real-world datasets show that our NeuroFire outperforms 9 baselines, demonstrating the performance of our approach by several visualizations. Moreover, we implement a citywide fire forecasting system named CityGuard to display the analysis and forecasting results, which can assist the fire rescue department in deploying fire prevention.},
	articleno    = 156,
	numpages     = 21,
	keywords     = {Fire risk, Neural network, Spatial-temporal data, Urban computing, Conditional random field}
}
@article{10.1145/3369812,
	title        = {RFID Tattoo: A Wireless Platform for Speech Recognition},
	author       = {Wang, Jingxian and Pan, Chengfeng and Jin, Haojian and Singh, Vaibhav and Jain, Yash and Hong, Jason I. and Majidi, Carmel and Kumar, Swarun},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369812},
	url          = {https://doi.org/10.1145/3369812},
	issue_date   = {December 2019},
	abstract     = {This paper presents an RF-based assistive technology for voice impairments (i.e., dysphonia), which occurs in an estimated 1% of the global population. We specifically focus on acquired voice disorders where users continue to be able to make facial and lip gestures associated with speech. Despite the rich literature on assistive technologies in this space, there remains a gap for a solution that neither requires external infrastructure in the environment, battery-powered sensors on skin or body-worn manual input devices.We present RFTattoo, which to our knowledge is the first wireless speech recognition system for voice impairments using batteryless and flexible RFID tattoos. We design specialized wafer-thin tattoos attached around the user's face and easily hidden by makeup. We build models that process signal variations from these tattoos to a portable RFID reader to recognize various facial gestures corresponding to distinct classes of sounds. We then develop natural language processing models that infer meaningful words and sentences based on the observed series of gestures. A detailed user study with 10 users reveals 86% accuracy in reconstructing the top-100 words in the English language, even without the users making any sounds.},
	articleno    = 155,
	numpages     = 24,
	keywords     = {Battery-free Networks, RFIDs}
}
@article{10.1145/3369805,
	title        = {Exploring the State-of-Receptivity for MHealth Interventions},
	author       = {K\"{u}nzler, Florian and Mishra, Varun and Kramer, Jan-Niklas and Kotz, David and Fleisch, Elgar and Kowatsch, Tobias},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369805},
	url          = {https://doi.org/10.1145/3369805},
	issue_date   = {December 2019},
	abstract     = {Recent advancements in sensing techniques for mHealth applications have led to successful development and deployments of several mHealth intervention designs, including Just-In-Time Adaptive Interventions (JITAI). JITAIs show great potential because they aim to provide the right type and amount of support, at the right time. Timing the delivery of a JITAI such as the user is receptive and available to engage with the intervention is crucial for a JITAI to succeed. Although previous research has extensively explored the role of context in users' responsiveness towards generic phone notifications, it has not been thoroughly explored for actual mHealth interventions. In this work, we explore the factors affecting users' receptivity towards JITAIs. To this end, we conducted a study with 189 participants, over a period of 6 weeks, where participants received interventions to improve their physical activity levels. The interventions were delivered by a chatbot-based digital coach -Ally - which was available on Android and iOS platforms.We define several metrics to gauge receptivity towards the interventions, and found that (1) several participant-specific characteristics (age, personality, and device type) show significant associations with the overall participant receptivity over the course of the study, and that (2) several contextual factors (day/time, phone battery, phone interaction, physical activity, and location), show significant associations with the participant receptivity, in-the-moment. Further, we explore the relationship between the effectiveness of the intervention and receptivity towards those interventions; based on our analyses, we speculate that being receptive to interventions helped participants achieve physical activity goals, which in turn motivated participants to be more receptive to future interventions. Finally, we build machine-learning models to detect receptivity, with up to a 77% increase in F1 score over a biased random classifier.},
	articleno    = 140,
	numpages     = 27,
	keywords     = {Receptivity, Interruption, Intervention, Mobile Health, Engagement}
}
@article{10.1145/3411828,
	title        = {VibroSense: Recognizing Home Activities by Deep Learning Subtle Vibrations on an Interior Surface of a House from a Single Point Using Laser Doppler Vibrometry},
	author       = {Sun, Wei and Chen, Tuochao and Zheng, Jiayi and Lei, Zhenyu and Wang, Lucy and Steeper, Benjamin and He, Peng and Dressa, Matthew and Tian, Feng and Zhang, Cheng},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411828},
	url          = {https://doi.org/10.1145/3411828},
	issue_date   = {September 2020},
	abstract     = {Smart homes of the future are envisioned to have the ability to recognize many types of home activities such as running a washing machine, flushing the toilet, and using a microwave. In this paper, we present a new sensing technology, VibroSense, which is able to recognize 18 different types of activities throughout a house by observing structural vibration patterns on a wall or ceiling using a laser Doppler vibrometer. The received vibration data is processed and sent to a deep neural network which is trained to distinguish between 18 activities. We conducted a system evaluation, where we collected data of 18 home activities in 5 different houses for 2 days in each house. The results demonstrated that our system can recognize 18 home activities with an average accuracy of up to 96.6%. After re-setup of the device on the second day, the average recognition accuracy decreased to 89.4%. We also conducted follow-up experiments, where we evaluated VibroSense under various scenarios to simulate real-world conditions. These included simulating online recognition, differentiating between specific stages of a device's activity, and testing the effects of shifting the laser's position during re-setup. Based on these results, we discuss the opportunities and challenges of applying VibroSense in real-world applications.},
	articleno    = 96,
	numpages     = 28,
	keywords     = {Home activity recognition, Structural vibration, Laser Doppler Vibrometry, Deep learning}
}
@article{10.1145/3397328,
	title        = {When Sharing Economy Meets IoT: Towards Fine-Grained Urban Air Quality Monitoring through Mobile Crowdsensing on Bike-Share System},
	author       = {Wu, Di and Xiao, Tao and Liao, Xuewen and Luo, Jie and Wu, Chao and Zhang, Shigeng and Li, Yong and Guo, Yike},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397328},
	url          = {https://doi.org/10.1145/3397328},
	issue_date   = {June 2020},
	abstract     = {Air pollution is a serious global issue impacting public health and social economy. In particular, exposure to small particulate matter of 2.5 microns or less in diameter (PM2.5) can cause cardiovascular and respiratory diseases, and cancer. Fine-grained urban air quality monitoring is crucial yet difficult to achieve. In this paper, we present the design, implementation, and evaluation of an ambient environment aware system, namely UbiAir, which can support fine-grained urban air quality monitoring through mobile crowdsensing on a bike-sharing system. We have built specific IoT box configured with multiple pollutant sensors and attached on shared bikes to sample micro-scale air quality data in the monitoring space that is split by a scalable grid structure. Both hardware and software data calibration methods are exploited in UbiAir to make the sampled data reliable. Then, we use Bayesian compressive sensing (BCS) as an inference model that leverages the calibrated samples to recover data points without direct measurements and reconstruct an accurate air quality map covering the entire monitoring space. In addition, red envelope based incentive schemes and differential rewarding strategies have been designed in UbiAir, and an adaptive BCS algorithm is proposed to deploy the red envelopes at the most informative positions to facilitate data sampling and inference. We have tested our system on campus with over 100k data measurements collected by 36 students through 18 days. Our real-world experiments show that UbiAir is a light-weight, low-cost, accurate and scalable system for fine-grained air quality monitoring, as compared with other solutions.},
	articleno    = 61,
	numpages     = 26,
	keywords     = {urban computing, sharing economy, Internet of things, mobile crowdsensing, air quality monitoring}
}
@article{10.1145/3397327,
	title        = {Foundations for Systematic Evaluation and Benchmarking of a Mobile Food Logger in a Large-Scale Nutrition Study},
	author       = {Jung, Jisu and Wellard-Cole, Lyndal and Cai, Colin and Koprinska, Irena and Yacef, Kalina and Allman-Farinelli, Margaret and Kay, Judy},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397327},
	url          = {https://doi.org/10.1145/3397327},
	issue_date   = {June 2020},
	abstract     = {Mobile food logging is important but people find it tedious and difficult to do. Our work tackles the challenging aspect of searching a large food database on a small mobile screen. We describe the design of the EaT (Eat and Track) app with its Search-Accelerator to support searches on &gt;6,000 foods. We designed a study to harness data from a large nutrition study to provide insights about the use and user experience of EaT. We report the results of our evaluation: a 12-participant lab study and a public health research field study where 1,027-participants entered their nutrition intake for 3 days, logging 30,715 food items. We also analysed 1,163 user-created food entries from 670 participants to gain insights about the causes of failures in the food search. Our core contributions are: 1) the design and evaluation of EaT's support for accurate and detailed food logging; 2) our study design that harnesses a nutrition research study to provide insights about timeliness of logging and the strengths and weaknesses of the search; 3) new performance benchmarks for mobile food logging.},
	articleno    = 47,
	numpages     = 25,
	keywords     = {Personal Informatics, Manual Tracking, Food Journaling, Health, Self-tracking, Self-monitoring, Quantified Self}
}
@article{10.1145/3381016,
	title        = {AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters},
	author       = {Wang, Xiyue and Takashima, Kazuki and Adachi, Tomoaki and Finn, Patrick and Sharlin, Ehud and Kitamura, Yoshifumi},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381016},
	url          = {https://doi.org/10.1145/3381016},
	issue_date   = {March 2020},
	abstract     = {Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children's stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child's playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children's stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children's mental health.},
	articleno    = 30,
	numpages     = 29,
	keywords     = {well being, PTSD, play, children, stress assessment, toy blocks, tangibles for health}
}
@article{10.1145/3381006,
	title        = {PMF: A Privacy-Preserving Human Mobility Prediction Framework via Federated Learning},
	author       = {Feng, Jie and Rong, Can and Sun, Funing and Guo, Diansheng and Li, Yong},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381006},
	url          = {https://doi.org/10.1145/3381006},
	issue_date   = {March 2020},
	abstract     = {With the popularity of mobile devices and location-based social network, understanding and modelling the human mobility becomes an important topic in the field of ubiquitous computing. With the model developing from personal models with own information to the joint models with population information, the prediction performance of proposed models become better and better. Meanwhile, the privacy issues of these models come into the view of community and the public: collecting and uploading private data to the centralized server without enough regulation. In this paper, we propose PMF, a privacy-preserving mobility prediction framework via federated learning, to solve this problem without significantly sacrificing the prediction performance. In our framework, based on the deep learning mobility model, no private data is uploaded into the centralized server and the only uploaded thing is the updated model parameters which are difficult to crack and thus more secure. Furthermore, we design a group optimization method for the training on local devices to achieve better trade-off between performance and privacy. Finally, we propose a fine-tuned personal adaptor for personal modelling to further improve the prediction performance. We conduct extensive experiments on three real-life mobility datasets to demonstrate the superiority and effectiveness of our methods in privacy protection settings.},
	articleno    = 10,
	numpages     = 21,
	keywords     = {Privacy-preserving system, Mobility prediction}
}
@article{10.1145/3381000,
	title        = {A Multi-Perspective Analysis of Social Context and Personal Factors in Office Settings for the Design of an Effective Mobile Notification System},
	author       = {Cavdar, Seyma Kucukozer and Taskaya-Temizel, Tugba and Musolesi, Mirco and Tino, Peter},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381000},
	url          = {https://doi.org/10.1145/3381000},
	issue_date   = {March 2020},
	abstract     = {In this study, we investigate the effects of social context, personal and mobile phone usage on the inference of work engagement/challenge levels of knowledge workers and their responsiveness to well-being related notifications. Our results show that mobile application usage is associated to the responsiveness and work engagement/challenge levels of knowledge workers. We also developed multi-level (within- and between-subjects) models for the inference of attentional states and engagement/challenge levels with mobile application usage indicators as inputs, such as the number of applications used prior to notifications, the number of switches between applications, and application category usage. The results of our analysis show that the following features are effective for the inference of attentional states and engagement/challenge levels: the number of switches between mobile applications in the last 45 minutes and the duration of application usage in the last 5 minutes before users' response to ESM messages.},
	articleno    = 15,
	numpages     = 38,
	keywords     = {personal factors, social context, attentional states, notifications, responsiveness}
}
@article{10.1145/3380985,
	title        = {A Systematic Study of Unsupervised Domain Adaptation for Robust Human-Activity Recognition},
	author       = {Chang, Youngjae and Mathur, Akhil and Isopoussu, Anton and Song, Junehwa and Kawsar, Fahim},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380985},
	url          = {https://doi.org/10.1145/3380985},
	issue_date   = {March 2020},
	abstract     = {Wearable sensors are increasingly becoming the primary interface for monitoring human activities. However, in order to scale human activity recognition (HAR) using wearable sensors to million of users and devices, it is imperative that HAR computational models are robust against real-world heterogeneity in inertial sensor data. In this paper, we study the problem of wearing diversity which pertains to the placement of the wearable sensor on the human body, and demonstrate that even state-of-the-art deep learning models are not robust against these factors. The core contribution of the paper lies in presenting a first-of-its-kind in-depth study of unsupervised domain adaptation (UDA) algorithms in the context of wearing diversity -- we develop and evaluate three adaptation techniques on four HAR datasets to evaluate their relative performance towards addressing the issue of wearing diversity. More importantly, we also do a careful analysis to learn the downsides of each UDA algorithm and uncover several implicit data-related assumptions without which these algorithms suffer a major degradation in accuracy. Taken together, our experimental findings caution against using UDA as a silver bullet for adapting HAR models to new domains, and serve as practical guidelines for HAR practitioners as well as pave the way for future research on domain adaptation in HAR.},
	articleno    = 39,
	numpages     = 30,
	keywords     = {Human Activity Recognition, Wearing Diversity, Unsupervised Domain Adaptation}
}
@article{10.1145/3380978,
	title        = {Will Online Digital Footprints Reveal Your Relationship Status? An Empirical Study of Social Applications for Sexual-Minority Men},
	author       = {Wang, Jiangtao and Ma, Junyi and Wang, Yasha and Wang, Ning and Wang, Leye and Zhang, Daqing and Wang, Feng and Lv, Qin},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380978},
	url          = {https://doi.org/10.1145/3380978},
	issue_date   = {March 2020},
	abstract     = {With the increasing social acceptance and openness, more and more sexual-minority men (SMM) have succeeded in creating and sustaining steady relationships in recent years. Maintaining steady relationships is beneficial to the wellbeing of SMM both mentally and physically. However, the relationship maintaining for them is also challenging due to the much less supports compared to the heterosexual couples, so that it is important to identify those SMM in steady relationship and provide corresponding personalized assistance. Furthermore, knowing SMM's relationship and the correlations with other visible features is also beneficial for optimizing the social applications' functionalities in terms of privacy preserving and friends recommendation. With the prevalence of SMM-oriented social apps (called SMMSA for short), this paper investigates the relationship status of SMM from a new perspective, that is, by introducing the SMM's online digital footprints left on SMMSA (e.g., presented profile, social interactions, expressions, sentiment, and mobility trajectories). Specifically, using a filtered dataset containing 2,359 active SMMSA users with their self-reported relationship status and publicly available app usage data, we explore the correlations between SMM's relationship status and their online digital footprints on SMMSA and present a set of interesting findings. Moreover, we demonstrate that by utilizing such correlations, it has the potential to construct machine-learning-based models for relationship status inference. Finally, we elaborate on the implications of our findings from the perspective of better understanding the SMM community and improving their social welfare.},
	articleno    = 29,
	numpages     = 23,
	keywords     = {online digital footprints, relationship status, Sexual-minority men}
}
@article{10.1145/3351283,
	title        = {CellTrans: Private Car or Public Transportation? Infer Users' Main Transportation Modes at Urban Scale with Cellular Data},
	author       = {Zhao, Yi and Wang, Xu and Li, Jianbo and Zhang, Desheng and Yang, Zheng},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351283},
	url          = {https://doi.org/10.1145/3351283},
	issue_date   = {September 2019},
	abstract     = {Understanding citizens' main transportation modes at urban scale is beneficial to a range of applications, such as urban planning, user profiling, transportation management, and precision marketing. Previous methods on mode inference are mostly focused on utilizing GPS data with high spatiotemporal granularity. However, due to high costs of GPS data collection, the previous work typically is in small scales. In contrast, the cellular data logging interactions between cellphone users and cell towers cover much higher population given the ubiquity of cellphones. Nevertheless, utilizing cellular data introduces new challenges given their low spatiotemporal granularity compared to GPS data. In this paper, we design CellTrans, a novel framework to survey users' main transportation modes (public transportation or private car) at urban scale with cellular data. CellTrans extracts various mobility features that are pertinent to users' main transportation modes and presents solutions for different application scenarios including when there are no labeled users in the studied cities. We evaluate CellTrans on two real-world large-scale cellular datasets covering 3 million users, among which 2,589 users are with labels. We assess our method not only quantitatively with labeled users, but also qualitatively with the whole population. The experiments show that CellTrans infers users' main transportation modes with accuracy over 80% (with a performance gain of 20% compared to state-of-the-art), and CellTrans remains effective when applied at urban scale to the whole population.},
	articleno    = 125,
	numpages     = 26,
	keywords     = {main transportation mode, cellular networks, human mobility}
}
@article{10.1145/3351282,
	title        = {Route Prediction for Instant Delivery},
	author       = {Zhang, Yan and Liu, Yunhuai and Li, Genjian and Ding, Yi and Chen, Ning and Zhang, Hao and He, Tian and Zhang, Desheng},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351282},
	url          = {https://doi.org/10.1145/3351282},
	issue_date   = {September 2019},
	abstract     = {Instant delivery has drawn much attention recently, as it greatly facilitates people's daily lives. Unlike postal services, instant delivery imposes a strict deadline on couriers after a customer places an order online. Therefore it is critical to dispatch the order to an appropriate courier to guarantee the timely delivery. Ideally couriers should choose the optimal routes with the lowest overdue rate (i.e., the rate of the deliveries that are not finished in time) and the minimal distance. In practice, however, decision-making of the couriers is quite complex because individuals have different psychological perception of the environments (e.g., distance) and delivery requirements (e.g., deadline). To well predict their behaviors, we design multiple features to model the decision-making psychology of individual couriers and predict couriers' route with a machine learning algorithm. In particular, we reveal that perceived distance is the main factor influencing couriers' decision, which should be modeled based on the subjective understanding of the actual distances. Our design is implemented, deployed and evaluated on Ele.me, which is one of the largest instant delivery platforms in the world. Experimental results show that the overdue rate can be reduced by 48.02%, which is a significant improvement.},
	articleno    = 124,
	numpages     = 25,
	keywords     = {route prediction, perceived distance, machine learning, instant delivery}
}
@article{10.1145/3351281,
	title        = {PDMove: Towards Passive Medication Adherence Monitoring of Parkinson's Disease Using Smartphone-Based Gait Assessment},
	author       = {Zhang, Hanbin and Xu, Chenhan and Li, Huining and Rathore, Aditya Singh and Song, Chen and Yan, Zhisheng and Li, Dongmei and Lin, Feng and Wang, Kun and Xu, Wenyao},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351281},
	url          = {https://doi.org/10.1145/3351281},
	issue_date   = {September 2019},
	abstract     = {The medicine adherence in Parkinson's disease (PD) treatment has attracted tremendous attention due to the critical consequences it can lead to otherwise. As a result, clinics need to ensure that the medicine intake is performed on time. Existing approaches, such as self-report, family reminder, and pill counts, heavily rely on the patients themselves to log the medicine intake (hereafter, patient involvement). Unfortunately, PD patients usually suffer from impaired cognition or memory loss, which leads to the so-called medication non-adherence, including missed doses, extra doses, and mistimed doses. These instances can nullify the treatment or even harm the patients. In this paper, we present PDMove, a smartphone-based passive sensing system to facilitate medication adherence monitoring without the need for patient involvement. Specifically, PDMove builds on the fact that PD patients will present gait abnormality if they do not follow medication treatment. To begin with, PDMove passively collects gait data while putting the smartphone in the pocket. Afterward, the gait preprocessor helps extract gait cycle containing the Parkinsonism-related biomarkers. Finally, the medicine intake detector consisting of a multi-view convolutional neural network predicts the medicine intake. In this way, PDMove enables the medication adherence monitoring. To evaluate PDMove, we enroll 247 participants with PD and collect more than 100,000 gait cycle samples. Our results show that smartphone-based gait assessment is a feasible approach to the AI-care strategy to monitor the medication adherence of PD patients.},
	articleno    = 123,
	numpages     = 23,
	keywords     = {Medication Adherence Monitoring, Parkinson's Disease, Gait Analysis, Mobile Health}
}
@article{10.1145/3351273,
	title        = {AcousticID: Gait-Based Human Identification Using Acoustic Signal},
	author       = {Xu, Wei and Yu, ZhiWen and Wang, Zhu and Guo, Bin and Han, Qi},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351273},
	url          = {https://doi.org/10.1145/3351273},
	issue_date   = {September 2019},
	abstract     = {Human identification plays an important role in our daily lives. Previous studies have successfully used characteristics such as fingerprints, irises, and facial features for identity recognition. However, these methods require the user being close to the sensing device, which may cause inconvenience to users. In this paper, we present AcousticID, a system that uses fine-grained gait information derived from acoustic signals generated by Commercial Off-The-Shelf devices to identify human beings. We demonstrate the feasibility of gait recognition by analyzing the Doppler effect of various body parts on acoustic signals while walking, and then extract fine-grained gait features that can distinguish different people from both macro and micro dimensions. Similar to an access control system in the home or office, AcousticID is a convenient, low cost, and universal solution. We evaluate AcousticID using experiments with 50 volunteers in an area of 60 m2, and the results show that it can identify different persons with an average accuracy of 96.6%.},
	articleno    = 115,
	numpages     = 25,
	keywords     = {Gait analysis, Device-free sensing, Acoustic sensing, Human identification}
}
@article{10.1145/3351267,
	title        = {FlexTouch: Enabling Large-Scale Interaction Sensing Beyond Touchscreens Using Flexible and Conductive Materials},
	author       = {Wang, Yuntao and Zhou, Jianyu and Li, Hanchuan and Zhang, Tengxiang and Gao, Minxuan and Cheng, Zhuolin and Yu, Chun and Patel, Shwetak and Shi, Yuanchun},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351267},
	url          = {https://doi.org/10.1145/3351267},
	issue_date   = {September 2019},
	abstract     = {In this paper, we present FlexTouch, a technique that enables large-scale interaction sensing beyond the spatial constraints of capacitive touchscreens using passive low-cost conductive materials. This is achieved by customizing 2D circuit-like patterns with an array of conductive strips that can be easily attached to the sensing nodes on the edge of the touchscreen. FlexTouch requires no hardware modification, and is compatible with various conductive materials (copper foil tape, silver nanoparticle ink, ITO frames, and carbon paint), as well as fabrication methods (cutting, coating, and ink-jet printing). Through a series of studies and illustrative examples, we demonstrate that FlexTouch can support long-range touch sensing for up to 4 meters and everyday object presence detection for up to 2 meters. Finally, we show the versatility and feasibility of FlexTouch through applications such as body posture recognition, human-object interaction as well as enhanced fitness training experiences.},
	articleno    = 109,
	numpages     = 20,
	keywords     = {touch interface, large-scale interaction, fabrication, posture detection, Capacitive sensing}
}
@article{10.1145/3351255,
	title        = {I3: Sensing Scrolling Human-Computer Interactions for Intelligent Interest Inference on Smartphones},
	author       = {Lu, Li and Yu, Jiadi and Chen, Yingying and Zhu, Yanmin and Li, Minglu and Xu, Xiangyu},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351255},
	url          = {https://doi.org/10.1145/3351255},
	issue_date   = {September 2019},
	abstract     = {The scrolling interaction is a pervasive human-computer interaction on smartphones, which can reflect intrinsic characteristics during dynamic browsings. Different from extrinsic statistical measures like frequency of visits and dwell time, intrinsic features underlying scrolling interactions reveal fine-grained implicit feedbacks about user interests. Toward this end, we explore user interest inference by extracting efficient browsing features from scrolling human-computer interactions on smartphones. In this paper, we first analyze browsing traces of 40 volunteers, and find two intrinsic browsing features underlying scrolling interactions, i.e., browsing velocity stability and browsing velocity sequence, which are tightly related to user interests. Inspired by the observation, we propose an Intelligent Interest Inference system, I3, which infers user interests through sensing scrolling interactions during browsings. Specifically, I3 first extracts the two intrinsic browsing features from users' scrolling interactions. Then, I3 applies a Naive Bayesian-based approach to construct an interest discriminator for coarse-grained user interest (i.e., like-preferred or dislike-preferred) inference. Furthermore, we develop a deep learning-based approach for I3 to train rating classifiers for fine-grained rating inference in like-preferred and dislike-preferred browsings respectively. Finally, I3 utilizes the interest discriminator and rating classifiers to infer exact user ratings about browsing contents on smartphones. Experimental results under browsing traces of 46 volunteers present that I3 achieves 92.4% overall accuracy in interest inference.},
	articleno    = 97,
	numpages     = 22,
	keywords     = {scrolling interactions, recommendation, Interest inference, smartphone}
}
@article{10.1145/3351253,
	title        = {FMT: A Wearable Camera-Based Object Tracking Memory Aid for Older Adults},
	author       = {Li, Franklin Mingzhe and Chen, Di Laura and Fan, Mingming and Truong, Khai N.},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351253},
	url          = {https://doi.org/10.1145/3351253},
	issue_date   = {September 2019},
	abstract     = {Older adults sometimes forget about whether or not they have completed routine actions and the states of objects that they have interacted with (e.g., the kitchen stove is on or off). In this work, we explore whether video clips captured from a body-worn camera every time objects of interest are found within its field of view can help older adults determine if they have completed certain actions with these objects and what their states are. We designed FMT ("Fiducial Marker Tracker")---a real-time capture and access application that opportunistically captures video clips of objects the user interacts with. To do this, the user places fiducial markers close to objects which would be captured when the marker enters the user's body-worn camera's field of view. We examine and discuss what objects this system would be best suited to track, and the usefulness and usability of this approach. FMT successfully captured direct interactions with an object at an average rate of 75.6% across all participants (SD = 9.9%). Our results also reveal how, what, and why users would use such a system for help.},
	articleno    = 95,
	numpages     = 25,
	keywords     = {technology probe, wearable camera, Memory aid, older adults, object tracking}
}
@article{10.1145/3351249,
	title        = {Micro-Stress EMA: A Passive Sensing Framework for Detecting in-the-Wild Stress in Pregnant Mothers},
	author       = {King, Zachary D. and Moskowitz, Judith and Egilmez, Begum and Zhang, Shibo and Zhang, Lida and Bass, Michael and Rogers, John and Ghaffari, Roozbeh and Wakschlag, Laurie and Alshurafa, Nabil},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351249},
	url          = {https://doi.org/10.1145/3351249},
	issue_date   = {September 2019},
	abstract     = {High levels of stress during pregnancy increase the chances of having a premature or low-birthweight baby. Perceived self-reported stress does not often capture or align with the physiological and behavioral response. But what if there was a self-report measure that could better capture the physiological response? Current perceived stress self-report assessments require users to answer multi-item scales at different time points of the day. Reducing it to one question, using microinteraction-based ecological momentary assessment (micro-EMA, collecting a single in situ self-report to assess behaviors) allows us to identify smaller or more subtle changes in physiology. It also allows for more frequent responses to capture perceived stress while at the same time reducing burden on the participant. We propose a framework for selecting the optimal micro-EMA that combines unbiased feature selection and unsupervised Agglomerative clustering. We test our framework in 18 women performing 16 activities in-lab wearing a Biostamp, a NeuLog, and a Polar chest strap. We validated our results in 17 pregnant women in real-world settings. Our framework shows that the question "How worried were you?" results in the highest accuracy when using a physiological model. Our results provide further in-depth exposure to the challenges of evaluating stress models in real-world situations.},
	articleno    = 91,
	numpages     = 22
}
@article{10.1145/3328919,
	title        = {Au-Id: Automatic User Identification and Authentication through the Motions Captured from Sequential Human Activities Using RFID},
	author       = {Huang, Anna and Wang, Dong and Zhao, Run and Zhang, Qian},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328919},
	url          = {https://doi.org/10.1145/3328919},
	issue_date   = {June 2019},
	abstract     = {The advancements of ambient intelligence and ubiquitous computing are driving the unprecedented development of smart spaces where enhanced services are provided based on activity recognition. Meanwhile, user identification, which can enable the personalization of the enhanced services for specific users and the access control of confidential information, becomes increasingly important. Traditional approaches to user identification require either attached wearable sensors or active user participation. This paper presents Au-Id, a non-intrusive automatic user identification and authentication system through human motions captured from their daily activities based on RFID. The key insight is that the RFID tag array can capture human's physical and behavioral characteristics for user identification. Particularly, phase and RSSI data streams of the RFID tag array are fused to incorporate the information from time, space and modality dimensions. Based on this, a novel sequence labeling based segmentation method is proposed for target motion extraction. Then Au-Id leverages a multi-modal Convolutional Neural Network (CNN) for user identification and significantly reduces the training efforts by transfer learning. In addition, Au-Id facilitates user authentication by integrating the feature representations extracted by CNN with one-class SVM classifiers. The evaluation shows that Au-Id can achieve accurate and robust user identification and authentication.},
	articleno    = 48,
	numpages     = 26,
	keywords     = {user identification, deep learning, transfer learning, user authentication, RFID}
}
@article{10.1145/3328916,
	title        = {Keyboard Snooping from Mobile Phone Arrays with Mixed Convolutional and Recurrent Neural Networks},
	author       = {Giallanza, Tyler and Siems, Travis and Smith, Elena and Gabrielsen, Erik and Johnson, Ian and Thornton, Mitchell A. and Larson, Eric C.},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328916},
	url          = {https://doi.org/10.1145/3328916},
	issue_date   = {June 2019},
	abstract     = {The ubiquity of modern smartphones, because they are equipped with a wide range of sensors, poses a potential security risk---malicious actors could utilize these sensors to detect private information such as the keystrokes a user enters on a nearby keyboard. Existing studies have examined the ability of phones to predict typing on a nearby keyboard but are limited by the realism of collected typing data, the expressiveness of employed prediction models, and are typically conducted in a relatively noise-free environment. We investigate the capability of mobile phone sensor arrays (using audio and motion sensor data) for classifying keystrokes that occur on a keyboard in proximity to phones around a table, as would be common in a meeting. We develop a system of mixed convolutional and recurrent neural networks and deploy the system in a human subjects experiment with 20 users typing naturally while talking. Using leave-one-user-out cross validation, we find that mobile phone arrays have the ability to detect 41.8% of keystrokes and 27% of typed words correctly in such a noisy environment---even without user specific training. To investigate the potential threat of this attack, we further developed the machine learning models into a realtime system capable of discerning keystrokes from an array of mobile phones and evaluated the system's ability with a single user typing in varying conditions. We conclude that, in order to launch a successful attack, the attacker would need advanced knowledge of the table from which a user types, and the style of keyboard on which a user types. These constraints greatly limit the feasibility of such an attack to highly capable attackers and we therefore conclude threat level of this attack to be low, but non-zero.},
	articleno    = 45,
	numpages     = 22,
	keywords     = {Security, Machine Learning, Keyboard Snooping}
}
@article{10.1145/3314415,
	title        = {LeakDoctor: Toward Automatically Diagnosing Privacy Leaks in Mobile Applications},
	author       = {Wang, Xiaolei and Continella, Andrea and Yang, Yuexiang and He, Yongzhong and Zhu, Sencun},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314415},
	url          = {https://doi.org/10.1145/3314415},
	issue_date   = {March 2019},
	abstract     = {With the enormous popularity of smartphones, millions of mobile apps are developed to provide rich functionalities for users by accessing certain personal data, leading to great privacy concerns. To address this problem, many approaches have been proposed to detect privacy disclosures in mobile apps, but they largely fail to automatically determine whether the privacy disclosures are necessary for the functionality of apps. As a result, security analysts may easily face with a large number of false positives when directly adopting such approaches for app analysis. In this paper, we propose LeakDoctor, an analysis system seeking to automatically diagnose privacy leaks by judging if a privacy disclosure from an app is necessary for some functionality of the app. Functionality-irrelevant privacy disclosures are not justifiable, so considered as potential privacy leak cases. To achieve this goal, LeakDoctor integrates dynamic response differential analysis with static response taint analysis. In addition, it employs a novel technique to locate the program statements of each privacy disclosure. We implement a prototype of LeakDoctor and evaluate it against 1060 apps, which contain 2,095 known disclosure cases. Our experimental results show that LeakDoctor can automatically determine that 71.9% of the privacy disclosure cases indeed serve apps' functionalities and are justifiable. Hence, with the diagnosis results of LeakDoctor, analysts may avoid analyzing many justifiable privacy disclosures and only focus on the those unjustifiable cases.},
	articleno    = 28,
	numpages     = 25,
	keywords     = {Taint Flow Analysis, Privacy Leak, Differential Analysis, Mobile Applications}
}
@article{10.1145/3314403,
	title        = {GoalKeeper: Exploring Interaction Lockout Mechanisms for Regulating Smartphone Use},
	author       = {Kim, Jaejeung and Jung, Hayoung and Ko, Minsam and Lee, Uichin},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314403},
	url          = {https://doi.org/10.1145/3314403},
	issue_date   = {March 2019},
	abstract     = {Many people often experience difficulties in achieving behavioral goals related to smartphone use. Most of prior studies approached this problem with various behavior change strategies such as self-reflection and social support. However, little is known about the effectiveness and user experiences of restrictive and coercive interventions such as blocking. In this work, we developed "GoalKeeper," a smartphone intervention app that locks the user into the self-defined daily use time limit with restrictive intervention mechanisms. We conducted a four-week field experiment with 36 participants to investigate the effects and user experiences of varying intensities of restrictive interventions. The results showed that restrictive mechanisms are more effective than non-restrictive mechanisms such as warning. However, we found that restrictive mechanisms caused more frustration and pressure to the users, mainly due to diversity of usage contexts and needs. Based on our study results, we extracted practical implications for designing restrictive mechanisms that balance the intervention effectiveness for behavioral changes and the flexibility for user acceptability.},
	articleno    = 16,
	numpages     = 29,
	keywords     = {commitment device, lockout mechanism, lockout intensity, restrictive technology, persuasive technology, smartphone non-use, behavior change, self-imposed restriction, ILM, interaction lockout}
}
@article{10.1145/3314402,
	title        = {A Deep Reinforcement Learning-Enabled Dynamic Redeployment System for Mobile Ambulances},
	author       = {Ji, Shenggong and Zheng, Yu and Wang, Zhaoyuan and Li, Tianrui},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314402},
	url          = {https://doi.org/10.1145/3314402},
	issue_date   = {March 2019},
	abstract     = {Protecting citizens' lives from emergent accidents (e.g. traffic accidents) and diseases (e.g. heart attack) is of vital importance in urban computing. Every day many people are caught in emergent accidents or diseases and thus need ambulances to transport them to hospitals. In this paper, we propose a dynamic ambulance redeployment system to reduce the time needed for ambulances to pick up patients and to increase the probability of patients being saved in time. For patients in danger, every second counts. Specifically, whenever there is an ambulance becoming available (e.g. finishing transporting a patient to a hospital), our dynamic ambulance redeployment system will redeploy it to a proper ambulance station such that it can better pick up future patients. However, the dynamic ambulance redeployment is challenging, as when we redeploy an available ambulance we need to simultaneously consider each station's multiple dynamic factors. To trade off these multiple factors using handcrafted rules are almost impossible. To deal with this issue, we propose using a deep neural network, called deep score network, to balance each station's dynamic factors into one score, leveraging the excellent representation ability of deep neural networks. And then we propose a deep reinforcement learning framework to learn the deep score network. Finally, based on the learned deep score network, we provide an effective dynamic ambulance redeployment algorithm. Experiment results using data collected in real world show clear advantages of our method over baselines, e.g. comparing with baselines, our method can save ~100 seconds (~20%) of average pickup time of patients and improve the ratio of patients being picked up within 10 minutes from 0.786 to 0.838. With our method, people in danger can be better saved.},
	articleno    = 15,
	numpages     = 20,
	keywords     = {Mobile computing, deep score network, dynamic redeployment system for mobile ambulances, deep reinforcement learning, urban computing}
}
@article{10.1145/3314399,
	title        = {Towards Reliable, Automated General Movement Assessment for Perinatal Stroke Screening in Infants Using Wearable Accelerometers},
	author       = {Gao, Yan and Long, Yang and Guan, Yu and Basu, Anna and Baggaley, Jessica and Ploetz, Thomas},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314399},
	url          = {https://doi.org/10.1145/3314399},
	issue_date   = {March 2019},
	abstract     = {Perinatal stroke (PS) is a serious condition that, if undetected and thus untreated, often leads to life-long disability, in particular Cerebral Palsy (CP). In clinical settings, Prechtl's General Movement Assessment (GMA) can be used to classify infant movements using a Gestalt approach, identifying infants at high risk of developing PS. Training and maintenance of assessment skills are essential and expensive for the correct use of GMA, yet many practitioners lack these skills, preventing larger-scale screening and leading to significant risks of missing opportunities for early detection and intervention for affected infants. We present an automated approach to GMA, based on body-worn accelerometers and a novel sensor data analysis method--Discriminative Pattern Discovery (DPD)--that is designed to cope with scenarios where only coarse annotations of data are available for model training. We demonstrate the effectiveness of our approach in a study with 34 newborns (21 typically developing infants and 13 PS infants with abnormal movements). Our method is able to correctly recognise the trials with abnormal movements with at least the accuracy that is required by newly trained human annotators (75%), which is encouraging towards our ultimate goal of an automated PS screening system that can be used population-wide.},
	articleno    = 12,
	numpages     = 22,
	keywords     = {Prechtl's General Movements Assessment, Human Activity Recognition, Perinatal Stroke, Health, Machine Learning, Wearables}
}
@article{10.1145/3314388,
	title        = {MORAL: An MHealth Model for Inferring Oral Hygiene Behaviors in-the-Wild Using Wrist-Worn Inertial Sensors},
	author       = {Akther, Sayma and Saleheen, Nazir and Samiei, Shahin Alan and Shetty, Vivek and Ertin, Emre and Kumar, Santosh},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314388},
	url          = {https://doi.org/10.1145/3314388},
	issue_date   = {March 2019},
	abstract     = {We address the open problem of reliably detecting oral health behaviors passively from wrist-worn inertial sensors. We present our model named mORAL (pronounced em oral) for detecting brushing and flossing behaviors, without the use of instrumented toothbrushes so that the model is applicable to brushing with still prevalent manual toothbrushes. We show that for detecting rare daily events such as toothbrushing, adopting a model that is based on identifying candidate windows based on events, rather than fixed-length timeblocks, leads to significantly higher performance. Trained and tested on 2,797 hours of sensor data collected over 192 days on 25 participants (using video annotations for ground truth labels), our brushing model achieves 100% median recall with a false positive rate of one event in every nine days of sensor wearing. The average error in estimating the start/end times of the detected event is 4.1% of the interval of the actual toothbrushing event.},
	articleno    = 1,
	numpages     = 25,
	keywords     = {flossing detection, hand-to-mouth gestures, mHealth, brushing detection}
}
@article{10.1145/3287078,
	title        = {Exploring Tangible Interactions with Radar Sensing},
	author       = {Yeo, Hui-Shyong and Minami, Ryosuke and Rodriguez, Kirill and Shaker, George and Quigley, Aaron},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287078},
	url          = {https://doi.org/10.1145/3287078},
	issue_date   = {December 2018},
	abstract     = {Research has explored miniature radar as a promising sensing technique for the recognition of gestures, objects, users' presence and activity. However, within Human-Computer Interaction (HCI), its use remains underexplored, in particular in Tangible User Interface (TUI). In this paper, we explore two research questions with radar as a platform for sensing tangible interaction with the counting, ordering, identification of objects and tracking the orientation, movement and distance of these objects. We detail the design space and practical use-cases for such interaction which allows us to identify a series of design patterns, beyond static interaction, which are continuous and dynamic. With a focus on planar objects, we report on a series of studies which demonstrate the suitability of this approach. This exploration is grounded in both a characterization of the radar sensing and our rigorous experiments which show that such sensing is accurate with minimal training. With these techniques, we envision both realistic and future applications and scenarios. The motivation for what we refer to as Solinteraction, is to demonstrate the potential for radar-based interaction with objects in HCI and TUI.},
	articleno    = 200,
	numpages     = 25,
	keywords     = {Tangible Interaction, Token+Constraint, Ubiquitous Computing, Radar Sensing, Machine Learning, Tangible User Interface, Context-Aware Interaction, Soli}
}
@article{10.1145/3287065,
	title        = {A Data-Driven Misbehavior Detection System for Connected Autonomous Vehicles},
	author       = {Sarker, Ankur and Shen, Haiying},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287065},
	url          = {https://doi.org/10.1145/3287065},
	issue_date   = {December 2018},
	abstract     = {In a Connected and Autonomous Vehicle (CAV) system, some malicious CAVs may send out false information in vehicle-to-vehicle communication to gain benefits or cause safety-related accidents. Previous false data detection methods are not sufficient to meet the accuracy and real-time requirements. In this paper, we propose a data-driven misbehavior detection system (MDS) (running by each CAV) that checks the consistency between the estimated and actually reported driving state (i.e., velocity, acceleration, brake status, steering angle) of an alerting CAV. First, MDS predicts the driving state using Gaussian mixture model based Mixture Density Network incorporating Recurrent Neural Network that can catch the driving behavior patterns of a CAV. Second, MDS extends the existing Krauss traffic flow model and uses it to consider the overall traffic flow of the road to make the predicted driving state more accurate. Finally, for a given received alert, a CAV validates the alert by checking the consistency between the predicted and actually reported driving states of the alerting CAV. We conduct extensive simulation studies based on a real driving dataset we collected from 29 participants and the Simulator for Urban MObility (SUMO) traffic simulator. The experimental results show that the false information detection rate of the proposed MDS is higher than other existing systems in different alert scenarios.},
	articleno    = 187,
	numpages     = 21,
	keywords     = {Mixture density network, Connected autonomous vehicles, Traffic flow model, Misbehavior detection system, Alert validations}
}
@article{10.1145/3287064,
	title        = {What Will You Do for the Rest of the Day? An Approach to Continuous Trajectory Prediction},
	author       = {Sadri, Amin and Salim, Flora D. and Ren, Yongli and Shao, Wei and Krumm, John C. and Mascolo, Cecilia},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287064},
	url          = {https://doi.org/10.1145/3287064},
	issue_date   = {December 2018},
	abstract     = {Understanding and predicting human mobility is vital to a large number of applications, ranging from recommendations to safety and urban service planning. In some travel applications, the ability to accurately predict the user's future trajectory is vital for delivering high quality of service. The accurate prediction of detailed trajectories would empower location-based service providers with the ability to deliver more precise recommendations to users. Existing work on human mobility prediction has mainly focused on the prediction of the next location (or the set of locations) visited by the user, rather than on the prediction of the continuous trajectory (sequences of further locations and the corresponding arrival and departure times). Furthermore, existing approaches often return predicted locations as regions with coarse granularity rather than geographical coordinates, which limits the practicality of the prediction.In this paper, we introduce a novel trajectory prediction problem: given historical data and a user's initial trajectory in the morning, can we predict the user's full trajectory later in the day (e.g. the afternoon trajectory)? The predicted continuous trajectory includes the sequence of future locations, the stay times, and the departure times. We first conduct a comprehensive analysis about the relationship between morning trajectories and the corresponding afternoon trajectories, and found there is a positive correlation between them. Our proposed method combines similarity metrics over the extracted temporal sequences of locations to estimate similar informative segments across user trajectories.Our evaluation shows results on both labeled and geographical trajectories with a prediction error reduced by 10-35% in comparison to the baselines. This improvement has the potential to enable precise location services, raising usefulness to users to unprecedented levels. We also present empirical evaluations with Markov model and Long Short Term Memory (LSTM), a state-of-the-art Recurrent Neural Network model. Our proposed method is shown to be more effective when smaller number of samples are used and is exponentially more efficient than LSTM.},
	articleno    = 186,
	numpages     = 26,
	keywords     = {Location-based services, Human daily trajectory, Trajectory prediction}
}
@article{10.1145/3287054,
	title        = {The Connected Shower: Studying Intimate Data in Everyday Life},
	author       = {Kwon, Hyosun and Fischer, Joel E. and Flintham, Martin and Colley, James},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287054},
	url          = {https://doi.org/10.1145/3287054},
	issue_date   = {December 2018},
	abstract     = {This paper presents the design and field study of the Connected Shower, a bespoke IoT device that captures water flow, temperature, shower-head movement, and shower product weight. We deployed the device in six UK homes for a week to understand the use of 'intimate data' as captured by IoT systems. Findings from our contextual interviews unpack a) how such intimate data is collaboratively made sense of by accounting for the social order of showering practices as part and parcel of everyday routines; b) how the data makes details of showering accountable to their partners; c) how people reason about sharing intimate data both with third parties and their partners. Our study shows that intimate data is not intimate per se, nor is intimacy a property of the data, but is an interactional outcome arising from the articulation of shower practices to their co-present partners. Thus, judgments as to whether the data is too sensitive, private, or intimate to share are contingent on situated sense-making and therefore subject to change; however, there was a general consensus that sharing intimate data with service providers was acceptable if the data was sufficiently abstract and anonymised. We discuss challenges in the design of trustworthy data-driven IoT systems, and how they need to be warranted to be both acceptable and adopted into our intimate practices.},
	articleno    = 176,
	numpages     = 22,
	keywords     = {Data Work, Intimacy, Internet of Things, Technology Probe, Privacy, Ubiquitous Computing, Intimate Data}
}
@article{10.1145/3287042,
	title        = {Fingers and Angles: Exploring the Comfort of Touch Input on Smartwatches},
	author       = {Gil, Hyunjae and Kim, Hongmin and Oakley, Ian},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287042},
	url          = {https://doi.org/10.1145/3287042},
	issue_date   = {December 2018},
	abstract     = {Smartwatches present a unique touch input context: small, fixed to one wrist and approachable from a limited range of angles by the touching hand. Techniques to expand their input expressivity often involve variations in how a watch must be touched, such as with different fingers, poses or from specific angles. While objective performance with such systems is commonly reported, subjective qualities such as comfort remain overlooked. We argue that techniques that involve uncomfortable input will be of limited value and contribute the first data on the comfort of input on smartwatches via two studies that combine subjective ratings of comfort with objective performance data. We examine both static and dynamic touches and three finger poses. Based on the study results, we contribute a set of design recommendations for comfortable, effective smartwatch input. We close by instantiating the recommendations in interface prototypes that we evaluate in a final qualitative study.},
	articleno    = 164,
	numpages     = 21,
	keywords     = {Smartwatch, Comfort, Angular Input, Touch Input}
}
@article{10.1145/3287035,
	title        = {Mobile Money: Understanding and Predicting Its Adoption and Use in a Developing Economy},
	author       = {Centellegher, Simone and Miritello, Giovanna and Villatoro, Daniel and Parameshwar, Devyani and Lepri, Bruno and Oliver, Nuria},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287035},
	url          = {https://doi.org/10.1145/3287035},
	issue_date   = {December 2018},
	abstract     = {Access to financial institutions is difficult in developing economies and especially for the poor. However, the widespread adoption of mobile phones has enabled the development of mobile money systems that deliver financial services through the mobile phone network. Despite the success of mobile money, there is a lack of quantitative studies that unveil which factors contribute to the adoption and sustained usage of such services. In this paper, we describe the results of a quantitative study that analyzes data from the world's leading mobile money service, M-Pesa. We analyzed millions of anonymized mobile phone communications and M-Pesa transactions in an African country. Our contributions are threefold: (1) we analyze the customers' usage of M-Pesa and report large-scale patterns of behavior; (2) we present the results of applying machine learning models to predict mobile money adoption (AUC=0.691), and mobile money spending (AUC=0.619) using multiple data sources: mobile phone data, M-Pesa agent information, the number of M-Pesa friends in the user's social network, and the characterization of the user's geographic location; (3) we discuss the most predictive features in both models and draw key implications for the design of mobile money services in a developing country. We find that the most predictive features are related to mobile phone activity, to the presence of M-Pesa users in a customer's ego-network and to mobility. We believe that our work will contribute to the understanding of the factors playing a role in the adoption and sustained usage of mobile money services in developing economies.},
	articleno    = 157,
	numpages     = 18,
	keywords     = {M-Pesa, Call Detail Records, Financial Inclusion, Mobile Money}
}
@article{10.1145/3264960,
	title        = {TeamSense: Assessing Personal Affect and Group Cohesion in Small Teams through Dyadic Interaction and Behavior Analysis with Wearable Sensors},
	author       = {Zhang, Yanxia and Olenick, Jeffrey and Chang, Chu-Hsiang and Kozlowski, Steve W. J. and Hung, Hayley},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264960},
	url          = {https://doi.org/10.1145/3264960},
	issue_date   = {September 2018},
	abstract     = {Continuous monitoring with unobtrusive wearable social sensors is becoming a popular method to assess individual affect states and team effectiveness in human research. A large number of applications have demonstrated the effectiveness of applying wearable sensing in corporate settings; for example, in short periodic social events or in a university campus. However, little is known of how we can automatically detect individual affect and group cohesion for long duration missions. Predicting negative affect states and low cohesiveness is vital for team missions. Knowing team members' negative states allows timely interventions to enhance their effectiveness. This work investigates whether sensing social interactions and individual behaviors with wearable sensors can provide insights into assessing individual affect states and group cohesion. We analyzed wearable sensor data from a team of six crew members who were deployed on a four-month simulation of a space exploration mission at a remote location. Our work proposes to recognize team members' affect states and group cohesion as a binary classification problem using novel behavior features that represent dyadic interaction and individual activities. Our method aggregates features from individual members into group levels to predict team cohesion. Our results show that the behavior features extracted from the wearable social sensors provide useful information in assessing personal affect and team cohesion. Group task cohesion can be predicted with a high performance of over 0.8 AUC. Our work demonstrates that we can extract social interactions from sensor data to predict group cohesion in longitudinal missions. We found that quantifying behavior patterns including dyadic interactions and face-to-face communications are important in assessing team process.},
	articleno    = 150,
	numpages     = 22,
	keywords     = {team cohesion, wearable social sensor, Behavior analysis, interaction, group dynamics}
}
@article{10.1145/3264956,
	title        = {SoberMotion: Leveraging the Force of Probation Officers to Reduce the Risk of DUI Recidivism},
	author       = {You, Chuang-Wen and Lin, Ya-Fang and Chuang, Yaliang and Lee, Ya-Han and Hsu, Pei-Yi and Lin, Shih-Yao and Chang, Chih-Chun and Chung, Yi-Ju and Chen, Yi-Ling and Huang, Ming-Chyi and Shen, Ping-Hsuan and Tseng, Hsin-Tung and Wang, Hao-Chuan},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264956},
	url          = {https://doi.org/10.1145/3264956},
	issue_date   = {September 2018},
	abstract     = {In this study, we sought to assist probation officers in their efforts to reduce the risk that offenders on probation would re-commit the offense of driving under the influence (DUI) of alcohol. We prototyped a support system called SoberMotion, in which a breathalyzer is connected to the offender's smart phone via Bluetooth. Development of the system was based on pilot interviews with probation officers, psychiatrists, and offenders aimed at identifying the challenges and opportunities associated with this type of technology-based support mechanism. A corresponding phone app logs alcohol use behavior and identifies situations in which offenders should evaluate their sobriety before operating a vehicle. We conducted a two-month field study involving eight DUI offenders on probation, with the aim of evaluating the feasibility of the SoberMotion system. Our results indicate that most of the participants who drank alcohol while following the program performed multiple alcohol screening tests before operating a vehicle in order to avoid re-committing DUI. Responses collected via qualitative interviews indicate that SoberMotion is an effective approach to extending the power of probation officers seeking to reduce the risk of DUI recidivism.},
	articleno    = 146,
	numpages     = 34,
	keywords     = {Therapeutic Jurisprudence (TJ), Alcohol Use Disorder, Mobile Support System, Driving Under the Influence (DUI)}
}
@article{10.1145/3264952,
	title        = {FocusVR: Effective 8 Usable VR Display Power Management},
	author       = {Wee, Tan Kiat and Cuervo, Eduardo and Balan, Rajesh},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264952},
	url          = {https://doi.org/10.1145/3264952},
	issue_date   = {September 2018},
	abstract     = {In this paper, we present the design and implementation of FocusVR, a system for effectively and efficiently reducing the power consumption of Virtual Reality (VR) devices by smartly dimming their displays. These devices are becoming increasingly common with large companies such as Facebook (Oculus Rift), and HTC and Valve (Vive), recently releasing high quality VR devices to the consumer market. However, these devices require increasingly higher screen resolutions and refresh rates to be effective, and this in turn, leads to high display power consumption costs. We show how the use of smart dimming techniques, vignettes and color mapping, can significantly reduce the power consumption of VR displays with minimal impact on usability. In particular, we describe the implementation of FocusVR in both Android and the Unity game engine and then present detailed measurement results across 3 different VR devices -- the Gear VR, the DK2, and the Vive. In addition, we present the results of 3 user studies, with 68 participants in total, that tested the usability of FocusVR. Overall, we show that FocusVR is able to save up to 80% of the display power and up to 50% of the overall system power, with negligible impact to usability.},
	articleno    = 142,
	numpages     = 25,
	keywords     = {Power Management, Tone Mapping, Mobile Games}
}
@article{10.1145/3264929,
	title        = {Handwritten Signature Verification Using Wrist-Worn Devices},
	author       = {Levy, Alona and Nassi, Ben and Elovici, Yuval and Shmueli, Erez},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264929},
	url          = {https://doi.org/10.1145/3264929},
	issue_date   = {September 2018},
	abstract     = {This paper suggests a novel verification system for handwritten signatures. The proposed system is based on capturing motion signals from the sensors of wrist-worn devices, such as smartwatches and fitness trackers, during the signing process, to train a machine learning classifier to determine whether a given signature is genuine or forged. Our system can be used to: (1) Verify signatures written on paper documents, such as checks, credit card receipts and vote by mail ballots. Unlike existing systems for signature verification, our system obtains a high degree of accuracy, without requiring an ad hoc digital signing device. (2) Authenticate a user of a secure system based on "who you are" traits. Unlike existing "motion-based" authentication methods that commonly rely on long-term user behavior, writing a signature is a relatively short-term process. In order to evaluate our system, we collected 1,980 genuine and forged signature recordings from 66 different subjects, captured using a smartwatch device. Applying our signature verification system on the collected dataset, we show that it significantly outperforms two other state-of-the-art systems, obtaining an EER of 2.36% and an AUC of 98.52%.},
	articleno    = 119,
	numpages     = 26,
	keywords     = {Machine Learning, Online Signature Verification, Wearables, Biometrics}
}
@article{10.1145/3264927,
	title        = {Beacon: Designing a Portable Device for Self-Administering a Measure of Critical Flicker Frequency},
	author       = {Karkar, Ravi and Kocielnik, Rafal and Zhang, Xiaoyi and Zia, Jasmine and Ioannou, George N. and Munson, Sean A. and Fogarty, James},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264927},
	url          = {https://doi.org/10.1145/3264927},
	issue_date   = {September 2018},
	abstract     = {Critical flicker frequency (CFF) is the minimum frequency at which a flickering light source appears fused to an observer. Measuring CFF can support early diagnosis of minimal hepatic encephalopathy (MHE), a condition affecting up to 80% of people with cirrhosis of the liver. However, adoption of CFF measurement in clinical practice has been hampered by the cost of a device for measuring CFF and the need for specialized training to administer the test. This paper presents Beacon, a portable, inexpensive device that enables people to measure their own critical flicker frequency. We adopt a mixed-methods approach to informing and evaluating the design of and potential opportunities for Beacon. We first report on a two-part formative study with 10 participants to evaluate the choice of certain parameters in the design of Beacon. We then report on a study of 41 healthy adults ranging from 18 to 99 years of age, finding that Beacon performs on par with Lafayette Flicker Fusion System, an established medical device, achieving a pearson correlation coefficient of 0.88. We finally report on a focus group with five hepatoligists who work with patients with cirrhosis of the liver, using our initial prototype development to examine their perspectives on potential opportunities and challenges in adoption of a device like Beacon. We discuss Beacon as an exploration of reframing critical flicker frequency measurement from a clinical screening tool into a self-administered self-tracking measure, thereby drawing upon and contributing to research in the health and personal informatics.},
	articleno    = 117,
	numpages     = 27,
	keywords     = {self-tracking, Critical flicker frequency, cirrhosis, hepatic encephalopathy}
}
@article{10.1145/3264925,
	title        = {CrowdProbe: Non-Invasive Crowd Monitoring with Wi-Fi Probe},
	author       = {Hong, Hande and De Silva, Girisha Durrel and Chan, Mun Choon},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264925},
	url          = {https://doi.org/10.1145/3264925},
	issue_date   = {September 2018},
	abstract     = {Devices with integrated Wi-Fi chips broadcast beacons for network connection management purposes. Such information can be captured with inexpensive monitors and used to extract user behavior. To understand the behavior of visitors, we deployed our passive monitoring system---CrowdProbe, in a multi-floor museum for six months. We used a Hidden Markov Models (HMM) based trajectory inference algorithm to infer crowd movement using more than 1.7 million opportunistically obtained probe request frames.However, as more devices adopt schemes to randomize their MAC addresses in the passive probe session to protect user privacy, it becomes more difficult to track crowd and understand their behavior. In this paper, we try to make use of historical transition probability to reason about the movement of those randomized devices with spatial and temporal constraints. With CrowdProbe, we are able to achieve sufficient accuracy to understand the movement of visitors carrying devices with randomized MAC addresses.},
	articleno    = 115,
	numpages     = 23,
	keywords     = {transition probability, Crowd movement, Passive tracking, randomization}
}
@article{10.1145/3264916,
	title        = {MultiCell: Urban Population Modeling Based on Multiple Cellphone Networks},
	author       = {Fang, Zhihan and Zhang, Fan and Yin, Ling and Zhang, Desheng},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264916},
	url          = {https://doi.org/10.1145/3264916},
	issue_date   = {September 2018},
	abstract     = {Exploring cellphone network data has been proved to be a very effective way to understand urban populations because of the high penetration rate of cellphones. However, the state-of-the-art population models driven by cellphone data are typically built upon single cellphone networks, assuming the users in a particular cellphone network used are representative of all residents in the studied city with multiple cellphone networks. This assumption usually does not hold in the real world due to strategic spatial coverages and business concentrations of cellphone companies, which lead to data biases, and thus overfitting of resultant population models. To address this issue, we design a model called MultiCell to model real-time urban populations from multiple cellphone networks with two novel techniques: (i) a network realignment technique to integrate individual cell-tower spatial distributions from multiple cellphone networks for finer granular population modeling; (ii) a data fusion technique based on cross-network training to design a population model based on multiple network data. We implement MultiCell in the Chinese city Shenzhen based on three cellphone networks with 10 million active users and their daily data records at 11 thousand cell towers. We evaluate MultiCell by comparing it to the state-of-the-art models driven by single cellphone networks, and the evaluation results show that MultiCell outperforms them by 27% in terms of accuracy. Finally, we cross-validate MultiCell with three transportation systems with more than 8 million passengers to investigate its performances.},
	articleno    = 106,
	numpages     = 25,
	keywords     = {data fusion, cellphone networks, data analysis}
}
@article{10.1145/3264907,
	title        = {ID'em: Inductive Sensing for Embedding and Extracting Information in Robust Materials},
	author       = {Chadalavada, Perumal Varun and Palaniappan, Goutham and Chandran, Vimal Kumar and Truong, Khai and Wigdor, Daniel},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264907},
	url          = {https://doi.org/10.1145/3264907},
	issue_date   = {September 2018},
	abstract     = {We present ID'em, a novel tagging and localization method that employs an array of Inductive Sensors to 'image' patterns of electrically conductive dots that are embedded underneath the surfaces of materials that cover the environments that we inhabit. ID'em addresses drawbacks found with existing tagging/localization technologies, while drawing on some of their attributes and strengths, thus creating a cost-effective, scalable system that is robust enough to be deployed pervasively. We present a detailed description of the system, applications that leverage ID'em's unique affordances, and address ID'em's strengths and limitations. With ID'em, we envision a future where the materials that we use to cover and build our everyday environments come imbued with information that can provide valuable context for rich, diverse interactions and capabilities.},
	articleno    = 97,
	numpages     = 28,
	keywords     = {Inductive Sensing, Smart Surfaces, Object Tagging, Indoor Localization, Object Identification}
}
@article{10.1145/3214290,
	title        = {Touch Sense: Touch Screen Based Mental Stress Sense},
	author       = {Zhang, Xiao and Lyu, Yongqiang and Luo, Xiaomin and Zhang, Jingyu and Yu, Chun and Yin, Hao and Shi, Yuanchun},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214290},
	url          = {https://doi.org/10.1145/3214290},
	issue_date   = {June 2018},
	abstract     = {Non-intrusive and sensitive measurement of users' stress is very crucial for computers to dynamically understand and respond to users' mental status while users are naturally interacting with them, such as context-aware reminding, smart assistant, health monitoring etc. Compared to others, physiological measures are known as more sensitive and reliable ones. However, most of the current physiological methods need explicit and obtrusive sensors that deter natural human-computer interactions. In this study, we propose a photoplethysmogram-based mental stress measuring method through infrared touchscreen, which, from the best of our knowledge, is the first to integrate with the touch modality to enable natural, sensitive and reliable measurement. We designed and conducted two user experiments with touch-and-hold mode and tap mode respectively. By using person-independent classifiers to handle the photoplethysmographic parameters, the results reached a 97% and 87% average stress recognition accuracy for static test and interaction test respectively.},
	articleno    = 87,
	numpages     = 18,
	keywords     = {infrared touch screen, photoplethysmogram, Mental stress}
}
@article{10.1145/3191787,
	title        = {IDial: Enabling a Virtual Dial Plate on the Hand Back for Around-Device Interaction},
	author       = {Zhang, Maotian and Dai, Qian and Yang, Panlong and Xiong, Jie and Tian, Chang and Xiang, Chaocan},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191787},
	url          = {https://doi.org/10.1145/3191787},
	issue_date   = {March 2018},
	abstract     = {Smart wearable devices have become pervasive and are playing a more important role in our everyday lives. However, the small screen size and very few buttons make the interaction and control cumbersome and inconvenient. Previous solutions to mitigate this problem either require extra dedicated hardware, or instrument the user's fingers with special purpose sensors, limiting their real-life applications. We present iDial, a novel real time approach that enables a virtual dial plate on the hand back, extending the interaction beyond the small screen of wearable devices. iDial only employs the already built-in microphone and motion sensors of the commercial-off-the-shelf (COTS) device to facilitate interactions between user and wearable, without any extra hardware involved. The key idea is to exploit the acoustic signatures extracted from passive subtle acoustic signals to accurately recognize the virtual keys input on the skin of the hand back. We innovatively locate the virtual keys on the 4 pieces of metacarpal bones to significantly reduce the possibility of casual inputs. iDial also takes advantages of the motion sensor fusion already available inside the wearable to achieve robustness against the ambient noise and human voices efficiently. We design and implement iDial on the Samsung Gear S2 smartwatch. Our extensive experiments show that iDial is able to achieve an average recognition accuracy of 96.7%, and maintain high accuracies across varying user behaviors and different environments. iDial achieves a below 0.5s end-to-end latency with all the computations and processes happening at the cheap commodity wearable.},
	articleno    = 55,
	numpages     = 20,
	keywords     = {virtual dial plate, Smartwatch, acoustic sensing, around-device interaction}
}
@article{10.1145/3191785,
	title        = {From Fresnel Diffraction Model to Fine-Grained Human Respiration Sensing with Commodity Wi-Fi Devices},
	author       = {Zhang, Fusang and Zhang, Daqing and Xiong, Jie and Wang, Hao and Niu, Kai and Jin, Beihong and Wang, Yuxiang},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191785},
	url          = {https://doi.org/10.1145/3191785},
	issue_date   = {March 2018},
	abstract     = {Non-intrusive respiration sensing without any device attached to the target plays a particular important role in our everyday lives. However, existing solutions either require dedicated hardware or employ special-purpose signals which are not cost-effective, significantly limiting their real-life applications. Also very few work concerns about the theory behind and can explain the large performance variations in different scenarios. In this paper, we employ the cheap commodity Wi-Fi hardware already ubiquitously deployed around us for respiration sensing. For the first time, we utilize the Fresnel diffraction model to accurately quantify the relationship between the diffraction gain and human target's subtle chest displacement and thus successfully turn the previously considered "destructive" obstruction diffraction in the First Fresnel Zone (FFZ) into beneficial sensing capability. By not just considering the chest displacement at the frontside as the existing solutions, but also the subtle displacement at the backside, we achieve surprisingly matching results with respect to the theoretical plots and become the first to clearly explain the theory behind the performance distinction between lying and sitting for respiration sensing. With two cheap commodity Wi-Fi cards each equipped with just one antenna, we are able to achieve higher than 98% accuracy of respiration rate monitoring at more than 60% of the locations in the FFZ. Furthermore, we are able to present the detail heatmap of the sensing capability at each location inside the FFZ to guide the respiration sensing so users clearly know where are the good positions for respiration monitoring and if located at a bad position, how to move just slightly to reach a good position.},
	articleno    = 53,
	numpages     = 23,
	keywords     = {Wi-Fi, Human respiration sensing, Fresnel diffraction model, Wireless sensing}
}
@article{10.1145/3191775,
	title        = {Tracking Depression Dynamics in College Students Using Mobile Phone and Wearable Sensing},
	author       = {Wang, Rui and Wang, Weichen and daSilva, Alex and Huckins, Jeremy F. and Kelley, William M. and Heatherton, Todd F. and Campbell, Andrew T.},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191775},
	url          = {https://doi.org/10.1145/3191775},
	issue_date   = {March 2018},
	abstract     = {There are rising rates of depression on college campuses. Mental health services on our campuses are working at full stretch. In response researchers have proposed using mobile sensing for continuous mental health assessment. Existing work on understanding the relationship between mobile sensing and depression, however, focuses on generic behavioral features that do not map to major depressive disorder symptoms defined in the standard mental disorders diagnostic manual (DSM-5). We propose a new approach to predicting depression using passive sensing data from students' smartphones and wearables. We propose a set of symptom features that proxy the DSM-5 defined depression symptoms specifically designed for college students. We present results from a study of 83 undergraduate students at Dartmouth College across two 9-week terms during the winter and spring terms in 2016. We identify a number of important new associations between symptom features and student self reported PHQ-8 and PHQ-4 depression scores. The study captures depression dynamics of the students at the beginning and end of term using a pre-post PHQ-8 and week by week changes using a weekly administered PHQ-4. Importantly, we show that symptom features derived from phone and wearable sensors can predict whether or not a student is depressed on a week by week basis with 81.5% recall and 69.1% precision.},
	articleno    = 43,
	numpages     = 26,
	keywords     = {Mobile Sensing, Mental Health, Depression}
}
@article{10.1145/3191774,
	title        = {SpiderWalk: Circumstance-Aware Transportation Activity Detection Using a Novel Contact Vibration Sensor},
	author       = {Wang, Liang and Cheng, Wen and Pan, Lijia and Gu, Tao and Wu, Tianheng and Tao, Xianping and Lu, Jian},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191774},
	url          = {https://doi.org/10.1145/3191774},
	issue_date   = {March 2018},
	abstract     = {This paper presents the design and implementation of the SpiderWalk system for circumstance-aware transportation activity detection using a novel contact vibration sensor. Different from existing systems that only report the type of activity, our system detects not only the activity but also its circumstances (e.g., road surface, vehicle, and shoe types) to provide better support for applications such as activity logging, location tracking, and smart persuasive applications. Inspired by but different from existing audio-based context detection approaches using microphones, the SpiderWalk system is designed and implemented using an ultra-sensitive, flexible contact vibration sensor which mimics the spiders' sensory slit organs. By sensing vibration patterns from the soles of shoes, the system can accurately detect transportation activities with rich circumstance information while resisting undesirable external signals from other sources or speech that may cause the data assignment and privacy preserving issues. Moreover, our system is implemented by reusing existing audio devices and can be used by an unmodified smartphone, making it ready for large-scale deployments. Finally, a novel temporal and spatial correlated classification approach is proposed to accurately detect the complex combinations of transportation activities and circumstances based on the output of each individual classifiers. Experiments conducted on a real-world data set suggest our system can accurately detect different transportation activities and their circumstances with an average detection accuracy of 93.8% with resource overheads comparable to existing audio- and GPS-based systems.},
	articleno    = 42,
	numpages     = 30,
	keywords     = {Crack-resistance Sensor, Circumstance-aware, Transportation}
}
@article{10.1145/3191769,
	title        = {Defining Adherence: Making Sense of Physical Activity Tracker Data},
	author       = {Tang, Lie Ming and Meyer, Jochen and Epstein, Daniel A. and Bragg, Kevin and Engelen, Lina and Bauman, Adrian and Kay, Judy},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191769},
	url          = {https://doi.org/10.1145/3191769},
	issue_date   = {March 2018},
	abstract     = {Increasingly, people are collecting detailed personal activity data from commercial trackers. Such data should be able to give important insights about their activity levels. However, people do not wear or carry tracking devices all day, every day and this means that tracker data is typically incomplete. This paper aims to provide a systematic way to take account of this incompleteness, by defining adherence, a measure of data completeness, based on how much people wore their tracker. We show the impact of different adherence definitions on 12 diverse datasets, for 753 users, with over 77,000 days with data, interspersed with over 73,000 days without data. For example, in one data set, one adherence measure gives an average step count of 6,952 where another gives 9,423. Our results show the importance of adherence when analysing and reporting activity tracker data. We provide guidelines for defining adherence, analysing its impact and reporting it along with the results of the tracker data analysis. Our key contribution is the foundation for analysis of physical activity data, to take account of data incompleteness.},
	articleno    = 37,
	numpages     = 22,
	keywords     = {data completeness, adherence, physical activity trackers, wear-time}
}
@article{10.1145/3191755,
	title        = {SignFi: Sign Language Recognition Using WiFi},
	author       = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan and Zhao, Hongyang and Jung, Woosub},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191755},
	url          = {https://doi.org/10.1145/3191755},
	issue_date   = {March 2018},
	abstract     = {We propose SignFi to recognize sign language gestures using WiFi. SignFi uses Channel State Information (CSI) measured by WiFi packets as the input and a Convolutional Neural Network (CNN) as the classification algorithm. Existing WiFi-based sign gesture recognition technologies are tested on no more than 25 gestures that only involve hand and/or finger gestures. SignFi is able to recognize 276 sign gestures, which involve the head, arm, hand, and finger gestures, with high accuracy. SignFi collects CSI measurements to capture wireless signal characteristics of sign gestures. Raw CSI measurements are pre-processed to remove noises and recover CSI changes over sub-carriers and sampling time. Pre-processed CSI measurements are fed to a 9-layer CNN for sign gesture classification. We collect CSI traces and evaluate SignFi in the lab and home environments. There are 8,280 gesture instances, 5,520 from the lab and 2,760 from the home, for 276 sign gestures in total. For 5-fold cross validation using CSI traces of one user, the average recognition accuracy of SignFi is 98.01%, 98.91%, and 94.81% for the lab, home, and lab+home environment, respectively. We also run tests using CSI traces from 5 different users in the lab environment. The average recognition accuracy of SignFi is 86.66% for 7,500 instances of 150 sign gestures performed by 5 different users.},
	articleno    = 23,
	numpages     = 21,
	keywords     = {Sign Language Recognition, Channel State Information, Convolutional Neural Network, Gesture Recognition, Wireless}
}
@article{10.1145/3191751,
	title        = {Vocal Resonance: Using Internal Body Voice for Wearable Authentication},
	author       = {Liu, Rui and Cornelius, Cory and Rawassizadeh, Reza and Peterson, Ronald and Kotz, David},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191751},
	url          = {https://doi.org/10.1145/3191751},
	issue_date   = {March 2018},
	abstract     = {We observe the advent of body-area networks of pervasive wearable devices, whether for health monitoring, personal assistance, entertainment, or home automation. For many devices, it is critical to identify the wearer, allowing sensor data to be properly labeled or personalized behavior to be properly achieved. In this paper we propose the use of vocal resonance, that is, the sound of the person's voice as it travels through the person's body -- a method we anticipate would be suitable for devices worn on the head, neck, or chest. In this regard, we go well beyond the simple challenge of speaker recognition: we want to know who is wearing the device. We explore two machine-learning approaches that analyze voice samples from a small throat-mounted microphone and allow the device to determine whether (a) the speaker is indeed the expected person, and (b) the microphone-enabled device is physically on the speaker's body. We collected data from 29 subjects, demonstrate the feasibility of a prototype, and show that our DNN method achieved balanced accuracy 0.914 for identification and 0.961 for verification by using an LSTM-based deep-learning model, while our efficient GMM method achieved balanced accuracy 0.875 for identification and 0.942 for verification.},
	articleno    = 19,
	numpages     = 23,
	keywords     = {mobile system security, wearable device, authentication, Biometric, vocal resonance}
}
@article{10.1145/3191750,
	title        = {Calibrating Low-Cost Sensors by a Two-Phase Learning Approach for Urban Air Quality Measurement},
	author       = {Lin, Yuxiang and Dong, Wei and Chen, Yuan},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191750},
	url          = {https://doi.org/10.1145/3191750},
	issue_date   = {March 2018},
	abstract     = {Urban air quality information, e.g., PM2.5 concentration, is of great importance to both the government and society. Recently, there is a growing interest in developing low-cost sensors, installed on moving vehicles, for fine-grained air quality measurement. However, low-cost mobile sensors typically suffer from low accuracy and thus need careful calibration to preserve a high measurement quality. In this paper, we propose a two-phase data calibration method consisting of a linear part and a nonlinear part. We use MLS (multiple least square) to train the linear part, and use RF (random forest) to train the nonlinear part. We propose an automatic feature selection algorithm based on AIC (Akaike information criterion) for the linear model, which helps avoid overfitting due to the inclusion of inappropriate features. We evaluate our method extensively. Results show that our method outperforms existing approaches, achieving an overall accuracy improvement of 16.4% in terms of PM2.5 levels compared with state-of-the-art approach.},
	articleno    = 18,
	numpages     = 18,
	keywords     = {Sensor calibration, Air quality, Mobile sensor network, Low-cost sensors}
}
@article{10.1145/3191744,
	title        = {BreathLive: Liveness Detection for Heart Sound Authentication with Deep Breathing},
	author       = {Huang, Chenyu and Chen, Huangxun and Yang, Lin and Zhang, Qian},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191744},
	url          = {https://doi.org/10.1145/3191744},
	issue_date   = {March 2018},
	abstract     = {Nowadays, considerable number of devices have been proposed to monitor cardiovascular health. To protect medical data on these devices from unauthorized access, researchers have proposed ECG-based and heart sound-based authentication methods. However, their vulnerabilities to replay attacks have recently been revealed. In this paper, we leverage liveness detection to enhance heart sound-based authentication against replay attacks. We utilize the inherent correlation between sounds and chest motion caused by deep breathing to realize a reliable liveness detection system, BreathLive. To be specific, BreathLive captures breathing sounds and chest motion simultaneously, and then eliminates signal delay caused by any imperfections of device components. Next, it extracts a set of features to characterize the correlation between sounds and motion signals, and uses them to train the classifier. We implement and evaluated BreathLive under different attacking scenarios and contexts. The results show that BreathLive achieves an equal error rate of 4.0%, 6.4% and 8.3% for random impersonation attacks, advanced impersonation attacks and advanced replay attacks respectively, which indicates its effectiveness in defending against different attacks. Also the extensive experiments prove the system can be robust to different contexts with a small training set.},
	articleno    = 12,
	numpages     = 25,
	keywords     = {Gyroscope, Liveness Detection, Microphone, Wearable Computing}
}
@article{10.1145/3191740,
	title        = {Mobile Device Type Substitution},
	author       = {Finley, Benjamin and Soikkeli, Tapio},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191740},
	url          = {https://doi.org/10.1145/3191740},
	issue_date   = {March 2018},
	abstract     = {Mobile users today interact with a variety of mobile device types including smartphones, tablets, smartwatches, and others. However research on mobile device type substitution has been limited in several respects including a lack of detailed and robust analyses. Therefore, in this work we study mobile device type substitution through analysis of multidevice usage data from a large US-based user panel. Specifically, we use regression analysis over paired user groups to test five device type substitution hypotheses. We find that both tablets and PCs are partial substitutes for smartphones with tablet and PC ownership decreasing smartphone usage by about 12.5 and 13 hours/month respectively. Additionally, we find that tablets and PCs also prompt about 20 and 57 hours/month respectively of additional (non-substituted) usage. We also illustrate significant inter-user diversity in substituted and additional usage. Overall, our results can help in understanding the relative positioning of different mobile device types and in parameterizing higher level mobile ecosystem models.},
	articleno    = 8,
	numpages     = 20,
	keywords     = {Mobile, Device Type Substitution}
}
@article{10.1145/3191743,
	title        = {You Are Sensing, but Are You Biased? A User Unaided Sensor Calibration Approach for Mobile Sensing},
	author       = {Grammenos, Andreas and Mascolo, Cecilia and Crowcroft, Jon},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191743},
	url          = {https://doi.org/10.1145/3191743},
	issue_date   = {March 2018},
	abstract     = {Mobile devices are becoming pervasive to our daily lives: they follow us everywhere and we use them for much more than just communication. These devices are also equipped with a myriad of different sensors that have the potential to allow the tracking of human activities, user patterns, location, direction and much more. Following this direction, many movements including sports, quantified self, and mobile health ones are starting to heavily rely on this technology, making it pivotal that the sensors offer high accuracy.However, heterogeneity in hardware manufacturing, slight substrate differences, electronic interference as well as external disturbances are just few of the reasons that limit sensor output accuracy which in turn hinders sensor usage in applications which need very high granularity and precision, such as quantified-self applications. Although, calibration of sensors is a widely studied topic in literature to the best of our knowledge no publicly available research exists that specifically tackles the calibration of mobile phones and existing methods that can be adapted for use in mobile devices not only require user interaction but they are also not adaptive to changes. Additionally, alternative approaches for performing more granular and accurate sensing exploit body-wide sensor networks using mobile phones and additional sensors; as one can imagine these techniques can be bulky, tedious, and not particularly user friendly. Moreover, existing techniques for performing data corrections post-acquisition can produce inconsistent results as they miss important context information provided from the device itself; which when used, has been shown to produce better results without a imposing a significant power-penalty.In this paper we introduce a novel multiposition calibration scheme that is specifically targeted at mobile devices Our scheme exploits machine learning techniques to perform an adaptive, power-efficient auto-calibration procedure with which achieves high output sensor accuracy when compared to state of the art techniques without requiring any user interaction or special equipment beyond device itself Moreover, the energy costs associated with our approach are lower than the alternatives (such as Kalman filter based solutions) and the overall power penalty is &lt; 5% when compared against power usage that is exhibited when using uncalibrated traces, thus, enabling our technique to be used efficiently on a wide variety of devices Finally, our evaluation illustrates that calibrated signals offer a tangible benefit in classification accuracy, ranging from 3 to 10%, over uncalibrated ones when using state of the art classifiers, on the other hand when using simpler SVM classifiers the classification improvement is boosted ranging from 8% to 12% making lower performing classifiers much more reliable Additionally, we show that for similar activities which are hard to distinguish otherwise, we reach an accuracy of &gt; 95% when using neural network classifiers and &gt; 88% when using SVM classifiers where uncalibrated data classification only reaches ~ 85% and ~ 80% respectively This can be a make or break factor in the use of accelerometer and gyroscope data in applications requiring high accuracy e g sports, health, games and others},
	articleno    = 11,
	numpages     = 26,
	keywords     = {IMU, Mobile Devices, Calibration, Sensor}
}
@article{10.1145/3191741,
	title        = {Crowdsourcing the Installation and Maintenance of Indoor Localization Infrastructure to Support Blind Navigation},
	author       = {Gleason, Cole and Ahmetovic, Dragan and Savage, Saiph and Toxtli, Carlos and Posthuma, Carl and Asakawa, Chieko and Kitani, Kris M. and Bigham, Jeffrey P.},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191741},
	url          = {https://doi.org/10.1145/3191741},
	issue_date   = {March 2018},
	abstract     = {Indoor navigation systems can make unfamiliar buildings more accessible for people with vision impairments, but their adoption is hampered by the effort of installing infrastructure and maintaining it over time. Most solutions in this space require augmenting the environment with add-ons, such as Bluetooth beacons. Installing and calibrating such infrastructure requires time and expertise. Once installed, localization accuracy often degrades over time as batteries die, beacons go missing, or otherwise stop working. Even localization systems installed by experts can become unreliable weeks, months, or years after the installation. To address this problem, we created LuzDeploy: a physical crowdsourcing system that organizes non-experts for the installation and long-term maintenance of a Bluetooth-based navigation system. LuzDeploy simplifies the tasks required to install and maintain the localization infrastructure, thus making a crowdsourcing approach feasible for non-experts. We report on a field deployment where 127 participants installed and maintained a blind navigation system over several months in a 7-story building, completing 455 tasks in total. We compare the accuracy of the system installed by participants to an installation completed by experts with specialized equipment. LuzDeploy aims to improve the sustainability of indoor navigation systems to encourage widespread adoption outside of research settings.},
	articleno    = 9,
	numpages     = 25,
	keywords     = {Real-world accessibility, Indoor navigation assistance, Volunteer crowd work, Physical crowdsourcing, Individuals with visual impairments}
}
@article{10.1145/3191737,
	title        = {Anatomy of a Vulnerable Fitness Tracking System: Dissecting the Fitbit Cloud, App, and Firmware},
	author       = {Classen, Jiska and Wegemer, Daniel and Patras, Paul and Spink, Tom and Hollick, Matthias},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191737},
	url          = {https://doi.org/10.1145/3191737},
	issue_date   = {March 2018},
	abstract     = {Fitbit fitness trackers record sensitive personal information, including daily step counts, heart rate profiles, and locations visited. By design, these devices gather and upload activity data to a cloud service, which provides aggregate statistics to mobile app users. The same principles govern numerous other Internet-of-Things (IoT) services that target different applications. As a market leader, Fitbit has developed perhaps the most secure wearables architecture that guards communication with end-to-end encryption. In this article, we analyze the complete Fitbit ecosystem and, despite the brand's continuous efforts to harden its products, we demonstrate a series of vulnerabilities with potentially severe implications to user privacy and device security. We employ a range of techniques, such as protocol analysis, software decompiling, and both static and dynamic embedded code analysis, to reverse engineer previously undocumented communication semantics, the official smartphone app, and the tracker firmware. Through this interplay and in-depth analysis, we reveal how attackers can exploit the Fitbit protocol to extract private information from victims without leaving a trace, and wirelessly flash malware without user consent. We demonstrate that users can tamper with both the app and firmware to selfishly manipulate records or circumvent Fitbit's walled garden business model, making the case for an independent, user-controlled, and more secure ecosystem. Finally, based on the insights gained, we make specific design recommendations that can not only mitigate the identified vulnerabilities, but are also broadly applicable to securing future wearable system architectures.},
	articleno    = 5,
	numpages     = 24,
	keywords     = {Wearables, Nexmon, health, firmware reverse engineering}
}
@article{10.1145/3191733,
	title        = {ComNSense: Grammar-Driven Crowd-Sourcing of Point Clouds for Automatic Indoor Mapping},
	author       = {Abdelaal, Mohamed and Reichelt, Daniel and D\"{u}rr, Frank and Rothermel, Kurt and Runceanu, Lavinia and Becker, Susanne and Fritsch, Dieter},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191733},
	url          = {https://doi.org/10.1145/3191733},
	issue_date   = {March 2018},
	abstract     = {Recently, point clouds have been efficiently utilized for medical imaging, modeling urban environments, and indoor modeling. In this realm, several mobile platforms, such as Google Tango and Apple ARKit, have been released leveraging 3D mapping, augmented reality, etc. In modeling applications, these modern mobile devices opened the door for crowd-sourcing point clouds to distribute the overhead of data collection. However, uploading these large points clouds from resources-constrained mobile devices to the back-end servers consumes excessive energy. Accordingly, participation rates in such crowd-sensing systems can be negatively influenced. To tackle this challenge, this paper introduces our ComNSense approach that dramatically reduces the energy consumption of processing and uploading point clouds. To this end, ComNSense reports only a set of extracted geometrical data to the servers. To optimize the geometry extraction, ComNSense leverages formal grammars which encode design-time knowledge, i.e. structural information. To demonstrate the effectiveness of ComNSense, we performed several experiments of collecting point clouds from two different buildings to extract the walls location, as a case study. We also assess the performance of ComNSense relative to a grammar-free method. The results showed a significant reduction of the energy consumption while achieving a comparable detection accuracy.},
	articleno    = 1,
	numpages     = 26,
	keywords     = {Data Acquisition, Formal Grammars, Crowd-Sensing, Energy Efficiency, Point Clouds}
}
@article{10.1145/3161407,
	title        = {Character Actor: Design and Evaluation of Expressive Robot Car Seat Motion},
	author       = {Tennent, Hamish and Moore, Dylan and Ju, Wendy},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161407},
	url          = {https://doi.org/10.1145/3161407},
	issue_date   = {December 2017},
	abstract     = {How might an actuated car seat become an expressive robot? To explore the possibilities of this novel interaction, we conducted a full design exploration from prototyping to validation, drawing on methods for embodied physical interaction design. First, we applied physical and digital puppeteering techniques to explore how a car seat can display emotional affect through movement with limited degrees of freedom in a semi-structured design workshop. Second, prototyped emotions were formalized with the Laban Effort framework and translated into computer animations. Third, we tested if lay users understood the expressions communicated by the animations in an online validation study on Amazon Mechanical Turk.Participants generally agreed with our interpretation of six prototyped expressive states for the robot car seat (Neutral, Aggressive, Confident, Cool, Excited, and Quirky), and reported quantitative and qualitative reactions to each including perceived safety, which varied across conditions. Participants reported more implied cognition for higher valence expressions, and also were more likely to agree with our design intent. This specific case of physical interaction design and evaluation serves as a vignette for how to design and validate novel physical expressions in non-anthropomorphic robot interfaces.},
	articleno    = 165,
	numpages     = 23,
	keywords     = {Robotics, Interaction Design, Movement Design}
}
@article{10.1145/3161182,
	title        = {Luciola: A Millimeter-Scale Light-Emitting Particle Moving in Mid-Air Based On Acoustic Levitation and Wireless Powering},
	author       = {Uno, Yuki and Qiu, Hao and Sai, Toru and Iguchi, Shunta and Mizutani, Yota and Hoshi, Takayuki and Kawahara, Yoshihiro and Kakehi, Yasuaki and Takamiya, Makoto},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161182},
	url          = {https://doi.org/10.1145/3161182},
	issue_date   = {December 2017},
	abstract     = {In this paper, we present an approach to realize the levitation of a small object with an embedded electronic circuit. Luciola is a light-emitting particle with a diameter of 3.5mm and a weight of 16.2mg moving in mid-air in a 10.4cm x 10.4cm x 5.4cm space through acoustic levitation using two 40-kHz 17 x 17 ultrasonic transducer arrays placed face-to-face at a distance of 20cm and wirelessly powered by 12.3-MHz resonant inductive coupling. The novelty of this paper is the acoustically levitated electronic object by the combined application of ultrasonic levitation and wireless powering to the levitated electronic object. A new shape of the levitated object and a new placement of the receiver coil to simultaneously realize acoustic levitation and wireless powering are proposed, achieving a stable wireless powering to a rotating levitated object at the bottom of an acoustic potential. To enable the levitation of a particle, a custom IC chip is essential in reducing the size and weight of the particle. In the design of the custom IC chip, a new voltage detector circuit enabling an accurate voltage detection and a correct output during the start-up is proposed to achieve an intermittent lighting of the LED to increase the maximum distance between the transmitter and the receiver coil. Luciola is applied to a self-luminous pixel in a mid-air display and drawings of characters in mid-air are demonstrated.},
	articleno    = 166,
	numpages     = 17,
	keywords     = {ultrasound, wireless powering, Millimeter-scale, levitation, IC chip}
}
@article{10.1145/3161172,
	title        = {Sensei: Sensing Educational Interaction},
	author       = {Saquib, Nazmus and Bose, Ayesha and George, Dwyane and Kamvar, Sepandar},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161172},
	url          = {https://doi.org/10.1145/3161172},
	issue_date   = {December 2017},
	abstract     = {We present Sensei, the first system designed to understand social interaction and learning in an early-childhood classroom using a distributed sensor network. Our unobtrusive sensors measure proximity between each node in a dynamic range-based mesh network. The sensors can be worn in the shoes, attached to selected landmarks in the classroom, and placed on Montessori materials. This data, accessible to teachers in a web dashboard, enables teachers to derive deeper insights from their classrooms. Sensei is currently deployed in three Montessori schools and we have evaluated the effectiveness of the system with teachers. Our user studies have shown that the system enhances teachers' capabilities and helps discover insights that would have otherwise been lost. From our evaluation interviews, we have established three major use cases of the system. Sensei augments teachers' manual observations, helps them plan individualized curriculum for each student, and identifies their needs for more interaction with some children. Further, the anonymized data can be used in large-scale research in early childhood development.},
	articleno    = 161,
	numpages     = 27,
	keywords     = {Sensor networks, Data visualization, Education}
}
@article{10.1145/3161165,
	title        = {ObjectSkin: Augmenting Everyday Objects with Hydroprinted Touch Sensors and Displays},
	author       = {Groeger, Daniel and Steimle, J\"{u}rgen},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161165},
	url          = {https://doi.org/10.1145/3161165},
	issue_date   = {December 2017},
	abstract     = {Augmenting everyday objects with interactive input and output surfaces is a long-standing topic in ubiquitous computing and HCI research. Existing approaches, however, fail to leverage the objects' full potential, particularly in highly curved organic geometries and in diverse visuo-haptic surface properties. We contribute ObjectSkin, a fabrication technique for adding conformal interactive surfaces to rigid and flexible everyday objects. It enables multi-touch sensing and display output that seamlessly integrates with highly curved and irregular geometries. The approach is based on a novel water-transfer process for interactive surfaces. It leverages off-the-shelf hobbyist equipment to fabricate thin, conformal, and translucent electronic circuits that preserve the surface characteristics of everyday objects. It offers two methods, for rapid low-fidelity and versatile high-fidelity prototyping, and is applicable to a wide variety of materials. Results from a series of technical experiments provide insights into the supported object geometries, compatible object materials, and robustness. Seven example cases demonstrate how ObjectSkin makes it possible to leverage geometries, surface properties, and unconventional objects for prototyping novel interactions for ubiquitous computing.},
	articleno    = 134,
	numpages     = 23,
	keywords     = {sensors, touch input, interactive objects, displays, prototyping, printed electronics, Fabrication}
}
@article{10.1145/3161163,
	title        = {Charging a Smartphone Across a Room Using Lasers},
	author       = {Iyer, Vikram and Bayati, Elyas and Nandakumar, Rajalakshmi and Majumdar, Arka and Gollakota, Shyamnath},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161163},
	url          = {https://doi.org/10.1145/3161163},
	issue_date   = {December 2017},
	abstract     = {We demonstrate a novel laser-based wireless power delivery system that can charge mobile devices such as smartphones across a room. The key challenges in achieving this are multi-fold: delivering greater than a watt of power across the room, minimizing the exposure of the resulting high-power lasers to human tissue, and finally ensuring that the design meets the form-factor requirements of a smartphone and requires minimal instrumentation to the environment. This paper presents a novel, and to the best of our knowledge, the first design, implementation and evaluation of an end-to-end power delivery system that satisfies all the above requirements. Our results show that we can deliver more than 2 W at ranges of 4.3 m and 12.2 m for a smartphone (25 cm2) and table-top form factor (100 cm2) receiver respectively. Further, extensive characterization of our safety system shows that we can turn off our laser source much before a human moving at a maximum speed of 44 m/s can even enter the high-power laser beam area.},
	articleno    = 143,
	numpages     = 21,
	keywords     = {Retroreflectors, Optical Backscatter, Wireless power, Optics, Acoustic Localization}
}
@article{10.1145/3132027,
	title        = {VibeBin: A Vibration-Based Waste Bin Level Detection System},
	author       = {Zhao, Yiran and Yao, Shuochao and Li, Shen and Hu, Shaohan and Shao, Huajie and Abdelzaher, Tarek F.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3132027},
	url          = {https://doi.org/10.1145/3132027},
	issue_date   = {September 2017},
	abstract     = {This paper presents the design and implementation of VibeBin, a low-cost, non-intrusive and easy-to-install waste bin level detection system. Recent popularity of Internet-of-Things (IoT) sensors has brought us unprecedented opportunities to enable a variety of new services for monitoring and controlling smart buildings. Indoor waste management is crucial to a healthy environment in smart buildings. Measuring the waste bin fill-level helps building operators schedule garbage collection more responsively and optimize the quantity and location of waste bins. Existing systems focus on directly and intrusively measuring the physical quantities of the garbage (weight, height, volume, etc.) or its appearance (image), and therefore require careful installation, laborious calibration or labeling, and can be costly. Our system indirectly measures fill-level by sensing the changes in motor-induced vibration characteristics on the outside surface of waste bins. VibeBin exploits the physical nature of vibration resonance of the waste bin and the garbage within, and learns the vibration features of different fill-levels through a few garbage collection (emptying) cycles in a completely unsupervised manner. VibeBin identifies vibration features of different fill-levels by clustering historical vibration samples based on a custom distance metric which measures the dissimilarity between two samples. We deploy our system on eight waste bins of different types and sizes, and show that under normal usage and real waste, it can deliver accurate level measurements after just 3 garbage collection cycles. The average F-score (harmonic mean of precision and recall) of measuring empty, half, and full levels achieves 0.912. A two-week deployment also shows that the false positive and false negative events are satisfactorily rare.},
	articleno    = 122,
	numpages     = 22,
	keywords     = {Smart Building, Waste Management, Internet-of-Things, Wireless sensors}
}
@article{10.1145/3132025,
	title        = {Building Cognition-Aware Systems: A Mobile Toolkit for Extracting Time-of-Day Fluctuations of Cognitive Performance},
	author       = {Dingler, Tilman and Schmidt, Albrecht and Machulla, Tonja},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3132025},
	url          = {https://doi.org/10.1145/3132025},
	issue_date   = {September 2017},
	abstract     = {People’s alertness fluctuates across the day: at some times we are highly focused while at others we feel unable to concentrate. So far, extracting fluctuation patterns has been time and cost-intensive. Using an in-the-wild approach with 12 participants, we evaluated three cognitive tasks regarding their adequacy as a mobile and economical assessment tool of diurnal changes in mental performance. Participants completed the five-minute test battery on their smartphones multiple times a day for a period of 1-2 weeks. Our results show that people’s circadian rhythm can be obtained under unregulated non-laboratory conditions. Along with this validation study, we release our test battery as an open source library for future work towards cognition-aware systems as well as a tool for psychological and medical research. We discuss ways of integrating the toolkit and possibilities for implicitly measuring performance variations in common applications. The ability to detect systematic patterns in alertness levels will allow cognition-aware systems to provide in-situ assistance in accordance with users’ current cognitive capabilities and limitations.},
	articleno    = 47,
	numpages     = 15,
	keywords     = {cognition-aware systems, circadian rhythm, alertness}
}
@article{10.1145/3131898,
	title        = {Detecting State Changes of Indoor Everyday Objects Using Wi-Fi Channel State Information},
	author       = {Ohara, Kazuya and Maekawa, Takuya and Matsushita, Yasuyuki},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131898},
	url          = {https://doi.org/10.1145/3131898},
	issue_date   = {September 2017},
	abstract     = {Detecting the events of indoor everyday objects such as door or window open/close events has been actively studied to implement such applications as intrusion detection, adaptive HVAC control, and monitoring an independently living elderly person. This study proposes a method for detecting the events and states of indoor everyday objects such as doors and windows without using distributed sensors attached to the objects. In this study, we achieve practical and unobtrusive event detection using a commodity Wi-Fi access point and a computer equipped with a commodity Wi-Fi module. Specifically, we detect the events using Wi-Fi channel state information (CSI), which describes how a signal propagates from a transmitter to a receiver, and is affected by such events. To handle CSI data that consists of the mixed effects of multiple indoor objects in an environment of interest, we employ independent component analysis to separate the events caused by the objects. The decomposed data are then fed into our event classifier based on convolutional and recurrent neural networks to automatically extract features from CSI data, as it is difficult to intuitively design features to be extracted from the CSI data. Moreover, we correct the neural network estimates by incorporating knowledge about the state transitions of an object using hidden Markov models. For example, because the “open” event of a door occurs only when the door is in a “closed” state. We correct impossible state transitions estimated by the neural network based on this knowledge.},
	articleno    = 88,
	numpages     = 28,
	keywords     = {pattern recognition, deep neural network, open/close event, Wi-Fi channel state information, Indoor context recognition}
}
@article{10.1145/3131892,
	title        = {Smartwatch Wearing Behavior Analysis: A Longitudinal Study},
	author       = {Jeong, Hayeon and Kim, Heepyung and Kim, Rihun and Lee, Uichin and Jeong, Yong},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131892},
	url          = {https://doi.org/10.1145/3131892},
	issue_date   = {September 2017},
	abstract     = {Smartwaches are the representative wearable or body-worn devices that provide convenient and easy information access. There is a growing body of research work on enabling novel interaction techniques and understanding user experiences of smartwatches. However, there is still lack of user experience research on wearing behaviors of smartwatches, which is critical for wearable device and service design. In this work, we investigate how college students wear smartwatches and what factors affect wearing behaviors by analyzing a longitudinal activity dataset collected from 50 smartwatch users for 203 days. Our results show that there are several temporal usage patterns and distinct groups of usage patterns. The factors affecting wearing behaviors are contextual, nuanced, and multifaceted. Our findings provide diverse design implications for improving wearability of smartwatches and leveraging smartwatches for behavioral changes.},
	articleno    = 60,
	numpages     = 31,
	keywords     = {Activity Tracker, Wearable Device, Smartwatch, Personal Informatics, User Experience}
}
@article{10.1145/3130985,
	title        = {FingerSound: Recognizing Unistroke Thumb Gestures Using a Ring},
	author       = {Zhang, Cheng and Waghmare, Anandghan and Kundra, Pranav and Pu, Yiming and Gilliland, Scott and Ploetz, Thomas and Starner, Thad E. and Inan, Omer T. and Abowd, Gregory D.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130985},
	url          = {https://doi.org/10.1145/3130985},
	issue_date   = {September 2017},
	abstract     = {We introduce FingerSound, an input technology to recognize unistroke thumb gestures, which are easy to learn and can be performed through eyes-free interaction. The gestures are performed using a thumb-mounted ring comprising a contact microphone and a gyroscope sensor. A K-Nearest-Neighbor(KNN) model with a distance function of Dynamic Time Warping (DTW) is built to recognize up to 42 common unistroke gestures. A user study, where the real-time classification results were given, shows an accuracy of 92%-98% by a machine learning model built with only 3 training samples per gesture. Based on the user study results, we further discuss the opportunities, challenges and practical limitations of FingerSound when deploying it to real-world applications in the future.},
	articleno    = 120,
	numpages     = 19,
	keywords     = {Wearable, Input, Gesture Recognition, Ring}
}
@article{10.1145/3130972,
	title        = {Gamification of Mobile Experience Sampling Improves Data Quality and Quantity},
	author       = {van Berkel, Niels and Goncalves, Jorge and Hosio, Simo and Kostakos, Vassilis},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130972},
	url          = {https://doi.org/10.1145/3130972},
	issue_date   = {September 2017},
	abstract     = {The Experience Sampling Method is used to capture high-quality in situ data from study participants. This method has become popular in studies involving smartphones, where it is often adapted to motivate participation through the use of gamification techniques. However, no work to date has evaluated whether gamification actually affects the quality and quantity of data collected through Experience Sampling. Our study systematically investigates the effect of gamification on the quantity and quality of experience sampling responses on smartphones. In a field study, we combine event contingent and interval contingent triggers to ask participants to describe their location. Subsequently, participants rate the quality of these entries by playing a game with a purpose. Our results indicate that participants using the gamified version of our ESM software provided significantly higher quality responses, slightly increased their response rate, and provided significantly more data on their own accord. Our findings suggest that gamifying experience sampling can improve data collection and quality in mobile settings.},
	articleno    = 107,
	numpages     = 21,
	keywords     = {sensing, EMA, ESM, location, labeling, experience sampling method, CSCW, crowdsensing, human behavior, motivation}
}
@article{10.1145/3130954,
	title        = {FootprintID: Indoor Pedestrian Identification through Ambient Structural Vibration Sensing},
	author       = {Pan, Shijia and Yu, Tong and Mirshekari, Mostafa and Fagert, Jonathon and Bonde, Amelie and Mengshoel, Ole J. and Noh, Hae Young and Zhang, Pei},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130954},
	url          = {https://doi.org/10.1145/3130954},
	issue_date   = {September 2017},
	abstract     = {We present FootprintID, an indoor pedestrian identification system that utilizes footstep-induced structural vibration to infer pedestrian identities for enabling various smart building applications. Previous studies have explored other sensing methods, including vision-, RF-, mobile-, and acoustic-based methods. They often require specific sensing conditions, including line-of-sight, high sensor density, and carrying wearable devices. Vibration-based methods, on the other hand, provide easy-to-install sparse sensing and utilize gait to distinguish different individuals. However, the challenge for these methods is that the signals are sensitive to the gait variations caused by different walking speeds and the floor variations caused by structural heterogeneity.We present FootprintID, a vibration-based approach that achieves robust pedestrian identification. The system uses vibration sensors to detect footstep-induced vibrations. It then selects vibration signals and classifiers to accommodate sensing variations, taking step location and frequency into account. We utilize the physical insight on how individual step signal changes with walking speeds and introduce an iterative transductive learning algorithm (ITSVM) to achieve robust classification with limited labeled training data. When trained only on the average walking speed and tested on different walking speeds, FootprintID achieves up to 96% accuracy and a 3X improvement in extreme speeds compared to the Support Vector Machine. Furthermore, it achieves up to 90% accuracy (1.5X improvement) in uncontrolled experiments.},
	articleno    = 89,
	numpages     = 31,
	keywords     = {pedestrian identification, ambient vibration sensing, structural vibration}
}
@article{10.1145/3130940,
	title        = {IndoTrack: Device-Free Indoor Human Tracking with Commodity Wi-Fi},
	author       = {Li, Xiang and Zhang, Daqing and Lv, Qin and Xiong, Jie and Li, Shengjie and Zhang, Yue and Mei, Hong},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130940},
	url          = {https://doi.org/10.1145/3130940},
	issue_date   = {September 2017},
	abstract     = {Indoor human tracking is fundamental to many real-world applications such as security surveillance, behavioral analysis, and elderly care. Previous solutions usually require dedicated device being carried by the human target, which is inconvenient or even infeasible in scenarios such as elderly care and break-ins. However, compared with device-based tracking, device-free tracking is particularly challenging because the much weaker reflection signals are employed for tracking. The problem becomes even more difficult with commodity Wi-Fi devices, which have limited number of antennas, small bandwidth size, and severe hardware noise.In this work, we propose IndoTrack, a device-free indoor human tracking system that utilizes only commodity Wi-Fi devices. IndoTrack is composed of two innovative methods: (1) Doppler-MUSIC is able to extract accurate Doppler velocity information from noisy Wi-Fi Channel State Information (CSI) samples; and (2) Doppler-AoA is able to determine the absolute trajectory of the target by jointly estimating target velocity and location via probabilistic co-modeling of spatial-temporal Doppler and AoA information. Extensive experiments demonstrate that IndoTrack can achieve a 35cm median error in human trajectory estimation, outperforming the state-of-the-art systems and provide accurate location and velocity information for indoor human mobility and behavioral analysis.},
	articleno    = 72,
	numpages     = 22,
	keywords     = {Device-free indoor tracking, Wi-Fi, Doppler}
}
@article{10.1145/3130903,
	title        = {How to Remember What to Remember: Exploring Possibilities for Digital Reminder Systems},
	author       = {Brewer, R. N. and Morris, M. R. and Lindley, S. E.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130903},
	url          = {https://doi.org/10.1145/3130903},
	issue_date   = {September 2017},
	abstract     = {Digital reminder systems typically use time and place as triggers to remind people to perform activities. In this paper, we investigate how digital reminder systems could better support the process of remembering in a wider range of situations. We report findings from a survey and one-week diary study, which reveal that people want to remember to perform a broad spectrum of activities in the future, many of which cannot be supported by simple time- and location-based reminders. In addition to these examples of prospective memory, or ‘remembering intentions’ [53], we also find that people want support in ‘retrieving’ [53] information and details, especially those encountered through social interactions or intended for use in conversations with others. Drawing on our analysis of what people want to remember and how they try to support this, we draw implications for the design of intelligent reminder systems such as digital assistants (e.g. Microsoft’s Cortana) and smart speaker systems (e.g. Amazon Echo), and highlight the possibilities afforded by drawing on conversation and giving material form to digital reminders.},
	articleno    = 38,
	numpages     = 20,
	keywords     = {memory, intelligent personal assistants, digital reminder systems, retrospective memory, Reminders, prospective memory}
}
@article{10.1145/3090092,
	title        = {RoboCOP: A Robotic Coach for Oral Presentations},
	author       = {Trinh, H. and Asadi, R. and Edge, D. and Bickmore, T.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090092},
	url          = {https://doi.org/10.1145/3090092},
	issue_date   = {June 2017},
	abstract     = {Rehearsing in front of a live audience is invaluable when preparing for important presentations. However, not all presenters take the opportunity to engage in such rehearsal, due to time constraints, availability of listeners who can provide constructive feedback, or public speaking anxiety. We present RoboCOP, an automated anthropomorphic robot head that acts as a coach to provide spoken feedback during presentation rehearsals at both the individual slide and overall presentation level. The robot offers conversational coaching on three key aspects of presentations: speech quality, content coverage, and audience orientation. The design of the feedback strategies was informed by findings from an exploratory study with academic professionals who were experienced in mentoring students on their presentations. In a within-subjects study comparing RoboCOP to visual feedback and spoken feedback without a robot, the robotic coach was shown to lead to significant improvement in the overall experience of presenters. Results of a second within-subjects evaluation study comparing RoboCOP with existing rehearsal practices show that our system creates a natural, interactive, and motivating rehearsal environment that leads to improved presentation quality.},
	articleno    = 27,
	numpages     = 24,
	keywords     = {Presentation rehearsal, robot, coaching, feedback}
}
@article{10.1145/3090091,
	title        = {Harnessing Long Term Physical Activity Data—How Long-Term Trackers Use Data and How an Adherence-Based Interface Supports New Insights},
	author       = {Tang, Lie Ming and Kay, Judy},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090091},
	url          = {https://doi.org/10.1145/3090091},
	issue_date   = {June 2017},
	abstract     = {Increasingly, people are amassing long term physical activity data which could play an important role for reflection. However, it is not clear if and how existing trackers use their long term data and incomplete data is a potential challenge. We introduced the notion of adherence to design iStuckWithIt, a custom calendar display that integrates and embeds daily adherence (days with data and days without), hourly adherence (hours of wear each day) and goal adherence (days people achieved their activity goals). Our study of 21 long term FitBit users (average: 23 months, 17 over 1 year) began with an interview about their use and knowledge of long term physical activity data followed by a think-aloud use of iStuckWithIt and a post-interview. Our participants gained new insights about their wearing patterns and they could then use this to overcome problems of missing data, to gain insights about their physical activity and goal achievement. This work makes two main contributions: new understanding of the ways that long term trackers have used and understand their data; the design and evaluation of iStuckWithIt demonstrating that people can gain new insights through designs that embed daily, hourly adherence data with goal adherence.},
	articleno    = 26,
	numpages     = 28,
	keywords     = {hourly adherence, goal adherence, physical activity trackers, daily adherence, long term physical activity data}
}
@article{10.1145/3090089,
	title        = {Inferring Person-to-Person Proximity Using WiFi Signals},
	author       = {Sapiezynski, Piotr and Stopczynski, Arkadiusz and Wind, David Kofoed and Leskovec, Jure and Lehmann, Sune},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090089},
	url          = {https://doi.org/10.1145/3090089},
	issue_date   = {June 2017},
	abstract     = {Today's societies are enveloped in an ever-growing telecommunication infrastructure. This infrastructure offers important opportunities for sensing and recording a multitude of human behaviors. Human mobility patterns are a prominent example of such a behavior which has been studied based on cell phone towers, Bluetooth beacons, and WiFi networks as proxies for location. While mobility is an important aspect of human behavior, it is also crucial to study physical interactions among individuals. Sensing proximity that enables social interactions on a large scale is a technical challenge and many commonly used approaches—including RFID badges or Bluetooth scanning—offer only limited scalability. Here we show that it is possible, in a scalable and robust way, to accurately infer person-to-person physical proximity from the lists of WiFi access points measured by smartphones carried by the two individuals. Based on a longitudinal dataset of approximately 800 participants with ground-truth Bluetooth proximity collected over a year, we show that our model performs better than the current state-of-the-art. Our results demonstrate the value of WiFi signals as a tool for social sensing and show how collections of WiFi data pose a potential threat to privacy.},
	articleno    = 24,
	numpages     = 20,
	keywords     = {social sensing, social networks, WiFi, proximity}
}
@article{10.1145/3536393,
	title        = {DiverSense: Maximizing Wi-Fi Sensing Range Leveraging Signal Diversity},
	author       = {Li, Yang and Wu, Dan and Zhang, Jie and Xu, Xuhai and Xie, Yaxiong and Gu, Tao and Zhang, Daqing},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3536393},
	url          = {https://doi.org/10.1145/3536393},
	issue_date   = {July 2022},
	abstract     = {The ubiquity of Wi-Fi infrastructure has facilitated the development of a range of Wi-Fi based sensing applications. Wi-Fi sensing relies on weak signal reflections from the human target and thus only supports a limited sensing range, which significantly hinders the real-world deployment of the proposed sensing systems. To extend the sensing range, traditional algorithms focus on suppressing the noise introduced by the imperfect Wi-Fi hardware. This paper picks a different direction and proposes to enhance the quality of the sensing signal by fully exploiting the signal diversity provided by the Wi-Fi hardware. We propose DiverSense, a system that combines sensing signal received from all subcarriers and all antennas in the array, to fully utilize the spatial and frequency diversity. To guarantee the diversity gain after signal combining, we also propose a time-diversity based signal alignment algorithm to align the phase of the multiple received sensing signals. We implement the proposed methods in a respiration monitoring system using commodity Wi-Fi devices and evaluate the performance in diverse environments. Extensive experimental results demonstrate that DiverSense is able to accurately monitor the human respiration even when the sensing signal is under noise floor, and therefore boosts sensing range to 40 meters, which is a 3x improvement over the current state-of-the-art. DiverSense also works robustly under NLoS scenarios, e.g., DiverSense is able to accurately monitor respiration even when the human and the Wi-Fi transceivers are separated by two concrete walls with wooden doors.},
	articleno    = 94,
	numpages     = 28,
	keywords     = {Wireless Sensing, Sensing Range, Channel State Information (CSI)}
}
@article{10.1145/3534596,
	title        = {DepreST-CAT: Retrospective Smartphone Call and Text Logs Collected during the COVID-19 Pandemic to Screen for Mental Illnesses},
	author       = {Tlachac, ML and Flores, Ricardo and Reisch, Miranda and Houskeeper, Katie and Rundensteiner, Elke A.},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534596},
	url          = {https://doi.org/10.1145/3534596},
	issue_date   = {July 2022},
	abstract     = {The rates of mental illness, especially anxiety and depression, have increased greatly since the start of the COVID-19 pandemic. Traditional mental illness screening instruments are too cumbersome and biased to screen an entire population. In contrast, smartphone call and text logs passively capture communication patterns and thus represent a promising screening alternative. To facilitate the advancement of such research, we collect and curate the DepreST Call and Text log (DepreST-CAT) dataset from over 365 crowdsourced participants during the COVID-19 pandemic. The logs are labeled with traditional anxiety and depression screening scores essential for training machine learning models. We construct time series ranging from 2 to 16 weeks in length from the retrospective smartphone logs. To demonstrate the screening capabilities of these time series, we then train a variety of unimodal and multimodal machine and deep learning models. These models provide insights into the relative screening value of the different types of logs, lengths of log time series, and classification methods. The DepreST-CAT dataset is a valuable resource for the research community to model communication patterns during the COVID-19 pandemic and further the development of machine learning algorithms for passive mental illness screening.},
	articleno    = 75,
	numpages     = 32,
	keywords     = {digital phenotype, recurrent modeling, mental health assessment, mobile health}
}
@article{10.1145/3534582,
	title        = {Leveraging Sound and Wrist Motion to Detect Activities of Daily Living with Commodity Smartwatches},
	author       = {Bhattacharya, Sarnab and Adaimi, Rebecca and Thomaz, Edison},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534582},
	url          = {https://doi.org/10.1145/3534582},
	issue_date   = {July 2022},
	abstract     = {Automatically recognizing a broad spectrum of human activities is key to realizing many compelling applications in health, personal assistance, human-computer interaction and smart environments. However, in real-world settings, approaches to human action perception have been largely constrained to detecting mobility states, e.g., walking, running, standing. In this work, we explore the use of inertial-acoustic sensing provided by off-the-shelf commodity smartwatches for detecting activities of daily living (ADLs). We conduct a semi-naturalistic study with a diverse set of 15 participants in their own homes and show that acoustic and inertial sensor data can be combined to recognize 23 activities such as writing, cooking, and cleaning with high accuracy. We further conduct a completely in-the-wild study with 5 participants to better evaluate the feasibility of our system in practical unconstrained scenarios. We comprehensively studied various baseline machine learning and deep learning models with three different fusion strategies, demonstrating the benefit of combining inertial and acoustic data for ADL recognition. Our analysis underscores the feasibility of high-performing recognition of daily activities using inertial-acoustic data from practical off-the-shelf wrist-worn devices while also uncovering challenges faced in unconstrained settings. We encourage researchers to use our public dataset to further push the boundary of ADL recognition in-the-wild.},
	articleno    = 42,
	numpages     = 28,
	keywords     = {Gesture Recognition, Smartwatch, Dataset, Wearable, Sound Sensing, Audio Classification, In-the-wild, Activities of Daily Living, Human Activity Recognition, Motion sensing, Multimodal classification}
}
@article{10.1145/3534578,
	title        = {Predicting Post-Operative Complications with Wearables: A Case Study with Patients Undergoing Pancreatic Surgery},
	author       = {Zhang, Jingwen and Li, Dingwen and Dai, Ruixuan and Cos, Heidy and Williams, Gregory A. and Raper, Lacey and Hammill, Chet W. and Lu, Chenyang},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534578},
	url          = {https://doi.org/10.1145/3534578},
	issue_date   = {July 2022},
	abstract     = {Post-operative complications and hospital readmission are of great concern to surgical patients and health care providers. Wearable devices such as Fitbit wristbands enable long-term and non-intrusive monitoring of patients outside clinical environments. To build accurate predictive models based on wearable data, however, requires effective feature engineering to extract high-level features from time series data collected by the wearable sensors. This paper presents a pipeline for developing clinical predictive models based on wearable sensors. The core of the pipeline is a multi-level feature engineering framework for extracting high-level features from fine-grained time series data. The framework integrates a set of techniques tailored for noisy and incomplete wearable data collected in real-world clinical studies: (1) singular spectrum analysis for extracting high-level features from daily features over the course of the study; (2) a set of daily features that are resilient to missing data in wearable time series data; (3) a K-Nearest Neighbors (KNN) method for imputing short missing heart rate segments; (4) the integration of patients' clinical characteristics and wearable features. We evaluated the feature engineering approach and machine learning models in a clinical study involving 61 patients undergoing pancreatic surgery. Linear support vector machine (SVM) with integrated feature engineering achieved an AUROC of 0.8802 for predicting post-operative readmission or severe complications, which significantly outperformed the existing rule-based model used in clinical practice and other state-of-the-art feature engineering approaches.},
	articleno    = 87,
	numpages     = 27,
	keywords     = {Wearable Devices, Machine Learning, Missing Data, Post-Surgical Prediction, Feature Engineering}
}
@article{10.1145/3534574,
	title        = {WePos: Weak-Supervised Indoor Positioning with Unlabeled WiFi for On-Demand Delivery},
	author       = {Guo, Baoshen and Zuo, Weijian and Wang, Shuai and Lyu, Wenjun and Hong, Zhiqing and Ding, Yi and He, Tian and Zhang, Desheng},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534574},
	url          = {https://doi.org/10.1145/3534574},
	issue_date   = {July 2022},
	abstract     = {On-demand delivery is an emerging business in recent years where accurate indoor locations of Gig couriers play an important role in the order dispatch and delivery process. To cater to this need, WiFi-based indoor positioning methods have become an alternative method for on-demand delivery thanks to extensive WiFi deployment in the indoor environment. Existing WiFi-based indoor localization and positioning methods are not suitable for large-scale on-demand delivery scenarios due to high costs (e.g., high labor cost to collect fingerprints) and limited coverage due to limited labeled data. In this work, we explore (i) massive crowdsourced WiFi data collecting from wearable or mobile devices of couriers with little extra effort and (ii) natural manual reports data in the delivery process as two opportunities to perform merchant-level indoor positioning in a weak-supervised manner. Specifically, we proposed WePos, an end-to-end weak-supervised-based merchant-level positioning framework, which consists of the following three parts: (i) a Bidirectional Encoder Representations from Transformers (BERT) based pre-training module to learn latent embeddings of WiFi access points, (ii) a contrastive label self-generate module to produce pseudos for WiFi scanning lists by matching similarity embedding clustering results and couriers' reporting behaviors. (iii) a deep neural network-based classifier to fine-tune the whole training process and conduct online merchant-level position inference. To evaluate the performance of our system, we conduct extensive experiments in both a large-scale public crowdsourcing dataset with over 50 GB of WiFi signal records and a real-world WiFi crowdsourced dataset collected from Eleme, (i.e., one of the largest on-demand delivery platforms in China) in four multi-floor malls in Shanghai. Experimental results show that WePos outperforms state-of-the-art baselines in the merchant-level positioning performance, offer up to 91.4% in positioning accuracy.},
	articleno    = 54,
	numpages     = 25,
	keywords     = {Weak-supervised Learning, Merchant-level Indoor Positioning, WiFi}
}
@article{10.1145/3517256,
	title        = {Testing a Drop of Liquid Using Smartphone LiDAR},
	author       = {Chan, Justin and Raghunath, Ananditha and Michaelsen, Kelly E. and Gollakota, Shyamnath},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517256},
	url          = {https://doi.org/10.1145/3517256},
	issue_date   = {March 2022},
	abstract     = {We present the first system to determine fluid properties using the LiDAR sensors present on modern smartphones. Traditional methods of measuring properties like viscosity require expensive laboratory equipment or a relatively large amount of fluid. In contrast, our smartphone-based method is accessible, contactless and works with just a single drop of liquid. Our design works by targeting a coherent LiDAR beam from the phone onto the liquid. Using the phone's camera, we capture the characteristic laser speckle pattern that is formed by the interference of light reflecting from light-scattering particles. By correlating the fluctuations in speckle intensity over time, we can characterize the Brownian motion within the liquid which is correlated with its viscosity. The speckle pattern can be captured on a range of phone cameras and does not require external magnifiers. Our results show that we can distinguish between different fat contents as well as identify adulterated milk. Further, algorithms can classify between ten different liquids using the smartphone LiDAR speckle patterns. Finally, we conducted a clinical study with whole blood samples across 30 patients showing that our approach can distinguish between coagulated and uncoagulated blood using a single drop of blood.},
	articleno    = 3,
	numpages     = 27,
	keywords     = {lidar, liquid testing, laser speckle}
}
@article{10.1145/3517247,
	title        = {BayesBeat: Reliable Atrial Fibrillation Detection from Noisy Photoplethysmography Data},
	author       = {Das, Sarkar Snigdha Sarathi and Shanto, Subangkar Karmaker and Rahman, Masum and Islam, Md Saiful and Rahman, Atif Hasan and Masud, Mohammad M. and Ali, Mohammed Eunus},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517247},
	url          = {https://doi.org/10.1145/3517247},
	issue_date   = {March 2022},
	abstract     = {Smartwatches or fitness trackers have garnered a lot of popularity as potential health tracking devices due to their affordable and longitudinal monitoring capabilities. To further widen their health tracking capabilities, in recent years researchers have started to look into the possibility of Atrial Fibrillation (AF) detection in real-time leveraging photoplethysmography (PPG) data, an inexpensive sensor widely available in almost all smartwatches. A significant challenge in AF detection from PPG signals comes from the inherent noise in the smartwatch PPG signals. In this paper, we propose a novel deep learning based approach, BayesBeat that leverages the power of Bayesian deep learning to accurately infer AF risks from noisy PPG signals, and at the same time provides an uncertainty estimate of the prediction. Extensive experiments on two publicly available dataset reveal that our proposed method BayesBeat outperforms the existing state-of-the-art methods. Moreover, BayesBeat is substantially more efficient having 40--200X fewer parameters than state-of-the-art baseline approaches making it suitable for deployment in resource constrained wearable devices.},
	articleno    = 8,
	numpages     = 21,
	keywords     = {Mobile health, Photoplethysmography (PPG), Atrial fibrillation, Bayesian deep learning}
}
@article{10.1145/3517245,
	title        = {ILLOC: In-Hall Localization with Standard LoRaWAN Uplink Frames},
	author       = {Guo, Dongfang and Gu, Chaojie and Jiang, Linshan and Luo, Wenjie and Tan, Rui},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517245},
	url          = {https://doi.org/10.1145/3517245},
	issue_date   = {March 2022},
	abstract     = {LoRaWAN is a narrowband wireless technology for ubiquitous connectivity. For various applications, it is desirable to localize LoRaWAN devices based on their uplink frames that convey application data. This localization service operates in an unobtrusive manner, in that it requires no special software instrumentation to the LoRaWAN devices. This paper investigates the feasibility of unobtrusive localization for LoRaWAN devices in hall-size indoor spaces like warehouses, airport terminals, sports centers, museum halls, etc. We study the TDoA-based approach, which needs to address two challenges of poor timing performance of LoRaWAN narrowband signal and nanosecond-level clock synchronization among anchors. We propose the ILLOC system featuring two LoRaWAN-specific techniques: (1) the cross-correlation among the differential phase sequences received by two anchors to estimate TDoA and (2) the just-in-time synchronization enabled by a specially deployed LoRaWAN end device providing time reference upon detecting a target device's transmission. In a long tunnel corridor, a 70 x 32 m2 sports hall, and a 110 x 70 m2 indoor plaza with extensive non-line-of-sight propagation paths, ILLOC achieves median localization errors of 6 m (with 2 anchors), 8.36 m (with 6 anchors), and 15.16 m (with 6 anchors and frame fusion), respectively. The achieved accuracy makes ILLOC useful for applications including zone-level asset tracking, misplacement detection, airport trolley management, and cybersecurity enforcement like detecting impersonation attacks launched by remote radios.},
	articleno    = 13,
	numpages     = 26,
	keywords     = {indoor localization, LoRaWAN, TDoA}
}
@article{10.1145/3517241,
	title        = {Towards Robust Gesture Recognition by Characterizing the Sensing Quality of WiFi Signals},
	author       = {Gao, Ruiyang and Li, Wenwei and Xie, Yaxiong and Yi, Enze and Wang, Leye and Wu, Dan and Zhang, Daqing},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517241},
	url          = {https://doi.org/10.1145/3517241},
	issue_date   = {March 2022},
	abstract     = {WiFi-based gesture recognition emerges in recent years and attracts extensive attention from researchers. Recognizing gestures via WiFi signal is feasible because a human gesture introduces a time series of variations to the received raw signal. The major challenge for building a ubiquitous gesture recognition system is that the mapping between each gesture and the series of signal variations is not unique, exact the same gesture but performed at different locations or with different orientations towards the transceivers generates entirely different gesture signals (variations). To remove the location dependency, prior work proposes to use gesture-level location-independent features to characterize the gesture instead of directly matching the signal variation pattern. We observe that gesture-level features cannot fully remove the location dependency since the signal qualities inside each gesture are different and also depends on the location. Therefore, we divide the signal time series of each gesture into segments according to their qualities and propose customized signal processing techniques to handle them separately. To realize this goal, we characterize signal's sensing quality by building a mathematical model that links the gesture signal with the ambient noise, from which we further derive a unique metric i.e., error of dynamic phase index (EDP-index) to quantitatively describe the sensing quality of signal segments of each gesture. We then propose a quality-oriented signal processing framework that maximizes the contribution of the high-quality signal segments and minimizes the impact of low-quality signal segments to improve the performance of gesture recognition applications. We develop a prototype on COTS WiFi devices. The extensive experimental results demonstrate that our system can recognize gestures with an accuracy of more than 94% on average, and significant improvements compared with state-of-arts.},
	articleno    = 11,
	numpages     = 26,
	keywords     = {Gesture Recognition, Human-Computer Interaction (HCI), Wireless Sensing}
}
@article{10.1145/3495002,
	title        = {VREED: Virtual Reality Emotion Recognition Dataset Using Eye Tracking &amp; Physiological Measures},
	author       = {Tabbaa, Luma and Searle, Ryan and Bafti, Saber Mirzaee and Hossain, Md Moinul and Intarasisrisawat, Jittrapol and Glancy, Maxine and Ang, Chee Siang},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3495002},
	url          = {https://doi.org/10.1145/3495002},
	issue_date   = {Dec 2021},
	abstract     = {The paper introduces a multimodal affective dataset named VREED (VR Eyes: Emotions Dataset) in which emotions were triggered using immersive 360° Video-Based Virtual Environments (360-VEs) delivered via Virtual Reality (VR) headset. Behavioural (eye tracking) and physiological signals (Electrocardiogram (ECG) and Galvanic Skin Response (GSR)) were captured, together with self-reported responses, from healthy participants (n=34) experiencing 360-VEs (n=12, 1--3 min each) selected through focus groups and a pilot trial. Statistical analysis confirmed the validity of the selected 360-VEs in eliciting the desired emotions. Preliminary machine learning analysis was carried out, demonstrating state-of-the-art performance reported in affective computing literature using non-immersive modalities. VREED is among the first multimodal VR datasets in emotion recognition using behavioural and physiological signals. VREED is made publicly available on Kaggle1. We hope that this contribution encourages other researchers to utilise VREED further to understand emotional responses in VR and ultimately enhance VR experiences design in applications where emotional elicitation plays a key role, i.e. healthcare, gaming, education, etc.},
	articleno    = 178,
	numpages     = 20,
	keywords     = {Dataset, Virtual Reality, Affective Computing, GSR, ECG}
}
@article{10.1145/3494996,
	title        = {Twin Meander Coil: Sensitive Readout of Battery-Free On-Body Wireless Sensors Using Body-Scale Meander Coils},
	author       = {Takahashi, Ryo and Yukita, Wakako and Sasatani, Takuya and Yokota, Tomoyuki and Someya, Takao and Kawahara, Yoshihiro},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494996},
	url          = {https://doi.org/10.1145/3494996},
	issue_date   = {Dec 2021},
	abstract     = {Energy-efficient and unconstrained wearable sensing platforms are essential for ubiquitous healthcare and activity monitoring applications. This paper presents Twin Meander Coil for wirelessly connecting battery-free on-body sensors to a textile-based reader knitted into clothing. This connection is based on passive inductive telemetry (PIT), wherein an external reader coil collects data from passive sensor coils via the magnetic field. In contrast to standard active sensing techniques, PIT does not require the reader to power up the sensors. Thus, the reader can be fabricated using a lossy conductive thread and industrial knitting machines. Furthermore, the sensors can superimpose information such as ID, touch, rotation, and pressure on its frequency response. However, conventional PIT technology needs a strong coupling between the reader and the sensor, requiring the reader to be small to the same extent as the sensors' size. Thus, applying this technology to body-scale sensing systems is challenging. To enable body-scale readout, Twin Meander Coil enhances the sensitivity of PIT technology by dividing the body-scale meander-shaped reader coils into two parts and integrating them so that they support the readout of each other. To demonstrate its feasibility, we built a prototype with a knitting machine, evaluated its sensing ability, and demonstrated several applications.},
	articleno    = 179,
	numpages     = 21,
	keywords     = {passive inductive telemetry, wireless, meander, knit, coil, sensing, battery-free}
}
@article{10.1145/3494994,
	title        = {IMU2Doppler: Cross-Modal Domain Adaptation for Doppler-Based Activity Recognition Using IMU Data},
	author       = {Bhalla, Sejal and Goel, Mayank and Khurana, Rushil},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494994},
	url          = {https://doi.org/10.1145/3494994},
	issue_date   = {Dec 2021},
	abstract     = {The proliferation of sensors powered by state-of-the-art machine learning techniques can now infer context, recognize activities and enable interactions. A key component required to build these automated sensing systems is labeled training data. However, the cost of collecting and labeling new data impedes our ability to deploy new sensors to recognize human activities. We tackle this challenge using domain adaptation i.e., using existing labeled data in a different domain to aid the training of a machine learning model for a new sensor. In this paper, we use off-the-shelf smartwatch IMU datasets to train an activity recognition system for mmWave radar sensor with minimally labeled data. We demonstrate that despite the lack of extensive datasets for mmWave radar, we are able to use our domain adaptation approach to build an activity recognition system that classifies between 10 activities with an accuracy of 70% with only 15 seconds of labeled doppler data. We also present results for a range of available labeled data (10 - 30 seconds) and show that our approach outperforms the baseline in every single scenario. We take our approach a step further and show that multiple IMU datasets can be combined together to act as a single source for our domain adaptation approach. Lastly, we discuss the limitations of our work and how it can impact future research directions.},
	articleno    = 145,
	numpages     = 20,
	keywords     = {domain adaptation, doppler sensor}
}
@article{10.1145/3494990,
	title        = {Quantifying the Causal Effect of Individual Mobility on Health Status in Urban Space},
	author       = {Zhang, Yunke and Xu, Fengli and Xia, Tong and Li, Yong},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494990},
	url          = {https://doi.org/10.1145/3494990},
	issue_date   = {Dec 2021},
	abstract     = {How does individual mobility in the urban environment impact their health status? Previous works have explored the correlation between human mobility behaviour and individual health, yet the study on the underlying causal effect is woefully inadequate. However, the correlation analysis can sometimes be bewildering because of the confounding effects. For example, older people visit park more often but have worse health status than younger people. The common associations with age will lead to a counter-intuitive negative correlation between park visits and health status. Obtaining causal effects from confounded observations remains a challenge. In this paper, we construct a causal framework based on propensity score matching on multi-level treatment to eliminate the bias brought by confounding effects and estimate the total treatment effects of mobility behaviours on health status. We demonstrate that the matching procedure approximates a de-confounded randomized experiment where confounding variables are balanced substantially. The analysis on the directions of estimated causal effects reveals that fewer neighbouring tobacco shops and frequent visits to sports facilities are related with higher risk in health status, which differs from their correlation directions. Physical mobility behaviours and environment features have more significant estimated effects on health status than contextual mobility behaviours. Moreover, we embed our causal analysis framework in health prediction models to filter out features with superficial correlation but insignificant effects that might lead to over-fitting. This strategy achieves better model robustness with more features filtered out than L1-regularization. Our findings shed light on individual healthy lifestyle and mobility-related health policymaking.},
	articleno    = 193,
	numpages     = 30,
	keywords     = {Causal inference, Urban mobility, Health status}
}
@article{10.1145/3494985,
	title        = {RFaceID: Towards RFID-Based Facial Recognition},
	author       = {Luo, Chengwen and Yang, Zhongru and Feng, Xingyu and Zhang, Jin and Jia, Hong and Li, Jianqiang and Wu, Jiawei and Hu, Wen},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494985},
	url          = {https://doi.org/10.1145/3494985},
	issue_date   = {Dec 2021},
	abstract     = {Face recognition (FR) has been widely used in many areas nowadays. However, the existing mainstream vision-based facial recognition has limitations such as vulnerability to spoofing attacks, sensitivity to lighting conditions, and high risk of privacy leakage, etc. To address these problems, in this paper we take a sparkly different approach and propose RFaceID, a novel RFID-based face recognition system. RFaceID only needs the users to shake their faces in front of the RFID tag matrix for a few seconds to get their faces recognized. Through theoretical analysis and experiment validations, the feasibility of the RFID-based face recognition is studied. Multiple data processing and data augmentation techniques are proposed to minimize the negative impact of environmental noises and user dynamics. A deep neural network (DNN) model is designed to characterize both the spatial and temporal feature of face shaking events. We implement the system and extensive evaluation results show that RFaceID achieves a high face recognition accuracy at 93.1% for 100 users, which shows the potential of RFaceID for future facial recognition applications.},
	articleno    = 170,
	numpages     = 21,
	keywords     = {neural network, RFID, data augmentation, face recognition}
}
@article{10.1145/3494970,
	title        = {MSLife: Digital Behavioral Phenotyping of Multiple Sclerosis Symptoms in the Wild Using Wearables and Graph-Based Statistical Analysis},
	author       = {Guo, Gabriel and Zhang, Hanbin and Yao, Liuyi and Li, Huining and Xu, Chenhan and Li, Zhengxiong and Xu, Wenyao},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494970},
	url          = {https://doi.org/10.1145/3494970},
	issue_date   = {Dec 2021},
	abstract     = {Treatment for multiple sclerosis (MS) focuses on managing its symptoms (e.g., depression, fatigue, poor sleep quality), varying with specific symptoms experienced. Thus, for optimal treatment, there arises the need to track these symptoms. Towards this goal, there is great interest in finding their relevant phenotypes. Prior research suggests links between activities of daily living (ADLs) and MS symptoms; therefore, we hypothesize that the behavioral phenotype (revealed through ADLs) is closely related to MS symptoms. Traditional approaches to finding behavioral phenotypes which rely on human observation or controlled clinical settings are burdensome and cannot account for all genuine ADLs. Here, we present MSLife, an end-to-end, burden-free approach to digital behavioral phenotyping of MS symptoms in the wild using wearables and graph-based statistical analysis. MSLife is built upon (1) low-cost, unobtrusive wearables (i.e., smartwatches) that can track and quantify ADLs among MS patients in the wild; (2) graph-based statistical analysis that can model the relationships between quantified ADLs (i.e., digital behavioral phenotype) and MS symptoms. We design, implement, and deploy MSLife with 30 MS patients across a one-week home-based IRB-approved clinical pilot study. We use the GENEActiv smartwatch to monitor ADLs and clinical behavioral instruments to collect MS symptoms. Then we develop a graph-based statistical analysis framework to model phenotyping relationships between ADLs and MS symptoms, incorporating confounding demographic factors. We discover 102 significant phenotyping relationships (e.g., later rise times are related to increased levels of depression, history of caffeine consumption is associated with lower fatigue levels, higher relative levels of moderate physical activity are linked with decreased sleep quality). We validate their healthcare implications, using them to track MS symptoms in retrospective analysis. To our best knowledge, this is one of the first practices to digital behavioral phenotyping of MS symptoms in the wild.},
	articleno    = 158,
	numpages     = 35,
	keywords     = {Digital Behavioral Phenotyping, Wearables, Multiple Sclerosis, Graph-Based Statistical Analysis, Mobile Health}
}
@article{10.1145/3478117,
	title        = {A City-Wide Crowdsourcing Delivery System with Reinforcement Learning},
	author       = {Ding, Yi and Guo, Baoshen and Zheng, Lin and Lu, Mingming and Zhang, Desheng and Wang, Shuai and Son, Sang Hyuk and He, Tian},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478117},
	url          = {https://doi.org/10.1145/3478117},
	issue_date   = {Sept 2021},
	abstract     = {The revolution of online shopping in recent years demands corresponding evolution in delivery services in urban areas. To cater to this trend, delivery by the crowd has become an alternative to the traditional delivery services thanks to the advances in ubiquitous computing. Notably, some studies use public transportation for crowdsourcing delivery, given its low-cost delivery network with millions of passengers as potential couriers. However, multiple practical impact factors are not considered in existing public-transport-based crowdsourcing delivery studies due to a lack of data and limited ubiquitous computing infrastructures in the past. In this work, we design a crowdsourcing delivery system based on public transport, considering the practical factors of time constraints, multi-hop delivery, and profits. To incorporate the impact factors, we build a reinforcement learning model to learn the optimal order dispatching strategies from massive passenger data and package data. The order dispatching problem is formulated as a sequential decision making problem for the packages routing, i.e., select the next station for the package. A delivery time estimation module is designed to accelerate the training process and provide statistical delivery time guarantee. Three months of real-world public transportation data and one month of package delivery data from an on-demand delivery platform in Shenzhen are used in the evaluation. Compared with existing crowdsourcing delivery algorithms and widely used baselines, we achieve a 40% increase in profit rates and a 29% increase in delivery rates. Comparison with other reinforcement learning algorithms shows that we can improve the profit rate and the delivery rate by 9% and 8% by using time estimation in action filtering. We share the data used in the project to the community for other researchers to validate our results and conduct further research.1 [1].},
	articleno    = 97,
	numpages     = 22,
	keywords     = {Reinforcement Learning, Crowdsourced Labor, Sharing Economy, Crowdsourcing}
}
@article{10.1145/3478108,
	title        = {HERMAS: A Human Mobility Embedding Framework with Large-Scale Cellular Signaling Data},
	author       = {Song, Yiwei and Jiang, Dongzhe and Liu, Yunhuai and Qin, Zhou and Tan, Chang and Zhang, Desheng},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478108},
	url          = {https://doi.org/10.1145/3478108},
	issue_date   = {Sept 2021},
	abstract     = {Efficient representations for spatio-temporal cellular Signaling Data (SD) are essential for many human mobility applications. Traditional representation methods are mainly designed for GPS data with high spatio-temporal continuity, and thus will suffer from poor embedding performance due to the unique Ping Pong Effect in SD. To address this issue, we explore the opportunity offered by a large number of human mobility traces and mine the inherent neighboring tower connection patterns. More specifically, we design HERMAS, a novel representation learning framework for large-scale cellular SD with three steps: (1) extract rich context information in each trajectory, adding neighboring tower information as extra knowledge in each mobility observation; (2) design a sequence encoding model to aggregate the embedding of each observation; (3) obtain the embedding for a trajectory. We evaluate the performance of HERMAS based on two human mobility applications, i.e. trajectory similarity measurement and user profiling. We conduct evaluations based on a 30-day SD dataset with 130,612 users and 2,369,267 moving trajectories. Experimental results show that (1) for the trajectory similarity measurement application, HERMAS improves the Hitting Rate (HR@10) from 15.2% to 39.2%; (2) for the user profiling application, HERMAS improves the F1-score for around 9%. More importantly, HERMAS significantly improves the computation efficiency by over 30x.},
	articleno    = 128,
	numpages     = 21,
	keywords     = {Signaling Data, Representation Learning, Trajectory Embedding, Regular Pattern Exploration}
}
@article{10.1145/3478106,
	title        = {The Crowd Wisdom for Location Privacy of Crowdsensing Photos: Spear or Shield?},
	author       = {Zhou, Tongqing and Cai, Zhiping and Liu, Fang},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478106},
	url          = {https://doi.org/10.1145/3478106},
	issue_date   = {Sept 2021},
	abstract     = {The incorporation of the mobile crowd in visual sensing provides a significant opportunity to explore and understand uncharted physical places. We investigate the gains and losses of the involvement of the crowd wisdom on users' location privacy in photo crowdsensing. For the negative effects, we design a novel crowdsensing photo location inference model, regardless of the robust location protection techniques, by jointly exploiting the visual representation, correlation, and geo-annotation capabilities extracted from the crowd. Compared with existing retrieval-based and model-based location inference techniques, our proposal poses more pernicious threats to location privacy by considering the no-reference-photos situations of crowdsensing. We conduct extensive analyses on the model with four photo datasets and crowdsourcing surveys for geo-annotation. The results indicate that being in a crowd of photos will, unfortunately, increase one's risk to be geo-identified, and highlights that the model can yield a considerable high inference accuracy (48%~70%) and serious privacy exposure (over 80% of users get privacy disclosed) with a small portion of geo-annotated samples. In view of the threats, we further propose an adaptive grouping-based signing model that hides a user's track with the camouflage of a crowd of users. Wherein, ring signature is tailored for crowdsensing to provide indistinguishable while valid identities for every user's submission. We theoretically analyze its adjustable privacy protection capability and develop a prototype to evaluate the effectiveness and performance.},
	articleno    = 142,
	numpages     = 23,
	keywords     = {crowd wisdom, crowdsourcing, location privacy, mobile crowdsensing}
}
@article{10.1145/3478101,
	title        = {Demystifying the Vetting Process of Voice-Controlled Skills on Markets},
	author       = {Wang, Dawei and Chen, Kai and Wang, Wei},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478101},
	url          = {https://doi.org/10.1145/3478101},
	issue_date   = {Sept 2021},
	abstract     = {Smart speakers, such as Google Home and Amazon Echo, have become popular. They execute user voice commands via their built-in functionalities together with various third-party voice-controlled applications, called skills. Malicious skills have brought significant threats to users in terms of security and privacy. As a countermeasure, only skills passing the strict vetting process can be released onto markets. However, malicious skills have been reported to exist on markets, indicating that the vetting process can be bypassed. This paper aims to demystify the vetting process of skills on main markets to discover weaknesses and protect markets better. To probe the vetting process, we carefully design numerous skills, perform the Turing test, a test for machine intelligence, to determine whether humans or machines perform vetting, and leverage natural language processing techniques to analyze their behaviors. Based on our comprehensive experiments, we gain a good understanding of the vetting process (e.g., machine or human testers and skill exploration strategies) and discover some weaknesses. In this paper, we design three types of attacks to verify our results and prove an attacker can embed sensitive behaviors in skills and bypass the strict vetting process. Accordingly, we also propose countermeasures to these attacks and weaknesses.},
	articleno    = 130,
	numpages     = 28,
	keywords     = {Skill, Google-Home, Alexa, vetting process}
}
@article{10.1145/3478090,
	title        = {RF Vital Sign Sensing under Free Body Movement},
	author       = {Gong, Jian and Zhang, Xinyu and Lin, Kaixin and Ren, Ju and Zhang, Yaoxue and Qiu, Wenxun},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478090},
	url          = {https://doi.org/10.1145/3478090},
	issue_date   = {Sept 2021},
	abstract     = {Radio frequency (RF) sensors such as radar are instrumental for continuous, contactless sensing of vital signs, especially heart rate (HR) and respiration rate (RR). However, decades of related research mainly focused on static subjects, because the motion artifacts from other body parts may easily overwhelm the weak reflections from vital signs. This paper marks a first step in enabling RF vital sign sensing under ambulant daily living conditions. Our solution is inspired by existing physiological research that revealed the correlation between vital signs and body movement. Specifically, we propose to combine direct RF sensing for static instances and indirect vital sign prediction based on movement power estimation. We design customized machine learning models to capture the sophisticated correlation between RF signal pattern, movement power, and vital signs. We further design an instant calibration and adaptive training scheme to enable cross-subjects generalization, without any explicit data labeling from unknown subjects. We prototype and evaluate the framework using a commodity radar sensor. Under a variety of moving conditions, our solution demonstrates an average estimation error of 5.57 bpm for HR and 3.32 bpm for RR across multiple subjects, which largely outperforms state-of-the-art systems.},
	articleno    = 101,
	numpages     = 22,
	keywords     = {millimeter wave radar, deep learning, heart rate, respiration rate}
}
@article{10.1145/3463527,
	title        = {Douleur: Creating Pain Sensation with Chemical Stimulant to Enhance User Experience in Virtual Reality},
	author       = {Jiang, Chutian and Chen, Yanjun and Fan, Mingming and Wang, Liuping and Shen, Luyao and Li, Nianlong and Sun, Wei and Zhang, Yu and Tian, Feng and Han, Teng},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463527},
	url          = {https://doi.org/10.1145/3463527},
	issue_date   = {June 2021},
	abstract     = {The imitation of pain sensation in Virtual Reality is considered valuable for safety education and training but has been seldom studied. This paper presents Douleur, a wearable haptic device that renders intensity-adjustable pain sensations with chemical stimulants. Different from mechanical, thermal, or electric stimulation, chemical-induced pain is more close to burning sensations and long-lasting. Douleur consists of a microfluidic platform that precisely emits capsaicin onto the skin and a microneedling component to help the stimulant penetrate the epidermis layer to activate the trigeminal nerve efficiently. Moreover, it embeds a Peltier module to apply the heating or cooling stimulus to the affected area to adjust the level of pain on the skin. To better understand how people would react to the chemical stimulant, we conducted a first study to quantify the enhancement of the sensation by changing the capsaicin concentration, skin temperature, and time and to determine suitable capsaicin concentration levels. In the second study, we demonstrated that Douleur could render a variety of pain sensations in corresponding virtual reality applications. In sum, Douleur is the first wearable prototype that leverages a combination of capsaicin and Peltier to induce rich pain sensations and opens up a wide range of applications for safety education and more.},
	articleno    = 66,
	numpages     = 26,
	keywords     = {capsaicin, Virtual reality, prototype, user experience, pain sensation, safety education}
}
@article{10.1145/3463525,
	title        = {Who Will Survive and Revive Undergoing the Epidemic: Analyses about POI Visit Behavior in Wuhan via Check-in Records},
	author       = {Han, Zhenyu and Fu, Haohao and Xu, Fengli and Tu, Zhen and Yu, Yang and Hui, Pan and Li, Yong},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463525},
	url          = {https://doi.org/10.1145/3463525},
	issue_date   = {June 2021},
	abstract     = {A rapid-spreading epidemic of COVID-19 hit China at the end of 2019, resulting in unignorable social and economic damage in the epicenter, Wuhan. POIs capture the microscopic behavior of citizens, providing valuable information to understand city reactions toward the epidemic. Leveraging large-scale check-in records, we analyze the POI visit trends over the epidemic period and normal times. We demonstrate that COVID-19 greatly influences the society, where most POIs demonstrate more than 60% of visit drops during the city lockdown period. Among them, Tourist Attractions received greatest impact with a 78.8% drop. Entertainment, Food, Medical and Shopping are sensible to the disease before lockdown, and we identify these "early birds" to investigate the public reaction in the early stage of the epidemic. We further analyze the revival trends, generating four different revival patterns that correlated with the necessity of POI functions. Finally, we analyze the perseverance during the COVID-19, finding no large-scale closures compared with the tremendous visit drop. The strong resilience in Wuhan supports the rapid recovery of society. These findings are important for researchers, industries, and governments to understand the city respondence under severe epidemic, proposing better regulations to respond, control, and prevent public emergencies.},
	articleno    = 64,
	numpages     = 20,
	keywords     = {data driven, COVID-19, check-in, POI, time series analyze}
}
@article{10.1145/3463524,
	title        = {LightGuide: Directing Visually Impaired People along a Path Using Light Cues},
	author       = {Yang, Ciyuan and Xu, Shuchang and Yu, Tianyu and Liu, Guanhong and Yu, Chun and Shi, Yuanchun},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463524},
	url          = {https://doi.org/10.1145/3463524},
	issue_date   = {June 2021},
	abstract     = {Precise and reliable directional feedback is crucial for electronic traveling aids that guide visually impaired people along safe paths. A large proportion of visually impaired people can determine light position using their light perception. This work presents LightGuide, a directional feedback solution that indicates a safe direction of travel via the position of a light within the user's visual field. We prototyped LightGuide using an LED strip attached to the brim of a cap, and conducted three user studies to explore the effectiveness of LightGuide compared to HapticBag, a state-of-the-art baseline solution that indicates directions through on-shoulder vibrations. Results showed that, with LightGuide, participants turned to target directions in place more quickly and smoothly, and navigated along basic and complex paths more efficiently, smoothly, and accurately than HapticBag. Users' subjective feedback implied that LightGuide was easy to learn and intuitive to use. The potential limitations of using LightGuide in real environments are subsequently discussed.},
	articleno    = 84,
	numpages     = 27,
	keywords     = {visual impairment, light perception, path-following tasks, visual feedback}
}
@article{10.1145/3463523,
	title        = {A Survey and Taxonomy of Electronics Toolkits for Interactive and Ubiquitous Device Prototyping},
	author       = {Lambrichts, Mannu and Ramakers, Raf and Hodges, Steve and Coppers, Sven and Devine, James},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463523},
	url          = {https://doi.org/10.1145/3463523},
	issue_date   = {June 2021},
	abstract     = {Over the past two decades, many toolkits for prototyping interactive and ubiquitous electronic devices have been developed. Although their technical specifications are often easy to look up, they vary greatly in terms of design, features and target audience, resulting in very real strengths and weaknesses depending on the intended application. These less technical characteristics are often reported inconsistently, if at all. In this paper we provide a comprehensive survey of interactive and ubiquitous device prototyping toolkits, systematically analysing their characteristics within the framework of a new taxonomy that we present. In addition to the specific characteristics we cover, we introduce a way to evaluate toolkits more holistically, covering user needs such as 'ease of construction' and 'ease of moving from prototype to product' rather than features. We also present results from an online survey which offers new insights on how the surveyed users prioritize these characteristics during prototyping, and what techniques they use to move beyond prototyping. We hope our analysis will be valuable for others in the community who need to build and potentially scale out prototypes as part of their research. We end by identifying gaps that have not yet been addressed by existing offerings and discuss opportunities for future research into electronics prototyping toolkits.},
	articleno    = 70,
	numpages     = 24,
	keywords     = {toolkits, electronics prototyping platforms, ubiquitous computing, interactive devices}
}
@article{10.1145/3463521,
	title        = {BlinkListener: "Listen" to Your Eye Blink Using Your Smartphone},
	author       = {Liu, Jialin and Li, Dong and Wang, Lei and Xiong, Jie},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463521},
	url          = {https://doi.org/10.1145/3463521},
	issue_date   = {June 2021},
	abstract     = {Eye blink detection plays a key role in many real-life applications such as Human-Computer Interaction (HCI), drowsy driving prevention and eye disease detection. Although traditional camera-based techniques are promising, multiple issues hinder their wide adoption including the privacy concern, strict lighting condition and line-of-sight (LoS) requirements. On the other hand, wireless sensing without a need for dedicated sensors gains a tremendous amount of attention in recent years. Among the wireless signals utilized for sensing, acoustic signals show a unique potential for fine-grained sensing owing to their low propagation speed in the air. Another trend favoring acoustic sensing is the wide availability of speakers and microphones in commodity devices. Promising progress has been achieved in fine-grained human motion sensing such as breathing using acoustic signals. However, it is still very challenging to employ acoustic signals for eye blink detection due to the unique characteristics of eye blink (i.e., subtle, sparse and aperiodic) and severe interference (i.e., from the human target himself and surrounding objects). We find that even the very subtle involuntary head movement induced by breathing can severely interfere with eye blink detection. In this work, for the first time, we propose a system called BlinkListener to sense the subtle eye blink motion using acoustic signals in a contact-free manner. We first quantitatively model the relationship between signal variation and the subtle movements caused by eye blink and interference. Then, we propose a novel method that exploits the "harmful" interference to maximize the subtle signal variation induced by eye blinks. We implement BlinkListener on both a research-purpose platform (Bela) and a commodity smartphone (iPhone 5c). Experiment results show that BlinkListener can achieve robust performance with a median detection accuracy of 95%. Our system can achieve high accuracies when the smartphone is held in hand, the target wears glasses/sunglasses and in the presence of strong interference with people moving around.},
	articleno    = 73,
	numpages     = 27,
	keywords     = {HCI, acoustic signals, contact-free sensing, eye blink detection}
}
@article{10.1145/3463511,
	title        = {NeckFace: Continuously Tracking Full Facial Expressions on Neck-Mounted Wearables},
	author       = {Chen, Tuochao and Li, Yaxuan and Tao, Songyun and Lim, Hyunchul and Sakashita, Mose and Zhang, Ruidong and Guimbretiere, Francois and Zhang, Cheng},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463511},
	url          = {https://doi.org/10.1145/3463511},
	issue_date   = {June 2021},
	abstract     = {Facial expressions are highly informative for computers to understand and interpret a person's mental and physical activities. However, continuously tracking facial expressions, especially when the user is in motion, is challenging. This paper presents NeckFace, a wearable sensing technology that can continuously track the full facial expressions using a neck-piece embedded with infrared (IR) cameras. A customized deep learning pipeline called NeckNet based on Resnet34 is developed to learn the captured infrared (IR) images of the chin and face and output 52 parameters representing the facial expressions. We demonstrated NeckFace on two common neck-mounted form factors: a necklace and a neckband (e.g., neck-mounted headphones), which was evaluated in a user study with 13 participants. The study results showed that NeckFace worked well when the participants were sitting, walking, or after remounting the device. We discuss the challenges and opportunities of using NeckFace in real-world applications.},
	articleno    = 58,
	numpages     = 31,
	keywords     = {Wearable, Infrared Imaging, Deep learning, Facial expressions}
}
@article{10.1145/3463500,
	title        = {LumNet: Learning to Estimate Vertical Visual Field Luminance for Adaptive Lighting Control},
	author       = {Songwa, Prince U.C. and Saeed, Aaqib and Bhardwaj, Sachin and Kruisselbrink, Thijs W. and Ozcelebi, Tanir},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463500},
	url          = {https://doi.org/10.1145/3463500},
	issue_date   = {June 2021},
	abstract     = {High-quality lighting positively influences visual performance in humans. The experienced visual performance can be measured using desktop luminance and hence several lighting control systems have been developed for its quantification. However, the measurement devices that are used to monitor the desktop luminance in existing lighting control systems are obtrusive to the users. As an alternative, ceiling-based luminance projection sensors are being used recently as these are unobtrusive and can capture the direct task area of a user. The positioning of these devices on the ceiling requires to estimate the desktop luminance in the user's vertical visual field, solely using ceiling-based measurements, to better predict the experienced visual performance of the user. For this purpose, we present LUMNET, an approach for estimating desktop luminance with deep models through utilizing supervised and self-supervised learning. Our model learns visual representations from ceiling-based images, which are collected in indoor spaces within the physical vicinity of the user to predict average desktop luminance as experienced in a real-life setting. We also propose a self-supervised contrastive method for pre-training LUMNET with unlabeled data and we demonstrate that the learned features are transferable onto a small labeled dataset which minimizes the requirement of costly data annotations. Likewise, we perform experiments on domain-specific datasets and show that our approach significantly improves over the baseline results from prior methods in estimating luminance, particularly in the low-data regime. LUMNET is an important step towards learning-based technique for luminance estimation and can be used for adaptive lighting control directly on-device thanks to its minimal computational footprint with an added benefit of preserving user's privacy.},
	articleno    = 79,
	numpages     = 20,
	keywords     = {ambient intelligence, self-supervised learning, HDR, deep learning, adaptive lighting, luminance estimation}
}
@article{10.1145/3463494,
	title        = {MTeeth: Identifying Brushing Teeth Surfaces Using Wrist-Worn Inertial Sensors},
	author       = {Akther, Sayma and Saleheen, Nazir and Saha, Mithun and Shetty, Vivek and Kumar, Santosh},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463494},
	url          = {https://doi.org/10.1145/3463494},
	issue_date   = {June 2021},
	abstract     = {Ensuring that all the teeth surfaces are adequately covered during daily brushing can reduce the risk of several oral diseases. In this paper, we propose the mTeeth model to detect teeth surfaces being brushed with a manual toothbrush in the natural free-living environment using wrist-worn inertial sensors. To unambiguously label sensor data corresponding to different surfaces and capture all transitions that last only milliseconds, we present a lightweight method to detect the micro-event of brushing strokes that cleanly demarcates transitions among brushing surfaces. Using features extracted from brushing strokes, we propose a Bayesian Ensemble method that leverages the natural hierarchy among teeth surfaces and patterns of transition among them. For training and testing, we enrich a publicly-available wrist-worn inertial sensor dataset collected from the natural environment with time-synchronized precise labels of brushing surface timings and moments of transition. We annotate 10,230 instances of brushing on different surfaces from 114 episodes and evaluate the impact of wide between-person and within-person between-episode variability on machine learning model's performance for brushing surface detection.},
	articleno    = 53,
	numpages     = 25,
	keywords     = {hand-to-mouth gestures, flossing detection, mHealth, brushing detection}
}
@article{10.1145/3448121,
	title        = {A Scalable Solution for Signaling Face Touches to Reduce the Spread of Surface-Based Pathogens},
	author       = {Rojas, Camilo and Poulsen, Niels and Van Tuyl, Mileva and Vargas, Daniel and Cohen, Zipporah and Paradiso, Joe and Maes, Pattie and Esvelt, Kevin and Adib, Fadel},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448121},
	url          = {https://doi.org/10.1145/3448121},
	issue_date   = {March 2021},
	abstract     = {Hand-to-Face transmission has been estimated to be a minority, yet non-negligible, vector of COVID-19 transmission and a major vector for multiple other pathogens. At the same time, as it cannot be effectively addressed with mainstream protection measures, such as wearing masks or tracing contacts, it remains largely untackled. To help address this issue, we have developed Saving Face - an app that alerts users when they are about to touch their faces, by analyzing the distortion patterns in the ultrasound signal emitted by their earphones. The system only relies on pre-existing hardware (a smartphone with generic earphones), which allows it to be rapidly scalable to billions of smartphone users worldwide. This paper describes the design, implementation and evaluation of the system, as well as the results of a user study testing the solution's accuracy, robustness, and user experience during various day-to-day activities (93.7% Sensitivity and 91.5% Precision, N=10). While this paper focuses on the system's application to detecting hand-to-face gestures, the technique can also be applicable to other types of gestures and gesture-based applications.},
	articleno    = 31,
	numpages     = 22,
	keywords     = {machine learning, signal processing, mobile devices, behavior change, wearables}
}
@article{10.1145/3448114,
	title        = {A Feature Adaptive Learning Method for High-Density SEMG-Based Gesture Recognition},
	author       = {Zhang, Yingwei and Chen, Yiqiang and Yu, Hanchao and Yang, Xiaodong and Sun, Ruizhe and Zeng, Bixiao},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448114},
	url          = {https://doi.org/10.1145/3448114},
	issue_date   = {March 2021},
	abstract     = {Surface electromyography (sEMG) array based gesture recognition, which is widely-used, could provide natural surfaces for human-computer interaction. Currently, most existing gesture recognition methods with sEMG array only work with the fixed and pre-defined electrodes configuration. However, changes in the number of electrodes (i.e., increment or decrement) is common in real scenarios due to the variability of physiological electrodes. In this paper, we study this challenging problem and propose a random forest based ensemble learning method, namely feature incremental and decremental ensemble learning (FIDE). FIDE is able to support continuous changes in the number of electrodes by dynamically maintaining the matrix sketches of every sEMG electrode and spatial structure of sEMG array. To evaluate the performance of FIDE, we conduct extensive experiments on three benchmark datasets, including NinaPro, CSL-hdemg, and CapgMyo. Experimental results demonstrate that FIDE outperforms other state-of-the-art methods and has the potential to adapt to the evolution of electrodes in the changing environments. Moreover, based on FIDE, we implement a multi clients/server collaboration system, namely McS, to support feature adaption in real-world environment. By collecting sEMG using two clients (smartphone and personal computer) and adaptively recognizing gestures in the cloud server, FIDE significantly improves the gesture recognition accuracy in electrode increment and decrement circumstances.},
	articleno    = 44,
	numpages     = 26,
	keywords     = {Feature Increment and Decrement, Gesture Recognition, Surface Electromyography (sEMG), High-Density, Ensemble Learning}
}
@article{10.1145/3448082,
	title        = {TagFi: Locating Ultra-Low Power WiFi Tags Using Unmodified WiFi Infrastructure},
	author       = {Soltanaghaei, Elahe and Dongare, Adwait and Prabhakara, Akarsh and Kumar, Swarun and Rowe, Anthony and Whitehouse, Kamin},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448082},
	url          = {https://doi.org/10.1145/3448082},
	issue_date   = {March 2021},
	abstract     = {Tag localization is crucial for many context-aware and automation applications in smart homes, retail stores, or warehouses. While custom localization technologies (e.g RFID) have the potential to support low-cost battery-free tag tracking, the cost and complexity of commissioning a space with beacons or readers has stifled adoption. In this paper, we explore how WiFi backscatter localization can be realized using the existing WiFi infrastructure already deployed for data applications. We present a new approach that leverages existing WiFi infrastructure to enable extremely low-power and accurate tag localization relative to a single scanning device. First, we adopt an ultra-low power tag design in which the tag blindly modulates ongoing WiFi packets using On-Off Keying (OOK). Then, we utilize the underlying physical properties of multipath propagation to detect the passive wireless reflection from the tag in the presence of rich multipath propagations. Finally, we localize the tag from a single receiver by forming a triangle between the tag reflection and the LoS path between the two WiFi transceivers. We implement TagFi using a customized backscatter tag and off-the-shelf WiFi chipsets. Our empirical results in a cluttered office building demonstrate that TagFi achieves a median localization accuracy of 0.2m up to 8 meters range.},
	articleno    = 34,
	numpages     = 29,
	keywords     = {CSI, Object localization, multipath propagation, Channel State Information, WiFi}
}
@article{10.1145/3448081,
	title        = {Ride Substitution Using Electric Bike Sharing: Feasibility, Cost, and Carbon Analysis},
	author       = {Wamburu, John and Lee, Stephen and Hajiesmaili, Mohammad H. and Irwin, David and Shenoy, Prashant},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448081},
	url          = {https://doi.org/10.1145/3448081},
	issue_date   = {March 2021},
	abstract     = {While ride-sharing has emerged as a popular form of transportation in urban areas due to its on-demand convenience, it has become a major contributor to carbon emissions, with recent studies suggesting it is 47% more carbon-intensive than personal car trips. In this paper, we examine the feasibility, costs, and carbon benefits of using electric bike-sharing---a low carbon form of ride-sharing---as a potential substitute for shorter ride-sharing trips, with the overall goal of greening the ride-sharing ecosystem. Using public datasets from New York City, our analysis shows that nearly half of the taxi and rideshare trips in New York are shorts trips of less than 3.5km, and that biking is actually faster than using a car for ultra-short trips of 2km or less. We analyze the cost and carbon benefits of different levels of ride substitution under various scenarios. We find that the additional bikes required to satisfy increased demand from ride substitution increases sub-linearly and results in 6.6% carbon emission reduction for 10% taxi ride substitution. Moreover, this reduction can be achieved through a hybrid mix that requires only a quarter of the bikes to be electric bikes, which reduces system costs. We also find that expanding bike-share systems to new areas that lack bike-share coverage requires additional investments due to the need for new bike stations and bike capacity to satisfy demand but also provides substantial carbon emission reductions. Finally, frequent station repositioning can reduce the number of bikes needed in the system by up to a third for a minimal increase in carbon emissions of 2% from the trucks required to perform repositioning, providing an interesting tradeoff between capital costs and carbon emissions.},
	articleno    = 38,
	numpages     = 28,
	keywords     = {Ride Substitution, Optimization, Electric Bikes, Carbon Emission}
}
@article{10.1145/3448079,
	title        = {Efficient Schedule of Energy-Constrained UAV Using Crowdsourced Buses in Last-Mile Parcel Delivery},
	author       = {Pan, Yan and Li, Shining and Chen, Qianwu and Zhang, Nan and Cheng, Tao and Li, Zhigang and Guo, Bin and Han, Qingye and Zhu, Ting},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448079},
	url          = {https://doi.org/10.1145/3448079},
	issue_date   = {March 2021},
	abstract     = {Stimulated by the dramatical service demand in the logistics industry, logistics trucks employed in last-mile parcel delivery bring critical public concerns, such as heavy cost burden, traffic congestion and air pollution. Unmanned Aerial Vehicles (UAVs) are a promising alternative tool in last-mile delivery, which is however limited by insufficient flight range and load capacity. This paper presents an innovative energy-limited logistics UAV schedule approach using crowdsourced buses. Specifically, when one UAV delivers a parcel, it first lands on a crowdsourced social bus to parcel destination, gets recharged by the wireless recharger deployed on the bus, and then flies from the bus to the parcel destination. This novel approach not only increases the delivery range and load capacity of battery-limited UAVs, but is also much more cost-effective and environment-friendly than traditional methods. New challenges therefore emerge as the buses with spatiotemporal mobility become the bottleneck during delivery. By landing on buses, an Energy-Neutral Flight Principle and a delivery scheduling algorithm are proposed for the UAVs. Using the Energy-Neutral Flight Principle, each UAV can plan a flying path without depleting energy given buses with uncertain velocities. Besides, the delivery scheduling algorithm optimizes the delivery time and number of delivered parcels given warehouse location, logistics UAVs, parcel locations and buses. Comprehensive evaluations using a large-scale bus dataset demonstrate the superiority of the innovative logistics UAV schedule approach.},
	articleno    = 28,
	numpages     = 23,
	keywords     = {Crowdsourced bus, Delivery Time, Last-mile delivery, Deliverable parcel number, Energy-Neutral Flight, Logistics UAV}
}
@article{10.1145/3448074,
	title        = {Unsupervised Human Activity Representation Learning with Multi-Task Deep Clustering},
	author       = {Ma, Haojie and Zhang, Zhijie and Li, Wenzhong and Lu, Sanglu},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448074},
	url          = {https://doi.org/10.1145/3448074},
	issue_date   = {March 2021},
	abstract     = {Human activity recognition (HAR) based on sensing data from wearable and mobile devices has become an active research area in ubiquitous computing, and it envisions a wide range of application scenarios in mobile social networking, environmental context sensing, health and well-being monitoring, etc. However, activity recognition based on manually annotated sensing data is manpower-expensive, time-consuming, and privacy-sensitive, which prevents HAR systems from being really deployed in scale. In this paper, we address the problem of unsupervised human activity recognition, which infers activities from unlabeled datasets without the need of domain knowledge. We propose an end-to-end multi-task deep clustering framework to solve the problem. Taking the unlabeled multi-dimensional sensing signals as input, we firstly apply a CNN-BiLSTM autoencoder to form a compressed latent feature representation. Then we apply a K-means clustering algorithm based on the extracted features to partition the dataset into different groups, which produces pseudo labels for the instances. We further train a deep neural network (DNN) with the latent features and pseudo labels for activity recognition. The tasks of feature representation, clustering, and classification are integrated into a uniform multi-task learning framework and optimized jointly to achieve unsupervised activity classification. We conduct extensive experiments based on three public datasets. It is shown that the proposed approach outperforms shallow unsupervised learning approaches, and it performs close to the state-of-the-art supervised approaches by fine-tuning with a small number of labeled data. The proposed approach significantly reduces the cost of human-based data annotation and narrows down the gap between unsupervised and supervised human activity recognition.},
	articleno    = 48,
	numpages     = 25,
	keywords     = {deep clustering, Human activity recognition, multi-task learning, unsupervised learning}
}
@article{10.1145/3432234,
	title        = {ComFeel: Productivity is a Matter of the Senses Too},
	author       = {Constantinides, Marios and \v{S}\'{c}epanovi\'{c}, Sanja and Quercia, Daniele and Li, Hongwei and Sassi, Ugo and Eggleston, Michael},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432234},
	url          = {https://doi.org/10.1145/3432234},
	issue_date   = {December 2020},
	abstract     = {Indoor environmental quality has been found to impact employees' productivity in the long run, yet it is unclear its meeting-level impact in the short term. We studied the relationship between sensorial pleasantness of a meeting's room and the meeting's productivity. By administering a 28-item questionnaire to 363 online participants, we indeed found that three factors captured 62% of people's experience of meetings: (a) productivity; (b) psychological safety; and (c) room pleasantness. To measure room pleasantness, we developed and deployed ComFeel, an indoor environmental sensing infrastructure, which captures light, temperature, and gas resistance readings through miniaturized and unobtrusive devices we built and named 'Geckos'. Across 29 real-world meetings, using ComFeel, we collected 1373 minutes of readings. For each of these meetings, we also collected whether each participant felt the meeting to have been productive, the setting to be psychologically safe, and the meeting room to be pleasant. As one expects, we found that, on average, the probability of a meeting being productive increased by 35% for each standard deviation increase in the psychological safety participants experienced. Importantly, that probability increased by as much as 25% for each increase in room pleasantness, confirming the significant short-term impact of the indoor environment on meetings' productivity.},
	articleno    = 123,
	numpages     = 21,
	keywords     = {perceptions, environmental conditions, meetings, productivity}
}
@article{10.1145/3432230,
	title        = {Incremental Real-Time Personalization in Human Activity Recognition Using Domain Adaptive Batch Normalization},
	author       = {Mazankiewicz, Alan and B\"{o}hm, Klemens and Berges, Mario},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432230},
	url          = {https://doi.org/10.1145/3432230},
	issue_date   = {December 2020},
	abstract     = {Human Activity Recognition (HAR) from devices like smartphone accelerometers is a fundamental problem in ubiquitous computing. Machine learning based recognition models often perform poorly when applied to new users that were not part of the training data. Previous work has addressed this challenge by personalizing general recognition models to the unique motion pattern of a new user in a static batch setting. They require target user data to be available upfront. The more challenging online setting has received less attention. No samples from the target user are available in advance, but they arrive sequentially. Additionally, the motion pattern of users may change over time. Thus, adapting to new and forgetting old information must be traded off. Finally, the target user should not have to do any work to use the recognition system by, say, labeling any activities. Our work addresses all of these challenges by proposing an unsupervised online domain adaptation algorithm. Both classification and personalization happen continuously and incrementally in real time. Our solution works by aligning the feature distributions of all subjects, be they sources or the target, in hidden neural network layers. To this end, we normalize the input of a layer with user-specific mean and variance statistics. During training, these statistics are computed over user-specific batches. In the online phase, they are estimated incrementally for any new target user.},
	articleno    = 144,
	numpages     = 20,
	keywords     = {human activity recognition, online learning, batch normalization, transfer learning, incremental personalization, convolutional neural networks, online domain adaptation}
}
@article{10.1145/3432218,
	title        = {Evaluating the Gradual Delivery of Knowledge-Focused and Mindset-Focused Messages for Facilitating the Acceptance of COPD},
	author       = {Zeng, Steven Y. and Wu, Robert and Truong, Khai N. and Chevalier, Fanny},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432218},
	url          = {https://doi.org/10.1145/3432218},
	issue_date   = {December 2020},
	abstract     = {Chronic Obstructive Pulmonary Disease (COPD) is a terminal, progressive lung condition which mainly affects older adults. The onset of symptoms, obstacles, and impairments brought about by COPD often necessitates a grieving process for patients. Acceptance is the stage of the grieving process in which the patient has healthily integrated the condition into his or her lifestyle and identity. Because of the progressive nature of COPD, the process of acceptance is a perpetual journey in which patients must continuously shift their mindsets and lifestyles to adapt to the increasing severity of the condition. Using the health belief model as a theoretical foundation, we explore the usage of daily automated SMS messages as an engaging and accessible means of facilitating and maintaining a patient's acceptance of COPD. The results of our investigation show that SMS messages serve as an effective tool for improving patients' acceptance of COPD while also reducing patients' proclivities to the nonacceptance stages of the grieving process.},
	articleno    = 160,
	numpages     = 23,
	keywords     = {coping, SMS messaging, acceptance, grief, COPD, chronic conditions}
}
@article{10.1145/3432214,
	title        = {D2Park: Diversified Demand-Aware On-Street Parking Guidance},
	author       = {Zhao, Dong and Cao, Zijian and Ju, Chen and Zhang, Desheng and Ma, Huadong},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432214},
	url          = {https://doi.org/10.1145/3432214},
	issue_date   = {December 2020},
	abstract     = {To address the increasingly serious parking pain, numerous mobile Apps have emerged to help drivers to find a convenient parking spot with various auxiliary information. However, the phenomenon of "multiple cars chasing the same spot" still exists, especially for on-street parking. Existing reservation-based resource allocation solutions could address the parking competition issue to some extent, but it is impractical to treat all spots as reservable resources. This paper first conducts a qualitative investigation based on the online survey data, which identifies diversified parking requirements involving i) reserved users, who request guaranteed spots with a reservation fee, ii) normal users, who request non-guaranteed spots with a "best-effort" service, and iii) external users, who do not use any guidance service. To this end, we design the D2Park system for diversified demand-aware parking guidance services. We formulate the problem as a novel Heterogeneous-Agent Dynamic Resource Allocation (HADRA) problem, which considers both current and future parking demands, and different constraints for diversified requirements. Two main modules are used in the system: 1) multi-step parking prediction, which makes multi-step parking inflow and occupancy rate predictions given the current parking events data and external factors; and 2) diversified parking guidance, which integrates the cooperation-based and competition-based resource allocation mechanisms based on a model predictive control framework to achieve a better performance balance among different user groups. Extensive experiments with a four-month real-world on-street parking dataset from the Chinese city Shenzhen demonstrate the effectiveness and efficiency of D2Park.},
	articleno    = 163,
	numpages     = 25,
	keywords     = {Dynamic Resource Allocation, On-street Parking Guidance, Diversified Requirements}
}
@article{10.1145/3432213,
	title        = {Understanding User Contexts and Coping Strategies for Context-Aware Phone Distraction Management System Design},
	author       = {Kim, Inyeop and Goh, Hwarang and Narziev, Nematjon and Noh, Youngtae and Lee, Uichin},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432213},
	url          = {https://doi.org/10.1145/3432213},
	issue_date   = {December 2020},
	abstract     = {Smartphones are often distraction for everyday life activities. In this work, we envision designing a context-aware system that helps users better manage smartphone distractions. This system design requires us to have an in-depth understanding of users' contexts of smartphone distractions and their coping strategies. However, there is a lack of knowledge regarding the contexts in which users perceive that smartphones are distracting in their everyday lives. Furthermore, prior studies did not systematically examine users' preferred coping strategies for handling interruptions caused by smartphones, possibly supported by context-aware systems that proactively manage smartphone distraction. To bridge this gap, we collect in-situ user contexts and their corresponding levels of perceived smartphone distraction as well as analyze the daily contexts in which users perceive smartphones as distracting. Moreover, we also explore how users want to manage phone distraction by asking them to write simple if-then rules. Our results on user contexts and coping strategies provide important implications for designing and implementing context-aware distraction management systems.},
	articleno    = 134,
	numpages     = 33,
	keywords     = {interruption handling, context-aware systems, Smartphone distraction, trigger-action programming}
}
@article{10.1145/3432196,
	title        = {AiSee: An Assistive Wearable Device to Support Visually Impaired Grocery Shoppers},
	author       = {Boldu, Roger and Matthies, Denys J.C. and Zhang, Haimo and Nanayakkara, Suranga},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432196},
	url          = {https://doi.org/10.1145/3432196},
	issue_date   = {December 2020},
	abstract     = {People with visual impairments (PVI) experience simple tasks, such as grocery shopping, to be an essential difficulty. Although the recent emergence of AI-technology has been dramatically improving visual recognition capabilities, the application to the daily life of PVI is still complex and erroneous. For example, image recognition engines require a clear shot of the targeted object and a contextual understanding of the information the user requires. In this paper, we aimed to understand the PVI's needs and their pain points in the task of identifying grocery items. Following a user-centered design process, we iteratively},
	articleno    = 119,
	numpages     = 25,
	keywords     = {Localization, Acoustic, Finger Touch, Force}
}
@article{10.1145/3432189,
	title        = {VibroMap: Understanding the Spacing of Vibrotactile Actuators across the Body},
	author       = {Elsayed, Hesham and Weigel, Martin and M\"{u}ller, Florian and Schmitz, Martin and Marky, Karola and G\"{u}nther, Sebastian and Riemann, Jan and M\"{u}hlh\"{a}user, Max},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432189},
	url          = {https://doi.org/10.1145/3432189},
	issue_date   = {December 2020},
	abstract     = {In spite of the great potential of on-body vibrotactile displays for a variety of applications, research lacks an understanding of the spacing between vibrotactile actuators. Through two experiments, we systematically investigate vibrotactile perception on the wrist, forearm, upper arm, back, torso, thigh, and leg, each in transverse and longitudinal body orientation. In the first experiment, we address the maximum distance between vibration motors that still preserves the ability to generate phantom sensations. In the second experiment, we investigate the perceptual accuracy of localizing vibrations in order to establish the minimum distance between vibration motors. Based on the results, we derive VibroMap, a spatial map of the functional range of inter-motor distances across the body. VibroMap supports hardware and interaction designers with design guidelines for constructing body-worn vibrotactile displays.},
	articleno    = 125,
	numpages     = 16,
	keywords     = {phantom sensation, wearable computing, actuator spacing, ERM vibration motors, haptic output, vibrotactile interfaces, design implications}
}
@article{10.1145/3369826,
	title        = {Scheduling Content in Pervasive Display Systems},
	author       = {Clinch, Sarah and Mikusz, Mateusz and Elhart, Ivan and Davies, Nigel and Langheinrich, Marc},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369826},
	url          = {https://doi.org/10.1145/3369826},
	issue_date   = {December 2019},
	abstract     = {Digital displays are a ubiquitous feature of public spaces; London recently deployed a whole network of new displays in its Underground stations, and the screens on One Time Square (New York) allow for presentation of over 16,000 square feet of digital media. However, despite decades of research into pervasive displays, the problem of scheduling content is under-served and there is little forward momentum in addressing the challenges brought with large-scale and open display networks. This paper presents the first comprehensive architectural model for scheduling in current and anticipated pervasive display systems. In contrast to prior work, our three-stage model separates out the process of high level goal setting from content filtering and selection. Our architecture is motivated by an extensive review of the literature and a detailed consideration of requirements. The architecture is realised with an implementation designed to serve the world's largest and longest-running research testbed of pervasive displays. A mixed-methods evaluation confirms the viability of the architecture from three angles: demonstrating capability to meet the articulated requirements, performance that comfortably fits within the demands of typical display deployments, and evidence of its ability to serve as the day-to-day scheduling platform for the previously described research testbed. Based on our evaluation and a reflection on paper as a whole, we identify ten implications that will shape future research and development in pervasive display scheduling.},
	articleno    = 129,
	numpages     = 37,
	keywords     = {multimedia scheduling, digital out-of-home advertising, Digital signage}
}
@article{10.1145/3369818,
	title        = {Cross-Dataset Activity Recognition via Adaptive Spatial-Temporal Transfer Learning},
	author       = {Qin, Xin and Chen, Yiqiang and Wang, Jindong and Yu, Chaohui},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369818},
	url          = {https://doi.org/10.1145/3369818},
	issue_date   = {December 2019},
	abstract     = {Human activity recognition (HAR) aims at recognizing activities by training models on the large quantity of sensor data. Since it is time-consuming and expensive to acquire abundant labeled data, transfer learning becomes necessary for HAR by transferring knowledge from existing domains. However, there are two challenges existing in cross-dataset activity recognition. The first challenge is source domain selection. Given a target task and several available source domains, it is difficult to determine how to select the most similar source domain to the target domain such that negative transfer can be avoided. The second one is accurately activity transfer. After source domain selection, how to achieve accurate knowledge transfer between the selected source and the target domain remains another challenge. In this paper, we propose an Adaptive Spatial-Temporal Transfer Learning (ASTTL) approach to tackle both of the above two challenges in cross-dataset HAR. ASTTL learns the spatial features in transfer learning by adaptively evaluating the relative importance between the marginal and conditional probability distributions. Besides, it captures the temporal features via incremental manifold learning. Therefore, ASTTL can learn the adaptive spatial-temporal features for cross-dataset HAR and can be used for both source domain selection and accurate activity transfer. We evaluate the performance of ASTTL through extensive experiments on 4 public HAR datasets, which demonstrates its effectiveness. Furthermore, based on ASTTL, we design and implement an adaptive cross-dataset HAR system called Client-Cloud Collaborative Adaptive Activity Recognition System (3C2ARS) to perform HAR in the real environment. By collecting activities in the smartphone and transferring knowledge in the cloud server, ASTTL can significantly improve the performance of source domain selection and accurate activity transfer.},
	articleno    = 148,
	numpages     = 25,
	keywords     = {Human Activity Recognition, Transfer Learning, Cross-Dataset Recognition, Domain Adaptation}
}
@article{10.1145/3369815,
	title        = {A Reliability-Aware Vehicular Crowdsensing System for Pothole Profiling},
	author       = {Zhong, Weida and Suo, Qiuling and Ma, Fenglong and Hou, Yunfei and Gupta, Abhishek and Qiao, Chunming and Su, Lu},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369815},
	url          = {https://doi.org/10.1145/3369815},
	issue_date   = {December 2019},
	abstract     = {Accurately profiling potholes on road surfaces not only helps eliminate safety related concerns and improve commuting efficiency for drivers, but also reduces unnecessary maintenance cost for transportation agencies. In this paper, we propose a smartphone-based system that is capable of precisely estimating the length and depth of potholes, and introduce a holistic design on pothole data collection, profile aggregation and pothole warning and reporting. The proposed system relies on the built-in inertial sensors of vehicle-carried smartphones to estimate pothole profiles, and warn the driver about incoming potholes. Because of the difference in driving behaviors and vehicle suspension systems, a major challenge in building such system is how to aggregate conflicting sensory reports from multiple participating vehicles. To tackle this challenge, we propose a novel reliability-aware data aggregation algorithm called Reliability Adaptive Truth Discovery (RATD). It infers the reliability for each data source and aggregates pothole profiles in an unsupervised fashion. Our field test shows that the proposed system can effectively estimate pothole profiles, and the RATD algorithm significantly improves the profiling accuracy compared with popular data aggregation methods.},
	articleno    = 160,
	numpages     = 26,
	keywords     = {pothole profiling, crowd sensing, truth discovery}
}
@article{10.1145/3369813,
	title        = {The Wearables Development Toolkit: An Integrated Development Environment for Activity Recognition Applications},
	author       = {Haladjian, Juan},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369813},
	url          = {https://doi.org/10.1145/3369813},
	issue_date   = {December 2019},
	abstract     = {Although the last two decades have seen an increasing number of activity recognition applications with wearable devices, there is still a lack of tools specifically designed to support their development. The development of activity recognition algorithms for wearable devices is particularly challenging because of the several requirements that have to be met simultaneously (e.g., low energy consumption, small and lightweight, accurate recognition). Activity recognition applications are usually developed in a series of iterations to annotate sensor data and to analyze, develop and assess the performance of a recognition algorithm. This paper presents the Wearables Development Toolkit, an Integrated Development Environment designed to lower the entrance barrier to the development of activity recognition applications with wearables. It specifically focuses on activity recognition using on-body inertial sensors. The toolkit offers a repository of high-level reusable components and a set of tools with functionality to annotate data, to analyze and develop activity recognition algorithms and to assess their recognition and computational performance. We demonstrate the versatility of the toolkit with three applications and describe how we developed it incrementally based on two user studies.},
	articleno    = 134,
	numpages     = 26,
	keywords     = {Wearables, Flow-based programming, Toolkit, Development Environment, Machine Learning, Human Activity Recognition}
}
@article{10.1145/3411843,
	title        = {HeartQuake: Accurate Low-Cost Non-Invasive ECG Monitoring Using Bed-Mounted Geophones},
	author       = {Park, Jaeyeon and Cho, Hyeon and Balan, Rajesh Krishna and Ko, JeongGil},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411843},
	url          = {https://doi.org/10.1145/3411843},
	issue_date   = {September 2020},
	abstract     = {This work presents HeartQuake, a low cost, accurate, non-intrusive, geophone-based sensing system for extracting accurate electrocardiogram (ECG) patterns using heartbeat vibrations that penetrate through a bed mattress. In HeartQuake, cardiac activity-originated vibration patterns are captured on a geophone and sent to a server, where the data is filtered to remove the sensor's internal noise and passed on to a bidirectional long short term memory (Bi-LSTM) deep learning model for ECG waveform estimation. To the best of our knowledge, this is the first solution that can non-intrusively provide accurate ECG waveform characteristics instead of more basic abstract features such as the heart rate using bed-mounted geophone sensors. Our extensive experimental results with a baseline dataset collected from 21 study participants and a longitudinal dataset from 15 study participants suggest that HeartQuake, even when using a general non-personalized model, can detect all five ECG peaks (e.g., P, Q, R, S, T) with an average error of 13 msec when participants are stationary on the bed. Furthermore, clinically used ECG metrics such as the RR interval and QRS segment width can be estimated with errors 3 msec and 10 msec, respectively. When additional noise factors are present (e.g., external vibration and various sleeping habits), the estimation error increases, but can be mitigated by using a personalized model. Finally, a qualitative study with 11 physicians on the clinically perceived quality of HeartQuake-generated ECG signals suggests that HeartQuake can effectively serve as a screening tool for detecting and diagnosing abnormal cardiovascular conditions. In addition, HeartQuake's low-cost and non-intrusive nature allow it to be deployed in larger scales compared to current ECG monitoring solutions.},
	articleno    = 93,
	numpages     = 28,
	keywords     = {ECG Waveform Prediction, Contactless Physiological Sensing}
}
@article{10.1145/3411840,
	title        = {What If Conversational Agents Became Invisible? Comparing Users' Mental Models According to Physical Entity of AI Speaker},
	author       = {Lee, Sunok and Cho, Minji and Lee, Sangsu},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411840},
	url          = {https://doi.org/10.1145/3411840},
	issue_date   = {September 2020},
	abstract     = {The popularity of conversational agents (CAs) in the form of AI speakers that support ubiquitous smart homes has increased because of their seamless interaction. However, recent studies have revealed that the use of AI speakers decreases over time, which shows that current agents do not fully support smart homes. Because of this problem, the possibility of unobtrusive, invisible intelligence without a physical device has been suggested. To explore CA design direction that enhances the user experience in smart homes, we aimed to understand each feature by comparing an invisible agent with visible ones embedded in stand-alone AI speakers. We conducted a drawing study to examine users' mental models formed through communicating with two different physical entities (i.e., visible and invisible CAs). From the drawings, interviews, and surveys, we identified how users' mental models and interactions differed depending on the presence of a physical entity. We found that a physical entity affected users' perceptions, expectations, and interactions toward the agent.},
	articleno    = 88,
	numpages     = 24,
	keywords     = {conversational agent, invisible intelligence, drawing study, Ubiquitous smart home, voice user interface}
}
@article{10.1145/3411833,
	title        = {Peer-to-Peer Localization for Single-Antenna Devices},
	author       = {Zhang, Xianan and Wang, Wei and Xiao, Xuedou and Yang, Hang and Zhang, Xinyu and Jiang, Tao},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411833},
	url          = {https://doi.org/10.1145/3411833},
	issue_date   = {September 2020},
	abstract     = {Some important indoor localization applications, such as localizing a lost kid in a shopping mall, call for a new peer-to-peer localization technique that can localize an individual's smartphone or wearables by directly using another's on-body devices in unknown indoor environments. However, current localization solutions either require pre-deployed infrastructures or multiple antennas in both transceivers, impending their wide-scale application. In this paper, we present P2PLocate, a peer-to-peer localization system that enables a single-antenna device co-located with a batteryless backscatter tag to localize another single-antenna device with decimeter-level accuracy. P2PLocate leverages the multipath variations intentionally created by an on-body backscatter tag, coupled with spatial information offered by user movements, to accomplish this objective without relying on any pre-deployed infrastructures or pre-training. P2PLocate incorporates novel algorithms to address two major challenges: (i) interference with strong direct-path signal while extracting multipath variations, and (ii) lack of direction information while using single-antenna transceivers. We implement P2PLocate on commercial off-the-shelf Google Nexus 6p, Intel 5300 WiFi card, and Raspberry Pi B4. Real-world experiments reveal that P2PLocate can localize both static and mobile targets with a median accuracy of 0.88 m.},
	articleno    = 105,
	numpages     = 25,
	keywords     = {Single-Antenna Device, Peer-to-Peer Localization, Backscatter, WiFi}
}
@article{10.1145/3411826,
	title        = {OptoSense: Towards Ubiquitous Self-Powered Ambient Light Sensing Surfaces},
	author       = {Zhang, Dingtian and Park, Jung Wook and Zhang, Yang and Zhao, Yuhui and Wang, Yiyang and Li, Yunzhi and Bhagwat, Tanvi and Chou, Wen-Fang and Jia, Xiaojia and Kippelen, Bernard and Fuentes-Hernandez, Canek and Starner, Thad and Abowd, Gregory D.},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411826},
	url          = {https://doi.org/10.1145/3411826},
	issue_date   = {September 2020},
	abstract     = {Ubiquitous computing requires robust and sustainable sensing techniques to detect users for explicit and implicit inputs. Existing solutions with cameras can be privacy-invasive. Battery-powered sensors require user maintenance, preventing practical ubiquitous sensor deployment. We present OptoSense, a general-purpose self-powered sensing system which senses ambient light at the surface level of everyday objects as a high-fidelity signal to infer user activities and interactions. To situate the novelty of OptoSense among prior work and highlight the generalizability of the approach, we propose a design framework of ambient light sensing surfaces, enabling implicit activity sensing and explicit interactions in a wide range of use cases with varying sensing dimensions (0D, 1D, 2D), fields of view (wide, narrow), and perspectives (egocentric, allocentric). OptoSense supports this framework through example applications ranging from object use and indoor traffic detection, to liquid sensing and multitouch input. Additionally, the system can achieve high detection accuracy while being self-powered by ambient light. On-going improvements that replace Optosense's silicon-based sensors with organic semiconductors (OSCs) enable devices that are ultra-thin, flexible, and cost effective to scale.},
	articleno    = 103,
	numpages     = 27,
	keywords     = {Ambient light sensing, multitouch and hover input, self-powered wireless systems, activity detection}
}
@article{10.1145/3411821,
	title        = {A Multi-Sensor Approach to Automatically Recognize Breaks and Work Activities of Knowledge Workers in Academia},
	author       = {Di Lascio, Elena and Gashi, Shkurta and Hidalgo, Juan Sebastian and Nale, Beatrice and Debus, Maike E. and Santini, Silvia},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411821},
	url          = {https://doi.org/10.1145/3411821},
	issue_date   = {September 2020},
	abstract     = {Personal informatics systems for the work environment can help improving workers' well-being and productivity. Using both self-reported data logged manually by the users and information automatically inferred from sensor measurements, such systems may track users' activities at work and help them reflect on their work habits through insightful data visualizations. They can further support interventions like, e.g., blocking distractions during work activities or suggest the user to take a break. The ability to automatically recognize when the user is engaged in a work activity or taking a break is thus a fundamental primitive such systems need to implement. In this paper, we explore the use of data collected from personal devices -- smartwatches, laptops, and smartphones -- to automatically recognize when users are working or taking breaks. We collect a data set of of continuous streams of sensor data captured from personal devices along with labels indicating whether a user is working or taking a break. We use multiple instruments to facilitate the collection of users' self-reported labels and discuss our experience with this approach. We analyse the available data -- 449 labelled activities of nine knowledge workers collected during a typical work week -- using machine learning techniques and show that user-independent models can achieve a (F1 score) of 94% for the identification of work activities and of 69% for breaks, outperforming baseline methods by 5-10 and 12-54 percentage points, respectively.},
	articleno    = 78,
	numpages     = 20,
	keywords     = {knowledge workers, human activity recognition, Multi-modal sensing}
}
@article{10.1145/3397330,
	title        = {Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables},
	author       = {Sheng, Taoran and Huber, Manfred},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397330},
	url          = {https://doi.org/10.1145/3397330},
	issue_date   = {June 2020},
	abstract     = {Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available.},
	articleno    = 57,
	numpages     = 18,
	keywords     = {Activity recognition, Weakly supervised learning, Wearable sensors, Person identification}
}
@article{10.1145/3397329,
	title        = {Rataplan: Resilient Automation of User Interface Actions with Multi-Modal Proxies},
	author       = {Veuskens, Tom and Luyten, Kris and Ramakers, Raf},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397329},
	url          = {https://doi.org/10.1145/3397329},
	issue_date   = {June 2020},
	abstract     = {We present Rataplan, a robust and resilient pixel-based approach for linking multi-modal proxies to automated sequences of actions in graphical user interfaces (GUIs). With Rataplan, users demonstrate a sequence of actions and answer human-readable follow-up questions to clarify their desire for automation. After demonstrating a sequence, the user can link a proxy input control to the action which can then be used as a shortcut for automating a sequence. Alternatively, output proxies use a notification model in which content is pushed when it becomes available. As an example use case, Rataplan uses keyboard shortcuts and tangible user interfaces (TUIs) as input proxies, and TUIs as output proxies. Instead of relying on available APIs, Rataplan automates GUIs using pixel-based reverse engineering. This ensures our approach can be used with all applications that offer a GUI, including web applications. We implemented a set of important strategies to support robust automation of modern interfaces that have a flat and minimal style, have frequent data and state changes, and have dynamic viewports.},
	articleno    = 60,
	numpages     = 23,
	keywords     = {tangible user interfaces, pixel-based reverse engineering, programming-by-demonstration, UI automation}
}
@article{10.1145/3397315,
	title        = {Towards Real-Time Cooperative Deep Inference over the Cloud and Edge End Devices},
	author       = {Zhang, Shigeng and Li, Yinggang and Liu, Xuan and Guo, Song and Wang, Weiping and Wang, Jianxin and Ding, Bo and Wu, Di},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397315},
	url          = {https://doi.org/10.1145/3397315},
	issue_date   = {June 2020},
	abstract     = {Deep neural networks (DNNs) have been widely used in many intelligent applications such as object recognition and automatic driving due to their superior performance in conducting inference tasks. However, DNN models are usually heavyweight in computation, hindering their utilization on the resource-constraint Internet of Things (IoT) end devices. To this end, cooperative deep inference is proposed, in which a DNN model is adaptively partitioned into two parts and different parts are executed on different devices (cloud or edge end devices) to minimize the total inference latency. One important issue is thus to find the optimal partition of the deep model subject to network dynamics in a real-time manner. In this paper, we formulate the optimal DNN partition as a min-cut problem in a directed acyclic graph (DAG) specially derived from the DNN and propose a novel two-stage approach named quick deep model partition (QDMP) to solve it. QDMP exploits the fact that the optimal partition of a DNN model must be between two adjacent cut vertices in the corresponding DAG. It first identifies the two cut vertices and considers only the subgraph in between when calculating the min-cut. QDMP can find the optimal model partition with response time less than 300ms even for large DNN models containing hundreds of layers (up to 66.3x faster than the state-of-the-art solution), and thus enables real-time cooperative deep inference over the cloud and edge end devices. Moreover, we observe one important fact that is ignored in all previous works: As many deep learning frameworks optimize the execution of DNN models, the execution latency of a series of layers in a DNN does not equal to the summation of each layer's independent execution latency. This results in inaccurate inference latency estimation in existing works. We propose a new execution latency measurement method, with which the inference latency can be accurately estimated in practice. We implement QDMP on real hardware and use a real-world self-driving car video dataset to evaluate its performance. Experimental results show that QDMP outperforms the state-of-the-art solution, reducing inference latency by up to 1.69x and increasing throughput by up to 3.81x.},
	articleno    = 69,
	numpages     = 24,
	keywords     = {deep model partitioning, deep model acceleration, cooperative deep inference, Edge AI}
}
@article{10.1145/3381754,
	title        = {KEHKey: Kinetic Energy Harvester-Based Authentication and Key Generation for Body Area Network},
	author       = {Lin, Qi and Xu, Weitao and Lan, Guohao and Cui, Yesheng and Jia, Hong and Hu, Wen and Hassan, Mahbub and Seneviratne, Aruna},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381754},
	url          = {https://doi.org/10.1145/3381754},
	issue_date   = {March 2020},
	abstract     = {For kinetic-powered body area networks, we explore the feasibility of converting energy harvesting patterns for device authentication and symmetric secret keys generation continuously. The intuition is that at any given time, multiple wearable devices harvest kinetic energy from the same user activity, such as walking, which allows them to independently observe a common secret energy harvesting pattern not accessible to outside devices. Such continuous KEH-based authentication and key generation is expected to be highly power efficient as it obviates the need to employ any extra sensors, such as accelerometer, to precisely track the walking patterns. Unfortunately, lack of precise activity tracking introduces bit mismatches between the independently generated keys, which makes KEH-based authentication and symmetric key generation a challenging problem. We propose KEHKey, a KEH-based authentication and key generation system that employs a compressive sensing-based information reconciliation protocol for wearable devices to effectively correct any mismatches in generated keys. We implement KEHKey using off-the-shelf piezoelectric energy harvesting products and evaluate its performance with data collected from 24 subjects wearing the devices on different body locations including head, torso and hands. Our results show that KEHKey is able to generate the same key for two KEH-embedded devices at a speed of 12.57 bps while reducing energy consumption by 59% compared to accelerometer-based methods, which makes it suitable for continuous operation. Finally, we demonstrate that KEHKey can successfully defend against typical adversarial attacks. In particular, KEHKey is found to be more resilient to video side channel attacks than its accelerometer-based counterparts.},
	articleno    = 41,
	numpages     = 26,
	keywords     = {kinetic energy harvester, key generation system, gait, continuous authentication system, compressive sensing}
}
@article{10.1145/3381017,
	title        = {Teacher Tracking with Integrity: What Indoor Positioning Can Reveal About Instructional Proxemics},
	author       = {Martinez-Maldonado, Roberto and Mangaroska, Katerina and Schulte, Jurgen and Elliott, Doug and Axisa, Carmen and Shum, Simon Buckingham},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381017},
	url          = {https://doi.org/10.1145/3381017},
	issue_date   = {March 2020},
	abstract     = {Automatic tracking of activity and location in the classroom is becoming increasingly feasible and inexpensive. However, although there is a growing interest in creating classrooms embedded with tracking capabilities using computer vision and wearables, more work is still needed to understand teachers' perceived opportunities and concerns about using indoor positioning data to reflect on their practice. This paper presents results from a qualitative study, conducted across three authentic educational settings, investigating the potential of making positioning traces available to teachers. Positioning data from 28 classes taught by 10 university teachers was captured using sensors in three different collaborative classroom spaces in the disciplines of design, health and science. The contributions of this paper to ubiquitous computing are the documented reflections of teachers from different disciplines provoked by visual representations of their classroom positioning data and that of others. These reflections point to: i) the potential benefit of using these digital traces to support teaching; and ii) concerns to be considered in the design of meaningful analytics systems for instructional proxemics.},
	articleno    = 22,
	numpages     = 27,
	keywords     = {Learning analytics, proxemics, indoor positioning, location analytics}
}
@article{10.1145/3381008,
	title        = {Endophasia: Utilizing Acoustic-Based Imaging for Issuing Contact-Free Silent Speech Commands},
	author       = {Zhang, Yongzhao and Huang, Wei-Hsiang and Yang, Chih-Yun and Wang, Wen-Ping and Chen, Yi-Chao and You, Chuang-Wen and Huang, Da-Yuan and Xue, Guangtao and Yu, Jiadi},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381008},
	url          = {https://doi.org/10.1145/3381008},
	issue_date   = {March 2020},
	abstract     = {Using silent speech to issue commands has received growing attention, as users can utilize existing command sets from voice-based interfaces without attracting other people's attention. Such interaction maintains privacy and social acceptance from others. However, current solutions for recognizing silent speech mainly rely on camera-based data or attaching sensors to the throat. Camera-based solutions require 5.82 times larger power consumption or have potential privacy issues; attaching sensors to the throat is not practical for commercial-off-the-shell (COTS) devices because additional sensors are required. In this paper, we propose a sensing technique that only needs a microphone and a speaker on COTS devices, which not only consumes little power but also has fewer privacy concerns. By deconstructing the received acoustic signals, a 2D motion profile can be generated. We propose a classifier based on convolutional neural networks (CNN) to identify the corresponding silent command from the 2D motion profiles. The proposed classifier can adapt to users and is robust when tested by environmental factors. Our evaluation shows that the system achieves 92.5% accuracy in classifying 20 commands.},
	articleno    = 37,
	numpages     = 26,
	keywords     = {silent command, acoustic-based imaging, mobile devices}
}
@article{10.1145/3380995,
	title        = {Soil-Monitoring Sensor Powered by Temperature Difference between Air and Shallow Underground Soil},
	author       = {Ikeda, Natsuki and Shigeta, Ryo and Shiomi, Junichiro and Kawahara, Yoshihiro},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380995},
	url          = {https://doi.org/10.1145/3380995},
	issue_date   = {March 2020},
	abstract     = {Energy harvesting (EH) technologies are useful for the semi-permanent operation of wireless sensor networks, especially, for agricultural monitoring as the networks need to be installed in large areas where power supply is unavailable. In this paper, we propose a battery-free soil-monitoring sensor for agriculture, which leverages the temperature difference between near-surface air and shallow underground soil using a thermoelectric generator (TEG). The performance of systems driven by the TEG mainly depends on the average temperature between the hot and cold sides of the TEG (T) and the temperature difference across the TEG (ΔT). If T is low and ΔT is small, it is challenging to earn enough power to drive wireless microcontroller unit; however, with our dedicated electric circuit, and thermal designs including impedance matching of thermal circuit and suppression of heat loss, the sensor can harvest more than a hundred microwatt on average from the temperature difference between the air and underground soil at a depth of 30 cm. The performance of the energy harvester is evaluated both by numerical analysis using temperature data collected from various farm fields and by a prototype implementation. Moreover, the prototype was deployed to farm fields in Japan and India. Our field experiment results revealed that the prototype could harvest 100 μW-370 μW on average, and drive a wireless microcontroller unit to perform soil monitoring.},
	articleno    = 13,
	numpages     = 22,
	keywords     = {Smart agriculture, Field experiment, Energy harvesting, Thermoelectric generation}
}
@article{10.1145/3380994,
	title        = {How Does Fitbit Measure Brainwaves: A Qualitative Study into the Credibility of Sleep-Tracking Technologies},
	author       = {Liang, Zilu and Ploderer, Bernd},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380994},
	url          = {https://doi.org/10.1145/3380994},
	issue_date   = {March 2020},
	abstract     = {Consumer sleep-tracking devices provide an unobtrusive and affordable way to learn about personal sleep habits. Recent research focused primarily on the information provided by such devices, i.e., whether the information is accurate and meaningful to people. However, little is known about how people judge the credibility of such information, and how the functionality and the design may influence such judgements. Hence, the aim of this research was to examine how consumers assess the credibility of sleep-tracking devices. We conducted a qualitative study with 22 participants who tracked their sleep for 3 nights with three different devices: Fitbit Charge 2, Neuroon EEG, and SleepScope, a medical sleep monitor. Based on semi-structured interviews, we found that people assess the credibility of sleep-tracking devices based not only on the credibility of sleep data per se, but also on device functionality, interface design and physical appearance. People found it difficult to judge credibility, because of the complexities of sleep stages and micro-arousals (sleep fallacy) and the black boxed nature of devices (black box fallacy), and also because of the misalignment between objective sleep measures and subjective sleep quality. We discuss the significance of design and functionality on the credibility of personal health technologies and highlight design challenges and opportunities to enhance their credibility.},
	articleno    = 17,
	numpages     = 29,
	keywords     = {Wearable sensors, Sleep-tracking, Personal informatics, Credibility, Consumer devices, Quantified self}
}
@article{10.1145/3380991,
	title        = {Using Sonar for Liveness Detection to Protect Smart Speakers against Remote Attackers},
	author       = {Lee, Yeonjoon and Zhao, Yue and Zeng, Jiutian and Lee, Kwangwuk and Zhang, Nan and Shezan, Faysal Hossain and Tian, Yuan and Chen, Kai and Wang, XiaoFeng},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380991},
	url          = {https://doi.org/10.1145/3380991},
	issue_date   = {March 2020},
	abstract     = {Smart speakers, which wait for voice commands and complete tasks for users, are becoming part of common households. While voice commands came with basic functionalities in the earlier days, as the market grew, various commands with critical functionalities were developed; e.g., access banking services, send money, open front door. Such voice commands can cause serious consequences once smart speakers are attacked. Recent research shows that smart speakers are vulnerable to malicious voice commands sent from other speakers (e.g., TV, baby monitor, radio) in the same area. In this work, we propose the Speaker-Sonar, a sonar-based liveness detection system for smart speakers. Our approach aims to protect the smart speakers from remote attackers that leverage network-connected speakers to send malicious commands. The key idea of our approach is to make sure that the voice command is indeed coming from the user. For this purpose, the Speaker-Sonar emits an inaudible sound and tracks the user's direction to compare it with the direction of the received voice command. The Speaker-Sonar does not require additional action from the user and works through an automatic consistency check. We built the Speaker-Sonar on a raspberry pi 3b, a circular microphone array, and a commodity speaker by imitating the Amazon Echo. Our evaluation shows that the Speaker-Sonar can reject remote voice attacks with an average accuracy of 95.5% in 2 meters, which significantly raises the bar for remote attackers. To the best of our knowledge, our defense is able to defend against known remote voice attack techniques.},
	articleno    = 16,
	numpages     = 28,
	keywords     = {liveness detection, smart speakers, sonar}
}
@article{10.1145/3380986,
	title        = {Dynamic Public Resource Allocation Based on Human Mobility Prediction},
	author       = {Ruan, Sijie and Bao, Jie and Liang, Yuxuan and Li, Ruiyuan and He, Tianfu and Meng, Chuishi and Li, Yanhua and Wu, Yingcai and Zheng, Yu},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380986},
	url          = {https://doi.org/10.1145/3380986},
	issue_date   = {March 2020},
	abstract     = {The objective of public resource allocation, e.g., the deployment of billboards, surveillance cameras, base stations, trash bins, is to serve more people. However, due to the dynamics of human mobility patterns, people are distributed unevenly on the spatial and temporal domains. As a result, in many cases, redundant resources have to be deployed to meet the crowd coverage requirements, which leads to high deployment costs and low usage. Fortunately, with the development of unmanned vehicles, the dynamic allocation of those public resources becomes possible. To this end, we provide the first attempt to design an effective and efficient scheduling algorithm for the dynamic public resource allocation. We formulate the problem as a novel multi-agent long-term maximal coverage scheduling (MALMCS) problem, which considers the crowd coverage and the energy limitation during a whole day. Two main components are employed in the system: 1) multi-step crowd flow prediction, which makes multi-step crowd flow prediction given the current crowd flows and external factors; and 2) energy adaptive scheduling, which employs a two-step heuristic algorithm, i.e., energy adaptive scheduling (EADS), to generate a scheduling plan that maximizes the crowd coverage within the service time for agents. Extensive experiments based on real crowd flow data in Happy Valley (a popular theme park in Beijing) demonstrate the effectiveness and efficiency of our approach.},
	articleno    = 25,
	numpages     = 22,
	keywords     = {Mobility Data Mining, Urban Computing, Dynamic Resource Allocation}
}
@article{10.1145/3380981,
	title        = {FingerDraw: Sub-Wavelength Level Finger Motion Tracking with WiFi Signals},
	author       = {Wu, Dan and Gao, Ruiyang and Zeng, Youwei and Liu, Jinyi and Wang, Leye and Gu, Tao and Zhang, Daqing},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380981},
	url          = {https://doi.org/10.1145/3380981},
	issue_date   = {March 2020},
	abstract     = {This paper explores the possibility of tracking finger drawings in the air leveraging WiFi signals from commodity devices. Prior solutions typically require user to hold a wireless transmitter, or need proprietary wireless hardware. They can only recognize a small set of pre-defined hand gestures. This paper introduces FingerDraw, the first sub-wavelength level finger motion tracking system using commodity WiFi devices, without attaching any sensor to finger. FingerDraw can reconstruct finger drawing trajectory such as digits, alphabets, and symbols with the setting of one WiFi transmitter and two WiFi receivers. It uses a two-antenna receiver to sense the sub-wavelength scale displacement of finger motion in each direction. The theoretical underpinning of FingerDraw is our proposed CSI-quotient model, which uses the channel quotient between two antennas of the receiver to cancel out the noise in CSI amplitude and the random offsets in CSI phase, and quantifies the correlation between CSI value dynamics and object displacement. This channel quotient is sensitive to and enables us to detect small changes in In-phase and Quadrature parts of channel state information due to finger movement. Our experimental results show that the overall median tracking accuracy is 1.27 cm, and the recognition of drawing ten digits in the air achieves an average accuracy of over 93.0%.},
	articleno    = 31,
	numpages     = 27,
	keywords     = {Channel Quotient, Finger-draw gesture, Channel state information (CSI), WiFi}
}
@article{10.1145/3380979,
	title        = {The StoryTeller: Scalable Building- and AP-Independent Deep Learning-Based Floor Prediction},
	author       = {Elbakly, Rizanne and Youssef, Moustafa},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380979},
	url          = {https://doi.org/10.1145/3380979},
	issue_date   = {March 2020},
	abstract     = {Due to the recent proliferation of location-based services indoors, the need for an accurate floor estimation technique that is easy to deploy in any typical multi-story building is higher than ever. Current approaches that attempt to solve the floor localization problem include sensor-based systems and 3D fingerprinting. Nevertheless, these systems incur high deployment and maintenance overhead, suffer from sensor drift and calibration issues, and/or are not available to all users.In this paper, we propose StoryTeller, a deep learning-based technique for floor prediction in multi-story buildings. StoryTeller leverages the ubiquitous WiFi signals to generate images that are input to a Convolutional Neural Network (CNN) which is trained to predict loors based on detected patterns in visible WiFi scans. Input images are created such that they capture the current WiFi-scan in an AP-independent manner. In addition, a novel virtual building concept is used to normalize the information in order to make them building-independent. This allows StoryTeller to reuse a trained network for a completely new building, significantly reducing the deployment overhead.We have implemented and evaluated StoryTeller using three different buildings with a side-by-side comparison with the state-of-the-art floor estimation techniques. The results show that StoryTeller can estimate the user's floor at least 98.3% within one floor of the actual ground truth floor. This accuracy is consistent across the different testbeds and for scenarios where the models used were trained in a completely different building than the tested building. This highlights StoryTeller's ability to generalize to new buildings and its promise as a scalable, low-overhead, high-accuracy floor localization system.},
	articleno    = 8,
	numpages     = 20,
	keywords     = {WiFi-based floor estimation, Sensor-based floor estimation, 3D indoor localization, deep learning}
}
@article{10.1145/3351284,
	title        = {Invisible QR Code Hijacking Using Smart LED},
	author       = {Zhou, Anfu and Su, Guangyuan and Zhu, Shilin and Ma, HuaDong},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351284},
	url          = {https://doi.org/10.1145/3351284},
	issue_date   = {September 2019},
	abstract     = {Quick response (QR) codes have found versatile usage in numerous applications, but have also posed severe security threats such as privacy leakage, phishing and even payment inception if the codes are hijacked. The hijacking is often assumed to be preventable by physically isolating the codes from possible attackers, e.g., putting the QR code inside a glass cabinet distant to outsiders. In this paper, we explore a new QR code hijacking attack, named Li-Man, that can subvert such protection using smart LED. The key idea is to illuminate a target victim QR code from afar using specialized flickering light waveforms, which can transform the code to be any other predefined malicious ones when being captured by smart-phone cameras, while keeping the attack invisible to human visual perception. Li-Man builds on a modeling framework that harnesses the disparity between camera and human imaging mechanisms. We develop a Li-Man simulator and also implement a prototype to verify the feasibility and threat level of Li-Man. Experiments demonstrate that Li-Man can successfully realize the invisible hijacking of QR codes from multiple hidden positions in constrained space. On the other hand, we propose and verify a primary countermeasure that is promising to defeat the Li-Man attack.},
	articleno    = 126,
	numpages     = 23,
	keywords     = {Invisible Attack, QR Code Security, SmartLED}
}
@article{10.1145/3351279,
	title        = {FarSense: Pushing the Range Limit of WiFi-Based Respiration Sensing with CSI Ratio of Two Antennas},
	author       = {Zeng, Youwei and Wu, Dan and Xiong, Jie and Yi, Enze and Gao, Ruiyang and Zhang, Daqing},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351279},
	url          = {https://doi.org/10.1145/3351279},
	issue_date   = {September 2019},
	abstract     = {The past few years have witnessed the great potential of exploiting channel state information retrieved from commodity WiFi devices for respiration monitoring. However, existing approaches only work when the target is close to the WiFi transceivers and the performance degrades significantly when the target is far away. On the other hand, most home environments only have one WiFi access point and it may not be located in the same room as the target. This sensing range constraint greatly limits the application of the proposed approaches in real life.This paper presents FarSense--the first real-time system that can reliably monitor human respiration when the target is far away from the WiFi transceiver pair. FarSense works well even when one of the transceivers is located in another room, moving a big step towards real-life deployment. We propose two novel schemes to achieve this goal: (1) Instead of applying the raw CSI readings of individual antenna for sensing, we employ the ratio of CSI readings from two antennas, whose noise is mostly canceled out by the division operation to significantly increase the sensing range; (2) The division operation further enables us to utilize the phase information which is not usable with one single antenna for sensing. The orthogonal amplitude and phase are elaborately combined to address the "blind spots" issue and further increase the sensing range. Extensive experiments show that FarSense is able to accurately monitor human respiration even when the target is 8 meters away from the transceiver pair, increasing the sensing range by more than 100%.1 We believe this is the first system to enable through-wall respiration sensing with commodity WiFi devices and the proposed method could also benefit other sensing applications.},
	articleno    = 121,
	numpages     = 26
}
@article{10.1145/3351265,
	title        = {Pedestrians and Visual Signs of Intent: Towards Expressive Autonomous Passenger Shuttles},
	author       = {Verma, Himanshu and Pythoud, Guillaume and Eden, Grace and Lalanne, Denis and Ev\'{e}quoz, Florian},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351265},
	url          = {https://doi.org/10.1145/3351265},
	issue_date   = {September 2019},
	abstract     = {Autonomous Passenger Shuttles (APS) are rapidly becoming an urban public transit alternative. Traversing populous commercial and residential centers, these shuttles are already operating in several cities. In the absence of a human driver and embedded means of communicating the autonomous shuttle's intent, the task of seamlessly navigating crosswalks and pedestrian-friendly zones becomes a challenging pursuit for pedestrians.We contribute to the emerging notion of AV-Pedestrian Interaction by examining the context of autonomous passenger shuttles (APS) in real-world settings, and by comparing four different classes of visual signals -- namely instructional, symbolic, metaphorical, and anthropomorphic -- designed to communicate the shuttle's intentions. Following a participatory methodology involving local residents and public transport service provider, and working within the framework of inflexible road traffic regulations concerning the operation and testing of autonomous vehicles, we conducted a participatory design workshop, a qualitative, and a survey study. The findings revealed differences across these four classes of signals in terms of pedestrians' subjective perceptions. Anthropomorphic signals were identified as the preferred and effective modality in terms of pedestrians' interpretation of the communicated intent and their perceived sense of attention, confidence, and calmness. Additionally, pedestrians' experiences while judging the intention of transitionary vehicular states (starting/slowing) were reported as perplexing and evoked stress. These findings were translated into design and policy implications in collaboration with other stakeholders, and exemplify a viable way for assimilating human factors research in urban mobility.},
	articleno    = 107,
	numpages     = 31,
	keywords     = {AV-Pedestrian Interaction, Autonomous Vehicles (AVs), AV Intentions, Classes of Visual Signals, Urban Public Transportation, Autonomous Passenger Shuttles (APS), Comparative Study}
}
@article{10.1145/3351245,
	title        = {Exploring the Touch and Motion Features in Game-Based Cognitive Assessments},
	author       = {Intarasirisawat, Jittrapol and Ang, Chee Siang and Efstratiou, Christos and Dickens, Luke William Feidhlim and Page, Rupert},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351245},
	url          = {https://doi.org/10.1145/3351245},
	issue_date   = {September 2019},
	abstract     = {Early detection of cognitive decline is important for timely intervention and treatment strategies to prevent further deterioration or development of more severe cognitive impairment, as well as identify at risk individuals for research. In this paper, we explore the feasibility of using data collected from built-in sensors of mobile phone and gameplay performance in mobile-game-based cognitive assessments. Twenty-two healthy participants took part in the two-session experiment where they were asked to take a series of standard cognitive assessments followed by playing three popular mobile games in which user-game interaction data were passively collected. The results from bivariate analysis reveal correlations between our proposed features and scores obtained from paper-based cognitive assessments. Our results show that touch gestural interaction and device motion patterns can be used as supplementary features on mobile game-based cognitive measurement. This study provides initial evidence that game related metrics on existing off-the-shelf games have potential to be used as proxies for conventional cognitive measures, specifically for visuospatial function, visual search capability, mental flexibility, memory and attention.},
	articleno    = 87,
	numpages     = 25,
	keywords     = {mobile health, serious games, cognitive assessment}
}
@article{10.1145/3351244,
	title        = {Integrating Activity Recognition and Nursing Care Records: The System, Deployment, and a Verification Study},
	author       = {Inoue, Sozo and Lago, Paula and Hossain, Tahera and Mairittha, Tittaya and Mairittha, Nattaya},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351244},
	url          = {https://doi.org/10.1145/3351244},
	issue_date   = {September 2019},
	abstract     = {In this paper, we introduce a system of integrating activity recognition and collecting nursing care records at nursing care facilities as well as activity labels and sensors through smartphones, and describe experiments at a nursing care facility for 4 months. A system designed to be used even by staff not familiar with smartphones could collected enough number of data without losing but improving their workload for recording. For collected data, we revealed the nature of the collected data as for activities, care details, and timestamps, and considering them, we show a reference accuracy of recognition of nursing activity which is durable to time skewness, overlaps, and class imbalances. Moreover, we demonstrate the near future prediction to predict the next day's activities from the previous day's records which could be useful for proactive care management. The dataset collected is to be opened to the research community, and can be the utilized for activity recognition and data mining in care facilities.},
	articleno    = 86,
	numpages     = 24,
	keywords     = {gaze detection, datasets, text tagging, neural networks}
}
@article{10.1145/3328931,
	title        = {Unsupervised Factory Activity Recognition with Wearable Sensors Using Process Instruction Information},
	author       = {Qingxin, Xia and Wada, Atsushi and Korpela, Joseph and Maekawa, Takuya and Namioka, Yasuo},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328931},
	url          = {https://doi.org/10.1145/3328931},
	issue_date   = {June 2019},
	abstract     = {This paper presents an unsupervised method for recognizing assembly work done by factory workers by using wearable sensor data. Such assembly work is a common part of line production systems and typically involves the factory workers performing a repetitive work process made up of a sequence of manual operations, such as setting a board on a workbench and screwing parts onto the board. This study aims to recognize the starting and ending times for individual operations in such work processes through analysis of sensor data collected from the workers along with analysis of the process instructions that detail and describe the flow of operations for each work process. We propose a particle-filter-based factory activity recognition method that leverages (i) trend changes in the sensor data detected by a nonparametric Bayesian hidden Markov model, (ii) semantic similarities between operations discovered in the process instructions, (iii) sensor-data similarities between consecutive repetitions of individual operations, and (iv) frequent sensor-data patterns (motifs) discovered in the overall assembly work processes. We evaluated the proposed method using sensor data from six workers collected in actual factories, achieving a recognition accuracy of 80% (macro-averaged F-measure).},
	articleno    = 60,
	numpages     = 23,
	keywords     = {wearable sensor, Activity recognition, factory work}
}
@article{10.1145/3328929,
	title        = {MenstruLoss: Sensor For Menstrual Blood Loss Monitoring},
	author       = {Mukherjee, Manideepa and Naqvi, Sana Ali and Verma, Anushika and Sengupta, Debarka and Parnami, Aman},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328929},
	url          = {https://doi.org/10.1145/3328929},
	issue_date   = {June 2019},
	abstract     = {Self-monitoring of menstrual blood loss volume could lead to early detection of multiple gynecological diseases. In this paper, we describe the development of a textile-based blood volume sensor which can be integrated into the sanitary napkin to quantify the menstrual blood loss during menstruation. It is based on sensing the resistance change detected as the output voltage change, with the added volume of fluid. Benchtop characterization tests with 5 mL of fluid determined the effect of spacing, orientation and weight, and location of fluid drop on the sensor. The sensor has been evaluated by intravenous blood samples collected from 18 participants and menstrual blood samples collected from 10 participants for four months. The collected intravenous blood samples and menstrual blood samples were used to create two regression model that can predict the blood volume and menstrual blood volume from the voltage input with Mean Absolute Percentage Error (MAPE) of 11-15% and 15-30% respectively.},
	articleno    = 58,
	numpages     = 21,
	keywords     = {Women Health Monitoring, Menstrual Health, Smart Sanitary Napkin, Blood volume sensor}
}
@article{10.1145/3328927,
	title        = {WearBreathing: Real World Respiratory Rate Monitoring Using Smartwatches},
	author       = {Liaqat, Daniyal and Abdalla, Mohamed and Abed-Esfahani, Pegah and Gabel, Moshe and Son, Tatiana and Wu, Robert and Gershon, Andrea and Rudzicz, Frank and Lara, Eyal De},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328927},
	url          = {https://doi.org/10.1145/3328927},
	issue_date   = {June 2019},
	abstract     = {Respiratory rate is a vital physiological signal that may be useful for a multitude of clinical applications, especially if measured in the wild rather than controlled settings. In-the-wild respiratory rate monitoring is currently done using dedicated chest band sensors, but these devices are specialized, expensive and cumbersome to wear day after day. While recent works have proposed using smartwatch based accelerometer and gyroscope data for respiratory rate monitoring, current methods are unreliable and inaccurate in the presence of motion and have therefore only been applied in controlled or low-motion settings. Thus, measuring respiratory rate in the wild remains a challenge.We observe that for many applications, having fewer accurate readings is better than having more, less accurate readings. Based on this, we develop WearBreathing, a novel system for respiratory rate monitoring. WearBreathing consists of a machine learning based filter that detects and rejects sensor data that are not suitable for respiratory rate extraction and a convolutional neural network model for extracting respiratory rate from accelerometer and gyroscope data. Using a diverse, out-of-the-lab dataset that we collected, we show that WearBreathing has a 2.5 to 5.8 times lower mean absolute error (MAE) than existing approaches. We show that WearBreathing is tunable and by changing a single threshold value, it can, for example, deliver a reading every 50 seconds with a MAE of 2.05 breaths/min or a reading every 5 minutes with an MAE of 1.09 breaths/min. Finally, we evaluate power consumption and find that with some power saving measures, WearBreathing can run on a smartwatch while providing a full day's worth of battery life.},
	articleno    = 56,
	numpages     = 22
}
@article{10.1145/3328918,
	title        = {Enhancing Indoor Inertial Odometry with WiFi},
	author       = {Venkatnarayan, Raghav H. and Shahzad, Muhammad},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328918},
	url          = {https://doi.org/10.1145/3328918},
	issue_date   = {June 2019},
	abstract     = {Accurately measuring the distance traversed by a subject, commonly referred to as odometry, in indoor environments is of fundamental importance in many applications such as augmented and virtual reality tracking, indoor navigation, and robot route guidance. While theoretically, odometry can be performed using a simple accelerometer, practically, it is well-known that the distances measured using accelerometers suffer from large drift errors. In this paper, we propose WIO, a WiFi-assisted Inertial Odometry technique that uses WiFi signals as an auxiliary source of information to correct these drift errors. The key intuition behind WIO is that among multiple reflections of a transmitted WiFi signal arriving at the WiFi receiver, WIO first isolates one reflection and then measures the change in the length of the path of that reflection as the subject moves. By identifying the extent through which the length of the path of that reflection changes, along with the direction of motion of the subject relative to that path, WIO can estimate the distance traversed by the subject using WiFi signals. WIO then uses this distance estimate to correct the drift errors. While researchers have previously proposed to use WiFi signals to correct drift errors, prior schemes suffer from one or more of the following six limitations: they 1) do not work indoors, 2) require manual exhaustive fingerprinting, 3) are not resilient against changes in environment including human movements, 4) do not work on commodity WiFi devices, 5) require multiple access points, and/or 6) can measure distance traversed by humans but not by non-human subjects. WIO addresses all of these limitations. We implemented WIO using commodity devices, and extensively evaluated it in a wide variety of complex indoor scenarios on both human and robotic subjects. Our results demonstrate that WIO achieved an average error of just 6.28% in estimating the distances traversed by the subjects.},
	articleno    = 47,
	numpages     = 27,
	keywords     = {WiFi, Inertial Odometry, CSI, Sensor Fusion, IMU}
}
@article{10.1145/3328908,
	title        = {Differentiating Higher and Lower Job Performers in the Workplace Using Mobile Sensing},
	author       = {Mirjafari, Shayan and Masaba, Kizito and Grover, Ted and Wang, Weichen and Audia, Pino and Campbell, Andrew T. and Chawla, Nitesh V. and Swain, Vedant Das and Choudhury, Munmun De and Dey, Anind K. and D'Mello, Sidney K. and Gao, Ge and Gregg, Julie M. and Jagannath, Krithika and Jiang, Kaifeng and Lin, Suwen and Liu, Qiang and Mark, Gloria and Martinez, Gonzalo J. and Mattingly, Stephen M. and Moskal, Edward and Mulukutla, Raghu and Nepal, Subigya and Nies, Kari and Reddy, Manikanta D. and Robles-Granda, Pablo and Saha, Koustuv and Sirigiri, Anusha and Striegel, Aaron},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328908},
	url          = {https://doi.org/10.1145/3328908},
	issue_date   = {June 2019},
	abstract     = {Assessing performance in the workplace typically relies on subjective evaluations, such as, peer ratings, supervisor ratings and self assessments, which are manual, burdensome and potentially biased. We use objective mobile sensing data from phones, wearables and beacons to study workplace performance and offer new insights into behavioral patterns that distinguish higher and lower performers when considering roles in companies (i.e., supervisors and non-supervisors) and different types of companies (i.e., high tech and consultancy). We present initial results from an ongoing year-long study of N=554 information workers collected over a period ranging from 2-8.5 months. We train a gradient boosting classifier that can classify workers as higher or lower performers with AUROC of 0.83. Our work opens the way to new forms of passive objective assessment and feedback to workers to potentially provide week by week or quarter by quarter guidance in the workplace.},
	articleno    = 37,
	numpages     = 24,
	keywords     = {mobile sensing, workplace performance, mobile behavioral pattern}
}
@article{10.1145/3314419,
	title        = {Fixing Mislabeling by Human Annotators Leveraging Conflict Resolution and Prior Knowledge},
	author       = {Zeni, Mattia and Zhang, Wanyi and Bignotti, Enrico and Passerini, Andrea and Giunchiglia, Fausto},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314419},
	url          = {https://doi.org/10.1145/3314419},
	issue_date   = {March 2019},
	abstract     = {According to the "human in the loop" paradigm, machine learning algorithms can improve when leveraging on human intelligence, usually in the form of labels or annotation from domain experts. However, in the case of research areas such as ubiquitous computing or lifelong learning, where the annotator is not an expert and is continuously asked for feedback, humans can provide significant fractions of incorrect labels. We propose to address this issue in a series of experiments where students are asked to provide information about their behavior via a dedicated mobile application. Their trustworthiness is tested by employing an architecture where the machine uses all its available knowledge to check the correctness of its own and the user labeling to build a uniform confidence measure for both of them to be used when a contradiction arises. The overarching system runs through a series of modes with progressively higher confidence and features a conflict resolution component to settle the inconsistencies. The results are very promising and show the pervasiveness of annotation mistakes, the extreme diversity of the users' behaviors which provides evidence of the impracticality of a uniform fits-it-all solution, and the substantially improved performance of a skeptical supervised learning strategy.},
	articleno    = 32,
	numpages     = 23,
	keywords     = {Ubiquitous and Mobile Devices, Collaborative and Social Computing, Annotation Errors}
}
@article{10.1145/3314409,
	title        = {Clinical Data in Context: Towards Sensemaking Tools for Interpreting Personal Health Data},
	author       = {Raj, Shriti and Lee, Joyce M. and Garrity, Ashley and Newman, Mark W.},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314409},
	url          = {https://doi.org/10.1145/3314409},
	issue_date   = {March 2019},
	abstract     = {Clinical data augmented with contextual data can help patients with chronic conditions make sense of their disease. However, existing tools do not support interpretation of multiple data streams. To better understand how individuals make sense of clinical and contextual data, we interviewed patients with Type 1 diabetes and their caregivers using context-enhanced visualizations of patients' data as probes to facilitate interpretation activities. We observed that our participants performed four analytical activities when interpreting their data -- finding context-based trends and explaining them, triangulating multiple factors, suggesting context-specific actions, and hypothesizing about alternate contextual factors affecting outcomes. We also observed two challenges encountered during analysis -- the inability to identify clear trends challenged action planning and counterintuitive insights compromised trust in data. Situating our findings within the existing sensemaking frameworks, we demonstrate that sensemaking can not only inform action but can guide the discovery of information needs for exploration. We further argue that sensemaking is a valuable approach for exploring contextual data. Informed by our findings and our reflection on existing sensemaking frameworks, we provide design guidelines for sensemaking tools to improve awareness of contextual factors affecting patients and to support patients' agency in making sense of health data.},
	articleno    = 22,
	numpages     = 20,
	keywords     = {data interpretation, sensemaking, context, diabetes, reflection, chronic disease management, patient-generated data, visualizations, Personal informatics}
}
@article{10.1145/3314405,
	title        = {Animo: Sharing Biosignals on a Smartwatch for Lightweight Social Connection},
	author       = {Liu, Fannie and Esparza, Mario and Pavlovskaia, Maria and Kaufman, Geoff and Dabbish, Laura and Monroy-Hern\'{a}ndez, Andr\'{e}s},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314405},
	url          = {https://doi.org/10.1145/3314405},
	issue_date   = {March 2019},
	abstract     = {We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch-based biosignals social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N=34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches.},
	articleno    = 18,
	numpages     = 19,
	keywords     = {biosignals, interpersonal communication, heart rate, smartwatches}
}
@article{10.1145/3314394,
	title        = {Identifying and Planning for Individualized Change: Patient-Provider Collaboration Using Lightweight Food Diaries in Healthy Eating and Irritable Bowel Syndrome},
	author       = {Chung, Chia-Fang and Wang, Qiaosi and Schroeder, Jessica and Cole, Allison and Zia, Jasmine and Fogarty, James and Munson, Sean A.},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314394},
	url          = {https://doi.org/10.1145/3314394},
	issue_date   = {March 2019},
	abstract     = {Identifying and planning strategies that support a healthy lifestyle or manage a chronic disease often require patient-provider collaboration. For example, people with healthy eating goals often share everyday food, exercise, or sleep data with health coaches or nutritionists to find opportunities for change, and patients with irritable bowel syndrome (IBS) often gather food and symptom data as part of working with providers to diagnose and manage symptoms. However, a lack of effective support often prevents health experts from reviewing large amounts of data in time-constrained visits, prevents focusing on individual goals, and prevents generating correct, individualized, and actionable recommendations. To examine how to design photo-based diaries to help people and health experts exchange knowledge and focus on collaboration goals when reviewing the data together, we designed and developed Foodprint, a photo-based food diary. Foodprint includes three components: (1) A mobile app supporting lightweight data collection, (2) a web app with photo-based visualization and quantitative visualizations supporting collaborative reflection, and (3) a pre-visit note communicating an individual's expectations and questions to experts. We deployed Foodprint in two studies: (1) with 17 people with healthy eating goals and 7 health experts, and (2) with 16 IBS patients and 8 health experts. Building upon the lens of boundary negotiating artifacts and findings from two field studies, our research contributes design principles to (1) prepare individuals to collect data relevant to their health goals and for collaboration, (2) help health experts focus on an individual's eating context, experiences, and goals in collaborative review, and (3) support individuals and experts to develop individualized, actionable plans and strategies.},
	articleno    = 7,
	numpages     = 27,
	keywords     = {patient-generated health data, collaboration, food, patient-provider collaboration, Self-tracking, personal informatics}
}
@article{10.1145/3314393,
	title        = {ICT: In-Field Calibration Transfer for Air Quality Sensor Deployments},
	author       = {Cheng, Yun and He, Xiaoxi and Zhou, Zimu and Thiele, Lothar},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314393},
	url          = {https://doi.org/10.1145/3314393},
	issue_date   = {March 2019},
	abstract     = {Recent years have witnessed a growing interest in urban air pollution monitoring, where hundreds of low-cost air quality sensors are deployed city-wide. To guarantee data accuracy and consistency, these sensors need periodic calibration after deployment. Since access to ground truth references is often limited in large-scale deployments, it is difficult to conduct city-wide post-deployment sensor calibration. In this work we propose In-field Calibration Transfer (ICT), a calibration scheme that transfers the calibration parameters of source sensors (with access to references) to target sensors (without access to references). On observing that (i) the distributions of ground truth in both source and target locations are similar and (ii) the transformation is approximately linear, ICT derives the transformation based on the similarity of distributions with a novel optimization formulation. The performance of ICT is further improved by exploiting spatial prediction of air quality levels and multi-source fusion. Experiments show that ICT is able to calibrate the target sensors as if they had direct access to the references.},
	articleno    = 6,
	numpages     = 19,
	keywords     = {Air Pollution, Sensor Calibration Transfer}
}
@article{10.1145/3314389,
	title        = {Deep Multi-Task Learning Based Urban Air Quality Index Modelling},
	author       = {Chen, Ling and Ding, Yifang and Lyu, Dandan and Liu, Xiaoze and Long, Hanyu},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314389},
	url          = {https://doi.org/10.1145/3314389},
	issue_date   = {March 2019},
	abstract     = {Obtaining comprehensive air quality information can help protect human health from air pollution. Existing spatially fine-grained estimation methods and forecasting methods have the following problems: 1) Only a part of data related to air quality is considered. 2) Features are defined and extracted artificially. 3) Due to the lack of training samples, they usually cannot achieve good generalization performance. Therefore, we propose a deep multi-task learning (MTL) based urban air quality index (AQI) modelling method (PANDA). On one hand, a variety of air quality-related urban big data (meteorology, traffic, factory air pollutant emission, point of interest (POI) distribution, road network distribution, etc.) are considered. Deep neural networks are used to learn the representations of these relevant spatial and sequential data, as well as to build the correlation between AQI and these representations. On the other hand, PANDA solves spatially fine-grained AQI level estimation task and AQI forecasting task jointly, which can leverage the commonalities and differences between these two tasks to improve generalization performance. We evaluate PANDA on the dataset of Hangzhou city. The experimental results show that our method can yield a better performance compared to the state-of-the-art methods.},
	articleno    = 2,
	numpages     = 17,
	keywords     = {air quality forecasting, graph embedding, multi-task learning, deep learning, Air quality estimation}
}
@article{10.1145/3287069,
	title        = {Rewind: Automatically Reconstructing Everyday Memories with First-Person Perspectives},
	author       = {Tan, Neille-Ann H. and Sha, Han and Celen, Eda and Tran, Phucanh and Wang, Kelly and Cheung, Gifford and Hinch, Philip and Huang, Jeff},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287069},
	url          = {https://doi.org/10.1145/3287069},
	issue_date   = {December 2018},
	abstract     = {Snapping photos or videos on a smartphone makes recording visual memories convenient, but what isn't captured may still be meaningful in retrospect. In this paper, digital mementos are automatically generated for participants using the Rewind system, which assists the recall of location-based minutiae. Rewind is a video-like medium describing people's daily excursions using a sequence of street-level images determined by self-tracked location data. The Rewinds are color-processed to reflect seasonal, time-of-day and weather characteristics. Through two user studies with a combined 40 users, Rewinds were shown to be used as memory artifacts, and are especially meaningful in the many situations when photos or videos are not available. The small cues in Rewinds evoke longer fragments of memory tied to nostalgic routines or significant events, and the sequence of images provide a first-person perspective of remembrance for users. While they are more generic and can possess inaccuracies, Rewinds give people anchors for memories that feel like their own that are used to craft a narrative for that day. Rewind strikes a balance between automatic logging and manually-curated memories by personalizing it in meaningful visuals and consolidating an otherwise overwhelming amount of data.},
	articleno    = 191,
	numpages     = 20,
	keywords     = {travel experiences, digital memories, location history}
}
@article{10.1145/3287048,
	title        = {FarmChat: A Conversational Agent to Answer Farmer Queries},
	author       = {Jain, Mohit and Kumar, Pratyush and Bhansali, Ishita and Liao, Q. Vera and Truong, Khai and Patel, Shwetak},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287048},
	url          = {https://doi.org/10.1145/3287048},
	issue_date   = {December 2018},
	abstract     = {Farmers constitute 54.6% of the Indian population, but earn only 13.9% of the national GDP. This gross mismatch can be alleviated by improving farmers' access to information and expert advice (e.g., knowing which seeds to sow and how to treat pests can significantly impact yield). In this paper, we report our experience of designing a conversational agent, called FarmChat, to meet the information needs of farmers in rural India. We conducted an evaluative study with 34 farmers near Ranchi in India, focusing on assessing the usability of the system, acceptability of the information provided, and understanding the user population's unique preferences, needs, and challenges in using the technology. We performed a comparative study with two different modalities: audio-only and audio+text. Our results provide a detailed understanding on how literacy level, digital literacy, and other factors impact users' preferences for the interaction modality. We found that a conversational agent has the potential to effectively meet the information needs of farmers at scale. More broadly, our results could inform future work on designing conversational agents for user populations with limited literacy and technology experience.},
	articleno    = 170,
	numpages     = 22,
	keywords     = {conversational agents, audio+text UI, India, smartphone, QnA system, developing world, farming, Chatbots, audio-only UI, ICT4D, user interfaces, speech-based interfaces, agriculture}
}
@article{10.1145/3287046,
	title        = {Multi-View Commercial Hotness Prediction Using Context-Aware Neural Network Ensemble},
	author       = {He, Zhiyuan and Yang, Su},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287046},
	url          = {https://doi.org/10.1145/3287046},
	issue_date   = {December 2018},
	abstract     = {Prediction over heterogeneous data attracts much attention in urban computing. Recently, satellite imagery provides a new chance for urban perception but raises the problem of how to fuse visual and non-visual features. So far, the practice is to concatenate the multimodal features into a vector, which may suppress important features. Therefore, we propose a new ensemble learning framework: (1) An estimator is developed for each predictor to score its confidence, which is input adaptive. (2) By applying the output of each predictor to the input of the corresponding estimator as feedback, the estimator learns the performance of the predictor in the input-output space. When a new input is applied to produce a prediction, the similar situations will be recalled by the estimator to score the confidence of the prediction. (3) Using end-to-end training, the estimator learns the weights automatically to minimize the total loss of the neural networks. With the proposed method, data mining based urban computing and computer vision rendered urban perception can be bridged at the task of commercial activeness prediction, where the prediction based on satellite images and social context data are fused to yield better prediction than those based on single view data in the experiments.},
	articleno    = 168,
	numpages     = 19,
	keywords     = {Urban perception, Ubiquitous Computing, Ensemble Learning}
}
@article{10.1145/3287044,
	title        = {Butterfly: Environment-Independent Physical-Layer Authentication for Passive RFID},
	author       = {Han, Jinsong and Qian, Chen and Yang, Yuqin and Wang, Ge and Ding, Han and Li, Xin and Ren, Kui},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287044},
	url          = {https://doi.org/10.1145/3287044},
	issue_date   = {December 2018},
	abstract     = {RFID tag authentication is challenging because most commodity tags cannot run cryptographic algorithms. Prior research demonstrates that physical layer information based authentication is a promising solution, which uses special features from the physical backscatter signals from tags as their fingerprints. However, our recent studies show that existing physical-layer authentication may fail if feature collection and authentication are conducted in different locations, due to location-dependent noises, environmental factors, or reader hardware differences.This paper presents a new physical layer authentication scheme, called Butterfly, which is resilient to environment and location changes. Butterfly utilizes a pair of adjacent tags as an identifier of each object. By using the difference between the RF signals of the two tags as their fingerprint, the environmental factors can be effectively canceled. Butterfly is fully compatible with commodity RFID systems and standards. We set up a prototype Butterfly using commodity readers, tags, and RF devices. Extensive experiments show that Butterfly achieves high authentication accuracy for substantially different environments and device changes.},
	articleno    = 166,
	numpages     = 21,
	keywords     = {Device authentication, Internet of things, RFID}
}
@article{10.1145/3287036,
	title        = {Performance Characterization of Deep Learning Models for Breathing-Based Authentication on Resource-Constrained Devices},
	author       = {Chauhan, Jagmohan and Rajasegaran, Jathushan and Seneviratne, Suranga and Misra, Archan and Seneviratne, Aruna and Lee, Youngki},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287036},
	url          = {https://doi.org/10.1145/3287036},
	issue_date   = {December 2018},
	abstract     = {Providing secure access to smart devices such as smartphones, wearables and various other IoT devices is becoming increasingly important, especially as these devices store a range of sensitive personal information. Breathing acoustics-based authentication offers a highly usable and possibly a secondary authentication mechanism for secure access. Executing sophisticated machine learning pipelines for such authentication on such devices remains an open problem, given their resource limitations in terms of storage, memory and computational power. To investigate this challenge, we compare the performance of an end-to-end system for both user identification and user verification tasks based on breathing acoustics on three type of smart devices: smartphone, smartwatch and Raspberry Pi using both shallow classifiers (i.e., SVM, GMM, Logistic Regression) and deep learning based classifiers (e.g., LSTM, MLP). Via detailed analysis, we conclude that LSTM models for acoustic classification are the smallest in size, have the lowest inference time and are more accurate than all other compared classifiers. An uncompressed LSTM model provides an average f-score of 80%-94% while requiring only 50--180 KB of storage (depending on the breathing gesture). The resulting inference can be done on smartphones and smartwatches within approximately 7--10 ms and 18--66 ms respectively, thereby making them suitable for resource-constrained devices. Further memory and computational savings can be achieved using model compression methods such as weight quantization and fully connected layer factorization: in particular, a combination of quantization and factorization achieves 25%--55% reduction in LSTM model size, with almost no loss in performance. We also compare the performance on GPUs and show that the use of GPU can reduce the inference time of LSTM models by a factor of 300%. These results provide a practical way to deploy breathing based biometrics, and more broadly LSTM-based classifiers, in future ubiquitous computing applications.},
	articleno    = 158,
	numpages     = 24,
	keywords     = {MLP, SVM, Authentication, Breathing Gestures, GMM, Wearables, Security, IoT, LSTM}
}
@article{10.1145/3287033,
	title        = {Amateur: Augmented Reality Based Vehicle Navigation System},
	author       = {Cao, Chu and Li, Zhenjiang and Zhou, Pengfei and Li, Mo},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287033},
	url          = {https://doi.org/10.1145/3287033},
	issue_date   = {December 2018},
	abstract     = {This paper presents Amateur, an augmented reality based vehicle navigation system using commodity smart phones. Amateur reads the navigation information from a digital map, matches it into live road condition video captured by smart phone, and directly annotates the navigation instructions on the video stream. The Amateur design entails two major challenges, including the lane identification and the intersection inference so as to correctly annotate navigation instructions for lane-changing and intersection-turning. In this paper, we propose a particle filter based design, assisted by inertial motion sensors and lane markers, to tolerate incomplete and even erroneous detection of road conditions. We further leverage traffic lights as land markers to estimate the position of each intersection to accurately annotate the navigation instructions. We develop a prototype system on Android mobile phones and test our system in a total number of more than 300km travel distance on different taxi cabs in a city. The evaluation results suggest that our system can timely provide correct instructions to navigate drivers. Our system can identify lanes in 2s with 92.7% accuracy and detect traffic lights with 95.29% accuracy. Overall, the accuracy of the navigation signs placement is less than 105 pixels on the screen throughout the experiments. The feedback from 50 taxi drivers indicates that Amateur provides an improved experience compared to traditional navigation systems.},
	articleno    = 155,
	numpages     = 24,
	keywords     = {Smartphone, Navigation Service, Augmented Reality}
}
@article{10.1145/3264962,
	title        = {BiLock: User Authentication via Dental Occlusion Biometrics},
	author       = {Zou, Yongpan and Zhao, Meng and Zhou, Zimu and Lin, Jiawei and Li, Mo and Wu, Kaishun},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264962},
	url          = {https://doi.org/10.1145/3264962},
	issue_date   = {September 2018},
	abstract     = {User authentication on smart devices is indispensable to keep data privacy and security. It is especially significant for emerging wearable devices such as smartwatches considering data sensitivity in them. However, conventional authentication methods are not applicable for wearables due to constraints of size and hardware, which makes present wearable devices lack convenient, secure and low-cost authentication schemes. To tackle this problem, we reveal a novel biometric authentication mechanism which makes use of sounds of human dental occlusion (i.e., tooth click). We demonstrate its feasibility by comprehensive measurement study, and design a prototype-BiLock with two Android platforms. Extensive real-world experiments have been conducted to evaluate the accuracy, robustness and security of BiLock in different environments. The results show that BiLock can achieve less than 5% average false reject rate and 0.95% average false accept rate even in a noisy environment. Comparative experiments also demonstrate that BiLock possesses advantages in robustness to noise and security against replay and observation attacks over existing voiceprinting schemes.},
	articleno    = 152,
	numpages     = 20,
	keywords     = {Dental occlusion, Biometric authentication, Mobile devices}
}
@article{10.1145/3264953,
	title        = {FreeSense: A Robust Approach for Indoor Human Detection Using Wi-Fi Signals},
	author       = {Xin, Tong and Guo, Bin and Wang, Zhu and Wang, Pei and Lam, Jacqueline Chi Kei and Li, Victor and Yu, Zhiwen},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264953},
	url          = {https://doi.org/10.1145/3264953},
	issue_date   = {September 2018},
	abstract     = {Human detection aims to monitor how people are moving in an area of interest. There are many potential applications such as asset security monitoring, emergency management, and elderly care, etc. With the development of wireless sensing technique, Wi-Fi-based human detection method carries great potential due to advantages of pervasive accessibility and coverage flexibility. Previous studies have investigated the detection of human movements via signal variations. However, affected by noises, such as multi-path effect and device difference, existing approaches cannot achieve high accuracy and low false alarm rate at the same time. In this paper, we propose FreeSense, a novel Wi-Fi-based approach for human detection. Different from previous studies that characterize the variation of temporal wireless signals or calculate the deviation of Channel State Information (CSIs) from a normal profile, we will detect human movements by identifying whether there is any phase difference between the amplitude waveforms of multiple receiving antennas. In addition, we also model the sensing coverage for movements of different granularities in open space and propose a method to estimate the coverage range. Extensive experiments demonstrate that FreeSense can achieve an average false positive rate (FP) of 0.53% and an average false negative rate (FN) of 1.40%. The coverage range estimation method can achieve an average accuracy of 1.36 m, sufficient to guide the deployment of devices for human detection indoors.},
	articleno    = 143,
	numpages     = 23,
	keywords     = {Wireless sensing, Wi-Fi CSI, sensing coverage model, human detection, human behavior understanding}
}
@article{10.1145/3264926,
	title        = {SkinWire: Fabricating a Self-Contained On-Skin PCB for the Hand},
	author       = {Kao, Hsin-Liu Cindy and Bedri, Abdelkareem and Lyons, Kent},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264926},
	url          = {https://doi.org/10.1145/3264926},
	issue_date   = {September 2018},
	abstract     = {Current wearable form factors often house electronics using an enclosure that is attached to the body. This form factor, while wearable, tends to protrude from the body and therefore can limit wearability. While emerging research in on-skin interfaces from the HCI and wearable communities have generated form factors with lower profiles, they often still require support by conventional electronics and associated form factors for the microprocessor, wireless communication, and battery units. In this work, we introduce SkinWire, a fabrication approach that extends the early work in on-skin interfaces to shift wearable devices from their traditional box-like forms to a fully self-contained on-skin form factor.The Skin Wire approach starts with the placement of electronic components into individual PCB islands, which are then distributed over the body surface. The islands are connected through a novel skin-wiring approach that deposits conformal multi-stranded metallic wires on thin silicon substrates through a sewing-based technique. The process affords on-skin interfaces with the needed wiring in limited surface areas. We exemplify the capacity of this approach by shifting an IMU-based hand gesture system - which traditionally come in bulky glove-based form factors - directly onto the skin. Inspired by the emerging body art trend of body wiring, the Skin Wire approach uses readily accessible materials and affords aesthetic customization. We evaluate fabrication parameters, and conduct a user study to uncover wearability concerns.},
	articleno    = 116,
	numpages     = 23,
	keywords     = {Fashion, Wearable Computing, Fabrication, Hand Gesture Recognition, On-Skin Interface}
}
@article{10.1145/3264913,
	title        = {Unobtrusive Assessment of Students' Emotional Engagement during Lectures Using Electrodermal Activity Sensors},
	author       = {Di Lascio, Elena and Gashi, Shkurta and Santini, Silvia},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264913},
	url          = {https://doi.org/10.1145/3264913},
	issue_date   = {September 2018},
	abstract     = {Modern wearable devices enable the continuous and unobtrusive monitoring of human physiological parameters, including heart rate and electrodermal activity. Through the definition of adequate models these parameters allow to infer the wellbeing, empathy, or engagement of humans in different contexts. In this paper, we show that off-the-shelf wearable devices can be used to unobtrusively monitor the emotional engagement of students during lectures. We propose the use of several novel features to capture students' momentary engagement and use existing methods to characterize the general arousal of students and their physiological synchrony with the teacher. To evaluate our method we collect a data set that -- after data cleaning -- contains data from 24 students, 9 teachers, and 41 lectures. Our results show that non-engaged students can be identified with high reliability. Using a Support Vector Machine, for instance, we achieve a recall of 81% -- which is a 25 percentage points improvement with respect to a Biased Random classifier. Overall, our findings may inform the design of systems that allow students to self-monitor their engagement and act upon the obtained feedback. Teachers could profit of information about non-engaged students too to perform self-reflection and to devise and evaluate methods to (re-)engage students.},
	articleno    = 103,
	numpages     = 21,
	keywords     = {Electrodermal, Activity, Wearable, Engagement, Students}
}
@article{10.1145/3264899,
	title        = {Up to a Limit? Privacy Concerns of Bystanders and Their Willingness to Share Additional Information with Visually Impaired Users of Assistive Technologies},
	author       = {Ahmed, Tousif and Kapadia, Apu and Potluri, Venkatesh and Swaminathan, Manohar},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264899},
	url          = {https://doi.org/10.1145/3264899},
	issue_date   = {September 2018},
	abstract     = {The emergence of augmented reality and computer vision based tools offer new opportunities to visually impaired persons (VIPs). Solutions that help VIPs in social interactions by providing information (age, gender, attire, expressions etc.) about people in the vicinity are becoming available. Although such assistive technologies are already collecting and sharing such information with VIPs, the views, perceptions, and preferences of sighted bystanders about such information sharing remain unexplored. Although bystanders may be willing to share more information for assistive uses it remains to be explored to what degree bystanders are willing to share various kinds of information and what might encourage additional sharing of information based on the contextual needs of VIPs. In this paper we describe the first empirical study of information sharing preferences of sighted bystanders of assistive devices. We conducted a survey based study using a contextual method of inquiry with 62 participants followed by nine semi-structured interviews to shed more insight on our key quantitative findings. We find that bystanders are more willing to share some kinds of personal information with VIPs and are willing to share additional information if higher security assurances can be made by improving their control over how their information is shared.},
	articleno    = 89,
	numpages     = 27,
	keywords     = {Assistive Technologies, Information Sharing, Paratyping}
}
@article{10.1145/3214283,
	title        = {IDrone: Robust Drone Identification through Motion Actuation Feedback},
	author       = {Ruiz, Carlos and Pan, Shijia and Bannis, Adeola and Chen, Xinlei and Joe-Wong, Carlee and Noh, Hae Young and Zhang, Pei},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214283},
	url          = {https://doi.org/10.1145/3214283},
	issue_date   = {June 2018},
	abstract     = {Swarms of Unmanned Aerial Vehicles (drones) could provide great benefit when used for disaster response and indoor search and rescue scenarios. In these harsh environments where GPS availability cannot be ensured, prior work often relies on cameras for control and localization. This creates the challenge of identifying each drone, i.e., finding the association between each physical ID (such as their radio address) and their visual ID (such as an object tracker output). To address this problem, prior work relies on visual cues such as LEDs or colored markers to provide unique information for identification. However, these methods often increase deployment difficulty, are sensitive to environmental changes, not robust to distance and might require hardware changes.In this paper, we present IDrone, a robust physical drone identification system through motion matching and actuation feedback. The intuition is to (1) identify each drone by matching the motion detected through their inertial sensors and from an external camera, and (2) control the drones so they move in unique patterns that allow for fast identification, while minimizing the risk of collision involved in controlling drones with uncertain identification. To validate our approach, we conduct both simulation and real experimentation with autonomous drones for the simplified case of a stationary Spotter (powerful drone equipped with the camera). Overall, our initial results show that our approach offers a great tradeoff between fast identification and small collision probability. In particular, IDrone achieves faster identification time than safety-based baseline actuation (one-at-a-time), and significantly higher survival rate compared to fast, non-safety-based baseline actuation (random motion).},
	articleno    = 80,
	numpages     = 22,
	keywords     = {multi-modal sensing, object identification, physical actuation, UAV swarm, camera}
}
@article{10.1145/3214282,
	title        = {RF Bandaid: A Fully-Analog and Passive Wireless Interface for Wearable Sensors},
	author       = {Ranganathan, Vaishnavi and Gupta, Sidhant and Lester, Jonathan and Smith, Joshua R. and Tan, Desney},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214282},
	url          = {https://doi.org/10.1145/3214282},
	issue_date   = {June 2018},
	abstract     = {This paper presents a passive wireless RF sensor platform (RFSP), with only analog components, that harvests energy from an RF source and reflects data as a direct subcarrier modulation, thus making it battery free. A fully-analog architecture results in an ultra-low power device (under 200 μW) with a low component count, reducing the physical footprint. We envision such a platform to enable medical sensing systems that fit on a small bandaid like flexible structure, require no-battery, or charging and are able to provide continuous physiological monitoring. To realize this vision, we have developed and optimized a novel RF architecture that 1) directly maps sensor output to frequency modulation and transmits it to a remote receiver processing unit (RPU). This direct frequency mapping allows all further digitization and computation to be moved to the RPU -- reducing power and size requirements on the RFSP; 2) harvests energy from the carrier signal transmitted by a simple continuous wave transmitter, thereby requiring no batteries or supercap; and 3) uses backscatter to communicate with the RPU enabling ultra-low power requirements. The total power consumption of our prototype device leveraging this architecture was measured to be between 35 μW and 160 μW. We demonstrate that the RFSP can harvest sufficient power, sense, and communicate continuously without necessity for energy storage at a distance of 4 m from a transmitter emitting a 915 MHz continuous wave at 26 dBm (0.39 W). Prior backscatter systems typically have power budgets of 1 mW and require energy storage (battery or supercap), RFSP's sub 200 μW power consumption provides a significant improvement and longer range for a given TX power. To demonstrate applicability to real-world health sensing and the flexibility to adapt to different sensors, this paper presents results from breathing, heart rate, temperature, and sound sensing applications.},
	articleno    = 79,
	numpages     = 21,
	keywords     = {energy harvesting, SDR, body-worn sensors, RFID, analog sensing, Passive devices}
}
@article{10.1145/3214281,
	title        = {When Virtual Reality Meets Internet of Things in the Gym: Enabling Immersive Interactive Machine Exercises},
	author       = {Rabbi, Fazlay and Park, Taiwoo and Fang, Biyi and Zhang, Mi and Lee, Youngki},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214281},
	url          = {https://doi.org/10.1145/3214281},
	issue_date   = {June 2018},
	abstract     = {With the advent of immersive virtual reality (VR) head-mounted displays (HMD), we envision that immersive VR will revolutionize the personal fitness experience in our daily lives. Toward this vision, we present JARVIS, a virtual exercise assistant that is able to provide an immersive and interactive gym exercise experience to a user. JARVIS is enabled by the synergy between Internet of Things (IoT) and immersive VR. JARVIS employs miniature IoT sensing devices removably attachable to exercise machines to track a multitude of exercise information including exercise types, repetition counts, and progress within each repetition in real time. Based on the tracked exercise information, JARVIS shows the user the proper way of doing the exercise in the virtual exercise environment, thereby helping the user to better focus on the target muscle group. We have conducted both in-lab experiments and a pilot user study to evaluate the performance and effectiveness of JARVIS, respectively. Our in-lab experiments with fifteen participants show that JARVIS is able to segment exercise repetitions with an average accuracy of 97.96% and recognize exercise types with an average accuracy of 99.08%. Our pilot user study with ten participants shows statistically significant improvements in perceived enjoyment, competence, and usefulness with JARVIS compared to a traditional machine exercise setting (p &lt; 0.05). Finally, our surface electromyography (sEMG) signal analysis conducted during the pilot user study shows statistically significant improvement in terms of muscle activation (p &lt; 0.01), indicating the potential of JARVIS in providing an engaging and effective guidance for machine exercises.},
	articleno    = 78,
	numpages     = 21,
	keywords     = {Virtual Reality, Human-Computer Interaction, Virtual Assistant, Activity Recognition, Internet of Things}
}
@article{10.1145/3214275,
	title        = {TwistIn: Tangible Authentication of Smart Devices via Motion Co-Analysis with a Smartwatch},
	author       = {Leung, Ho-Man Colman and Fu, Chi-Wing and Heng, Pheng-Ann},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214275},
	url          = {https://doi.org/10.1145/3214275},
	issue_date   = {June 2018},
	abstract     = {Smart devices contain sensitive information that has to be guarded against unauthorized access through authentication. Existing authentication methods become obsolete as they are designed either for logging-in one device at a time or are ineffective in a multi-user multi-device environment. This paper presents TwistIn, a simple gesture that takes a smartwatch as an authentication token for fast access and control of other smart devices. Our mechanism is particularly useful for devices such as smartphones, smart glasses, and small IoT objects. To log in a device, one simply need to pick it up and twist it a few times. Then, by co-analyzing the motion data from the device and the watch, our method can extend the user authentication on the watch to the device. This is a simple and tangible interaction that takes only one to two seconds to perform. Furthermore, to account for user variation in wrist bending, we decompose wrist and forearm rotations via an optimization to improve the method accuracy. We implemented TwistIn, collected thousands of gesture samples, and conducted various experiments to evaluate our prototype system and show that it achieved over 95% detection accuracy.},
	articleno    = 72,
	numpages     = 24,
	keywords     = {motion analysis, smart device, Tangible authentication, cross-device interaction}
}
@article{10.1145/3214270,
	title        = {Lightitude: Indoor Positioning Using Uneven Light Intensity Distribution},
	author       = {Hu, Yiqing and Xiong, Yan and Huang, Wenchao and Li, Xiang-Yang and Yang, Panlong and Zhang, Yanan and Mao, Xufei},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214270},
	url          = {https://doi.org/10.1145/3214270},
	issue_date   = {June 2018},
	abstract     = {In this paper, we propose an indoor positioning system, Lightitude, which utilizes the already existed, uneven indoor light intensity distribution established by densely deployed indoor lights as the medium. As common indoor lights cannot act as landmarks due to lack of unique features (e.g., unique intensity or flicker frequency), systems that exploiting the received light intensity (RLI) usually impose strong constraints on user's motion and make ideal assumptions about the indoor environment. Different from these approaches, we first propose a realistic light intensity model to reconstruct the RLI distribution given any motion (position, orientation) of the receiver, thus RLI collected with every motion of the receiver could be used for positioning. Then we design a particle-filter-based positioning module, which harnesses user's natural mobility to eliminate the ambiguity of a single RLI. Experiment results show that Lightitude achieves mean accuracy 1.93m and 1.98m in an office (720m2) and a library (960m2) respectively. Lightitude is still robust against interferences like sunlight, shading of human-body and several user behaviors.},
	articleno    = 67,
	numpages     = 25,
	keywords     = {Ubiquitous visible lights, Indoor localization, Smart device}
}
@article{10.1145/3214269,
	title        = {DeActive: Scaling Activity Recognition with Active Deep Learning},
	author       = {Hossain, H. M. Sajjad and Al Haiz Khan, MD Abdullah and Roy, Nirmalya},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214269},
	url          = {https://doi.org/10.1145/3214269},
	issue_date   = {June 2018},
	abstract     = {Deep learning architectures have been applied increasingly in multi-modal problems which has empowered a large number of application domains needing much less human supervision in the process. As unlabeled data are abundant in most of the application domains, deep architectures are getting increasingly popular to extract meaningful information out of these large volume of data. One of the major caveat of these architectures is that the training phase demands both computational time and system resources much higher than shallow learning algorithms and it is posing a difficult challenge for the researchers to implement the architectures in low-power resource constrained devices. In this paper, we propose a deep and active learning enabled activity recognition model, DeActive, which is optimized according to our problem domain and reduce the resource requirements. We incorporate active learning in the process to minimize the human supervision along with the effort needed for compiling ground truth. The DeActive model has been validated using real data traces from a retirement community center (IRB #HP-00064387) and 4 public datasets. Our experimental results show that our model can contribute better accuracy while ensuring less amount of resource usages in reduced time compared to other traditional deep learning approaches in activity recognition.},
	articleno    = 66,
	numpages     = 23,
	keywords     = {Active learning, Deep learning, Smart home, Activity recognition}
}
@article{10.1145/3214268,
	title        = {Energy-Ball: Wireless Power Transfer for Batteryless Internet of Things through Distributed Beamforming},
	author       = {Fan, Xiaoran and Ding, Han and Li, Sugang and Sanzari, Michael and Zhang, Yanyong and Trappe, Wade and Han, Zhu and Howard, Richard E.},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214268},
	url          = {https://doi.org/10.1145/3214268},
	issue_date   = {June 2018},
	abstract     = {Wireless power transfer (WPT) promises to deliver energy to devices that are otherwise hard to charge or replace batteries for. This paper presents a new power transfer approach by aligning the phases of a collection of radio frequency (RF) energy chargers at the target receiver device. Our approach can ship energy over tens of meters and to mobile targets. More importantly, our approach leads to a highly asymmetric energy density distribution in the charging area: the energy density at the target receiver is much higher than the energy density at other locations. It is a departure from existing beamforming based WPT systems that have high energy along the energy beam path. Such a technology can enable a large array of batteryless Internet of Things applications and render them much more robust and long-running. Thanks to its asymmetric energy distribution, our approach potentially can be scaled up to ship higher level of energy over longer distances.In this paper, we design, prototype, and evaluate the proposed energy transfer approach, referred to as Energy-Ball. We implement an Energy-Ball testbed that consists of 17 N210 and 4 B210 Universal Software Radio Peripheral (USRP) nodes, yielding a 20 x 20 m2 energy delivery area. We conduct carefully designed experiments on the testbed. We demo that the energy density of Energy-Ball at the target spot is considerably higher than the energy density elsewhere, with the peak to average power ratio of 8.72. We show that Energy-Ball can transfer energy to any point within the area. When the receiver moves at a speed of 0.5 m/s, Energy-Ball can transfer 80% of optimal power to the mobile receiver. Further, our results also show Energy-Ball can deliver over 0.6mw RF power that enables batteryless sensors at any point across the area.},
	articleno    = 65,
	numpages     = 22,
	keywords     = {Batteryless IoT, Wireless power transfer, Mobile Receiver, Distributed Beamforming}
}
@article{10.1145/3191780,
	title        = {SharedEdge: GPS-Free Fine-Grained Travel Time Estimation in State-Level Highway Systems},
	author       = {Yang, Yu and Zhang, Fan and Zhang, Desheng},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191780},
	url          = {https://doi.org/10.1145/3191780},
	issue_date   = {March 2018},
	abstract     = {Estimating travel time on the highway in real time is of great importance for transportation services. Previous work has been mainly focusing on the city scale for a particular transportation system, e.g., taxi, bus, and metro. Little research has been conducted to estimate fine-grained real-time travel time in state-level highway systems. This is because the traditional solutions based on probe vehicles or loop sensors cannot scale to state-level highway systems due to their large spatial coverage. Recently, the adoption of Electric Toll Collection (ETC) systems (e.g. EZ-pass) brings a new opportunity to estimate the real-time travel time in the highway systems with little marginal cost. However, the key challenge is that ETC data only record the coarse-grained total travel time between a pair of toll stations rather than fine-grained travel time in each individual highway edge. To address this challenge, we design SharedEdge to estimate the fine-grained edge travel time with large-scale streaming ETC data. The key novelty is that we estimate real-time fine-grained travel time (i.e., edge travel time) without using fine-grained data (i.e. GPS trajectories or loop sensor data), by a few techniques based on Bayesian Graphical models and Expectation Maximization. More importantly, we implement our SharedEdge in the Guangdong Province, China with an ETC system covering 69 highways and 773 toll stations with a length of 7, 000 km. Based on this implementation, we evaluate SharedEdge in details by comparing it with some baselines and the state-of-the-art models. The evaluation results show that SharedEdge outperforms other methods in terms of travel time estimation accuracy when compared with the ground truth obtained by 114 thousand GPS-equipped vehicles.},
	articleno    = 48,
	numpages     = 26,
	keywords     = {Cyber-physical System, Highway System, Travel Time Estimation}
}
@article{10.1145/3191777,
	title        = {PrivateHunt: Multi-Source Data-Driven Dispatching in For-Hire Vehicle Systems},
	author       = {Xie, Xiaoyang and Zhang, Fan and Zhang, Desheng},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191777},
	url          = {https://doi.org/10.1145/3191777},
	issue_date   = {March 2018},
	abstract     = {Recently, for-hire vehicle services (FHV, e.g., Uber and Lyft) have become essential to people's daily transportation. Similar to taxis, how to effectively dispatch these FHV based on demand and supply is important for both FHV passengers and drivers. Based on real-world multi-source data, we identify two new challenges for FHV dispatching: (i) diverse demand: FHV passengers are a mix of passengers previously using taxis, buses, subways, or private vehicles; (ii) uncertain supply: FHV drivers join and leave the FHV system with spatiotemporal dynamics. As a result, the state-of-the-art taxi dispatching techniques cannot be applied to FHV systems. In this paper, we design the first FHV dispatching system PrivateHunt based on extremely large-scale urban transportation data from New York City and Shenzhen in China. In particular, we present (i) a passenger demand model based on taxi, bus, subway, and private vehicle data; (ii) a driver supply model based on small-scale FHV data; (iii) a dispatching technique for FHV vehicles based on proposed demand/supply models to reduce idle driving time. We implement PrivateHunt based on 14 thousand taxis, 13 thousand buses, and 8-line subway system and 10 thousand private vehicles. The experimental results show that our data-driven dispatching strategy significantly outperforms the state-of-the-art dispatching strategies without data-driven FHV insights.},
	articleno    = 45,
	numpages     = 26,
	keywords     = {Mobility Models, Urban Computing, Transportation, Multi-modal Interaction, Design}
}
@article{10.1145/3191754,
	title        = {What Makes Smartphone Use Meaningful or Meaningless?},
	author       = {Lukoff, Kai and Yu, Cissy and Kientz, Julie and Hiniker, Alexis},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191754},
	url          = {https://doi.org/10.1145/3191754},
	issue_date   = {March 2018},
	abstract     = {Prior research indicates that many people wish to limit aspects of their smartphone use. Why is it that certain smartphone use feels so meaningless? We examined this question by using interviews, the experience sampling method, and mobile logging of 86,402 sessions of app use. One motivation for use (habitual use to pass the time) and two types of use (entertainment and passive social media) were associated with a lower sense of meaningfulness. In interviews, participants reported feeling a loss of autonomy when using their phone in these ways. These reports were corroborated by experience sampling data showing that motivation to achieve a specific purpose declined over the course of app use, particularly for passive social media and entertainment usage. In interviews, participants pointed out that even when smartphone use itself was meaningless, it could sometimes still be meaningful in the context of broader life as a 'micro escape' from negative situations. We discuss implications for how mobile apps can be used and designed to reduce meaningless experiences.},
	articleno    = 22,
	numpages     = 26,
	keywords     = {social media, Meaning, eudaimonia, uses 8 gratifications, habits, persuasive design, positive computing, self-regulation}
}
@article{10.1145/3191752,
	title        = {Third-Eye: A Mobilephone-Enabled Crowdsensing System for Air Quality Monitoring},
	author       = {Liu, Liang and Liu, Wu and Zheng, Yu and Ma, Huadong and Zhang, Cheng},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191752},
	url          = {https://doi.org/10.1145/3191752},
	issue_date   = {March 2018},
	abstract     = {Air pollution has raised people's public health concerns in major cities, especially for Particulate Matter under 2.5μm (PM2.5) due to its significant impact on human respiratory and circulation systems. In this paper, we present the design, implementation, and evaluation of a mobile application, Third-Eye, that can turn mobile phones into high-quality PM2.5 monitors, thereby enabling a crowdsensing way for fine-grained PM2.5 monitoring in the city. We explore two ways, crowdsensing and web crawling, to efficiently build large-scale datasets of the outdoor images taken by mobile phone, weather data, and air-pollution data. Then, we leverage two deep learning models, Convolutional Neural Network (CNN) for images and Long Short Term Memory (LSTM) network for weather and air-pollution data, to build an end-to-end framework for training PM2.5 inference models. Our App has been downloaded more than 2,000 times and runs more than 1 year. The real user data based evaluation shows that Third-Eye achieves 17.38 μg/m3 average error and 81.55% classification accuracy, which outperforms 5 state-of-the-art methods, including three scattered interpolations and two image based estimation methods. The results also demonstrate how Third-Eye offers substantial enhancements over typical portable PM2.5 monitors by simultaneously improving accessibility, portability, and accuracy.},
	articleno    = 20,
	numpages     = 26,
	keywords     = {Air quality, CNN, crowdsensing, PM2.5 monitoring, LSTM}
}
@article{10.1145/3191742,
	title        = {SiFi: Pushing the Limit of Time-Based WiFi Localization Using a Single Commodity Access Point},
	author       = {Gong, Wei and Liu, Jiangchuan},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191742},
	url          = {https://doi.org/10.1145/3191742},
	issue_date   = {March 2018},
	abstract     = {There has been a booming interest in developing WiFi localization using multi-antenna (MIMO) access points (APs). Recent advances have demonstrated promising results that break the meter-accuracy barrier using commodity APs. Yet these state-of-the-art solutions require either multiple APs that are not necessarily available in practice, or multiple-channel measurements that disrupt normal data communication. In this paper, we present SiFi, a single AP-based indoor localization system that for the first time achieves sub-meter accuracy with a single channel only. The SiFi design is based on a key observation: with MIMO, the multiple (typically three) antennas of an AP are frequency-locked; although the accurate Time-of-Arrival (ToA) estimation on commodity APs is fundamentally limited by the imperfect time and frequency synchronization between the transmitter and receiver, there should be only one value for the ToA distortion that can cause three direct-path ToAs of the antennas to intersect at a single point, i.e., the position of the target. We develop the theoretical foundations of SiFi and demonstrate its realworld implementation with off-the-shelf WiFi cards. Our implementation introduces no hardware modification and is fully compatible with concurrent data transmission. It achieves a median accuracy of 0.93 m, which significantly outperforms the best known single AP single channel solution.},
	articleno    = 10,
	numpages     = 21,
	keywords     = {Single Access Point, Indoor Localization, MIMO, WiFi OFDM}
}
@article{10.1145/3161196,
	title        = {Snoopy: Sniffing Your Smartwatch Passwords via Deep Sequence Learning},
	author       = {Lu, Chris Xiaoxuan and Du, Bowen and Wen, Hongkai and Wang, Sen and Markham, Andrew and Martinovic, Ivan and Shen, Yiran and Trigoni, Niki},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161196},
	url          = {https://doi.org/10.1145/3161196},
	issue_date   = {December 2017},
	abstract     = {Demand for smartwatches has taken off in recent years with new models which can run independently from smartphones and provide more useful features, becoming first-class mobile platforms. One can access online banking or even make payments on a smartwatch without a paired phone. This makes smartwatches more attractive and vulnerable to malicious attacks, which to date have been largely overlooked. In this paper, we demonstrate Snoopy, a password extraction and inference system which is able to accurately infer passwords entered on Android/Apple watches within 20 attempts, just by eavesdropping on motion sensors. Snoopy uses a uniform framework to extract the segments of motion data when passwords are entered, and uses novel deep neural networks to infer the actual passwords. We evaluate the proposed Snoopy system in the real-world with data from 362 participants and show that our system offers a ~ 3-fold improvement in the accuracy of inferring passwords compared to the state-of-the-art, without consuming excessive energy or computational resources. We also show that Snoopy is very resilient to user and device heterogeneity: it can be trained on crowd-sourced motion data (e.g. via Amazon Mechanical Turk), and then used to attack passwords from a new user, even if they are wearing a different model.This paper shows that, in the wrong hands, Snoopy can potentially cause serious leaks of sensitive information. By raising awareness, we invite the community and manufacturers to revisit the risks of continuous motion sensing on smart wearable devices.},
	articleno    = 152,
	numpages     = 29,
	keywords     = {APL, Motion Sensors, Smartwatch}
}
@article{10.1145/3161188,
	title        = {C-FMCW Based Contactless Respiration Detection Using Acoustic Signal},
	author       = {Wang, Tianben and Zhang, Daqing and Zheng, Yuanqing and Gu, Tao and Zhou, Xingshe and Dorizzi, Bernadette},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161188},
	url          = {https://doi.org/10.1145/3161188},
	issue_date   = {December 2017},
	abstract     = {Recent advances in ubiquitous sensing technologies have exploited various approaches to monitoring vital signs. One of the vital signs is human respiration which typically requires reliable monitoring with low error rate in practice. Previous works in respiration monitoring however either incur high cost or suffer from poor error rate. In this paper, we propose a Correlation based Frequency Modulated Continuous Wave method (C-FMCW) which is able to achieve high ranging resolution. Based on C-FMCW, we present the design and implementation of an audio-based highly-accurate system for human respiration monitoring, leveraging on commodity speaker and microphone widely available in home environments. The basic idea behind the audio-based method is that when a user is close to a pair of speaker and microphone, body movement during respiration causes periodic audio signal changes, which can be extracted to obtain the respiration rate. However, several technical challenges exist when applying C-FMCW to detect respiration with commodity acoustic devices. First, the sampling frequency offset between speakers and microphones if not being corrected properly would cause high ranging errors. Second, the uncertain starting time difference between the speaker and microphone varies over time. Moreover, due to multipath effect, weak periodic components due to respiration can easily be overwhelmed by strong static components in practice. To address those challenges, we 1) propose an algorithm to compensate dynamically acoustic signal and counteract the offset between speaker and microphone; 2) co-locate speaker and microphone and use the received signal without reflection (self-interference) as a reference to eliminate the starting time difference; and 3) leverage the periodicity of respiration to extract weak periodic components with autocorrelation. Extensive experimental results show that our system detects respiration in real environments with the median error lower than 0.35 breaths/min, outperforming the state-of-the-arts.},
	articleno    = 170,
	numpages     = 20,
	keywords     = {Health Monitoring, Acoustic sensing, Respiration Detection, Device-free Ranging, Contactless Sensing}
}
@article{10.1145/3161184,
	title        = {Predicting the Suitability of Service Animals Using Instrumented Dog Toys},
	author       = {Byrne, Ceara and Zuerndorfer, Jay and Freil, Larry and Han, Xiaochuang and Sirolly, Andrew and Cilliland, Scott and Starner, Thad and Jackson, Melody},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161184},
	url          = {https://doi.org/10.1145/3161184},
	issue_date   = {December 2017},
	abstract     = {Working dogs1 are significantly beneficial to society; however, a substantial number of dogs are released from time consuming and expensive training programs because of unsuitability in behavior. Early prediction of successful service dog placement could save time, resources, and funding. Our research focus is to explore whether aspects of canine temperament can be detected from interactions with sensors, and to develop classifiers that correlate sensor data to predict the success (or failure) of assistance dogs in advanced training. In a 2-year longitudinal study, our team tested a cohort of dogs entering advanced training in the Canine Companions for Independence (CCI) Program with 2 instrumented dog toys: a silicone ball and a silicone tug sensor. We then create a logistic model tree classifier to predict service dog success using only 5 features derived from dog-toy interactions. During randomized 10-fold cross validation where 4 of the 40 dogs were kept in an independent test set for each fold, our classifier predicts the dogs' outcomes with 87.5% average accuracy. We assess the reliability of our model by performing the testing routine 10 times over 1.5 years for a single suitable working dog, which predicts that the dog would pass each time. We calculate the resource benefit of identifying dogs who will fail early in their training, and the value for a cohort of 40 dogs using our toys and our methods for prediction is over $70,000. With CCI's 6 training centers, annual savings could be upwards of $5 million per year.},
	articleno    = 127,
	numpages     = 20,
	keywords     = {Tangibles, temperament, tangible computing, service dogs, smart objects}
}
@article{10.1145/3161183,
	title        = {FallDeFi: Ubiquitous Fall Detection Using Commodity Wi-Fi Devices},
	author       = {Palipana, Sameera and Rojas, David and Agrawal, Piyush and Pesch, Dirk},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161183},
	url          = {https://doi.org/10.1145/3161183},
	issue_date   = {December 2017},
	abstract     = {Falling or tripping among elderly people living on their own is recognized as a major public health worry that can even lead to death. Fall detection systems that alert caregivers, family members or neighbours can potentially save lives. In the past decade, an extensive amount of research has been carried out to develop fall detection systems based on a range of different detection approaches, i.e, wearable and non-wearable sensing and detection technologies. In this paper, we consider an emerging non-wearable fall detection approach based on WiFi Channel State Information (CSI). Previous CSI based fall detection solutions have considered only time domain approaches. Here, we take an altogether different direction, time-frequency analysis as used in radar fall detection. We use the conventional Short-Time Fourier Transform (STFT) to extract time-frequency features and a sequential forward selection algorithm to single out features that are resilient to environment changes while maintaining a higher fall detection rate. When our system is pre-trained, it has a 93% accuracy and compared to RTFall and CARM, this is a 12% and 15% improvement respectively. When the environment changes, our system still has an average accuracy close to 80% which is more than a 20% to 30% and 5% to 15% improvement respectively.},
	articleno    = 155,
	numpages     = 25,
	keywords     = {Feature extraction, Activity recognition, Device-free, Wi-Fi}
}
@article{10.1145/3161179,
	title        = {SafeDrive: Detecting Distracted Driving Behaviors Using Wrist-Worn Devices},
	author       = {Jiang, Landu and Lin, Xinye and Liu, Xue and Bi, Chongguang and Xing, Guoliang},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161179},
	url          = {https://doi.org/10.1145/3161179},
	issue_date   = {December 2017},
	abstract     = {Distracted driving causes a large number of fatalities every year and is now becoming an important issue in the traffic safety study. In this paper, we present SafeDrive, a driving safety system that leverages wearable wrist sensing techniques to detect and analyze driver distracted behaviors. Existing wrist-worn sensing approaches, however, do not address challenges under real driving environments, such as less distinguishable gesture patterns due to in-vehicle physical constraints, various gesture hallmarks produced by different drivers and significant noise introduced by various driving conditions. In response, SafeDrive adopts a semi-supervised machine learning model for in-vehicle distracting activity detection. To improve the detection accuracy, we provide online updated classifiers by collecting real-time gesture data, while at the same time utilize smartphone sensing to generate soft hints filtering out anomalies and non-distracted hand movements. In the evaluation, we conduct extensive real-road experiments involving 20 participants (10 males and 10 females) and 5 vehicles (a sedan, a minivan and three SUVs). Our approach can achieve an average classification accuracy of over 90% with a error rate of a few percent, which demonstrate that SafeDrive is robust to real driving environments, and has great potential to help drivers shape safe driving habits.},
	articleno    = 144,
	numpages     = 22,
	keywords     = {Wrist-Worn Sensing, Mobile Sensing, Driving Safety, Activity Recognition}
}
@article{10.1145/3131900,
	title        = {Participatory Sensing or Participatory Nonsense? Mitigating the Effect of Human Error on Data Quality in Citizen Science},
	author       = {Budde, Matthias and Schankin, Andrea and Hoffmann, Julien and Danz, Marcel and Riedel, Till and Beigl, Michael},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131900},
	url          = {https://doi.org/10.1145/3131900},
	issue_date   = {September 2017},
	abstract     = {Citizen Science with mobile and wearable technology holds the possibility of unprecedented observation systems. Experts and policy makers are torn between enthusiasm and scepticism regarding the value of the resulting data, as their decision making traditionally relies on high-quality instrumentation and trained personnel measuring in a standardized way. In this paper, we (1) present an empirical behavior taxonomy of errors exhibited in non-expert smartphone-based sensing, based on four small exploratory studies, and discuss measures to mitigate their effects. We then present a large summative study (N=535) that compares instructions and technical measures to address these errors, both from the perspective of improvements to error frequency and perceived usability. Our results show that (2) technical measures without explanation notably reduce the perceived usability and (3) technical measures and instructions nicely complement each other: Their combination achieves a significant reduction in observed error rates while not affecting the user experience negatively.},
	articleno    = 39,
	numpages     = 23,
	keywords     = {Amateur Science, Human Error, Crowd Science, Non-Expert Sensing, Novice Sensing, Participatory Sensing, Design Space, User Mistakes, Empirical Study, User Study, Volunteer Monitoring}
}
@article{10.1145/3130983,
	title        = {Predicting Commercial Activeness over Urban Big Data},
	author       = {Yang, Su and Wang, Minjie and Wang, Wenshan and Sun, Yi and Gao, Jun and Zhang, Weishan and Zhang, Jiulong},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130983},
	url          = {https://doi.org/10.1145/3130983},
	issue_date   = {September 2017},
	abstract     = {This study aims at revealing how commercial hotness of urban commercial districts (UCDs) is shaped by social contexts of surrounding areas so as to render predictive business planning. We define social contexts for a given region as the number of visitors, the region functions, the population and buying power of local residents, the average price of services, and the rating scores of customers, which are computed from heterogeneous data including taxi GPS trajectories, point of interests, geographical data, and user-generated comments. Then, we apply sparse representation to discover the impactor factor of each variable of the social contexts in terms of predicting commercial activeness of UCDs under a linear predictive model. The experiments show that a linear correlation between social contexts and commercial activeness exists for Beijing and Shanghai based on an average prediction accuracy of 77.69% but the impact factors of social contexts vary from city to city, where the key factors are rich life services, diversity of restaurants, good shopping experience, large number of local residents with relatively high purchasing power, and convenient transportation. This study reveals the underlying mechanism of urban business ecosystems, and promise social context-aware business planning over heterogeneous urban big data.},
	articleno    = 119,
	numpages     = 20,
	keywords     = {Crowdsourcing, Social Intelligence, Context Awareness, Economic Ecosystems, Urban Informatics}
}
@article{10.1145/3130968,
	title        = {Robust Indoor Localization across Smartphone Models with Ellipsoid Features from Multiple RSSIs},
	author       = {Sugasaki, Masato and Shimosaka, Masamichi},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130968},
	url          = {https://doi.org/10.1145/3130968},
	issue_date   = {September 2017},
	abstract     = {Localization for mobile devices has become important as the basis technology for various ubiquitous computing applications. While GPS is leveraged as the de-facto standard technology in outdoor localization, its accuracy is poor indoors. For twenty years, researchers have tried to investigate indoor localization technology using fingerprinting from received signal strength indicators (RSSIs). With the widespread use of smartphones in the last decade, device dependency (e.g. antenna characteristics) must be considered to avoid performance degradation, while most of the recent localization approaches assume that all the smartphone models have the same device characteristics.In this paper, we propose a novel feature representation based on multiple RSSIs for compensating performance degradation against smartphone models changes. In contrast to the previous feature representation based on a single RSSI, our new feature representation, which we call Ellipsoid features, employs tuples of pair of RSSIs to eliminate device dependence in the path loss model for wave propagation. In contrast to recent advances in machine learning methods such as domain adaptation, multi-task learning, and semi-supervised learning, our approach requires no additional dataset nor retraining for the new target models. This simplicity would promote ubiquity of indoor localization in the era of smartphones. Moreover, our feature representation works well compared to the state-of-the-arts in feature representations based on multiple RSSIs even when only a small number of access points (APs) are available. Experimental result using smartphone devices including Android Nexus5, Nexus5X, Nexus6P, and Xperia X Performance shows that our approach achieves superior performance over the state-of-the-art indoor localization models as well as robust performance against device changes.},
	articleno    = 103,
	numpages     = 16,
	keywords     = {Wi-Fi, positioning system, RSSI variance problem, Location fingerprint, indoor localization}
}
@article{10.1145/3130967,
	title        = {VRShop: A Mobile Interactive Virtual Reality Shopping Environment Combining the Benefits of On- and Offline Shopping},
	author       = {Speicher, Marco and Cucerca, Sebastian and Kr\"{u}ger, Antonio},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130967},
	url          = {https://doi.org/10.1145/3130967},
	issue_date   = {September 2017},
	abstract     = {In this work, we explored the main characteristics of on- and offline shops with regard to customer shopping behavior and frequency. Thus, we designed and implemented an immersive virtual reality (VR) online shopping environment. We tried to maintain the benefits of online shops, like search functionality and availability, while simultaneously focusing on shopping experience and immersion. By touching the third dimension, VR provides a more advanced form of visualization, which can increase the customer’s satisfaction and thus shopping experience. We further introduced the Virtual Reality Shopping Experience (VRSE) model based on customer satisfaction, task performance and user preference. A case study of a first VR shop prototype was conducted and evaluated with respect to the VRSE model. The results showed that the usability and user experience of our system is above average overall. In summary, searching for a product in a WebVR online shop using speech input in combination with VR output proved to be the best regarding user performance (speed, error rate) and preference (usability, user experience, immersion, motion sickness).},
	articleno    = 102,
	numpages     = 31,
	keywords     = {Virtual reality, shopping experience, customer satisfaction, virtual environment, user-centered design, 3D user interfaces}
}
@article{10.1145/3130961,
	title        = {Distant Emotion Recognition},
	author       = {Salekin, Asif and Chen, Zeya and Ahmed, Mohsin Y. and Lach, John and Metz, Donna and De La Haye, Kayla and Bell, Brooke and Stankovic, John A.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130961},
	url          = {https://doi.org/10.1145/3130961},
	issue_date   = {September 2017},
	abstract     = {Distant emotion recognition (DER) extends the application of speech emotion recognition to the very challenging situation that is determined by variable speaker to microphone distances. The performance of conventional emotion recognition systems degrades dramatically as soon as the microphone is moved away from the mouth of the speaker. This is due to a broad variety of effects such as background noise, feature distortion with distance, overlapping speech from other speakers, and reverberation. This paper presents a novel solution for DER, addressing the key challenges by identification and deletion of features from consideration which are significantly distorted by distance, creating a novel, called Emo2vec, feature modeling and overlapping speech filtering technique, and the use of an LSTM classifier to capture the temporal dynamics of speech states found in emotions. A comprehensive evaluation is conducted on two acted datasets (with artificially generated distance effect) as well as on a new emotional dataset of spontaneous family discussions with audio recorded from multiple microphones placed in different distances. Our solution achieves an average 91.6%, 90.1% and 89.5% accuracy for emotion happy, angry and sad, respectively, across various distances which is more than a 16% increase on average in accuracy compared to the best baseline method.},
	articleno    = 96,
	numpages     = 25,
	keywords     = {Distant emotion detection, word2vec}
}
@article{10.1145/3130939,
	title        = {Gazture: Design and Implementation of a Gaze Based Gesture Control System on Tablets},
	author       = {Li, Yinghui and Cao, Zhichao and Wang, Jiliang},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130939},
	url          = {https://doi.org/10.1145/3130939},
	issue_date   = {September 2017},
	abstract     = {We present Gazture, a light-weight gaze based real-time gesture control system on commercial tablets. Unlike existing approaches that require dedicated hardware (e.g., high resolution camera), high computation overhead (powerful CPU) or specific user behavior (keeping head steady), Gazture provides gesture recognition based on easy-to-control user gaze input with a small overhead. To achieve this goal, Gazture incorporates a two-layer structure: The first layer focuses on real-time gaze estimation with acceptable tracking accuracy while incurring a small overhead. The second layer implements a robust gesture recognition algorithm while compensating gaze estimation error. To address user posture change while using mobile device, we design a online transfer function based method to convert current eye features into corresponding eye features in reference posture, which then facilitates efficient gaze position estimation. We implement Gazture on Lenovo Tab3 8 Plus tablet with Android 6.0.1, and evaluate its performance in different scenarios. The evaluation results show that Gazture can achieve a high accuracy in gesture recognition while incurring a low overhead.},
	articleno    = 74,
	numpages     = 17
}
@article{10.1145/3130935,
	title        = {Mining User Reviews for Mobile App Comparisons},
	author       = {Li, Yuanchun and Jia, Baoxiong and Guo, Yao and Chen, Xiangqun},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130935},
	url          = {https://doi.org/10.1145/3130935},
	issue_date   = {September 2017},
	abstract     = {As the number of mobile apps keeps increasing, users often need to compare many apps, in order to choose one that best fits their needs. Fortunately, as there are so many users sharing an app market, it is likely that some other users with the same preferences have already made the comparisons and shared their opinions. For example, a user may state that an app is better in power consumption than another app in a review, then the review would help other users who care about battery life while choosing apps. This paper presents a method to identify comparative reviews for mobile apps from an app market, which can be used to provide fine-grained app comparisons based on different topics. According to experiments on 5 million reviews from Google Play and manual assessments on 900 reviews, our method is able to identify opinions accurately and provide meaningful comparisons between apps, which could in turn help users find desired apps based on their preferences.},
	articleno    = 75,
	numpages     = 15,
	keywords     = {Mobile application, user review, comparative opinion, text processing}
}
@article{10.1145/3130930,
	title        = {OmniTrack: A Flexible Self-Tracking Approach Leveraging Semi-Automated Tracking},
	author       = {Kim, Young-Ho and Jeon, Jae Ho and Lee, Bongshin and Choe, Eun Kyoung and Seo, Jinwook},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130930},
	url          = {https://doi.org/10.1145/3130930},
	issue_date   = {September 2017},
	abstract     = {We now see an increasing number of self-tracking apps and wearable devices. Despite the vast number of available tools, however, it is still challenging for self-trackers to find apps that suit their unique tracking needs, preferences, and commitments. Furthermore, people are bounded by the tracking tools’ initial design because it is difficult to modify, extend, or mash up existing tools. In this paper, we present OmniTrack, a mobile self-tracking system, which enables self-trackers to construct their own trackers and customize tracking items to meet their individual tracking needs. To inform the OmniTrack design, we first conducted semi-structured interviews (N = 12) and analyzed existing mobile tracking apps (N = 62). We then designed and developed OmniTrack as an Android mobile app, leveraging a semi-automated tracking approach that combines manual and automated tracking methods. We evaluated OmniTrack through a usability study (N = 10) and improved its interfaces based on the feedback. Finally, we conducted a 3-week deployment study (N = 21) to assess if people can capitalize on OmniTrack’s flexible and customizable design to meet their tracking needs. From the study, we showed how participants used OmniTrack to create, revise, and appropriate trackers—ranging from a simple mood tracker to a sophisticated daily activity tracker. We discuss how OmniTrack positively influences and supports self-trackers’ tracking practices over time, and how to further improve OmniTrack by providing more appropriate visualizations and sharable templates, incorporating external contexts, and supporting researchers’ unique data collection needs.},
	articleno    = 67,
	numpages     = 28,
	keywords     = {semi-automated tracking, self-monitoring, wellness, tracking apps, health, customization., Self-tracking, personal informatics, mobile apps}
}
@article{10.1145/3130928,
	title        = {Let’s FOCUS: Mitigating Mobile Phone Use in College Classrooms},
	author       = {Kim, Inyeop and Jung, Gyuwon and Jung, Hayoung and Ko, Minsam and Lee, Uichin},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130928},
	url          = {https://doi.org/10.1145/3130928},
	issue_date   = {September 2017},
	abstract     = {With the increasingly frequent appearance of mobile phones in college classrooms, there have been growing concerns regarding their negative aspects including distractive off-task multitasking. In this work, we design and evaluate Let’s FOCUS, a software-based intervention service that assists college students in self-regulating their mobile phone use in classrooms. Our preliminary survey study (with 47 professors and 283 students) reveals that it is critical to encourage voluntary participation by framing intervention as a learning tool and to raise awareness regarding appropriate mobile phone usage by establishing social norms in colleges. Let’s FOCUS introduces a virtual limiting space for each class (or a virtual classroom) where the students can explicitly restrict their mobile phone use voluntarily. Furthermore, it promotes students’ willing participation by leveraging social facilitation and context-aware reminders associated with virtual classrooms. We conducted a campus-wide campaign for approximately six weeks to evaluate the feasibility of the proposed approach. The results confirm that 379 students used the app to limit 9,335 hours of mobile phone usage over 233 classrooms. Let’s FOCUS was used in diverse learning contexts and for different purposes and its social learning and context-awareness features significantly motivated prolonged participation. We present the design considerations of software-based intervention.},
	articleno    = 63,
	numpages     = 29,
	keywords     = {mobile phone usage, context awareness, persuasive technology, Software-based intervention, mobile application, college students, off-task multitasking}
}
@article{10.1145/3130921,
	title        = {QuickTalk: An Association-Free Communication Method for IoT Devices in Proximity},
	author       = {Ham, Seongmin and Lee, Jihyung and Lee, Kyunghan},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130921},
	url          = {https://doi.org/10.1145/3130921},
	issue_date   = {September 2017},
	abstract     = {IoT devices are in general considered to be straightforward to use. However, we find that there are a number of situations where the usability becomes poor. The situations include but not limited to the followings: 1) when initializing an IoT device, 2) when trying to control an IoT device which is initialized by another person, and 3) when trying to control an IoT device out of many of the same type. We tackle these situations by proposing a new association-free communication method, QuickTalk. QuickTalk lets a user device such as a smartphone pinpoint and activate an IoT device with the help of an IR transmitter and communicate with the pinpointed IoT device through the broadcast channel of WiFi without a conventional association process. This nature, QuickTalk allows a user device to immediately give a command to a specific IoT device in proximity even when the IoT device is uninitialized, unassociated with the control interface of the user, or associated but visually indistinguishable from others of the same kind. Our experiments of QuickTalk implemented on Raspberry Pi 2 devices show that QuickTalk does its job quickly and intuitively. The end-to-end delay of QuickTalk for transmitting an IoT command is on average about 0.74 seconds, and is upper bounded by 2.5 seconds. We further confirm that even when an IoT device has ongoing data sessions with other devices, which disturb the broadcast channel, QuickTalk can still reliably communicate with the IoT device at the cost of minor throughput degradation.},
	articleno    = 56,
	numpages     = 18,
	keywords     = {Communication, Proximity Communication, IoT Networking}
}
@article{10.1145/3130906,
	title        = {Rapid: A Multimodal and Device-Free Approach Using Noise Estimation for Robust Person Identification},
	author       = {Chen, Yuanying and Dong, Wei and Gao, Yi and Liu, Xue and Gu, Tao},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130906},
	url          = {https://doi.org/10.1145/3130906},
	issue_date   = {September 2017},
	abstract     = {Device-free human sensing is a key technology to support many applications such as indoor navigation and activity recognition. By exploiting WiFi signals reflected by human body, there have been many WiFi-based device-free human sensing applications. Among these applications, person identification is a fundamental technology to enable user-specific services. In this paper, we present Rapid, a system that can perform robust person identification in a device-free and low-cost manner, using fine-grained channel information (i.e., CSI) of WiFi and acoustic information from footstep sound. In order to achieve high accuracy in real-life scenarios with both system and environment noise, we perform noise estimation and include two different confidence values to quantify the impact of noise to both CSI and acoustic measurements. Based on an accurate gait analysis, we then adaptively fuse CSI and acoustic measurements to achieve robust person identification. We implement low-cost Rapid nodes and evaluate our system using experiments at multiple locations with a total of 1800 gait instances from 20 volunteers, and the results show that Rapid identifies a subject with an average accuracy of 92% to 82% from a group of 2 to 6 subjects, respectively.},
	articleno    = 41,
	numpages     = 27,
	keywords     = {audio sensing, Multimodal person identification, noise estimation, Channel State Information (CSI)}
}
@article{10.1145/3090095,
	title        = {SoundTrak: Continuous 3D Tracking of a Finger Using Active Acoustics},
	author       = {Zhang, Cheng and Xue, Qiuyue and Waghmare, Anandghan and Jain, Sumeet and Pu, Yiming and Hersek, Sinan and Lyons, Kent and Cunefare, Kenneth A. and Inan, Omer T. and Abowd, Gregory D.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090095},
	url          = {https://doi.org/10.1145/3090095},
	issue_date   = {June 2017},
	abstract     = {The small size of wearable devices limits the efficiency and scope of possible user interactions, as inputs are typically constrained to two dimensions: the touchscreen surface. We present SoundTrak, an active acoustic sensing technique that enables a user to interact with wearable devices in the surrounding 3D space by continuously tracking the finger position with high resolution. The user wears a ring with an embedded miniature speaker sending an acoustic signal at a specific frequency (e.g., 11 kHz), which is captured by an array of miniature, inexpensive microphones on the target wearable device. A novel algorithm is designed to localize the finger’s position in 3D space by extracting phase information from the received acoustic signals. We evaluated SoundTrak in a volume of space (20cm \texttimes{} 16cm \texttimes{} 11cm) around a smartwatch, and show an average accuracy of 1.3 cm. We report on results from a Fitts’ Law experiment with 10 participants as the evaluation of the real-time prototype. We also present a set of applications which are supported by this 3D input technique, and show the practical challenges that need to be addressed before widespread use.},
	articleno    = 30,
	numpages     = 25,
	keywords     = {Finger Tracking, 3D input, Wearable, Acoustic}
}
@article{10.1145/3090082,
	title        = {UbiEar: Bringing Location-Independent Sound Awareness to the Hard-of-Hearing People with Smartphones},
	author       = {Sicong, Liu and Zimu, Zhou and Junzhao, Du and Longfei, Shangguan and Han, Jun and Wang, Xin},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090082},
	url          = {https://doi.org/10.1145/3090082},
	issue_date   = {June 2017},
	abstract     = {Non-speech sound-awareness is important to improve the quality of life for the deaf and hard-of-hearing (DHH) people. DHH people, especially the young, are not always satisfied with their hearing aids. According to the interviews with 60 young hard-of-hearing students, a ubiquitous sound-awareness tool for emergency and social events that works in diverse environments is desired. In this paper, we design UbiEar, a smartphone-based acoustic event sensing and notification system. Core techniques in UbiEar are a light-weight deep convolution neural network to enable location-independent acoustic event recognition on commodity smartphons, and a set of mechanisms for prompt and energy-efficient acoustic sensing. We conducted both controlled experiments and user studies with 86 DHH students and showed that UbiEar can assist the young DHH students in awareness of important acoustic events in their daily life.},
	articleno    = 17,
	numpages     = 21
}
@article{10.1145/3090055,
	title        = {PocketThumb: A Wearable Dual-Sided Touch Interface for Cursor-Based Control of Smart-Eyewear},
	author       = {Dobbelstein, David and Winkler, Christian and Haas, Gabriel and Rukzio, Enrico},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090055},
	url          = {https://doi.org/10.1145/3090055},
	issue_date   = {June 2017},
	abstract     = {We present PocketThumb, a wearable touch interface for smart-eyewear that is embedded into the fabrics of the front trouser pocket. The interface is reachable from outside and inside of the pocket to allow for a combined dual-sided touch input. The user can control an absolute cursor with their thumb sliding along the fabric from the inside, while at the same time tapping or swiping with fingers from the outside to perform joint gestures. This allows for resting the hand in a comfortable and quickly accessible position, while performing interaction with a high expressiveness that is feasible in mobile scenarios. In a cursor-based target selection study, we found that our introduced dual-sided touch interaction is significantly faster in comparison to common single-sided absolute as well as relative touch interaction (~19%, resp. ~23% faster). The effect is largest in the mobile conditions standing and walking (up to ~31% faster).},
	articleno    = 9,
	numpages     = 17,
	keywords     = {dual-sided touch, smart-eyewear, Wearable input}
}
@article{10.1145/3090051,
	title        = {Detecting Drinking Episodes in Young Adults Using Smartphone-Based Sensors},
	author       = {Bae, Sangwon and Ferreira, Denzil and Suffoletto, Brian and Puyana, Juan C. and Kurtz, Ryan and Chung, Tammy and Dey, Anind K.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090051},
	url          = {https://doi.org/10.1145/3090051},
	issue_date   = {June 2017},
	abstract     = {Alcohol use in young adults is common, with high rates of morbidity and mortality largely due to periodic, heavy drinking episodes (HDEs). Behavioral interventions delivered through electronic communication modalities (e.g., text messaging) can reduce the frequency of HDEs in young adults, but effects are small. One way to amplify these effects is to deliver support materials proximal to drinking occasions, but this requires knowledge of when they will occur. Mobile phones have built-in sensors that can potentially be useful in monitoring behavioral patterns associated with the initiation of drinking occasions. The objective of our work is to explore the detection of daily-life behavioral markers using mobile phone sensors and their utility in identifying drinking occasions. We utilized data from 30 young adults aged 21-28 with past hazardous drinking and collected mobile phone sensor data and daily Experience Sampling Method (ESM) of drinking for 28 consecutive days. We built a machine learning-based model that is 96.6% accurate at identifying non-drinking, drinking and heavy drinking episodes. We highlight the most important features for detecting drinking episodes and identify the amount of historical data needed for accurate detection. Our results suggest that mobile phone sensors can be used for automated, continuous monitoring of at-risk populations to detect drinking episodes and support the delivery of timely interventions.},
	articleno    = 5,
	numpages     = 36,
	keywords     = {Young adults, Alcohol consumption, Behavioral model, Smartphone sensors, Machine learning}
}
@article{10.1145/3090050,
	title        = {Using Thermal Stimuli to Enhance Photo-Sharing in Social Media},
	author       = {Akazue, Moses and Halvey, Martin and Baillie, Lynne},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090050},
	url          = {https://doi.org/10.1145/3090050},
	issue_date   = {June 2017},
	abstract     = {Limited work has been undertaken to show how the emotive ability of thermal stimuli can be used for interaction purposes. One potential application area is using thermal stimuli to influence emotions in images shared online such as social media platforms. This paper presents a two-part study, which examines how the documented emotive property of thermal stimuli can be applied to enhance social media images. Participants in part-one supplied images from their personal collection or social media profiles, and were asked to augment each image with thermal stimuli based on the emotions they wanted to enhance or reduce. Part-one participants were interviewed to understand the effects they wanted augmented images to have. In part-two, these augmented images were perceived by a different set of participants in a simulated social media interface. Results showed strong agreement between the emotions augmented images were designed to evoke and the emotions they actually evoked as perceived by part-two participants. Participants in part-one selected thermal stimuli augmentation intended to modulate valence and arousal in images as a way of enhancing the realism of the images augmented. Part-two results indicate this was achieved as participants perceived thermal stimuli augmentation reduced valence in negative images and modulated valence and arousal in positive images.},
	articleno    = 4,
	numpages     = 21,
	keywords     = {experimentation, human factors, visual, valence, stimulation, emotion, dominance, Design, thermal feedback, arousal, thermal stimuli}
}
@article{10.1145/3543194,
	title        = {First-Gen Lens: Assessing Mental Health of First-Generation Students across Their First Year at College Using Mobile Sensing},
	author       = {Wang, Weichen and Nepal, Subigya and Huckins, Jeremy F. and Hernandez, Lessley and Vojdanovski, Vlado and Mack, Dante and Plomp, Jane and Pillai, Arvind and Obuchi, Mikio and daSilva, Alex and Murphy, Eilis and Hedlund, Elin and Rogers, Courtney and Meyer, Meghan and Campbell, Andrew},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3543194},
	url          = {https://doi.org/10.1145/3543194},
	issue_date   = {July 2022},
	abstract     = {The transition from high school to college is a taxing time for young adults. New students arriving on campus navigate a myriad of challenges centered around adapting to new living situations, financial needs, academic pressures and social demands. First-year students need to gain new skills and strategies to cope with these new demands in order to make good decisions, ease their transition to independent living and ultimately succeed. In general, first-generation students are less prepared when they enter college in comparison to non-first-generation students. This presents additional challenges for first-generation students to overcome and be successful during their college years. We study first-year students through the lens of mobile phone sensing across their first year at college, including all academic terms and breaks. We collect longitudinal mobile sensing data for N=180 first-year college students, where 27 of the students are first-generation, representing 15% of the study cohort and representative of the number of first-generation students admitted each year at the study institution, Dartmouth College. We discuss risk factors, behavioral patterns and mental health of first-generation and non-first-generation students. We propose a deep learning model that accurately predicts the mental health of first-generation students by taking into account important distinguishing behavioral factors of first-generation students. Our study, which uses the StudentLife app, offers data-informed insights that could be used to identify struggling students and provide new forms of phone-based interventions with the goal of keeping students on track.},
	articleno    = 95,
	numpages     = 32,
	keywords     = {mobile sensing, mental health, first-generation students, first year}
}
@article{10.1145/3534624,
	title        = {One-Handed Input for Mobile Devices via Motion Matching and Orbits Controls},
	author       = {Esteves, Augusto and Bouquet, Elizabeth and Pfeuffer, Ken and Alt, Florian},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534624},
	url          = {https://doi.org/10.1145/3534624},
	issue_date   = {July 2022},
	abstract     = {We introduce a novel one-handed input technique for mobile devices that is not based on pointing, but on motion matching -where users select a target by mimicking its unique animation. Our work is motivated by the findings of a survey (N=201) on current mobile use, from which we identify lingering opportunities for one-handed input techniques. We follow by expanding on current motion matching implementations - previously developed in the context of gaze or mid-air input - so these take advantage of the affordances of touch-input devices. We validate the technique by characterizing user performance via a standard selection task (N=24) where we report success rates (&gt;95%), selection times (~1.6 s), input footprint, grip stability, usability, and subjective workload - in both phone and tablet conditions. Finally, we present a design space that illustrates six ways in which motion matching can be embedded into mobile interfaces via a camera prototype application.},
	articleno    = 51,
	numpages     = 24,
	keywords     = {reachability, mobile use survey, motion matching, one-handed input, orbits}
}
@article{10.1145/3534615,
	title        = {Total VREcall: Using Biosignals to Recognize Emotional Autobiographical Memory in Virtual Reality},
	author       = {Gupta, Kunal and Chan, Sam W. T. and Pai, Yun Suen and Strachan, Nicholas and Su, John and Sumich, Alexander and Nanayakkara, Suranga and Billinghurst, Mark},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534615},
	url          = {https://doi.org/10.1145/3534615},
	issue_date   = {July 2022},
	abstract     = {Our memories and past experiences contribute to guiding our perception and action of future affective experiences. Virtual Reality (VR) experiences are more vividly memorized and recalled than non-VR ones, but there is little research on how to detect this recall in VR. We investigate the feasibility of recognizing autobiographical memory (AM) recall in VR using physiological cues: skin conductance, heart-rate variability, eye gaze, and pupillary response. We devised a methodology replicating an existing AM Test in VR. We conducted a user study with 20 participants recalling AM using three valence categories cue words: positive, negative, and neutral. We found a significant effect of AM recalls on EDA peak, and eye blink rate, with a generalized recognition accuracy of 77.1% and person dependent accuracy of up to 95.1%. This shows a promising approach for detecting AM recall in VR and we discuss the implications for VR experience design.},
	articleno    = 55,
	numpages     = 21,
	keywords     = {Virtual Reality, Physiological Signals, Electrodermal Activity, Eye-tracking, Memory, Biosignals, Autobiographical Memory, Emotion}
}
@article{10.1145/3534609,
	title        = {Effects of Scene Detection, Scene Prediction, and Maneuver Planning Visualizations on Trust, Situation Awareness, and Cognitive Load in Highly Automated Vehicles},
	author       = {Colley, Mark and R\"{a}dler, Max and Glimmann, Jonas and Rukzio, Enrico},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534609},
	url          = {https://doi.org/10.1145/3534609},
	issue_date   = {July 2022},
	abstract     = {The successful introduction of automated vehicles (AVs) depends on the user's acceptance. To gain acceptance, the intended user must trust the technology, which itself relies on an appropriate understanding. Visualizing internal processes could aid in this. For example, the functional hierarchy of autonomous vehicles distinguishes between perception, prediction, and maneuver planning. In each of these stages, visualizations including possible uncertainties (or errors) are possible. Therefore, we report the results of an online study (N=216) comparing visualizations and their combinations on these three levels using a pre-recorded real-world video with visualizations shown on a simulated augmented reality windshield. Effects on trust, cognitive load, situation awareness, and perceived safety were measured. Situation Prediction-related visualizations were perceived as worse than the remaining levels. Based on a negative evaluation of the visualization, the abilities of the AV were also judged worse. In general, the results indicate the presence of overtrust in AVs.},
	articleno    = 49,
	numpages     = 21,
	keywords     = {intention prediction, self-driving vehicles, machine learning, Autonomous vehicles, semantic segmentation}
}
@article{10.1145/3534608,
	title        = {Sensor-Free Soil Moisture Sensing Using LoRa Signals},
	author       = {Chang, Zhaoxin and Zhang, Fusang and Xiong, Jie and Ma, Junqi and Jin, Beihong and Zhang, Daqing},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534608},
	url          = {https://doi.org/10.1145/3534608},
	issue_date   = {July 2022},
	abstract     = {Soil moisture sensing is one of the most important components in smart agriculture. It plays a critical role in increasing crop yields and reducing water waste. However, existing commercial soil moisture sensors are either expensive or inaccurate, limiting their real-world deployment. In this paper, we utilize wide-area LoRa signals to sense soil moisture without a need of dedicated soil moisture sensors. Different from traditional usage of LoRa in smart agriculture which is only for sensor data transmission, we leverage LoRa signal itself as a powerful sensing tool. The key insight is that the dielectric permittivity of soil which is closely related to soil moisture can be obtained from phase readings of LoRa signals. Therefore, antennas of a LoRa node can be placed in the soil to capture signal phase readings for soil moisture measurements. Though promising, it is non-trivial to extract accurate phase information due to unsynchronization of LoRa transmitter and receiver. In this work, we propose to include a low-cost switch to equip the LoRa node with two antennas to address the issue. We develop a delicate chirp ratio approach to cancel out the phase offset caused by transceiver unsynchronization to extract accurate phase information. The proposed system design has multiple unique advantages including high accuracy, robustness against motion interference and large sensing range for large-scale deployment in smart agriculture. Experiments with commodity LoRa nodes show that our system can accurately estimate soil moisture at an average error of 3.1%, achieving a performance comparable to high-end commodity soil moisture sensors. Field studies show that the proposed system can accurately sense soil moisture even when the LoRa gateway is 100 m away from the LoRa node, enabling wide-area soil moisture sensing for the first time.},
	articleno    = 45,
	numpages     = 27,
	keywords     = {Soil moisture sensing, LoRa, Long range sensing}
}
@article{10.1145/3534605,
	title        = {GoPose: 3D Human Pose Estimation Using WiFi},
	author       = {Ren, Yili and Wang, Zi and Wang, Yichao and Tan, Sheng and Chen, Yingying and Yang, Jie},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534605},
	url          = {https://doi.org/10.1145/3534605},
	issue_date   = {July 2022},
	abstract     = {This paper presents GoPose, a 3D skeleton-based human pose estimation system that uses WiFi devices at home. Our system leverages the WiFi signals reflected off the human body for 3D pose estimation. In contrast to prior systems that need specialized hardware or dedicated sensors, our system does not require a user to wear or carry any sensors and can reuse the WiFi devices that already exist in a home environment for mass adoption. To realize such a system, we leverage the 2D AoA spectrum of the signals reflected from the human body and the deep learning techniques. In particular, the 2D AoA spectrum is proposed to locate different parts of the human body as well as to enable environment-independent pose estimation. Deep learning is incorporated to model the complex relationship between the 2D AoA spectrums and the 3D skeletons of the human body for pose tracking. Our evaluation results show GoPose achieves around 4.7cm of accuracy under various scenarios including tracking unseen activities and under NLoS scenarios.},
	articleno    = 69,
	numpages     = 25,
	keywords     = {WiFi Sensing, Channel State Information (CSI), Human Pose Estimation, Deep Learning}
}
@article{10.1145/3534601,
	title        = {IndexPen: Two-Finger Text Input with Millimeter-Wave Radar},
	author       = {Wei, Haowen and Li, Ziheng and Galvan, Alexander D. and Su, Zhuoran and Zhang, Xiao and Pahlavan, Kaveh and Solovey, Erin T.},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534601},
	url          = {https://doi.org/10.1145/3534601},
	issue_date   = {July 2022},
	abstract     = {In this paper, we introduce IndexPen, a novel interaction technique for text input through two-finger in-air micro-gestures, enabling touch-free, effortless, tracking-based interaction, designed to mirror real-world writing. Our system is based on millimeter-wave radar sensing, and does not require instrumentation on the user. IndexPen can successfully identify 30 distinct gestures, representing the letters A-Z, as well as Space, Backspace, Enter, and a special Activation gesture to prevent unintentional input. Additionally, we include a noise class to differentiate gesture and non-gesture noise. We present our system design, including the radio frequency (RF) processing pipeline, classification model, and real-time detection algorithms. We further demonstrate our proof-of-concept system with data collected over ten days with five participants yielding 95.89% cross-validation accuracy on 31 classes (including noise). Moreover, we explore the learnability and adaptability of our system for real-world text input with 16 participants who are first-time users to IndexPen over five sessions. After each session, the pre-trained model from the previous five-user study is calibrated on the data collected so far for a new user through transfer learning. The F-1 score showed an average increase of 9.14% per session with the calibration, reaching an average of 88.3% on the last session across the 16 users. Meanwhile, we show that the users can type sentences with IndexPen at 86.2% accuracy, measured by string similarity. This work builds a foundation and vision for future interaction interfaces that could be enabled with this paradigm.},
	articleno    = 79,
	numpages     = 39,
	keywords     = {Deep Learning, Millimeter wave FMCW radar, Cursor interaction, Micro-gesture sensing, Text input, In-air gestures}
}
@article{10.1145/3534597,
	title        = {PPGface: Like What You Are Watching? Earphones Can "Feel" Your Facial Expressions},
	author       = {Choi, Seokmin and Gao, Yang and Jin, Yincheng and Kim, Se jun and Li, Jiyang and Xu, Wenyao and Jin, Zhanpeng},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534597},
	url          = {https://doi.org/10.1145/3534597},
	issue_date   = {July 2022},
	abstract     = {Recognition of facial expressions has been widely explored to represent people's emotional states. Existing facial expression recognition systems primarily rely on external cameras which make it less accessible and efficient in many real-life scenarios to monitor an individual's facial expression in a convenient and unobtrusive manner. To this end, we propose PPGface, a ubiquitous, easy-to-use, user-friendly facial expression recognition platform that leverages earable devices with built-in PPG sensor. PPGface understands the facial expressions through the dynamic PPG patterns resulting from facial muscle movements. With the aid of the accelerometer sensor, PPGface can detect and recognize the user's seven universal facial expressions and relevant body posture unobtrusively. We conducted an user study (N=20) using multimodal ResNet to evaluate the performance of PPGface, and showed that PPGface can detect different facial expressions with 93.5 accuracy and 0.93 fl-score. In addition, to explore the robustness and usability of our proposed platform, we conducted several comprehensive experiments under real-world settings. Overall results of this work validate a great potential to be employed in future commodity earable devices.},
	articleno    = 48,
	numpages     = 32,
	keywords     = {Facial Expression, Photoplethysmogram, Ear Canal, Blood Vessel Deformation, PPG}
}
@article{10.1145/3534595,
	title        = {BreathMentor: Acoustic-Based Diaphragmatic Breathing Monitor System},
	author       = {Gong, Yanbin and Zhang, Qian and NG, Bobby H.P. and Li, Wei},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534595},
	url          = {https://doi.org/10.1145/3534595},
	issue_date   = {July 2022},
	abstract     = {Chronic Obstructive Pulmonary Disease (COPD) is currently the third major cause of death--more than three million people died from it in 2019. Given that COPD cannot be cured currently, immediate treatment is crucial. Pulmonary rehabilitation (PR) is widely used to prevent COPD deterioration. Patients are advised to undergo a PR at home to get sufficient treatment in time. Monitoring patients during home rehabilitation can help not only improve patient adherence but also collect data on patients' recovery progress from rehabilitation team's perspective. However, how to track if proper diaphragmatic breathing, an essential part of PR, is taken by a patient has remained challenging. The current monitoring solution still appears obtrusive as it requires the patient to wear two uncomfortable respiration belts. Alternatively, therapists need to monitor the patients remotely through several cameras, which consumes substantial medical resources and causes privacy issues.In this work, we present BreathMentor, a smart speaker based diaphragmatic breathing monitoring system targeting early COPD stages I and II. BreathMentor is both unobtrusive and preventive of privacy invasion, so that it can solve the existing pain points and suits home care. BreathMentor converts the smart speaker into an active sonar system that continuously perceives and analyses the changes in surroundings, thereby detecting the user's respiration rate, deriving the breathing phases, and classifying whether the patient is practising diaphragmatic breathing. BreathMentor formulates breathing monitoring as a Temporal Action Localization task that enables us to detect each breathing cycle and classify its type. Our key insight is that breathing periodicity and phase duration are natural properties to localize and segment the breaths. Our key design to classify the breathing type is a hybrid architecture encompassing signal processing and deep learning techniques. Further, we evaluate the system performance on fifteen healthy subjects who would not breathe abnormally during diaphragmatic breathing under the supervision of therapists. In conclusion, BreathMentor can achieve robust performance for monitoring diaphragmatic breathing in different environments, as demonstrated in the results. The median error rate of respiration detection is 0.2 BPM, and the I/E ratio derivation is accurate with a mean absolute percentage error of less than 5.9% for breathing phase detection, together with a recall of 98.2%, and a precision of 95.5% in detecting diaphragmatic breathing. Above results indicate that BreathMentor can be used to track the patients' adherence and help monitor their breathing capacity.},
	articleno    = 53,
	numpages     = 28,
	keywords     = {smart speaker, breathing training, respiration diseases, acoustic sensing, breathing classification}
}
@article{10.1145/3534589,
	title        = {Semantic-Discriminative Mixup for Generalizable Sensor-Based Cross-Domain Activity Recognition},
	author       = {Lu, Wang and Wang, Jindong and Chen, Yiqiang and Pan, Sinno Jialin and Hu, Chunyu and Qin, Xin},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534589},
	url          = {https://doi.org/10.1145/3534589},
	issue_date   = {July 2022},
	abstract     = {It is expensive and time-consuming to collect sufficient labeled data to build human activity recognition (HAR) models. Training on existing data often makes the model biased towards the distribution of the training data, thus the model might perform terribly on test data with different distributions. Although existing efforts on transfer learning and domain adaptation try to solve the above problem, they still need access to unlabeled data on the target domain, which may not be possible in real scenarios. Few works pay attention to training a model that can generalize well to unseen target domains for HAR. In this paper, we propose a novel method called Semantic-Discriminative Mixup (SDMix) for generalizable cross-domain HAR. Firstly, we introduce semantic-aware Mixup that considers the activity semantic ranges to overcome the semantic inconsistency brought by domain differences. Secondly, we introduce the large margin loss to enhance the discrimination of Mixup to prevent misclassification brought by noisy virtual labels. Comprehensive generalization experiments on five public datasets demonstrate that our SDMix substantially outperforms the state-of-the-art approaches with 6% average accuracy improvement on cross-person, cross-dataset, and cross-position HAR.},
	articleno    = 65,
	numpages     = 19,
	keywords     = {Transfer Learning, Domain Generalization, Human Activity Recognition}
}
@article{10.1145/3534588,
	title        = {RFCam: Uncertainty-Aware Fusion of Camera and Wi-Fi for Real-Time Human Identification with Mobile Devices},
	author       = {Chen, Hongkai and Munir, Sirajum and Lin, Shan},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534588},
	url          = {https://doi.org/10.1145/3534588},
	issue_date   = {July 2022},
	abstract     = {As cameras and Wi-Fi access points are widely deployed in public places, new mobile applications and services can be developed by connecting live video analytics to the mobile Wi-Fi-enabled devices of the relevant users. To achieve this, a critical challenge is to identify the person who carries a device in the video with the mobile device's network ID, e.g., MAC address. To address this issue, we propose RFCam, a system for human identification with a fusion of Wi-Fi and camera data. RFCam uses a multi-antenna Wi-Fi radio to collect CSI of Wi-Fi packets sent by mobile devices, and a camera to monitor users in the area. With low sampling rate CSI data, RFCam derives heterogeneous embedding features on location, motion, and user activity for each device over time, and fuses them with visual user features generated from video analytics to find the best matches. To mitigate the impacts of multi-user environments on wireless sensing, we develop video-assisted learning models for different features and quantify their uncertainties, and incorporate them with video analytics to rank moments and features for robust and efficient fusion. RFCam is implemented and tested in indoor environments for over 800 minutes with 25 volunteers, and extensive evaluation results demonstrate that RFCam achieves real-time identification average accuracy of 97.01% in all experiments with up to ten users, significantly outperforming existing solutions.},
	articleno    = 47,
	numpages     = 29,
	keywords     = {human identification, sensor fusion, video analytics, activity recognition, localization, embedding, CSI, Wi-Fi}
}
@article{10.1145/3534585,
	title        = {Are You Left Out? An Efficient and Fair Federated Learning for Personalized Profiles on Wearable Devices of Inferior Networking Conditions},
	author       = {Zhou, Pengyuan and Xu, Hengwei and Lee, Lik Hang and Fang, Pei and Hui, Pan},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534585},
	url          = {https://doi.org/10.1145/3534585},
	issue_date   = {July 2022},
	abstract     = {Wearable computers engage in percutaneous interactions with human users and revolutionize the way of learning human activities. Due to rising privacy concerns, federated learning has been recently proposed to train wearable data with privacy preservation collaboratively. However, under the state-of-the-art (SOTA) schemes, user profiles on wearable devices of inferior networking conditions are regarded as 'left out'. Such schemes suffer from three fundamental limitations: (1) the widely adopted network-capacity-based client selection leads to biased training; (2) the aggregation has low communication efficiency; (3) users lack convenient channels for providing feedback on wearable devices.Therefore, this paper proposes a Fair and Communication-efficient Federated Learning scheme, namely FCFL. FCFL is a full-stack learning system specifically designed for wearable computers, improving the SOTA performance in terms of communication efficiency, fairness, personalization, and user experience. To this end, we design a technique named ThrowRightAway (TRA) to loose the network capacity constraints. Clients with poor networks are allowed to be selected as participators to improve the representation and guarantee the model's fairness. Remarkably, we propose Movement Aware Federated Learning (MAFL) to aggregate only the model updates with top contributions to the global model for the sake of communication efficiency. Accordingly, we implemented an FCFL-supported prototype as a sports application on smartwatches. Our comprehensive evaluation demonstrated that FCFL is a communication efficient scheme significantly reducing uploaded data by up to 29.77%, with a prominent feature of guaranteeing enhanced fairness up to 65.07%. Also, FCFL achieves robust personalization performance (i.e., 20% improvements of global model accuracy) in the face of packet loss below a certain fraction (10%-30%). A follow-up user survey shows that our FCFL-supported prototypical system on wearable devices significantly reduces users' workload.},
	articleno    = 91,
	numpages     = 25,
	keywords     = {Federated learning, Personalization, Wearable computers, Fairness, Loss tolerance}
}
@article{10.1145/3534573,
	title        = {The City as a Personal Assistant: Turning Urban Landmarks into Conversational Agents for Serving Hyper Local Information},
	author       = {Acer, Utku G\"{u}nay and Broeck, Marc van den and Min, Chulhong and Dasari, Mallesham and Kawsar, Fahim},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534573},
	url          = {https://doi.org/10.1145/3534573},
	issue_date   = {July 2022},
	abstract     = {Conversational agents are increasingly becoming digital partners in our everyday computational experiences. Although rich and fresh in content, they are oblivious to users' locality beyond geospatial weather and traffic conditions. We introduce Lingo, a hyper-local conversational agent embedded deeply into the urban infrastructure that provides rich, purposeful, detailed, and in some cases, playful information relevant to a neighbourhood. Drawing lessons from a mixed-method contextual study (online survey, n = 1992 and semi-structured interviews, n = 21), we identify requirements for such a hyper-local conversational agent and a sample set of questions serving urban neighbourhoods of Belgium. Our agent design is manifested into a two-part system. First, a multi-modal reasoning engine serves as a hyper-local information source using automated machine-learning models operating on camera, microphone, and environmental sensor data. Second, a smart conversational speaker and a smartphone application serve as hyper-local information access points. Finally, we introduce a covert communication mechanism over Wi-Fi management frames that bridges the two parts of our Lingo system and enables the privacy-preserving proxemic interactions. We describe the design, implementation, and technical assessment of Lingo together with usability (n = 20) and real-world deployment (n = 5) studies. We reflect on information quality, accessibility benefits, and interaction dynamics and demonstrate the efficacy of Lingo in offering hyper-local information at the finest granularity in urban neighbourhoods while reducing access time up to a factor of 25.},
	articleno    = 40,
	numpages     = 31,
	keywords     = {Citizen Engagement, Edge AI, Spontaneous Interaction, Conversational Agent}
}
@article{10.1145/3517332,
	title        = {Integrating Handcrafted Features with Deep Representations for Smartphone Authentication},
	author       = {Song, Yunpeng and Cai, Zhongmin},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517332},
	url          = {https://doi.org/10.1145/3517332},
	issue_date   = {March 2022},
	abstract     = {Recent research demonstrates the potential of touch dynamics as a usable and privacy-preserving scheme for smartphone authentication. Most existing approaches rely on handcrafted features since deep models may be vulnerable to behavioral uncertainty due to the lack of consistent semantic information. Toward this end, we propose an approach to integrating handcrafted features into two phases of the deep learning process. On one hand, we present three fine-grained behavior representations by encoding semantic handcrafted features into the raw touch actions. On the other hand, we devise a deep Feature Regularization Net (FRN) architecture to combine the complementary information in both handcrafted and deep features. FRN involves handcrafted features as regularization to guide the learning process of deep features and selectively fuses these two feature types through a feature re-weighting mechanism. Experimental findings demonstrate that FRN outperforms the existing handcrafted or deep features even with smaller training and template sets. The framework also works for SOTA deep models and further boosts the accuracy. Results show that our approach is more reliable to alleviate behavioral variability and is competitively robust to statistical attacks compared with the most effective handcrafted features, suggesting a promising mechanism to improve the effectiveness and usability of behavioral authentication for multi-touch enabled mobile devices.},
	articleno    = 27,
	numpages     = 27,
	keywords     = {Behavioral authentication, Mobile devices, Biometrics, Feature fusion}
}
@article{10.1145/3517257,
	title        = {WearSign: Pushing the Limit of Sign Language Translation Using Inertial and EMG Wearables},
	author       = {Zhang, Qian and Jing, JiaZhen and Wang, Dong and Zhao, Run},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517257},
	url          = {https://doi.org/10.1145/3517257},
	issue_date   = {March 2022},
	abstract     = {Sign language translation (SLT) is considered as the core technology to break the communication barrier between the deaf and hearing people. However, most studies only focus on recognizing the sequence of sign gestures (sign language recognition (SLR)), ignoring the significant difference of linguistic structures between sign language and spoken language. In this paper, we approach SLT as a spatio-temporal machine translation task and propose a wearable-based system, WearSign, to enable direct translation from the sign-induced sensory signals into spoken texts. WearSign leverages a smartwatch and an armband of ElectroMyoGraphy (EMG) sensors to capture the sophisticated sign gestures. In the design of the translation network, considering the significant modality and linguistic gap between sensory signals and spoken language, we design a multi-task encoder-decoder framework which uses sign glosses (sign gesture labels) for intermediate supervision to guide the end-to-end training. In addition, due to the lack of sufficient training data, the performance of prior studies usually degrades drastically when it comes to sentences with complex structures or unseen in the training set. To tackle this, we borrow the idea of back-translation and leverage the much more available spoken language data to synthesize the paired sign language data. We include the synthetic pairs into the training process, which enables the network to learn better sequence-to-sequence mapping as well as generate more fluent spoken language sentences.We construct an American sign language (ASL) dataset consisting of 250 commonly used sentences gathered from 15 volunteers. WearSign achieves 4.7% and 8.6% word error rate (WER) in user-independent tests and unseen sentence tests respectively. We also implement a real-time version of WearSign which runs fully on the smartphone with a low latency and energy overhead.},
	articleno    = 35,
	numpages     = 27,
	keywords     = {wearables, sign language translation, multi-task encoder-decoder, data synthesis}
}
@article{10.1145/3517250,
	title        = {Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices},
	author       = {Zhao, Yingying and Chang, Yuhu and Lu, Yutian and Wang, Yujiang and Dong, Mingzhi and Lv, Qin and Dick, Robert P. and Yang, Fan and Lu, Tun and Gu, Ning and Shang, Li},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517250},
	url          = {https://doi.org/10.1145/3517250},
	issue_date   = {March 2022},
	abstract     = {Emotion recognition in smart eyewear devices is valuable but challenging. One key limitation of previous works is that the expression-related information like facial or eye images is considered as the only evidence of emotion. However, emotional status is not isolated; it is tightly associated with people's visual perceptions, especially those with emotional implications. However, little work has examined such associations to better illustrate the causes of emotions. In this paper, we study the emotionship analysis problem in eyewear systems, an ambitious task that requires classifying the user's emotions and semantically understanding their potential causes. To this end, we describe EMOShip, a deep-learning-based eyewear system that can automatically detect the wearer's emotional status and simultaneously analyze its associations with semantic-level visual perception. Experimental studies with 20 participants demonstrate that, thanks to its awareness of emotionship, EMOShip achieves superior emotion recognition accuracy compared to existing methods (80.2% vs. 69.4%) and provides a valuable understanding of the causes of emotions. Further pilot studies with 20 additional participants further motivate the potential use of EMOShip to empower emotion-aware applications, such as emotionship self-reflection and emotionship life-logging.},
	articleno    = 38,
	numpages     = 29,
	keywords     = {Emotionship, Visual Question Answering, Sentiment Analysis, Emotion Recognition, Smart Eyewear System, Image Captioning}
}
@article{10.1145/3517237,
	title        = {Placement Matters: Understanding the Effects of Device Placement for WiFi Sensing},
	author       = {Wang, Xuanzhi and Niu, Kai and Xiong, Jie and Qian, Bochong and Yao, Zhiyun and Lou, Tairong and Zhang, Daqing},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517237},
	url          = {https://doi.org/10.1145/3517237},
	issue_date   = {March 2022},
	abstract     = {WiFi-based contactless sensing has found numerous applications in the fields of smart home and health care owning to its low-cost, non-intrusive and privacy-preserving characteristics. While promising in many aspects, the limited sensing range and interference issues still exist, hindering the adoption of WiFi sensing in real world. In this paper, inspired by the SNR (signal-to-noise ratio) metric in communication theory, we propose a new metric named SSNR (sensing-signal-to-noise-ratio) to quantify the sensing capability of WiFi systems. We theoretically model the effect of transmitter-receiver distance on sensing coverage. We show that in LoS scenario, the sensing coverage area increases first from a small oval to a maximal one and then decreases. When the transmitter-receiver distance further increases, the coverage area is separated into two ovals located around the two transceivers respectively. We demonstrate that, instead of applying complex signal processing scheme or advanced hardware, by just properly placing the transmitter and receiver, the two well-known issues in WiFi sensing (i.e., small range and severe interference) can be greatly mitigated. Specifically, by properly placing the transmitter and receiver, the coverage of human walking sensing can be expanded by around 200%. By increasing the transmitter-receiver distance, a target's fine-grained respiration can still be accurately sensed with one interferer sitting just 0.5 m away.},
	articleno    = 32,
	numpages     = 25,
	keywords     = {Capability, SSNR, Wireless Sensing, Coverage Model, WiFi Sensing}
}
@article{10.1145/3517235,
	title        = {ARticulate: One-Shot Interactions with Intelligent Assistants in Unfamiliar Smart Spaces Using Augmented Reality},
	author       = {Clark, Meghan and Newman, Mark W. and Dutta, Prabal},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517235},
	url          = {https://doi.org/10.1145/3517235},
	issue_date   = {March 2022},
	abstract     = {Smart space technologies have entered the mainstream home market. Most users currently interact with smart homes that they (or an acquaintance) have set up and know well. However, as these technologies spread to commercial or public environments, users will need to frequently interact with unfamiliar smart spaces where they are unaware of the available capabilities and the system maintainer will not be present to help. Users will need to quickly and independently 1) discover what is and is not possible, and 2) make use of available functionality. Widespread adoption of smart space systems will not be possible until this discoverability issue is solved. We design and evaluate ARticulate, an interface that allows users to have successful smart space interactions with an intelligent assistant while learning transferable information about the overall set of devices in an unfamiliar space. Our method of using Snapchat-like contextual photo messages enhanced by two technologies---augmented reality and autocomplete---allows users to determine available functionality and achieve their goals in one attempt with a smart space they have never seen before, something no existing interface supports. The ability to easily operate unfamiliar smart spaces improves the usability of existing systems and removes a significant obstacle to the vision of ubiquitous computing.},
	articleno    = 7,
	numpages     = 24,
	keywords     = {smart homes, augmented reality, smart spaces, intelligent assistants}
}
@article{10.1145/3517232,
	title        = {SkillFence: A Systems Approach to Practically Mitigating Voice-Based Confusion Attacks},
	author       = {Hooda, Ashish and Wallace, Matthew and Jhunjhunwalla, Kushal and Fernandes, Earlence and Fawaz, Kassem},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517232},
	url          = {https://doi.org/10.1145/3517232},
	issue_date   = {March 2022},
	abstract     = {Voice assistants are deployed widely and provide useful functionality. However, recent work has shown that commercial systems like Amazon Alexa and Google Home are vulnerable to voice-based confusion attacks that exploit design issues. We propose a systems-oriented defense against this class of attacks and demonstrate its functionality for Amazon Alexa. We ensure that only the skills a user intends execute in response to voice commands. Our key insight is that we can interpret a user's intentions by analyzing their activity on counterpart systems of the web and smartphones. For example, the Lyft ride-sharing Alexa skill has an Android app and a website. Our work shows how information from counterpart apps can help reduce dis-ambiguities in the skill invocation process. We build SkilIFence, a browser extension that existing voice assistant users can install to ensure that only legitimate skills run in response to their commands. Using real user data from MTurk (N = 116) and experimental trials involving synthetic and organic speech, we show that SkillFence provides a balance between usability and security by securing 90.83% of skills that a user will need with a False acceptance rate of 19.83%.},
	articleno    = 16,
	numpages     = 26,
	keywords     = {Defense, Voice Attacks, Alexa, Skill, Skill-Squatting}
}
@article{10.1145/3517226,
	title        = {CornerRadar: RF-Based Indoor Localization Around Corners},
	author       = {Yue, Shichao and He, Hao and Cao, Peng and Zha, Kaiwen and Koizumi, Masayuki and Katabi, Dina},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517226},
	url          = {https://doi.org/10.1145/3517226},
	issue_date   = {March 2022},
	abstract     = {Unmanned robots are increasingly used around humans in factories, malls, and hotels. As they navigate our space, it is important to ensure that such robots do not collide with people who suddenly appear as they turn a corner. Today, however, there is no practical solution for localizing people around corners. Optical solutions try to track hidden people through their visible shadows on the floor or a sidewall, but they can easily fail depending on the ambient light and the environment. More recent work has considered the use of radio frequency (RF) signals to track people and vehicles around street corners. However, past RF-based proposals rely on a simplistic ray-tracing model that fails in practical indoor scenarios. This paper introduces CornerRadar, an RF-based method that provides accurate around-corner indoor localization. CornerRadar addresses the limitations of the ray-tracing model used in past work. It does so through a novel encoding of how RF signals bounce off walls and occlusions. The encoding, which we call the hint map, is then fed to a neural network along with the radio signals to localize people around corners. Empirical evaluation with people moving around corners in 56 indoor environments shows that CornerRadar achieves a median error that is 3x to 12x smaller than past RF-based solutions for localizing people around corners.},
	articleno    = 34,
	numpages     = 24,
	keywords     = {See-through-wall, Contactless Sensing, Localization, Around-Corner, Wireless Sensing, Deep Learning, Signal Processing}
}
@article{10.1145/3494995,
	title        = {Zero-Shot Learning for IMU-Based Activity Recognition Using Video Embeddings},
	author       = {Tong, Catherine and Ge, Jinchen and Lane, Nicholas D.},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494995},
	url          = {https://doi.org/10.1145/3494995},
	issue_date   = {Dec 2021},
	abstract     = {The Activity Recognition Chain generally precludes the challenging scenario of recognizing new activities that were unseen during training, despite this scenario being a practical and common one as users perform diverse activities at test time. A few prior works have adopted zero-shot learning methods for IMU-based activity recognition, which work by relating seen and unseen classes through an auxiliary semantic space. However, these methods usually rely heavily on a hand-crafted attribute space which is costly to define, or a learnt semantic space based on word embedding, which lacks motion-related information crucial for distinguishing IMU features. Instead, we propose a strategy to exploit videos of human activities to construct an informative semantic space. With our approach, knowledge from state-of-the-art video action recognition models is encoded into video embeddings to relate seen and unseen activity classes. Experiments on three public datasets find that our approach outperforms other learnt semantic spaces, with an additional desirable feature of scalability, as recognition performance is seen to scale with the amount of data used. More generally, our results indicate that exploiting information from the video domain for IMU-based tasks is a promising direction, with tangible returns in a zero-shot learning scenario.},
	articleno    = 180,
	numpages     = 23,
	keywords     = {zero-shot learning, cross-modal knowledge transfer, human activity recognition}
}
@article{10.1145/3494987,
	title        = {SpeeChin: A Smart Necklace for Silent Speech Recognition},
	author       = {Zhang, Ruidong and Chen, Mingyang and Steeper, Benjamin and Li, Yaxuan and Yan, Zihan and Chen, Yizhuo and Tao, Songyun and Chen, Tuochao and Lim, Hyunchul and Zhang, Cheng},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494987},
	url          = {https://doi.org/10.1145/3494987},
	issue_date   = {Dec 2021},
	abstract     = {This paper presents SpeeChin, a smart necklace that can recognize 54 English and 44 Chinese silent speech commands. A customized infrared (IR) imaging system is mounted on a necklace to capture images of the neck and face from under the chin. These images are first pre-processed and then deep learned by an end-to-end deep convolutional-recurrent-neural-network (CRNN) model to infer different silent speech commands. A user study with 20 participants (10 participants for each language) showed that SpeeChin could recognize 54 English and 44 Chinese silent speech commands with average cross-session accuracies of 90.5% and 91.6%, respectively. To further investigate the potential of SpeeChin in recognizing other silent speech commands, we conducted another study with 10 participants distinguishing between 72 one-syllable nonwords. Based on the results from the user studies, we further discuss the challenges and opportunities of deploying SpeeChin in real-world applications.},
	articleno    = 192,
	numpages     = 23,
	keywords     = {Deep learning, Silent Speech recognition, Computer vision}
}
@article{10.1145/3494983,
	title        = {Smart Webcam Cover: Exploring the Design of an Intelligent Webcam Cover to Improve Usability and Trust},
	author       = {Do, Youngwook and Park, Jung Wook and Wu, Yuxi and Basu, Avinandan and Zhang, Dingtian and Abowd, Gregory D. and Das, Sauvik},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494983},
	url          = {https://doi.org/10.1145/3494983},
	issue_date   = {Dec 2021},
	abstract     = {Laptop webcams can be covertly activated by malware and law enforcement agencies. Consequently, 59% percent of Americans manually cover their webcams to avoid being surveilled. However, manual covers are prone to human error---through a survey with 200 users, we found that 61.5% occasionally forget to re-attach their cover after using their webcam. To address this problem, we developed Smart Webcam Cover (SWC): a thin film that covers the webcam (PDLC-overlay) by default until a user manually uncovers the webcam, and automatically covers the webcam when not in use. Through a two-phased design iteration process, we evaluated SWC with 20 webcam cover users through a remote study with a video prototype of SWC, compared to manual operation, and discussed factors that influence users' trust in the effectiveness of SWC and their perceptions of its utility.},
	articleno    = 154,
	numpages     = 21,
	keywords     = {privacy-invasive sensor, webcam cover, usable security and privacy}
}
@article{10.1145/3494965,
	title        = {Understanding User Perceptions of Proactive Smart Speakers},
	author       = {Wei, Jing and Dingler, Tilman and Kostakos, Vassilis},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494965},
	url          = {https://doi.org/10.1145/3494965},
	issue_date   = {Dec 2021},
	abstract     = {Voice assistants, such as Amazon's Alexa and Google Home, increasingly find their way into consumer homes. Their functionality, however, is currently limited to being passive answer machines rather than proactively engaging users in conversations. Speakers' proactivity would open up a range of important application scenarios, including health services, such as checking in on patient states and triggering medication reminders. It remains unclear how passive speakers should implement proactivity. To better understand user perceptions, we ran a 3-week field study with 13 participants where we modified the off-the-shelf Google Home to become proactive. During the study, our speaker proactively triggered conversations that were essentially Experience Sampling probes allowing us to identify when to engage users. Applying machine-learning, we are able to predict user responsiveness with a 71.6% accuracy and find predictive features. We also identify self-reported factors, such as boredom and mood, that are significantly correlated with users' perceived availability. Our prototype and findings inform the design of proactive speakers that verbally engage users at opportune moments and contribute to the design of proactive application scenarios and voice-based experience sampling studies.},
	articleno    = 185,
	numpages     = 28,
	keywords     = {Experience Sampling Method, Conversational Agents, Smart Speakers, Interruptibility}
}
@article{10.1145/3494959,
	title        = {CSMC: Cellular Signal Map Construction via Mobile Crowdsensing},
	author       = {Wang, Hai and Guo, Baoshen and Wang, Shuai and He, Tian and Zhang, Desheng},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494959},
	url          = {https://doi.org/10.1145/3494959},
	issue_date   = {Dec 2021},
	abstract     = {The rise concern about mobile communication performance has driven the growing demand for the construction of mobile network signal maps which are widely utilized in network monitoring, spectrum management, and indoor/outdoor localization. Existing studies such as time-consuming and labor-intensive site surveys are difficult to maintain an update-to-date finegrained signal map within a large area. The mobile crowdsensing (MCS) paradigm is a promising approach for building signal maps because collecting large-scale MCS data is low-cost and with little extra-efforts. However, the dynamic environment and the mobility of the crowd cause spatio-temporal uncertainty and sparsity of MCS. In this work, we leverage MCS as an opportunity to conduct the city-wide mobile network signal map construction. We propose a fine-grained city-wide Cellular Signal Map Construction (CSMC) framework to address two challenges including (i) the problem of missing and unreliable MCS data; (ii) spatio-temporal uncertainty of signal propagation. In particular, CSMC captures spatio-temporal characteristics of signals from both inter- and intra- cellular base stations and conducts missing signal recovery with Bayesian tensor decomposition to build large-area fine-grained signal maps. Furthermore, CSMC develops a context-aware multi-view fusion network to make full use of external information and enhance signal map construction accuracy. To evaluate the performance of CSMC, we conduct extensive experiments and ablation studies on a large-scale dataset with over 200GB MCS signal records collected from Shanghai. Experimental results demonstrate that our model outperforms state-of-the-art baselines in the accuracy of signal estimation and user localization.},
	articleno    = 183,
	numpages     = 22,
	keywords     = {Signal map, Tensor decomposition, Cellular networks, Mobile Crowdsensing}
}
@article{10.1145/3478129,
	title        = {FaceSense: Sensing Face Touch with an Ear-Worn System},
	author       = {Kakaraparthi, Vimal and Shao, Qijia and Carver, Charles J. and Pham, Tien and Bui, Nam and Nguyen, Phuc and Zhou, Xia and Vu, Tam},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478129},
	url          = {https://doi.org/10.1145/3478129},
	issue_date   = {Sept 2021},
	abstract     = {Face touch is an unconscious human habit. Frequent touching of sensitive/mucosal facial zones (eyes, nose, and mouth) increases health risks by passing pathogens into the body and spreading diseases. Furthermore, accurate monitoring of face touch is critical for behavioral intervention. Existing monitoring systems only capture objects approaching the face, rather than detecting actual touches. As such, these systems are prone to false positives upon hand or object movement in proximity to one's face (e.g., picking up a phone). We present FaceSense, an ear-worn system capable of identifying actual touches and differentiating them between sensitive/mucosal areas from other facial areas. Following a multimodal approach, FaceSense integrates low-resolution thermal images and physiological signals. Thermal sensors sense the thermal infrared signal emitted by an approaching hand, while physiological sensors monitor impedance changes caused by skin deformation during a touch. Processed thermal and physiological signals are fed into a deep learning model (TouchNet) to detect touches and identify the facial zone of the touch. We fabricated prototypes using off-the-shelf hardware and conducted experiments with 14 participants while they perform various daily activities (e.g., drinking, talking). Results show a macro-F1-score of 83.4% for touch detection with leave-one-user-out cross-validation and a macro-F1-score of 90.1% for touch zone identification with a personalized model.},
	articleno    = 110,
	numpages     = 27,
	keywords     = {face touch detection, multimodal deep learning, thermo-physiological sensing}
}
@article{10.1145/3478122,
	title        = {Who Am I? A Design Probe Exploring Real-Time Transparency about Online and Offline User Profiling Underlying Targeted Ads},
	author       = {Barbosa, Nat\~{a} M. and Wang, Gang and Ur, Blase and Wang, Yang},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478122},
	url          = {https://doi.org/10.1145/3478122},
	issue_date   = {Sept 2021},
	abstract     = {To enable targeted ads, companies profile Internet users, automatically inferring potential interests and demographics. While current profiling centers on users' web browsing data, smartphones and other devices with rich sensing capabilities portend profiling techniques that draw on methods from ubiquitous computing. Unfortunately, even existing profiling and ad-targeting practices remain opaque to users, engendering distrust, resignation, and privacy concerns. We hypothesized that making profiling visible at the time and place it occurs might help users better understand and engage with automatically constructed profiles. To this end, we built a technology probe that surfaces the incremental construction of user profiles from both web browsing and activities in the physical world. The probe explores transparency and control of profile construction in real time. We conducted a two-week field deployment of this probe with 25 participants. We found that increasing the visibility of profiling helped participants anticipate how certain actions can trigger specific ads. Participants' desired engagement with their profile differed in part based on their overall attitudes toward ads. Furthermore, participants expected algorithms would automatically determine when an inference was inaccurate, no longer relevant, or off-limits. Current techniques typically do not do this. Overall, our findings suggest that leveraging opportunistic moments within pervasive computing to engage users with their own inferred profiles can create more trustworthy and positive experiences with targeted ads.},
	articleno    = 88,
	numpages     = 32,
	keywords     = {transparency, technology probe, profiling, online behavioral advertising}
}
@article{10.1145/3478118,
	title        = {Duco: Autonomous Large-Scale Direct-Circuit-Writing (DCW) on Vertical Everyday Surfaces Using A Scalable Hanging Plotter},
	author       = {Cheng, Tingyu and Li, Bu and Zhang, Yang and Li, Yunzhi and Ramey, Charles and Jung, Eui Min and Cui, Yepu and Swaminathan, Sai Ganesh and Do, Youngwook and Tentzeris, Manos and Abowd, Gregory D. and Oh, HyunJoo},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478118},
	url          = {https://doi.org/10.1145/3478118},
	issue_date   = {Sept 2021},
	abstract     = {Human environments are filled with large open spaces that are separated by structures like walls, facades, glass windows, etc. Most often, these structures are largely passive offering little to no interactivity. In this paper, we present Duco, a large-scale electronics fabrication robot that enables room-scale &amp; building-scale circuitry to add interactivity to vertical everyday surfaces. Duco negates the need for any human intervention by leveraging a hanging robotic system that automatically sketches multi-layered circuity to enable novel large-scale interfaces. The key idea behind Duco is that it achieves single-layer or multi-layer circuit fabrication on 2D surfaces as well as 2D cutouts that can be assembled into 3D objects by loading various functional inks (e.g., conductive, dielectric, or cleaning) to the wall-hanging drawing robot, as well as employing an optional laser cutting head as a cutting tool. Our technical evaluation shows that Duco's mechanical system works reliably on various surface materials with a wide range of roughness and surface morphologies. The system achieves superior mechanical tolerances (0.1mm XY axis resolution and 1mm smallest feature size). We demonstrate our system with five application examples, including an interactive piano, an IoT coffee maker controller, an FM energy-harvester printed on a large glass window, a human-scale touch sensor and a 3D interactive lamp.},
	articleno    = 92,
	numpages     = 25,
	keywords     = {ubiquitous computing, digital fabrication, smart environments, large-scale circuit, printed electronics}
}
@article{10.1145/3478115,
	title        = {RF-Ray: Joint RF and Linguistics Domain Learning for Object Recognition},
	author       = {Ding, Han and Zhai, Linwei and Zhao, Cui and Hou, Songjiang and Wang, Ge and Xi, Wei and Zhao, Jizhong and Gong, Yihong},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478115},
	url          = {https://doi.org/10.1145/3478115},
	issue_date   = {Sept 2021},
	abstract     = {This paper presents a non-invasive design, namely RF-ray, to recognize the shape and material of an object simultaneously. RF-ray puts the object approximate to an RFID tag array, and explores the propagation effect as well as coupling effect between RFIDs and the object for sensing. In contrast to prior proposals, RF-ray is capable to recognize unseen objects, including unseen shape-material pairs and unseen materials within a certain container. To make it real, RF-ray introduces a sensing capability enhancement module and leverages a two-branch neural network for shape profiling and material identification respectively. Furthermore, we incorporate a Zero-Shot Learning based embedding module that incorporates the well-learned linguistic features to generalize RF-ray to recognize unseen materials. We build a prototype of RF-ray using commodity RFID devices. Comprehensive real-world experiments demonstrate our system can achieve high object recognition performance.},
	articleno    = 96,
	numpages     = 24,
	keywords     = {zero-shot learning, object recognition, RFIDs}
}
@article{10.1145/3478114,
	title        = {DualRing: Enabling Subtle and Expressive Hand Interaction with Dual IMU Rings},
	author       = {Liang, Chen and Yu, Chun and Qin, Yue and Wang, Yuntao and Shi, Yuanchun},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478114},
	url          = {https://doi.org/10.1145/3478114},
	issue_date   = {Sept 2021},
	abstract     = {We present DualRing, a novel ring-form input device that can capture the state and movement of the user's hand and fingers. With two IMU rings attached to the user's thumb and index finger, DualRing can sense not only the absolute hand gesture relative to the ground but also the relative pose and movement among hand segments. To enable natural thumb-to-finger interaction, we develop a high-frequency AC circuit for on-body contact detection. Based on the sensing information of DualRing, we outline the interaction space and divide it into three sub-spaces: within-hand interaction, hand-to-surface interaction, and hand-to-object interaction. By analyzing the accuracy and performance of our system, we demonstrate the informational advantage of DualRing in sensing comprehensive hand gestures compared with single-ring-based solutions. Through the user study, we discovered the interaction space enabled by DualRing is favored by users for its usability, efficiency, and novelty.},
	articleno    = 115,
	numpages     = 27,
	keywords     = {hand interaction, hand gesture sensing, ring form device}
}
@article{10.1145/3478113,
	title        = {SquiggleMilli: Approximating SAR Imaging on Mobile Millimeter-Wave Devices},
	author       = {Regmi, Hem and Saadat, Moh Sabbir and Sur, Sanjib and Nelakuditi, Srihari},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478113},
	url          = {https://doi.org/10.1145/3478113},
	issue_date   = {Sept 2021},
	abstract     = {This paper proposes SquiggleMilli, a system that approximates traditional Synthetic Aperture Radar (SAR) imaging on mobile millimeter-wave (mmWave) devices. The system is capable of imaging through obstructions, such as clothing, and under low visibility conditions. Unlike traditional SAR that relies on mechanical controllers or rigid bodies, SquiggleMilli is based on the hand-held, fluidic motion of the mmWave device. It enables mmWave imaging in hand-held settings by re-thinking existing motion compensation, compressed sensing, and voxel segmentation. Since mmWave imaging suffers from poor resolution due to specularity and weak reflectivity, the reconstructed shapes could be imperceptible by machines and humans. To this end, SquiggleMilli designs a machine learning model to recover the high spatial frequencies in the object to reconstruct an accurate 2D shape and predict its 3D features and category. We have customized SquiggleMilli for security applications, but the model is adaptable to other applications with limited training samples. We implement SquiggleMilli on off-the-shelf components and demonstrate its performance improvement over the traditional SAR qualitatively and quantitatively.},
	articleno    = 125,
	numpages     = 26,
	keywords     = {See-through Imaging, Millimeter-Wave, Generative Adversarial Networks}
}
@article{10.1145/3478111,
	title        = {How Should Automated Vehicles Communicate Critical Situations? A Comparative Analysis of Visualization Concepts},
	author       = {Colley, Mark and Krauss, Svenja and Lanzer, Mirjam and Rukzio, Enrico},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478111},
	url          = {https://doi.org/10.1145/3478111},
	issue_date   = {Sept 2021},
	abstract     = {Passengers of automated vehicles will likely engage in non-driving related activities like reading and, therefore, be disengaged from the driving task. However, especially in critical situations such as unexpected pedestrian crossings, it can be assumed that passengers request information about the vehicle's intention and an explanation. Some concepts were proposed for such communication from the automated vehicle to the passenger. However, results are not comparable due to varying information content and scenarios. We present a comparative study in Virtual Reality (N=20) of four visualization concepts and a baseline with Augmented Reality, a Head-Up Display, or Lightbands. We found that all concepts were rated reasonable and necessary and increased trust, perceived safety, perceived intelligence, and acceptance compared to no visualization. However, when visualizations were compared, there were hardly any significant differences between them.},
	articleno    = 94,
	numpages     = 23,
	keywords     = {Autonomous vehicles, interface design}
}
@article{10.1145/3478109,
	title        = {GlucoMine: A Case for Improving the Use of Wearable Device Data in Diabetes Management},
	author       = {Bartolome, Abigail and Shah, Sahaj and Prioleau, Temiloluwa},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478109},
	url          = {https://doi.org/10.1145/3478109},
	issue_date   = {Sept 2021},
	abstract     = {The growing popularity of wearable devices for continuous sensing has made personal health data increasingly available, yet methods for data interpretation are still a work in progress. This paper investigates potential under-utilization of wearable device data in diabetes management and develops an analytic approach - GlucoMine - to uncover individualized patterns in extended periods of such data to support and improve care. In addition, we conduct a user study with clinicians to assess and compare conventional tools used for reviewing wearable device data in diabetes management with the proposed solution. Using 3-6 months of continuous glucose monitor (CGM) data from 54 patients with type 1 diabetes, we found that: 1) the recommended practice of reviewing only short periods (e.g., the most recent 2-weeks) of CGM data based on correlation analysis is not sufficient for finding hidden patterns of poor management; 2) majority of subjects (96% in this study) had clinically-recognized episodes of recurrent adverse glycemic events observable from analysis of extended periods of their CGM data; 3) majority of clinicians (89% in this study) believe there is benefit to be gained in having an algorithm for extracting patterns of adverse glycemic events from longer periods of wearable device data. Findings from our user study also provides insights, including strengths and weakness of various data presentation tools, to guide development of better solutions that improve the use of wearable device data for patient care.},
	articleno    = 90,
	numpages     = 24,
	keywords     = {continuous glucose monitor, personal informatics, pattern discovery, data visualization, user experience}
}
@article{10.1145/3478097,
	title        = {Honeysuckle: Annotation-Guided Code Generation of In-App Privacy Notices},
	author       = {Li, Tianshi and Neundorfer, Elijah B. and Agarwal, Yuvraj and Hong, Jason I.},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478097},
	url          = {https://doi.org/10.1145/3478097},
	issue_date   = {Sept 2021},
	abstract     = {In-app privacy notices can help smartphone users make informed privacy decisions. However, they are rarely used in real-world apps, since developers often lack the knowledge, time, and resources to design and implement them well. We present Honeysuckle, a programming tool that helps Android developers build in-app privacy notices using an annotation-based code generation approach facilitated by an IDE plugin, a build system plugin, and a library. We conducted a within-subjects study with 12 Android developers to evaluate Honeysuckle. Each participant was asked to implement privacy notices for two popular open-source apps using the Honeysuckle library as a baseline as well as the annotation-based approach. Our results show that the annotation-based approach helps developers accomplish the task faster with significantly lower cognitive load. Developers preferred the annotation-based approach over the library approach because it was much easier to learn and use and allowed developers to achieve various types of privacy notices using a unified code format, which can enhance code readability and benefit team collaboration.},
	articleno    = 112,
	numpages     = 27,
	keywords     = {Java Annotation, Privacy, Code Generation, IDE Plugin, Transparency, Android Development, Privacy Notice, Programming/Development Support}
}
@article{10.1145/3478084,
	title        = {DriverSonar: Fine-Grained Dangerous Driving Detection Using Active Sonar},
	author       = {Jiang, Hongbo and Hu, Jingyang and Liu, Daibo and Xiong, Jie and Cai, Mingjie},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478084},
	url          = {https://doi.org/10.1145/3478084},
	issue_date   = {Sept 2021},
	abstract     = {Dangerous driving due to drowsiness and distraction is the main cause of traffic accidents, resulting in casualties and economic loss. There is an urgent need to address this problem by accurately detecting dangerous driving behaviors and generating real-time alerts. Inspired by the observation that dangerous driving actions induce unique acoustic features that respond to the signal of an acoustic source, we present the DriverSonar system in this paper. The proposed system detects dangerous driving actions and generates real-time alarms using off-the-shelf smartphones. Compared with the state-of-the-arts, the DriverSonar system does not require dedicated sensors but just uses the built-in speaker and microphone in a smartphone. Specifically, DriverSonar is able to recognize head/hand motions such as nodding, yawning, and abrupt adjustment of the steering wheel. We design, implement and evaluate DriverSonar with extensive experiments. We conduct both simulator-based and and real driving-based experiments (IRB-approved) with 30 volunteers for a period over 12 months. Experiment results show that the proposed system can detect drowsy and distraction related dangerous driving actions at an precision up to 93.2% and a low false acceptance rate of 3.6%.},
	articleno    = 108,
	numpages     = 22,
	keywords     = {Dangerous Driving Identification, Device-free, Acoustic Tracking}
}
@article{10.1145/3478082,
	title        = {SwingNet: Ubiquitous Fine-Grained Swing Tracking Framework via Stochastic Neural Architecture Search and Adversarial Learning},
	author       = {Jia, Hong and Hu, Jiawei and Hu, Wen},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478082},
	url          = {https://doi.org/10.1145/3478082},
	issue_date   = {Sept 2021},
	abstract     = {Sports analytics in the wild (i.e., ubiquitously) is a thriving industry. Swing tracking is a key feature in sports analytics. Therefore, a centimeter-level tracking resolution solution is required. Recent research has explored deep neural networks for sensor fusion to produce consistent swing-tracking performance. This is achieved by combining the advantages of two sensor modalities (IMUs and depth sensors) for golf swing tracking. Here, the IMUs are not affected by occlusion and can support high sampling rates. Meanwhile, depth sensors produce significantly more accurate motion measurements than those produced by IMUs. Nevertheless, this method can be further improved in terms of accuracy and lacking information for different domains (e.g., subjects, sports, and devices). Unfortunately, designing a deep neural network with good performance is time consuming and labor intensive, which is challenging when a network model is deployed to be used in new settings. To this end, we propose a network based on Neural Architecture Search (NAS), called SwingNet, which is a regression-based automatic generated deep neural network via stochastic neural network search. The proposed network aims to learn the swing tracking feature for better prediction automatically. Furthermore, SwingNet features a domain discriminator by using unsupervised learning and adversarial learning to ensure that it can be adaptive to unobserved domains. We implemented SwingNet prototypes with a smart wristband (IMU) and smartphone (depth sensor), which are ubiquitously available. They enable accurate sports analytics (e.g., coaching, tracking, analysis and assessment) in the wild.Our comprehensive experiment shows that SwingNet achieves less than 10 cm errors of swing tracking with a subject-independent model covering multiple sports (e.g., golf and tennis) and depth sensor hardware, which outperforms state-of-the-art approaches.},
	articleno    = 106,
	numpages     = 21,
	keywords     = {Neural Architecture Search, Adversarial Learning, Sport Analytics, Swing Tracking}
}
@article{10.1145/3478081,
	title        = {ALWAES: An Automatic Outdoor Location-Aware Correction System for Online Delivery Platforms},
	author       = {Jiang, Dongzhe and Ding, Yi and Zhang, Hao and Liu, Yunhuai and He, Tian and Yang, Yu and Zhang, Desheng},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478081},
	url          = {https://doi.org/10.1145/3478081},
	issue_date   = {Sept 2021},
	abstract     = {For an online delivery platform, accurate physical locations of merchants are essential for delivery scheduling. It is challenging to maintain tens of thousands of merchant locations accurately because of potential errors introduced by merchants for profits (e.g., potential fraud). In practice, a platform periodically sends a dedicated crew to survey limited locations due to high workforce costs, leaving many potential location errors. In this paper, we design and implement ALWAES, a system that automatically identifies and corrects location errors based on fundamental tradeoffs of five measurement strategies from manual, physical, and virtual data collection infrastructures for online delivery platforms. ALWAES explores delivery data already collected by platform infrastructures to measure the travel time of couriers between merchants and verify all merchants' locations by cross-validation automatically. We explore tradeoffs between performance and cost of different measurement approaches. By comparing with the manually-collected ground truth, the experimental results show that ALWAES outperforms three other baselines by 32.2%, 41.8%, and 47.2%, respectively. More importantly, ALWAES saves 3,846 hours of the delivery time of 35,005 orders in a month and finds new erroneous locations that initially were not in the ground truth but are verified by our field study later, accounting for 3% of all merchants with erroneous locations.},
	articleno    = 107,
	numpages     = 24,
	keywords     = {Machine Learning, CrowdSourcing, Online delivery, Localization}
}
@article{10.1145/3463526,
	title        = {Unlocking the Beamforming Potential of LoRa for Long-Range Multi-Target Respiration Sensing},
	author       = {Zhang, Fusang and Chang, Zhaoxin and Xiong, Jie and Zheng, Rong and Ma, Junqi and Niu, Kai and Jin, Beihong and Zhang, Daqing},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463526},
	url          = {https://doi.org/10.1145/3463526},
	issue_date   = {June 2021},
	abstract     = {Despite extensive research effort in contact-free sensing using RF signals in the last few years, there still exist significant barriers preventing their wide adoptions. One key issue is the inability to sense multiple targets due to the intrinsic nature of relying on reflection signals for sensing: the reflections from multiple targets get mixed at the receiver and it is extremely difficult to separate these signals to sense each individual. This problem becomes even more severe in long-range LoRa sensing because the sensing range is much larger compared to WiFi and acoustic based sensing. In this work, we address the challenging multi-target sensing issue, moving LoRa sensing one big step towards practical adoption. The key idea is to effectively utilize multiple antennas at the LoRa gateway to enable spatial beamforming to support multi-target sensing. While traditional beamforming methods adopted in WiFi and Radar systems rely on accurate channel information or transmitter-receiver synchronization, these requirements can not be satisfied in LoRa systems: the transmitter and receiver are not synchronized and no channel state information can be obtained from the cheap LoRa nodes. Another interesting observation is that while beamforming helps to increase signal strength, the phase/amplitude information which is critical for sensing can get corrupted during the beamforming process, eventually compromising the sensing capability. In this paper, we propose novel signal processing methods to address the issues above to enable long-range multi-target reparation sensing with LoRa. Extensive experiments show that our system can monitor the respiration rates of five human targets simultaneously at an average accuracy of 98.1%.},
	articleno    = 85,
	numpages     = 25,
	keywords     = {LoRa Beamforming, Contactless sensing, Multi-target respiration sensing, Long range sensing}
}
@article{10.1145/3463518,
	title        = {FabHandWear: An End-to-End Pipeline from Design to Fabrication of Customized Functional Hand Wearables},
	author       = {Paredes, Luis and Reddy, Sai Swarup and Chidambaram, Subramanian and Vagholkar, Devashri and Zhang, Yunbo and Benes, Bedrich and Ramani, Karthik},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463518},
	url          = {https://doi.org/10.1145/3463518},
	issue_date   = {June 2021},
	abstract     = {Current hand wearables have limited customizability, they are loose-fit to an individual's hand and lack comfort. The main barrier in customizing hand wearables is the geometric complexity and size variation in hands. Moreover, there are different functions that the users can be looking for; some may only want to detect hand's motion or orientation; others may be interested in tracking their vital signs. Current wearables usually fit multiple functions and are designed for a universal user with none or limited customization. There are no specialized tools that facilitate the creation of customized hand wearables for varying hand sizes and provide different functionalities. We envision an emerging generation of customizable hand wearables that supports hand differences and promotes hand exploration with additional functionality. We introduce FabHandWear, a novel system that allows end-to-end design and fabrication of customized functional self-contained hand wearables. FabHandWear is designed to work with off-the-shelf electronics, with the ability to connect them automatically and generate a printable pattern for fabrication. We validate our system by using illustrative applications, a durability test, and an empirical user evaluation. Overall, FabHandWear offers the freedom to create customized, functional, and manufacturable hand wearables.},
	articleno    = 76,
	numpages     = 22,
	keywords     = {screen print, customization, 3D design, fabrication, hand, interface, Wearables, inks, electronics, textiles}
}
@article{10.1145/3463504,
	title        = {Towards Position-Independent Sensing for Gesture Recognition with Wi-Fi},
	author       = {Gao, Ruiyang and Zhang, Mi and Zhang, Jie and Li, Yang and Yi, Enze and Wu, Dan and Wang, Leye and Zhang, Daqing},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463504},
	url          = {https://doi.org/10.1145/3463504},
	issue_date   = {June 2021},
	abstract     = {Past decades have witnessed the extension of the Wi-Fi signals as a useful tool sensing human activities. One common assumption behind it is that there is a one-to-one mapping between human activities and Wi-Fi received signal patterns. However, this assumption does not hold when the user conducts activities in different locations and orientations. Actually, the received signal patterns of the same activity would become inconsistent when the relative location and orientation of the user with respect to transceivers change, leading to unstable sensing performance. This problem is known as the position-dependent problem, hindering the actual deployment of Wi-Fi-based sensing applications. In this paper, to tackle this fundamental problem, we develop a new position-independent sensing strategy and use gesture recognition as an application example to demonstrate its effectiveness. The key idea is to shift our observation from the traditional transceiver view to the hand-oriented view, and extract features that are irrespective of position-specific factors. Following the strategy, we design a position-independent feature, denoted as Motion Navigation Primitive(MNP). MNP captures the pattern of moving direction changes of the hand, which shares consistent patterns when the user performs the same gesture with different position-specific factors. By analyzing the pattern of MNP, we convert gestures into sequences of strokes (e.g, line, arc and corner) which makes them easy to be recognized. We build a prototype WiFi gesture recognition system, i.e., WiGesture to validate the effectiveness of the proposed strategy. Experiments show that our system can outperform the start-of-arts significantly in different settings. Given its novelty and superiority, we believe the proposed method symbolizes a major step towards gesture recognition and would inspire other solutions to position-independent activity recognition in the future.},
	articleno    = 61,
	numpages     = 28,
	keywords     = {Wireless Sensing, Gesture Recognition, Channel State Information (CSI)}
}
@article{10.1145/3448113,
	title        = {Voice In Ear: Spoofing-Resistant and Passphrase-Independent Body Sound Authentication},
	author       = {Gao, Yang and Jin, Yincheng and Chauhan, Jagmohan and Choi, Seokmin and Li, Jiyang and Jin, Zhanpeng},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448113},
	url          = {https://doi.org/10.1145/3448113},
	issue_date   = {March 2021},
	abstract     = {With the rapid growth of wearable computing and increasing demand for mobile authentication scenarios, voiceprint-based authentication has become one of the prevalent technologies and has already presented tremendous potentials to the public. However, it is vulnerable to voice spoofing attacks (e.g., replay attacks and synthetic voice attacks). To address this threat, we propose a new biometric authentication approach, named EarPrint, which aims to extend voiceprint and build a hidden and secure user authentication scheme on earphones. EarPrint builds on the speaking-induced body sound transmission from the throat to the ear canal, i.e., different users will have different body sound conduction patterns on both sides of ears. As the first exploratory study, extensive experiments on 23 subjects show the EarPrint is robust against ambient noises and body motions. EarPrint achieves an Equal Error Rate (EER) of 3.64% with 75 seconds enrollment data. We also evaluate the resilience of EarPrint against replay attacks. A major contribution of EarPrint is that it leverages two-level uniqueness, including the body sound conduction from the throat to the ear canal and the body asymmetry between the left and the right ears, taking advantage of earphones' paring form-factor. Compared with other mobile and wearable biometric modalities, EarPrint is a low-cost, accurate, and secure authentication solution for earphone users.},
	articleno    = 12,
	numpages     = 25,
	keywords     = {authentication, Voiceprint, earphones}
}
@article{10.1145/3448110,
	title        = {Pantomime: Mid-Air Gesture Recognition with Sparse Millimeter-Wave Radar Point Clouds},
	author       = {Palipana, Sameera and Salami, Dariush and Leiva, Luis A. and Sigg, Stephan},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448110},
	url          = {https://doi.org/10.1145/3448110},
	issue_date   = {March 2021},
	abstract     = {We introduce Pantomime, a novel mid-air gesture recognition system exploiting spatio-temporal properties of millimeter-wave radio frequency (RF) signals. Pantomime is positioned in a unique region of the RF landscape: mid-resolution mid-range high-frequency sensing, which makes it ideal for motion gesture interaction. We configure a commercial frequency-modulated continuous-wave radar device to promote spatial information over the temporal resolution by means of sparse 3D point clouds and contribute a deep learning architecture that directly consumes the point cloud, enabling real-time performance with low computational demands. Pantomime achieves 95% accuracy and 99% AUC in a challenging set of 21 gestures articulated by 41 participants in two indoor environments, outperforming four state-of-the-art 3D point cloud recognizers. We further analyze the effect of the environment in 5 different indoor environments, the effect of articulation speed, angle, and the distance of the person up to 5m. We have publicly made available the collected mmWave gesture dataset consisting of nearly 22,000 gesture instances along with our radar sensor configuration, trained models, and source code for reproducibility. We conclude that pantomime is resilient to various input conditions and that it may enable novel applications in industrial, vehicular, and smart home scenarios.},
	articleno    = 27,
	numpages     = 27,
	keywords     = {gesture recognition, radar sensing, deep learning}
}
@article{10.1145/3448098,
	title        = {EarDynamic: An Ear Canal Deformation Based Continuous User Authentication Using In-Ear Wearables},
	author       = {Wang, Zi and Tan, Sheng and Zhang, Linghan and Ren, Yili and Wang, Zhi and Yang, Jie},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448098},
	url          = {https://doi.org/10.1145/3448098},
	issue_date   = {March 2021},
	abstract     = {Biometric-based authentication is gaining increasing attention for wearables and mobile applications. Meanwhile, the growing adoption of sensors in wearables also provides opportunities to capture novel wearable biometrics. In this work, we propose EarDynamic, an ear canal deformation based user authentication using in-ear wearables. EarDynamic provides continuous and passive user authentication and is transparent to users. It leverages ear canal deformation that combines the unique static geometry and dynamic motions of the ear canal when the user is speaking for authentication. It utilizes an acoustic sensing approach to capture the ear canal deformation with the built-in microphone and speaker of the in-ear wearable. Specifically, it first emits well-designed inaudible beep signals and records the reflected signals from the ear canal. It then analyzes the reflected signals and extracts fine-grained acoustic features that correspond to the ear canal deformation for user authentication. Our extensive experimental evaluation shows that EarDynamic can achieve a recall of 97.38% and an F1 score of 96.84%. Results also show that our system works well under different noisy environments with various daily activities.},
	articleno    = 39,
	numpages     = 27,
	keywords     = {Biometrics, Ear Canal, Mobile Authentication, Wearable, Acoustic Sensing}
}
@article{10.1145/3448086,
	title        = {Ray Tracing-Based Light Energy Prediction for Indoor Batteryless Sensors},
	author       = {Kim, Daeyong and Ahn, Junick and Shin, Jun and Cha, Hojung},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448086},
	url          = {https://doi.org/10.1145/3448086},
	issue_date   = {March 2021},
	abstract     = {Light energy harvesting is a valuable technique for batteryless sensors located indoors. A key challenge is finding the right locations to deploy sensors to provide sufficient harvesting capability. A trial-and-error approach or energy prediction method is used as the solution, but existing schemes are either time-consuming or employing a na\"{\i}ve prediction mechanism primarily developed for outdoor environments. In this paper, we propose a light energy prediction technique, called Solacle, which accounts for various factors in indoor light harvesting to provide accuracy at any given location. Exploiting the ray tracing technique, Solacle estimates the illuminance and the luminous efficacy of light sources to predict the harvesting capability, by considering the spatiotemporal characteristics of the surrounding environment. To this end, we defined the optical properties of a space, and devised an optimization approach, specifically a gradient-free-based scheme, to acquire adequate values for the combination of optical properties. We implemented the system and evaluated its efficacy in controlled and real environments. The experiment results show that the proposed approach delivers a significant improvement over previous work in light energy prediction of indoor space.},
	articleno    = 17,
	numpages     = 27,
	keywords     = {batteryless sensors, Additional Keywords and Phrases: light energy harvesting, ray tracing}
}
@article{10.1145/3432233,
	title        = {VerHealth: Vetting Medical Voice Applications through Policy Enforcement},
	author       = {Shezan, Faysal Hossain and Hu, Hang and Wang, Gang and Tian, Yuan},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432233},
	url          = {https://doi.org/10.1145/3432233},
	issue_date   = {December 2020},
	abstract     = {Healthcare applications on Voice Personal Assistant System (e.g., Amazon Alexa), have shown a great promise to deliver personalized health services via a conversational interface. However, concerns are also raised about privacy, safety, and service quality. In this paper, we propose VerHealth, to systematically assess health-related applications on Alexa for how well they comply with existing privacy and safety policies. VerHealth contains a static module and a dynamic module based on machine learning that can trigger and detect violation behaviors hidden deep in the interaction threads. We use VerHealth to analyze 813 health-related applications on Alexa by sending over 855,000 probing questions and analyzing 863,000 responses. We also consult with three medical school students (domain experts) to confirm and assess the potential violations. We show that violations are quite common, e.g., 86.36% of them miss disclaimers when providing medical information; 30.23% of them store user physical or mental health data without approval. Domain experts believe that the applications' medical suggestions are often factually-correct but are of poor relevance, and applications should have asked more questions before providing suggestions for over half of the cases. Finally, we use our results to discuss possible directions for improvements.},
	articleno    = 153,
	numpages     = 21,
	keywords     = {Alexa, policy-enforcement, Skill, Medical-voice-applications, dynamic-analysis, Google-Home}
}
@article{10.1145/3432220,
	title        = {Evaluating the Reproducibility of Physiological Stress Detection Models},
	author       = {Mishra, Varun and Sen, Sougata and Chen, Grace and Hao, Tian and Rogers, Jeffrey and Chen, Ching-Hua and Kotz, David},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432220},
	url          = {https://doi.org/10.1145/3432220},
	issue_date   = {December 2020},
	abstract     = {Recent advances in wearable sensor technologies have led to a variety of approaches for detecting physiological stress. Even with over a decade of research in the domain, there still exist many significant challenges, including a near-total lack of reproducibility across studies. Researchers often use some physiological sensors (custom-made or off-the-shelf), conduct a study to collect data, and build machine-learning models to detect stress. There is little effort to test the applicability of the model with similar physiological data collected from different devices, or the efficacy of the model on data collected from different studies, populations, or demographics.This paper takes the first step towards testing reproducibility and validity of methods and machine-learning models for stress detection. To this end, we analyzed data from 90 participants, from four independent controlled studies, using two different types of sensors, with different study protocols and research goals. We started by evaluating the performance of models built using data from one study and tested on data from other studies. Next, we evaluated new methods to improve the performance of stress-detection models and found that our methods led to a consistent increase in performance across all studies, irrespective of the device type, sensor type, or the type of stressor. Finally, we developed and evaluated a clustering approach to determine the stressed/not-stressed classification when applying models on data from different studies, and found that our approach performed better than selecting a threshold based on training data. This paper's thorough exploration of reproducibility in a controlled environment provides a critical foundation for deeper study of such methods, and is a prerequisite for tackling reproducibility in free-living conditions.},
	articleno    = 147,
	numpages     = 29,
	keywords     = {Stress detection, wearable sensing, mobile health (mHealth), mental health}
}
@article{10.1145/3432206,
	title        = {Not Quite Yourself Today: Behaviour-Based Continuous Authentication in IoT Environments},
	author       = {Kra\v{s}ovec, Andra\v{z} and Pellarini, Daniel and Geneiatakis, Dimitrios and Baldini, Gianmarco and Pejovi\'{c}, Veljko},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432206},
	url          = {https://doi.org/10.1145/3432206},
	issue_date   = {December 2020},
	abstract     = {The shortcomings of the traditional password-based authentication mechanism are becoming increasingly apparent as we transition from "one user - one device" to a richer "multiple users - multiple devices" computing paradigm. The currently dominant research direction focuses on on-device biometrics, which require sensitive information, such as images of a user's face, to be constantly streamed from a single recording source, often the device on which a user is getting authenticated. Instead, in this work we explore the possibilities offered by heterogeneous devices that opportunistically collect non-sensitive data in smart environments. We construct an IoT testbed in which we gather data pertaining to a person's movement in space, interaction with certain physical objects, PC terminal usage, and keyboard typing, and construct machine learning models capturing the person's behaviour traits. We commence our examination with models constructed from data sensed during a previously-completed task run and with such models we achieve up to 68% user identification accuracy (c.f. 7% baseline) among up to 20 individuals. Taking into account the limits of behaviour persistence we then revise our approach to continuously refine the model with the most recently sampled sensor data. This method allows us to achieve 99.3% user verification accuracy and successfully prevent a session takeover attack within 12 seconds with less than 1% of false attack detection.},
	articleno    = 136,
	numpages     = 29,
	keywords     = {continuous authentication, Internet of Things (IoT), machine learning}
}
@article{10.1145/3432203,
	title        = {ContAuth: Continual Learning Framework for Behavioral-Based User Authentication},
	author       = {Chauhan, Jagmohan and Kwon, Young D. and Hui, Pan and Mascolo, Cecilia},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432203},
	url          = {https://doi.org/10.1145/3432203},
	issue_date   = {December 2020},
	abstract     = {User authentication is key in user authorization on smart and personal devices. Over the years, several authentication mechanisms have been proposed: these also include behavioral-based biometrics. However, behavioral-based biometrics suffer from two issues: they are prone to degradation in performance (accuracy) over time (e.g., due to data distribution changes arising from user behavior) and the need to learn the machine learning model from scratch, when adding new users. In this paper, we propose ContAuth, a system that can enhance the robustness of behavioral-based authentication. ContAuth continuously adapts to new incoming data (data incremental learning) and is able to add new users without retraining (class incremental learning). Specifically, ContAuth combines deep learning models with online learning models to achieve learning on the fly, thereby preventing a severe drop in the accuracy between sessions (over time). To add new users, ContAuth employs class incremental learning methods. We evaluate ContAuth on multiple behavior-based user authentication modalities: breathing, gait. and EMG. Our results show that our framework can help True Positive Rate (TPR) to remain high (&gt;85 %) compared to other methods for all the modalities except EMG (&gt;70%) across the sessions while keeping False Positive Rates (FPR) at a minimum (0-10%). It can achieve up to 35% improvement in TPR over a traditional deep learning model. Additionally, iCaRL (an incremental learning method) enables ContAuth to allow the addition of new users by alleviating catastrophic forgetting, to a large extent. Finally, we also show that ContAuth can be deployed efficiently and effectively on device, further providing data privacy.},
	articleno    = 122,
	numpages     = 23,
	keywords     = {Breathing Gestures, Behavior based Authentication, Incremental Learning, Neural Networks, EMG, Gait}
}
@article{10.1145/3432201,
	title        = {Knitted Sensors: Designs and Novel Approaches for Real-Time, Real-World Sensing},
	author       = {McDonald, Denisa Qori and Vallett, Richard and Solovey, Erin and Dion, Genevi\`{e}ve and Shokoufandeh, Ali},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432201},
	url          = {https://doi.org/10.1145/3432201},
	issue_date   = {December 2020},
	abstract     = {Recent work has shown the feasibility of producing knitted capacitive touch sensors through digital fabrication with little human intervention in the textile production process. Such sensors can be designed and manufactured at scale and require only two connection points, regardless of the touch sensor form factor and size of the fabric, opening many possibilities for new designs and applications in textile sensors. To bring this technology closer to real-world use, we go beyond previous work on coarse touch discrimination to enable fine, accurate touch localization on a knitted sensor, using a recognition model able to capture the temporal behavior of the sensor. Moreover, signal acquisition and processing are performed in real-time, using swept frequency Bode analysis to quantify distortion from induced capacitance. After training our network model, we conducted a study with new users, and achieved a subject-independent accuracy of 66% in identifying the touch location on the 36-button sensor, while chance accuracy is approximately 3%. Additionally, we conducted a study demonstrating the viability of taking this solution closer to real-world scenarios by testing the sensor's resistance to potential deformation from everyday conditions. We also introduce several other knitted designs and related application prototypes to explore potential uses of the technology.},
	articleno    = 145,
	numpages     = 25,
	keywords     = {touch detection, evaluation study, robustness study, real-world sensing, real-time sensing, neural networks, knitted sensors}
}
@article{10.1145/3432194,
	title        = {It Didn't Sound Good with My Cochlear Implants: Understanding the Challenges of Using Smart Assistants for Deaf and Hard of Hearing Users},
	author       = {Blair, Johnna and Abdullah, Saeed},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432194},
	url          = {https://doi.org/10.1145/3432194},
	issue_date   = {December 2020},
	abstract     = {How do deaf and hard of hearing (DHH) individuals use smart assistants (SAs)? Does the prominent use of voice interfaces in most SAs pose unique challenges for DHH users? In this work, we aim to answer these questions by conducting 4 in-depth interviews, as well as collecting survey data from 73 DHH individuals. Our findings show that individuals, even with profound deafness, can leverage SAs to accomplish complex daily tasks. However, we also identified a number of common challenges DHH individuals face when interacting with SAs (e.g., high pitch used in the default SA voice interfaces can be incompatible with hearing aids, difficulty using mobile SAs in public places with loud background noise). Based on these insights, we provide a set of suggestions for designing SAs that can better accommodate a wide range of hearing abilities. Specifically, SAs should provide more customization options to allow the user to tailor their SA to meet their hearing needs over time. For example, using a pitch-frequency test feature, much like audiograms conducted by audiologists, could allow users to calibrate their SA's voice to fit within their optimal range. We also see a need to provide more clear and actionable error messages conveyed beyond audio notifications, such as more meaningful light notifications. These recommendations and findings provide the first step forward toward a more inclusive SA by addressing accessibility needs unique to this group.},
	articleno    = 118,
	numpages     = 27,
	keywords     = {Deaf, Hearing Loss, Conversational Agents, Voice Interfaces, Accessibility, Smart Assistants}
}
@article{10.1145/3369836,
	title        = {LimbMotion: Decimeter-Level Limb Tracking for Wearable-Based Human-Computer Interaction},
	author       = {Zhou, Han and Gao, Yi and Song, Xinyi and Liu, Wenxin and Dong, Wei},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369836},
	url          = {https://doi.org/10.1145/3369836},
	issue_date   = {December 2019},
	abstract     = {Wearable-based human-computer interaction is a promising technology to enable various applications. This paper aims to track the 3D posture of the entire limb, both wrist/ankle and elbow/knee, of a user wearing a smart device. This limb tracking technology can trace the geometric motion of the limb, without introducing any training stage usually required in gesture recognition approaches. Nonetheless, the tracked limb motion can also be used as a generic input for gesture-based applications. The 3D posture of a limb is defined by the relative positions among main joints, e.g., shoulder, elbow, and wrist for an arm. When a smartwatch is worn on the wrist of a user, its position is affected by both elbow and shoulder motions. It is challenging to infer the entire 3D posture when only given a single point of sensor data from the smartwatch. In this paper, we propose LimbMotion, an accurate and real-time limb tracking system. The performance gain of LimbMotion comes from multiple key technologies, including an accurate attitude estimator based on a novel two-step filter, fast acoustic ranging, and point clouds-based positioning. We implemented LimbMotion and evaluated its performance using extensive experiments, including different gestures, moving speeds, users, and limbs. Results show that LimbMotion achieves real-time tracking with a median error of 7.5cm to 8.9cm, which outperforms the state-of-the-art approach by about 32%.},
	articleno    = 161,
	numpages     = 24,
	keywords     = {Acoustic sensing, Limb tracking, Wearable computing, Additional Key Words and Phrases, Human-computer interaction}
}
@article{10.1145/3369834,
	title        = {Probing Sucrose Contents in Everyday Drinks Using Miniaturized Near-Infrared Spectroscopy Scanners},
	author       = {Jiang, Weiwei and Marini, Gabriele and van Berkel, Niels and Sarsenbayeva, Zhanna and Tan, Zheyu and Luo, Chu and He, Xin and Dingler, Tilman and Goncalves, Jorge and Kawahara, Yoshihiro and Kostakos, Vassilis},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369834},
	url          = {https://doi.org/10.1145/3369834},
	issue_date   = {December 2019},
	abstract     = {Near-Infrared Spectroscopy (NIRS) is a non-invasive sensing technique which can be used to acquire information on an object's chemical composition. Although NIRS is conventionally used in dedicated laboratories, the recent introduction of miniaturized NIRS scanners has greatly expanded the use cases of this technology. Previous work from the UbiComp community shows that miniaturized NIRS can be successfully adapted to identify medical pills and alcohol concentration. In this paper, we further extend this technology to identify sugar (sucrose) contents in everyday drinks. We developed a standalone mobile device which includes inter alia a NIRS scanner and a 3D printed clamp. The clamp can be attached to a straw-like tube to sense a liquid's sucrose content. Through a series of studies, we show that our technique can accurately measure sucrose levels in both lab-made samples and commercially available drinks, as well as classify commercial drinks. Furthermore, we show that our method is robust to variations in the ambient temperature and lighting conditions. Overall, our system can estimate the concentration of sugar with ±0.29 g/100ml error in lab-made samples and &lt; 2.0 g/100ml error in 18 commercial drinks, and can identify everyday drinks with &gt; 99% accuracy. Furthermore, in our analysis, we are able to discern three characteristic wavelengths in the near-infrared region (1055 nm, 1235 nm and 1545 nm) with acute responses to sugar (sucrose). Our proposed protocol contributes to the development of everyday "food scanners" consumers.},
	articleno    = 136,
	numpages     = 25,
	keywords     = {mobile sensing, Near-Infrared spectroscopy, food scanner, liquid sensing, machine learning}
}
@article{10.1145/3369811,
	title        = {Secure Your Voice: An Oral Airflow-Based Continuous Liveness Detection for Voice Assistants},
	author       = {Wang, Yao and Cai, Wandong and Gu, Tao and Shao, Wei and Li, Yannan and Yu, Yong},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369811},
	url          = {https://doi.org/10.1145/3369811},
	issue_date   = {December 2019},
	abstract     = {Voice control has attracted extensive attention recently as it is a prospective User Interface (UI) to substitute for conventional touch control on smart devices. Voice assistants have become increasingly popular in our daily lives, especially for those people who are visually impaired. However, the inherently insecure nature of voice biometrics means that voice assistants are vulnerable to spoofing attacks as evidenced by security experts. To secure the commands for voice assistants, in this paper, we present a liveness detection system that provides continuous speaker verification on smart devices. The basic aim is to match the voice received by the smart device's microphone with the oral airflow of the user when speaking a command. The airflow is captured by an auxiliary commercial off-the-shelf airflow sensor. Specifically, we establish a theoretical model to depict the relationship between the oral airflow pressure and the phonemes in users' speech. The system estimates a series of pressures from the speech according to the theoretical model, and then calculates the consistency between the estimated pressure signal and the actual pressure signal measured by the airflow sensor to determine whether a command is a genuine "live" voice or an artificially generated one. We evaluate the system with 26 participants and 30 different voice commands. The evaluation showed that our system achieves an overall accuracy of 97.25% with an Equal Error Rate (EER) of 2.08%.},
	articleno    = 157,
	numpages     = 28,
	keywords     = {Liveness detection, Additional Key Words and Phrases, Voice control, Oral airflow}
}
@article{10.1145/3369808,
	title        = {ER-Rhythm: Coupling Exercise and Respiration Rhythm Using Lightweight COTS RFID},
	author       = {Yang, Yanni and Cao, Jiannong and Liu, Xiulong},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369808},
	url          = {https://doi.org/10.1145/3369808},
	issue_date   = {December 2019},
	abstract     = {The locomotor-respiratory coupling (LRC) ratio of a person doing exercise is an important parameter to reflect the exercise safety and effectiveness. Existing approaches that can measure LRC either rely on specialized and costly devices or use heavy sensors, bringing much inconvenience to people during exercise. To overcome these limitations, we propose ER-Rhythm using low-cost and lightweight RFID tags attached on the human body to simultaneously extract and couple the exercise and respiration rhythm for LRC estimation. ER-Rhythm captures exercise locomotion rhythm from the signals of the tags on limbs. However, extracting respiration rhythm from the signals of the tags on the chest during exercise is a challenging task because the minute respiration movement can be overwhelmed by the large torso movement. To address this challenge, we first leverage the unique characteristic of human respiratory mechanism to measure the chest movement while breathing, and then perform dedicated signal fusion of multiple tags interrogated by a pair of antennas to remove the torso movement effect. In addition, we take advantage of the multi-path effect of RF signals to reduce the number of needed antennas for respiration pattern extraction to save the system cost. To couple the exercise and respiration rhythm, we adopt a correlation-based approach to facilitate LRC estimation. The experimental results show that LRC can be estimated accurately up to 92% -- 95% of the time.},
	articleno    = 158,
	numpages     = 24,
	keywords     = {RFID, phase, exercise and respiration rhythm}
}
@article{10.1145/3411842,
	title        = {The OnHW Dataset: Online Handwriting Recognition from IMU-Enhanced Ballpoint Pens with Machine Learning},
	author       = {Ott, Felix and Wehbi, Mohamad and Hamann, Tim and Barth, Jens and Eskofier, Bj\"{o}rn and Mutschler, Christopher},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411842},
	url          = {https://doi.org/10.1145/3411842},
	issue_date   = {September 2020},
	abstract     = {This paper presents a handwriting recognition (HWR) system that deals with online character recognition in real-time. Our sensor-enhanced ballpoint pen delivers sensor data streams from triaxial acceleration, gyroscope, magnetometer and force signals at 100 Hz. As most existing datasets do not meet the requirements of online handwriting recognition and as they have been collected using specific equipment under constrained conditions, we propose a novel online handwriting dataset acquired from 119 writers consisting of 31,275 uppercase and lowercase English alphabet character recordings (52 classes) as part of the UbiComp 2020 Time Series Classification Challenge. Our novel OnHW-chars dataset allows for the evaluations of uppercase, lowercase and combined classification tasks, on both writer-dependent (WD) and writer-independent (WI) classes and we show that properly tuned machine learning pipelines as well as deep learning classifiers (such as CNNs, LSTMs, and BiLSTMs) yield accuracies up to 90 % for the WD task and 83 % for the WI task for uppercase characters. Our baseline implementations together with the rich and publicly available OnHW dataset serve as a baseline for future research in that area.},
	articleno    = 92,
	numpages     = 20,
	keywords     = {writer-(in)dependent, multi-stroke gestures, time-series data, inertial measurement unit, sensor-based pen, embedded, character dataset, Online handwriting recognition}
}
@article{10.1145/3411834,
	title        = {WiBorder: Precise Wi-Fi Based Boundary Sensing via Through-Wall Discrimination},
	author       = {Li, Shengjie and Liu, Zhaopeng and Zhang, Yue and Lv, Qin and Niu, Xiaopeng and Wang, Leye and Zhang, Daqing},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411834},
	url          = {https://doi.org/10.1145/3411834},
	issue_date   = {September 2020},
	abstract     = {Recent research has shown great potential of exploiting Channel State Information (CSI) retrieved from commodity Wi-Fi devices for contactless human sensing in smart homes. Despite much work on Wi-Fi based indoor localization and motion/intrusion detection, no prior solution is capable of detecting a person entering a room with a precise sensing boundary, making room-based services infeasible in the real world. In this paper, we present WiBorder, an innovative technique for accurate determination of Wi-Fi sensing boundary. The key idea is to harness antenna diversity to effectively eliminate random phase shifts while amplifying through-wall amplitude attenuation. By designing a novel sensing metric and correlating it with human's through-wall discrimination, WiBorder is able to precisely determine Wi-Fi sensing boundaries by leveraging walls in our daily environments. To demonstrate the effectiveness of WiBorder, we have developed an intrusion detection system and an area detection system. Extensive results in real-life scenarios show that our intrusion detection system achieves a high detection rate of 99.4% and a low false alarm rate of 0.68%, and the area detection system's accuracy can be as high as 97.03%. To the best of our knowledge, WiBorder is the first work that enables precise sensing boundary determination via through-wall discrimination, which can immediately benefit other Wi-Fi based applications.},
	articleno    = 89,
	numpages     = 30,
	keywords     = {Device-free Sensing, Intrusion Detection, Wi-Fi, Area Detection, Channel State Information (CSI), Sensing boundary}
}
@article{10.1145/3411831,
	title        = {ScraTouch: Extending Interaction Technique Using Fingernail on Unmodified Capacitive Touch Surfaces},
	author       = {Ikematsu, Kaori and Yamanaka, Shota},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411831},
	url          = {https://doi.org/10.1145/3411831},
	issue_date   = {September 2020},
	abstract     = {We present ScraTouch, an interaction technique using fingernails, as a new input modality by leveraging capacitive touch sensing. Differentiating between fingertip and fingernail touches requires only tens of milliseconds worth of shunt current data from unmodified capacitive touch surfaces, thus requires no hardware modification. ScraTouch is simple but practical technique for command invocation and mode switching. An evaluation using a point-and-select task on a touchpad showed that although the switching between the finger and nail in ScraTouch required a little more time compared with the baseline (finger touching without mode switching), in overall the operations, ScraTouch was just as fast as the baseline, and on average, 29 % faster than a long press with 500-ms threshold. We also confirmed that setting a simple threshold on the measured shunt current for recognition works robustly across users (97 % accuracy).},
	articleno    = 81,
	numpages     = 19,
	keywords     = {Touch Interaction, Capacitive Touch Surface, Mobile Computing, Fingernail}
}
@article{10.1145/3411817,
	title        = {Semantic-Aware Spatio-Temporal App Usage Representation via Graph Convolutional Network},
	author       = {Yu, Yue and Xia, Tong and Wang, Huandong and Feng, Jie and Li, Yong},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411817},
	url          = {https://doi.org/10.1145/3411817},
	issue_date   = {September 2020},
	abstract     = {Recent years have witnessed a rapid proliferation of personalized mobile Apps, which poses a pressing need for user experience improvement. A promising solution is to model App usage by learning semantic-aware App usage representations which can capture the relation among time, locations and Apps. However, it is non-trivial due to the complexity, dynamics, and heterogeneity characteristics of App usage. To smooth over these obstacles and achieve the goal, we propose SA-GCN, a novel representation learning model to map Apps, location, and time units into dense embedding vectors considering spatio-temporal characteristics and unit properties simultaneously. To handle complexity and dynamics, we build an App usage graph by regarding App, time, and location units as nodes and their co-occurrence relations as edges. For heterogeneity, we develop a Graph Convolutional Network with meta path-based objective function to combine the structure of the graph and the attribute of units into the semantic-aware representations. We evaluate the performance of SA-GCN via a large-scale real-world dataset. In-depth analysis shows that SA-GCN characterizes the complex relationships among different units and recover meaningful spatio-temporal patterns. Moreover, we make use of the learned representations in App usage prediction task without post-training and achieve 8.3% of the performance gain compared with state-of-the-art baselines.},
	articleno    = 101,
	numpages     = 24,
	keywords     = {Representation Learning, App Usage Modeling, Graph Convolutional Network}
}
@article{10.1145/3411815,
	title        = {Dyadic Mirror: Everyday Second-Person Live-View for Empathetic Reflection upon Parent-Child Interaction},
	author       = {Kim, Wonjung and Lee, Seungchul and Kim, Seonghoon and Jo, Sungbin and Yoo, Chungkuk and Hwang, Inseok and Kang, Seungwoo and Song, Junehwa},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411815},
	url          = {https://doi.org/10.1145/3411815},
	issue_date   = {September 2020},
	abstract     = {A parent's capacity to understand the mental states of both him/herself and the child is considered to play a significant role in various aspects of parent-child relationship-e.g., lowering parental stress and supporting cognitive development of the child. We propose Dyadic Mirror, a wearable smart mirror which is designed to foster the aforementioned parental capacity in everyday parent-child interaction. Its key feature is to provide a parent with a second-person live-view from the child, i.e., the parent's own face as seen by the child, during their face-to-face interaction. Dyadic Mirror serves as a straightforward cue that helps the parent be aware of (1) his/her emotional state, and (2) the way he/she would be now being seen by the child, thereby facilitate the parent to infer the child's mental state. To evaluate Dyadic Mirror under unconstrained parent-child interactions in real-life, we implemented the working prototype of Dyadic Mirror and deployed it to 6 families over 4 weeks. The participating parents reported extensive experiences with Dyadic Mirror, supporting that Dyadic Mirror has helped them be aware of their recurring but unconscious behaviors, understand their children's feelings, reason with the children's behaviors, and find self-driven momenta to better their attitude and expressions towards their children.},
	articleno    = 86,
	numpages     = 29,
	keywords     = {parent-child interaction, reflective functioning, wearable service, perspective-taking, second-person live-view, self-awareness}
}
@article{10.1145/3411814,
	title        = {Virtual Paving: Rendering a Smooth Path for People with Visual Impairment through Vibrotactile and Audio Feedback},
	author       = {Xu, Shuchang and Yang, Ciyuan and Ge, Wenhao and Yu, Chun and Shi, Yuanchun},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411814},
	url          = {https://doi.org/10.1145/3411814},
	issue_date   = {September 2020},
	abstract     = {Tactile pavings are public works for visually impaired people, designed to indicate a particular path to follow by providing haptic cues underfoot. However, they face many limitations such as installation errors, obstructions, degradation, and limited coverage. To address these issues, we propose Virtual Paving, which aims to assist independent navigation by rendering a smooth path to visually impaired people through multi-modal feedback. This work assumes that a path has been planned to avoid obstacles and focuses on the feedback design to guide users along the path safely, smoothly, and efficiently. Firstly, we extracted the design guidelines of Virtual Paving based on an investigation into visually impaired people's current practices and issues with tactile pavings. Next, we developed a multi-modal solution through co-design and evaluation with visually impaired users. This solution included (1) vibrotactile feedback on the shoulders and waist to give readily-perceivable directional cues and (2) audio feedback to describe road conditions ahead of the user. Finally, we evaluated the proposed solution through user tests. Guided by the designed feedback, 16 visually impaired participants successfully completed 127 out of 128 trials with 2.1m-wide basic paths, including straight and curved paths. Subjective feedback indicated that our solution to render Virtual Paving was easy for users to learn, and it also enabled them to walk smoothly. The feasibility and potential limitations for Virtual Paving to support independent navigation in real environments are discussed.},
	articleno    = 99,
	numpages     = 25,
	keywords     = {auditory feedback, path following, visual impairment, accessibility, vibrotactile feedback, tactile paving}
}
@article{10.1145/3411808,
	title        = {Zygarde: Time-Sensitive On-Device Deep Inference and Adaptation on Intermittently-Powered Systems},
	author       = {Islam, Bashima and Nirjon, Shahriar},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411808},
	url          = {https://doi.org/10.1145/3411808},
	issue_date   = {September 2020},
	abstract     = {We propose Zygarde --- which is an energy- and accuracy-aware soft real-time task scheduling framework for batteryless systems that flexibly execute deep learning tasks1 that are suitable for running on microcontrollers. The sporadic nature of harvested energy, resource constraints of the embedded platform, and the computational demand of deep neural networks (DNNs) pose a unique and challenging real-time scheduling problem for which no solutions have been proposed in the literature. We empirically study the problem and model the energy harvesting pattern as well as the trade-off between the accuracy and execution of a DNN. We develop an imprecise computing-based scheduling algorithm that improves the timeliness of DNN tasks on intermittently powered systems. We evaluate Zygarde using four standard datasets as well as by deploying it in six real-life applications involving audio and camera sensor systems. Results show that Zygarde decreases the execution time by up to 26% and schedules 9% -- 34% more tasks with up to 21% higher inference accuracy, compared to traditional schedulers such as the earliest deadline first (EDF).},
	articleno    = 82,
	numpages     = 29,
	keywords     = {Deep Neural Network, On-Device Learning, Real-Time Systems, Energy harvesting System, Batteryless System, On-Device Computation, Semi-Supervised Learning, Intermittent Computing}
}
@article{10.1145/3411807,
	title        = {SUME: Semantic-Enhanced Urban Mobility Network Embedding for User Demographic Inference},
	author       = {Xu, Fengli and Lin, Zongyu and Xia, Tong and Guo, Diansheng and Li, Yong},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411807},
	url          = {https://doi.org/10.1145/3411807},
	issue_date   = {September 2020},
	abstract     = {Recent years have witnessed a rapid proliferation of personalized mobile applications, which poses a pressing need for accurate user demographics inference. Facilitated by the prevalent smart devices, the ubiquitously collected mobility trace presents a promising opportunity to infer user demographics at large-scale. In this paper, we propose a novel Semantic-enhanced Urban Mobility Embedding (SUME) model, which learns dense representation vectors for user demographic inference by jointly modelling the physical mobility patterns and the semantic of urban mobility. Specifically, SUME models urban mobility as a heterogeneous network of users and locations, with various types of edges denoting the physical visitation and semantic similarities. Moreover, SUME optimizes the node representation vectors with two alternating objective functions that preserve the feature in physical and semantic domains, respectively. As a result, it is able to capture the effective signals in the heterogeneous urban mobility network. Empirical experiments on two real-world mobility traces show the proposed model significantly out-performs all state-of-the-art baselines with an accuracy margin of 8.6%~14.3% for occupation, gender, age, education and income inference. In addition, further experiments show SUME is able to reveal meaningful correlations between user demographics and the mobility patterns in spatial, temporal and urban structure domain.},
	articleno    = 98,
	numpages     = 25,
	keywords     = {user profiling, demographic inference, representation learning, human mobility}
}
@article{10.1145/3397323,
	title        = {Adversarial Multi-View Networks for Activity Recognition},
	author       = {Bai, Lei and Yao, Lina and Wang, Xianzhi and Kanhere, Salil S. and Guo, Bin and Yu, Zhiwen},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397323},
	url          = {https://doi.org/10.1145/3397323},
	issue_date   = {June 2020},
	abstract     = {Human activity recognition (HAR) plays an irreplaceable role in various applications and has been a prosperous research topic for years. Recent studies show significant progress in feature extraction (i.e., data representation) using deep learning techniques. However, they face significant challenges in capturing multi-modal spatial-temporal patterns from the sensory data, and they commonly overlook the variants between subjects. We propose a Discriminative Adversarial MUlti-view Network (DAMUN) to address the above issues in sensor-based HAR. We first design a multi-view feature extractor to obtain representations of sensory data streams from temporal, spatial, and spatio-temporal views using convolutional networks. Then, we fuse the multi-view representations into a robust joint representation through a trainable Hadamard fusion module, and finally employ a Siamese adversarial network architecture to decrease the variants between the representations of different subjects. We have conducted extensive experiments under an iterative left-one-subject-out setting on three real-world datasets and demonstrated both the effectiveness and robustness of our approach.},
	articleno    = 42,
	numpages     = 22,
	keywords     = {Adversarial Training, Deep Learning, Multi-view Representation, Activity Recognition}
}
@article{10.1145/3397320,
	title        = {VocalLock: Sensing Vocal Tract for Passphrase-Independent User Authentication Leveraging Acoustic Signals on Smartphones},
	author       = {Lu, Li and Yu, Jiadi and Chen, Yingying and Wang, Yan},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397320},
	url          = {https://doi.org/10.1145/3397320},
	issue_date   = {June 2020},
	abstract     = {Recent years have witnessed the surge of biometric-based user authentication for mobile devices due to its promising security and convenience. As a natural and widely-existed behavior, human speaking has been exploited for user authentication. Existing voice-based user authentication explores the unique characteristics from either the voiceprint or mouth movements, which is vulnerable to replay attacks and mimic attacks. During speaking, the vocal tract, including the static shape and dynamic movements, also exhibits the individual uniqueness, and they are hardly eavesdropped and imitated by adversaries. Hence, our work aims to employ the individual uniqueness of vocal tract to realize user authentication on mobile devices. Moreover, most voice-based user authentications are passphrase-dependent, which significantly degrade the user experience. Thus, such user authentications are pressed to be implemented in a passphrase-independent manner while being able to resist various attacks. In this paper, we propose a user authentication system, VocalLock, which senses the whole vocal tract during speaking to identify different individuals in a passphrase-independent manner on smartphones leveraging acoustic signals. VocalLock first utilizes FMCW on acoustic signals to characterize both the static shape and dynamic movements of the vocal tract during speaking, and then constructs a passphrase-independent user authentication model based on the unique characteristics of vocal tract through GMM-UBM. The proposed VocalLock can resist various spoofing attacks, while achieving a satisfactory user experience. Extensive experiments in real environments demonstrate VocalLock can accurately authenticate user identity in a passphrase-independent manner and successfully resist various attacks.},
	articleno    = 51,
	numpages     = 24,
	keywords     = {acoustic signal, passphrase-independent, FMCW, vocal-tract behavior, User authentication}
}
@article{10.1145/3397313,
	title        = {NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in Free-Living Conditions},
	author       = {Zhang, Shibo and Zhao, Yuqi and Nguyen, Dzung Tri and Xu, Runsheng and Sen, Sougata and Hester, Josiah and Alshurafa, Nabil},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397313},
	url          = {https://doi.org/10.1145/3397313},
	issue_date   = {June 2020},
	abstract     = {We present the design, implementation, and evaluation of a multi-sensor, low-power necklace, NeckSense, for automatically and unobtrusively capturing fine-grained information about an individual's eating activity and eating episodes, across an entire waking day in a naturalistic setting. NeckSense fuses and classifies the proximity of the necklace from the chin, the ambient light, the Lean Forward Angle, and the energy signals to determine chewing sequences, a building block of the eating activity. It then clusters the identified chewing sequences to determine eating episodes. We tested NeckSense on 11 participants with and 9 participants without obesity, across two studies, where we collected more than 470 hours of data in a naturalistic setting. Our results demonstrate that NeckSense enables reliable eating detection for individuals with diverse body mass index (BMI) profiles, across an entire waking day, even in free-living environments. Overall, our system achieves an F1-score of 81.6% in detecting eating episodes in an exploratory study. Moreover, our system can achieve an F1-score of 77.1% for episodes even in an all-day-long free-living setting. With more than 15.8 hours of battery life, NeckSense will allow researchers and dietitians to better understand natural chewing and eating behaviors. In the future, researchers and dietitians can use NeckSense to provide appropriate real-time interventions when an eating episode is detected or when problematic eating is identified.},
	articleno    = 72,
	numpages     = 26,
	keywords     = {neck-worn sensor, automated dietary monitoring, eating activity detection, human activity recognition, wearable, free-living studies, sensor fusion}
}
@article{10.1145/3381012,
	title        = {METIER: A Deep Multi-Task Learning Based Activity and User Recognition Model Using Wearable Sensors},
	author       = {Chen, Ling and Zhang, Yi and Peng, Liangying},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381012},
	url          = {https://doi.org/10.1145/3381012},
	issue_date   = {March 2020},
	abstract     = {Activity recognition (AR) and user recognition (UR) using wearable sensors are two key tasks in ubiquitous and mobile computing. Currently, they still face some challenging problems. For one thing, due to the variations in how users perform activities, the performance of a well-trained AR model typically drops on new users. For another, existing UR models are powerless to activity changes, as there are significant differences between the sensor data in different activity scenarios. To address these problems, we propose METIER (deep multi-task learning based activity and user recognition) model, which solves AR and UR tasks jointly and transfers knowledge across them. User-related knowledge from UR task helps AR task to model user characteristics, and activity-related knowledge from AR task guides UR task to handle activity changes. METIER softly shares parameters between AR and UR networks, and optimizes these two networks jointly. The commonalities and differences across tasks are exploited to promote AR and UR tasks simultaneously. Furthermore, mutual attention mechanism is introduced to enable AR and UR tasks to exploit their knowledge to highlight important features for each other. Experiments are conducted on three public datasets, and the results show that our model can achieve competitive performance on both tasks.},
	articleno    = 5,
	numpages     = 18,
	keywords     = {mutual attention mechanism, Activity recognition, user recognition, multi-task learning}
}
@article{10.1145/3381004,
	title        = {Auto-Key: Using Autoencoder to Speed Up Gait-Based Key Generation in Body Area Networks},
	author       = {Wu, Yuezhong and Lin, Qi and Jia, Hong and Hassan, Mahbub and Hu, Wen},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381004},
	url          = {https://doi.org/10.1145/3381004},
	issue_date   = {March 2020},
	abstract     = {With the rising popularity of wearable devices and sensors, shielding Body Area Networks (BANs) from eavesdroppers has become an urgent problem to solve. Since the conventional key distribution systems are too onerous for resource-constrained wearable sensors, researchers are pursuing a new light-weight key generation approach that enables two wearable devices attached at different locations of the user body to generate an identical key simultaneously simply from their independent observations of user gait. A key challenge for such gait-based key generation lies in matching the bits of the keys generated by independent devices despite the noisy sensor measurements, especially when the devices are located far apart on the body affected by different sources of noise. To address the challenge, we propose a novel machine learning framework, called Auto-Key, that uses an autoencoder to help one device predict the gait observations at another distant device attached to the same body and generate the key using the predicted sensor data. We prototype the proposed method and evaluate it using a public acceleration dataset collected from 15 real subjects wearing accelerometers attached to seven different locations of the body. Our results show that, on average, Auto-Key increases the matching rate of independently generated bits from two sensors attached at two different locations by 16.5%, which speeds up the successful generation of fully-matching symmetric keys at independent wearable sensors by a factor of 1.9. In the proposed framework, a subject-specific model can be trained with 50% fewer data and 88% less time by retraining a pre-trained general model when compared to training a new model from scratch. The reduced training complexity makes Auto-Key more practical for edge computing, which provides better privacy protection to biometric and behavioral data compared to cloud-based training.},
	articleno    = 32,
	numpages     = 23,
	keywords     = {Autoencoder, Body Sensor Networks, Body Area Networks, Biometric Key Generation, Wearable Communications, Machine Learning, Transfer learning, Device Pairing, Autonomic Symmetric Key Generation}
}
@article{10.1145/3381003,
	title        = {FairCharge: A Data-Driven Fairness-Aware Charging Recommendation System for Large-Scale Electric Taxi Fleets},
	author       = {Wang, Guang and Zhang, Yongfeng and Fang, Zhihan and Wang, Shuai and Zhang, Fan and Zhang, Desheng},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381003},
	url          = {https://doi.org/10.1145/3381003},
	issue_date   = {March 2020},
	abstract     = {Our society is witnessing a rapid taxi electrification process. Compared to conventional gas taxis, a key drawback of electric taxis is their prolonged charging time, which potentially reduces drivers' daily operation time and income. In addition, insufficient charging stations, intensive charging peaks, and heuristic-based charging station choice of drivers also significantly decrease the charging efficiency of electric taxi charging networks. To improve the charging efficiency (e.g., reduce queuing time in stations) of electric taxi charging networks, in this paper, we design a fairness-aware Pareto efficient charging recommendation system called FairCharge, which aims to minimize the total charging idle time (traveling time + queuing time) in a fleet-oriented fashion combined with fairness constraints. Different from existing works, FairCharge considers fairness as a constraint to potentially achieve long-term social benefits. In addition, our FairCharge considers not only current charging requests, but also possible charging requests of other nearby electric taxis in a near-future duration. More importantly, we simulate and evaluate FairCharge with real-world streaming data from the Chinese city Shenzhen, including GPS data and transaction data from more than 16,400 electric taxis, coupled with the data of 117 charging stations, which constitute, to our knowledge, the largest electric taxi network in the world. The extensive experimental results show that our fairness-aware FairCharge effectively reduces queuing time and idle time of the Shenzhen electric taxi fleet by 80.2% and 67.7%, simultaneously.},
	articleno    = 28,
	numpages     = 25,
	keywords     = {Electric taxi, Pareto efficiency, charging recommendation, recommendation system, fairness-aware}
}
@article{10.1145/3381001,
	title        = {Predicting Brain Functional Connectivity Using Mobile Sensing},
	author       = {Obuchi, Mikio and Huckins, Jeremy F. and Wang, Weichen and daSilva, Alex and Rogers, Courtney and Murphy, Eilis and Hedlund, Elin and Holtzheimer, Paul and Mirjafari, Shayan and Campbell, Andrew},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381001},
	url          = {https://doi.org/10.1145/3381001},
	issue_date   = {March 2020},
	abstract     = {Brain circuit functioning and connectivity between specific regions allow us to learn, remember, recognize and think as humans. In this paper, we ask the question if mobile sensing from phones can predict brain functional connectivity. We study the brain resting-state functional connectivity (RSFC) between the ventromedial prefrontal cortex (vmPFC) and the amygdala, which has been shown by neuroscientists to be associated with mental illness such as anxiety and depression. We discuss initial results and insights from the NeuroSence study, an exploratory study of 105 first year college students using neuroimaging and mobile sensing across one semester. We observe correlations between several behavioral features from students' mobile phones and connectivity between vmPFC and amygdala, including conversation duration (r=0.365, p&lt;0.001), sleep onset time (r=0.299, p&lt;0.001) and the number of phone unlocks (r=0.253, p=0.029). We use a support vector classifier and 10-fold cross validation and show that we can classify whether students have higher (i.e., stronger) or lower (i.e., weaker) vmPFC-amygdala RSFC purely based on mobile sensing data with an F1 score of 0.793. To the best of our knowledge, this is the first paper to report that resting-state brain functional connectivity can be predicted using passive sensing data from mobile phones.},
	articleno    = 23,
	numpages     = 22,
	keywords     = {Neuroscience, Brain Imaging, Mobile Sensing}
}
@article{10.1145/3380992,
	title        = {CARIN: Wireless CSI-Based Driver Activity Recognition under the Interference of Passengers},
	author       = {Bai, Yunhao and Wang, Xiaorui},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3380992},
	url          = {https://doi.org/10.1145/3380992},
	issue_date   = {March 2020},
	abstract     = {Recent studies have proposed to use the Channel State Information (CSI) of WiFi wireless channel for human gesture recognition. As an important application, CSI-based driver activity recognition in passenger vehicles has received increasing research attention. However, a serious limitation of almost all the existing WiFi-based recognition solutions is that they can only recognize the activity of a single person at a time, because the activities of other people (if performed at the same time) can interfere with the WiFi signals. In a sharp contrast, there can often be one or more passengers in any vehicles.In this paper, we propose CARIN, CSI-based driver Activity Recognition under the INterference of passengers. CARIN features a combination-based solution that profiles all the possible activity combinations of driver and (one or more) passengers in offline training and then performs recognition online. To attack possible combination explosion, we first leverage in-car pressure sensors to significantly reduce combinations, because there are only limited seating options in a passenger vehicle. We then formulate a distance minimization problem for fast runtime recognition. In addition, a period analysis methodology is designed based on the kNN classifier to recognize activities that have a sequence of body movements, like continuous head nodding due to driver fatigue. Our results in a real car with 3,000 real-world traces show that CARIN can achieve an overall F1 score of 90.9%, and outperforms the three state-of-the-art solutions by 32.2%.},
	articleno    = 2,
	numpages     = 28,
	keywords     = {Wireless Sensing, Activity Recognition, ADAS}
}
@article{10.1145/3351280,
	title        = {WiDetect: Robust Motion Detection with a Statistical Electromagnetic Model},
	author       = {Zhang, Feng and Wu, Chenshu and Wang, Beibei and Lai, Hung-Quoc and Han, Yi and Liu, K. J. Ray},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351280},
	url          = {https://doi.org/10.1145/3351280},
	issue_date   = {September 2019},
	abstract     = {Motion detection acts as a key component for a range of applications such as home security, occupancy and activity monitoring, retail analytics, etc. Most existing solutions, however, require special installation and calibration and suffer from frequent false alarms with very limited coverage. In this paper, we propose WiDetect, a highly accurate, robust, and calibration-free wireless motion detector that achieves almost zero false alarm rate and large through-the-wall coverage. Different from previous approaches that either extract data-driven features or assume a few reflection multipaths, we model the problem from a perspective of statistical electromagnetic (EM) by accounting for all multipaths indoors. By exploiting the statistical theory of EM waves, we establish a connection between the autocorrelation function of the physical layer channel state information (CSI) and target motion in the environment. On this basis, we devise a novel motion statistic that is independent of environment, location, orientation, and subjects, and then perform a hypothesis testing for motion detection. By harnessing abundant multipaths indoors, WiDetect can detect arbitrary motion, be it in Line-Of-Sight vicinity or behind multiple walls, providing sufficient whole-home coverage for typical apartments and houses using a single link on commodity WiFi. We conduct extensive experiments in a typical office, an apartment, and a single house with different users for an overall period of more than 5 weeks. The results show that WiDetect achieves a remarkable detection accuracy of 99.68% with a zero false rate, significantly outperforming the state-of-the-art solutions and setting up the stage for ubiquitous motion sensing in practice.},
	articleno    = 122,
	numpages     = 24,
	keywords     = {Statistical EM wave theory, Motion detection, WiFi sensing}
}
@article{10.1145/3351264,
	title        = {Tracking Fatigue and Health State in Multiple Sclerosis Patients Using Connnected Wellness Devices},
	author       = {Tong, Catherine and Craner, Matthew and Vegreville, Matthieu and Lane, Nicholas D.},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351264},
	url          = {https://doi.org/10.1145/3351264},
	issue_date   = {September 2019},
	abstract     = {Multiple Sclerosis requires long-term disease management, but tracking patients through the use of clinical surveys is hindered by high costs and patient burden. In this work, we investigate the feasibility of using data from ubiquitous sensing to predict MS patients' fatigue and health status, as measured by the Fatigue Severity Scale (FSS) and EQ-5D index. We collected data from 198 MS patients who are given connected wellness devices for over 6 months. We examine how accurately can the collected data predict reported FSS and EQ-5D scores per patient using an ensemble of regressors. In predicting for both FSS and EQ-5D, we are able to achieve errors aligning with the instrument' standard measurement error (SEM), as well as strong and significant correlations between predicted and ground truth values. We also show a simple adaptation method that greatly reduces prediction errors through the use of just 1 user-supplied ground truth datapoint. For FSS (SEM 0.7), the universal model predicts weekly scores with MAE 1.00, while an adapted model predicts with MAE 0.58. For EQ-5D (SEM 0.093), the universal model predicts weekly scores with MAE 0.097, while an adapted model predicts with MAE 0.065. Our study represents the first sets of results showing that fatigue and health state of MS patients can be measured using data from connected wellness devices and a small number of background features, with promising prediction performance with errors within the accepted range of error in the widely used clinically-validated questionnaires. Future extensions and potential applications of our results can positively impact MS patient disease management and support clinical research.},
	articleno    = 106,
	numpages     = 19,
	keywords     = {Machine Learning, Connected devices, Health data analysis, Multimodal prediction}
}
@article{10.1145/3351263,
	title        = {Enhancing Augmented VR Interaction via Egocentric Scene Analysis},
	author       = {Tian, Yang and Fu, Chi-Wing and Zhao, Shengdong and Li, Ruihui and Tang, Xiao and Hu, Xiaowei and Heng, Pheng-Ann},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351263},
	url          = {https://doi.org/10.1145/3351263},
	issue_date   = {September 2019},
	abstract     = {Augmented virtual reality (AVR) takes portions of the physical world into the VR world to enable VR users to access physical objects. State-of-the-art solutions mainly focus on extracting and showing physical objects in the VR world. In this work, we go beyond previous solutions and propose a novel approach to realize AVR. We first analyze the physical environment in the user's egocentric view through depth sensing and deep learning, then acquire the layout and geometry of the surrounding objects, and further explore their affordances. Based on the above information, we create visual guidance (hollowed guiding path) and hybrid user interfaces (augmented physical notepad, LR finger slider, and LRRL finger slider) to augment the AVR interaction. Empirical evaluations showed that the participants responded positively to our AVR techniques.},
	articleno    = 105,
	numpages     = 24,
	keywords     = {depth sensing, virtual reality, visual tool, scene analysis, visual guidance, deep learning, egocentric view, augmented VR interaction}
}
@article{10.1145/3351261,
	title        = {VLA: A Practical Visible Light-Based Attack on Face Recognition Systems in Physical World},
	author       = {Shen, Meng and Liao, Zelin and Zhu, Liehuang and Xu, Ke and Du, Xiaojiang},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351261},
	url          = {https://doi.org/10.1145/3351261},
	issue_date   = {September 2019},
	abstract     = {Adversarial example attacks have become a growing menace to neural network-based face recognition systems. Generated by composing facial images with pixel-level perturbations, adversarial examples change key features of inputs and thereby lead to misclassification of neural networks. However, the perturbation loss caused by complex physical environments sometimes prevents existing attack methods from taking effect.In this paper, we focus on designing new attacks that are effective and inconspicuous in the physical world. Motivated by the differences in image-forming principles between cameras and human eyes, we propose VLA, a novel attack against black-box face recognition systems using visible light. In VLA, visible light-based adversarial perturbations are crafted and projected on human faces, which allows an adversary to conduct targeted or un-targeted attacks. VLA decomposes adversarial perturbations into a perturbation frame and a concealing frame, where the former adds modifications on human facial images while the latter makes these modifications inconspicuous to human eyes. We conduct extensive experiments to demonstrate the effectiveness, inconspicuousness, and robustness of the adversarial examples crafted by VLA in physical scenarios.},
	articleno    = 103,
	numpages     = 19,
	keywords     = {visible light attack, adversarial example}
}
@article{10.1145/3351259,
	title        = {Two Tell-Tale Perspectives of PTSD: Neurobiological Abnormalities and Bayesian Regulatory Network of the Underlying Disorder in a Refugee Context},
	author       = {Shahid, Farhana and Rahman, Wasifur and Islam, Anika Binte and Paul, Nipi and Khan, Nabila and Rahman, Mohammad Saifur and Haque, Md Munirul and Al Islam, A. B. M. Alim},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351259},
	url          = {https://doi.org/10.1145/3351259},
	issue_date   = {September 2019},
	abstract     = {Global refugee crisis around the world has displaced millions of people from their homes. Although some of them adjust well, many suffer from significant psychological distress, such as post-traumatic stress disorder (PTSD), owing to exposure to traumatic events and hardships. Here, diagnosis and access to psychological health care present particular challenges for various human-centered design issues. Therefore, analyzing the case of Rohingya refugees in Bangladesh, we propose a two-way diagnosis of PTSD using (i) short inexpensive questionnaire to determine its prevalence, and (ii) low-cost portable EEG headset to identify potential neurobiological markers of PTSD. To the best of our knowledge, this study is the first to use consumer-grade EEG devices in the scarce-resource settings of refugees. Moreover, we explored the underlying structure of PTSD and its symptoms via developing various hybrid models based on Bayesian inference by combining aspects from both reflective and formative models of PTSD, which is also the first of its kind. Our findings revealed several key components of PTSD and its neurobiological abnormality. Moreover, challenges faced during our study would inform design processes of screening tools and treatments of PTSD to incorporate refugee experience in a more meaningful way during contemporary and future humanitarian crisis.},
	articleno    = 101,
	numpages     = 45,
	keywords     = {Correlation, Refugee mental heath, Bayesian networks, EEG power}
}
@article{10.1145/3351252,
	title        = {Quadmetric Optimized Thumb-to-Finger Interaction for Force Assisted One-Handed Text Entry on Mobile Headsets},
	author       = {Lee, Lik Hang and Lam, Kit Yung and Li, Tong and Braud, Tristan and Su, Xiang and Hui, Pan},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351252},
	url          = {https://doi.org/10.1145/3351252},
	issue_date   = {September 2019},
	abstract     = {Augmented reality head-worn computers often feature small-sized touch interfaces that complicate interaction with content, provide insufficient space for comfortable text input, and can be awkward to use in social situations. This paper presents a novel one-handed thumb-to-finger text entry solution for augmented reality head-worn computers. We design a glove composed of 12 force-sensitive nodes featuring an ambiguous keyboard layout. We first explore the viability of force disambiguation to evaluate the force division within the force spectrum. We select a 3-level force division as it allows to considerably reduce the number of keys while featuring a high (83.9%) accuracy. Following this pilot study, we map the 26 English characters onto the 9 nodes located on the index, middle and ring fingers in a 3-3-3 configuration, and attribute the space, enter and backspace keys to the remaining three nodes. We consider text entry performance as a quadmetric optimization problem considering the following criteria: goodness of character pairs, layout similarity to the QWERTY keyboard, easiness of force interaction, and comfort level of thumb reach. The resulting layout strikes a balance between performance and usability. We finally evaluate the quadmetric optimized layout over 6 sessions with 12 participants. The participants achieve an average text entry rate of 6.47 WPM with 6.85% error rate in the final session, which is significantly faster than existing thumb-to-finger solutions. In addition, our one-handed text entry system enhances the user mobility compared to other state-of-the-art solutions by freeing one hand, while allowing the user to direct his visual attention to other activities.},
	articleno    = 94,
	numpages     = 27,
	keywords     = {Thumb-to-finger interaction, Text input, Smart wearable, Human-computer interaction}
}
@article{10.1145/3351239,
	title        = {EarEcho: Using Ear Canal Echo for Wearable Authentication},
	author       = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351239},
	url          = {https://doi.org/10.1145/3351239},
	issue_date   = {September 2019},
	abstract     = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
	articleno    = 81,
	numpages     = 24,
	keywords     = {authentication, echo, ear canal, Acoustic, biometric, wearable devices}
}
@article{10.1145/3351235,
	title        = {ProspecFit: In Situ Evaluation of Digital Prospective Memory Training for Older Adults},
	author       = {Chan, Samantha W. T. and Buddhika, Thisum and Zhang, Haimo and Nanayakkara, Suranga},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351235},
	url          = {https://doi.org/10.1145/3351235},
	issue_date   = {September 2019},
	abstract     = {Prospective Memory (PM), which involves remembering to perform intended actions, is the primary source of everyday memory lapses. While existing solutions mostly focus on supportive memory aids and reminders, it is also crucial to maintain PM functions and independent living for older adults. We present ProspecFit, which digitises implementation intentions, a lab-based memory intervention, making it available on smartphones through iterative design that draws insights from a focus group and preliminary studies. We evaluated its usability and effectiveness in enhancing PM through user studies that included a 12-day in situ study, and pre- and post-testing with 10 adults (61 to 80 years old). Participants in the digital PM training group were more prompt in performing the in situ PM tasks, compared to the control group without digital training, and reported improvement in their PM compared to before the training. We also show findings from diary entries, reports on forgetful moments and user reactions. Our work provides implications for creating digital memory training tools in HCI.},
	articleno    = 77,
	numpages     = 20,
	keywords     = {In Situ Study, Memory Training, Prospective Memory}
}
@article{10.1145/3351233,
	title        = {Prediction of Mood Instability with Passive Sensing},
	author       = {Morshed, Mehrab Bin and Saha, Koustuv and Li, Richard and D'Mello, Sidney K. and De Choudhury, Munmun and Abowd, Gregory D. and Pl\"{o}tz, Thomas},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351233},
	url          = {https://doi.org/10.1145/3351233},
	issue_date   = {September 2019},
	abstract     = {Mental health issues, which can be difficult to diagnose, are a growing concern worldwide. For effective care and support, early detection of mood-related health concerns is of paramount importance. Typically, survey based instruments including Ecologically Momentary Assessments (EMA) and Day Reconstruction Method (DRM) are the method of choice for assessing mood related health. While effective, these methods require some effort and thus both compliance rates as well as quality of responses can be limited. As an alternative, We present a study that used passively sensed data from smartphones and wearables and machine learning techniques to predict mood instabilities, an important aspect of mental health. We explored the effectiveness of the proposed method on two large-scale datasets, finding that as little as three weeks of continuous, passive recordings were sufficient to reliably predict mood instabilities.},
	articleno    = 75,
	numpages     = 21,
	keywords     = {Wearable Sensing, Mood Instability, Early Intervention, Smartphone Sensing, Mental Health}
}
@article{10.1145/3351231,
	title        = {Mobile Gait Analysis Using Foot-Mounted UWB Sensors},
	author       = {Anderson, Boyd and Shi, Mingqian and Tan, Vincent Y. F. and Wang, Ye},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351231},
	url          = {https://doi.org/10.1145/3351231},
	issue_date   = {September 2019},
	abstract     = {We demonstrate a new foot-mounted sensor system for mobile gait analysis which is based on Ultra Wideband (UWB) technology. Our system is wireless, inexpensive, portable, and able to estimate clinical measurements that are not currently available in traditional Inertial Measurement Unit (IMU) based wearables such as step width and foot positioning. We collect a dataset of over 2000 steps across 21 people to test our system in comparison with the clinical gold-standard GAITRite, and other IMU-based algorithms. We propose methods to calculate gait metrics from the UWB data that our system collects. Our system is then validated against the GAITRite mat, measuring step width, step length, and step time with mean absolute errors of 0.033m, 0.032m, and 0.012s respectively. This system has the potential for use in many fields including sports medicine, neurological diagnostics, fall risk assessment, and monitoring of the elderly.},
	articleno    = 73,
	numpages     = 22,
	keywords     = {Inertial Measurement Units, Ultra Wideband, Gait Measurement}
}
@article{10.1145/3351228,
	title        = {Leveraging Active Learning and Conditional Mutual Information to Minimize Data Annotation in Human Activity Recognition},
	author       = {Adaimi, Rebecca and Thomaz, Edison},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351228},
	url          = {https://doi.org/10.1145/3351228},
	issue_date   = {September 2019},
	abstract     = {A difficulty in human activity recognition (HAR) with wearable sensors is the acquisition of large amounts of annotated data for training models using supervised learning approaches. While collecting raw sensor data has been made easier with advances in mobile sensing and computing, the process of data annotation remains a time-consuming and onerous process. This paper explores active learning as a way to minimize the labor-intensive task of labeling data. We train models with active learning in both offline and online settings with data from 4 publicly available activity recognition datasets and show that it performs comparably to or better than supervised methods while using around 10% of the training data. Moreover, we introduce a method based on conditional mutual information for determining when to stop the active learning process while maximizing recognition performance. This is an important issue that arises in practice when applying active learning to unlabeled datasets.},
	articleno    = 70,
	numpages     = 23,
	keywords     = {Stopping Criterion, Human Activity Recognition, Data Annotation, Active Learning, Conditional Mutual Information}
}
@article{10.1145/3351227,
	title        = {Classifying Attention Types with Thermal Imaging and Eye Tracking},
	author       = {Abdelrahman, Yomna and Khan, Anam Ahmad and Newn, Joshua and Velloso, Eduardo and Safwat, Sherine Ashraf and Bailey, James and Bulling, Andreas and Vetere, Frank and Schmidt, Albrecht},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351227},
	url          = {https://doi.org/10.1145/3351227},
	issue_date   = {September 2019},
	abstract     = {Despite the importance of attention in user performance, current methods for attention classification do not allow to discriminate between different attention types. We propose a novel method that combines thermal imaging and eye tracking to unobtrusively classify four types of attention: sustained, alternating, selective, and divided. We collected a data set in which we stimulate these four attention types in a user study (N = 22) using combinations of audio and visual stimuli while measuring users' facial temperature and eye movement. Using a Logistic Regression on features extracted from both sensing technologies, we can classify the four attention types with high AUC scores up to 75.7% for the user independent-condition independent, 87% for the user-independent-condition dependent, and 77.4% for the user-dependent prediction. Our findings not only demonstrate the potential of thermal imaging and eye tracking for unobtrusive classification of different attention types but also pave the way for novel applications for attentive user interfaces and attention-aware computing.},
	articleno    = 69,
	numpages     = 27,
	keywords     = {Eye Tracking, Thermal Imaging, Attention Classification, Attention Types}
}
@article{10.1145/3328937,
	title        = {MegaLight: Learning-Based Color Adaptation for Barcode Stream Recognition over Screen-Camera Links},
	author       = {Zhan, Tong and Li, Wenzhong and Chen, Xu and Lu, Sanglu},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328937},
	url          = {https://doi.org/10.1145/3328937},
	issue_date   = {June 2019},
	abstract     = {Screen-camera communication using dynamic barcode streaming has emerged as a convenient and secure method for short-range, impromptu device-to-device communication. Conventional dynamic barcode systems adopt a rule-based approach to recognize the color barcode stream in the receiver, which is empirical, inflexible, and lacks self-adaptiveness. In this paper, we propose a novel solution framework for color barcode stream recognition basing on machine learning techniques. By including a number of training frames into the barcode stream to build a classification model, the proposed framework can achieve high accuracy in color barcode recognition and is adaptive to different ambient lighting conditions (without sudden changes during transmission). A semi-supervised learning approach basing on the Mixture of Experts (MoE) model is further proposed to reduce the start-up time. We implement MegaLight on both black-white and color barcode systems. Extensive experiments demonstrate that MegaLight can significantly reduce the frame demodulation error and reach up to 3x improvement in system goodput comparing to conventional barcode stream recognition approaches.},
	articleno    = 66,
	numpages     = 23,
	keywords     = {color barcode stream, Screen-camera communication, machine learning}
}
@article{10.1145/3328930,
	title        = {Drinks &amp; Crowds: Characterizing Alcohol Consumption through Crowdsensing and Social Media},
	author       = {Phan, Thanh-Trung and Muralidhar, Skanda and Gatica-Perez, Daniel},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328930},
	url          = {https://doi.org/10.1145/3328930},
	issue_date   = {June 2019},
	abstract     = {The design of computational methods to recognize alcohol intake is a relevant problem in ubiquitous computing. While mobile crowdsensing and social media analytics are two current approaches to characterize alcohol consumption in everyday life, the question of how they can be integrated, to examine their relative value as informative of the drinking phenomenon and to exploit their complementarity towards the classification of drinking-related attributes, remains as an open issue. In this paper, we present a comparative study based on five years of Instagram data about alcohol consumption and a 200+ person crowdsensing campaign collected in the same country (Switzerland). Our contributions are two-fold. First, we conduct data analyses that uncover temporal, spatial, and social contextual patterns of alcohol consumption on weekend nights as represented by both crowdsensing and social media. This comparative analysis provides a contextual snapshot of the alcohol drinking practices of urban youth dwellers. Second, we use a machine learning framework to classify individual drinking events according to alcohol and non-alcohol categories, using images features and contextual cues from individual and joint data sources. Our best performing models give an accuracy of 82.3% on alcohol category classification (against a baseline of 48.5%) and 90% on alcohol/non-alcohol classification (against a baseline of 65.9%) using a fusion of image features and contextual cues in this task. Our work uncovers important patterns in drinking behaviour across these two datasets and the results of study are promising towards developing systems that use machine learning for self-monitoring of alcohol consumption.},
	articleno    = 59,
	numpages     = 30,
	keywords     = {context, nightlife, youth, mobile crowdsensing, social media}
}
@article{10.1145/3328913,
	title        = {MAC: Measuring the Impacts of Anomalies on Travel Time of Multiple Transportation Systems},
	author       = {Fang, Zhihan and Yang, Yu and Wang, Shuai and Fu, Boyang and Song, Zixing and Zhang, Fan and Zhang, Desheng},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328913},
	url          = {https://doi.org/10.1145/3328913},
	issue_date   = {June 2019},
	abstract     = {Urban anomalies have a large impact on passengers' travel behavior and city infrastructures, which can cause uncertainty on travel time estimation. Understanding the impact of urban anomalies on travel time is of great value for various applications such as urban planning, human mobility studies and navigation systems. Most existing studies on travel time have been focused on the total riding time between two locations on an individual transportation modality. However, passengers often take different modes of transportation, e.g., taxis, subways, buses or private vehicles, and a significant portion of the travel time is spent in the uncertain waiting. In this paper, we study the fine-grained travel time patterns in multiple transportation systems under the impact of urban anomalies. Specifically, (i) we investigate implicit components, including waiting and riding time, in multiple transportation systems; (ii) we measure the impact of real-world anomalies on travel time components; (iii) we design a learning-based model for travel time component prediction with anomalies. Different from existing studies, we implement and evaluate our measurement framework on multiple data sources including four city-scale transportation systems, which are (i) a 14-thousand taxicab network, (ii) a 13-thousand bus network, (iii) a 10-thousand private vehicle network, and (iv) an automatic fare collection system for a public transit network (i.e., subway and bus) with 5 million smart cards.},
	articleno    = 42,
	numpages     = 24,
	keywords     = {travel time components, anomalies, cyber physical systems}
}
@article{10.1145/3328909,
	title        = {RAMT: Real-Time Attitude and Motion Tracking for Mobile Devices in Moving Vehicle},
	author       = {Bi, Chongguang and Xing, Guoliang},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328909},
	url          = {https://doi.org/10.1145/3328909},
	issue_date   = {June 2019},
	abstract     = {Recently a class of new in-vehicle technologies based on off-the-shelf mobile devices have been developed to improve driving safety and experience. For instance, wearables like the smartwatches are utilized to monitor the action of the driver and detect possible secondary tasks. Moreover, wearables can allow a driver to use gesture for in-vehicle controls, reducing distractions to driving. The accuracy of these systems can be significantly improved by tracking the real-time attitude of mobile devices. This paper proposes a novel system called Real-time Attitude and Motion Tracking (RAMT) that can enable a mobile device to accurately learn the coordinate system of a moving vehicle, and hence track its attitude and motion in real time. RAMT consists of a series of lightweight algorithms to sense the vehicle's movement and calculate the device's attitude. It provides a solution for trajectory-based gesture recognition. We have implemented RAMT on a smartphone and a smartwatch and evaluated the performance in 10 real driving trips. Our results show that the overall error of the coordinate system alignment is around 5° for the smartphone and 10° for the smartwatch, and over 84% of customized hand gestures can be accurately recognized with the result of RAMT. A video demo of RAMT is available at https://youtu.be/9rZp7HxyRts.},
	articleno    = 38,
	numpages     = 21,
	keywords     = {motion tracking, driving, mobile devices}
}
@article{10.1145/3328906,
	title        = {Scaling Crowdsourcing with Mobile Workforce: A Case Study with Belgian Postal Service},
	author       = {Acer, Utku G\"{u}nay and Broeck, Marc van den and Forlivesi, Claudio and Heller, Florian and Kawsar, Fahim},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328906},
	url          = {https://doi.org/10.1145/3328906},
	issue_date   = {June 2019},
	abstract     = {Traditional urban-scale crowdsourcing approaches suffer from three caveats - lack of complete spatiotemporal coverage, lack of accurate information and lack of sustained engagement of crowd workers. In this paper, we argue that these caveats can be addressed by embedding crowdsourcing tasks into the daily routine of mobile workforces that roam around an urban area. As a use case, we take the bpost who deliver the letters and parcels to the citizens across entire Belgium. We present a study that explores the behavioural attributes of these mobile postal workers both quantitatively (6.3K) and qualitatively (6) to assess the opportunity of leveraging them for crowdsourcing tasks. We report their mobility pattern, workflow, and behavioural traits which collectively inform the design of a purpose-built crowdsourcing solution. In particular, our solution operates on two key techniques - route augmentation, and on-wearable interruptibility management. Together, these mechanisms enhance the spatial coverage, response accuracy and increase workers' engagement with crowdsourcing tasks. We describe these principal components in a wearable smartwatch application supported by a data management infrastructure. Finally, we report a first-of-its-kind real-world trial with ten postal workers for two weeks to assess the quality of road signs at the city centre of Antwerp. Our findings suggest that our solution was effective in achieving 89% spatial coverage and increasing response rate (83.6%) and accuracy (100%) of the crowdsourcing tasks. Although limited in scale, these and the rest of our findings highlight the way of building an efficient and purposeful crowdsourcing solution of the future.},
	articleno    = 35,
	numpages     = 32,
	keywords     = {interruptibility, wearable computing, behaviour modelling, mobile crowdsourcing}
}
@article{10.1145/3314421,
	title        = {GEVR: An Event Venue Recommendation System for Groups of Mobile Users},
	author       = {Zhang, Jason Shuo and Gartrell, Mike and Han, Richard and Lv, Qin and Mishra, Shivakant},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314421},
	url          = {https://doi.org/10.1145/3314421},
	issue_date   = {March 2019},
	abstract     = {In this paper, we present GEVR, the first Group Event Venue Recommendation system that incorporates mobility via individual location traces and context information into a "social-based" group decision model to provide venue recommendations for groups of mobile users. Our study leverages a real-world dataset collected using the OutWithFriendz mobile app for group event planning, which contains 625 users and over 500 group events. We first develop a novel "social-based" group location prediction model, which adaptively applies different group decision strategies to groups with different social relationship strength to aggregate each group member's location preference, to predict where groups will meet. Evaluation results show that our prediction model not only outperforms commonly used and state-of-the-art group decision strategies with over 80% accuracy for predicting groups' final meeting location clusters, but also provides promising qualities in cold-start scenarios. We then integrate our prediction model with the Foursquare Venue Recommendation API to construct an event venue recommendation framework for groups of mobile users. Evaluation results show that GEVR outperforms the comparative models by a significant margin.},
	articleno    = 34,
	numpages     = 25,
	keywords     = {Event Venue, Group Location Cluster, User Mobility, Group Recommendation}
}
@article{10.1145/3314404,
	title        = {Audio-Based Activities of Daily Living (ADL) Recognition with Large-Scale Acoustic Embeddings from Online Videos},
	author       = {Liang, Dawei and Thomaz, Edison},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314404},
	url          = {https://doi.org/10.1145/3314404},
	issue_date   = {March 2019},
	abstract     = {Over the years, activity sensing and recognition has been shown to play a key enabling role in a wide range of applications, from sustainability and human-computer interaction to health care. While many recognition tasks have traditionally employed inertial sensors, acoustic-based methods offer the benefit of capturing rich contextual information, which can be useful when discriminating complex activities. Given the emergence of deep learning techniques and leveraging new, large-scale multimedia datasets, this paper revisits the opportunity of training audio-based classifiers without the onerous and time-consuming task of annotating audio data. We propose a framework for audio-based activity recognition that can make use of millions of embedding features from public online video sound clips. Based on the combination of oversampling and deep learning approaches, our framework does not require further feature processing or outliers filtering as in prior work. We evaluated our approach in the context of Activities of Daily Living (ADL) by recognizing 15 everyday activities with 14 participants in their own homes, achieving 64.2% and 83.6% averaged within-subject accuracy in terms of top-1 and top-3 classification respectively. Individual class performance was also examined in the paper to further study the co-occurrence characteristics of the activities and the robustness of the framework.},
	articleno    = 17,
	numpages     = 18,
	keywords     = {Audio Processing, Multi-Class Classification, Activity Recognition, Deep Learning}
}
@article{10.1145/3314400,
	title        = {Using Unobtrusive Wearable Sensors to Measure the Physiological Synchrony Between Presenters and Audience Members},
	author       = {Gashi, Shkurta and Di Lascio, Elena and Santini, Silvia},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314400},
	url          = {https://doi.org/10.1145/3314400},
	issue_date   = {March 2019},
	abstract     = {The widespread adoption of mobile and wearable devices enables new approaches for the unobtrusive and continuous monitoring of humans' behavior, physiological state, interactions and more. Within this line of research, we focus on the physiological synchrony between a presenter and her audience and investigate whether it can be used to characterize the experience of presenters and audience members during presentations. To this end, we collect data from 17 presenters and six audience members during a two-days conference. For 40, unique presenter-audience pairs we gather electrodermal activity (EDA) signals and self-reports on different aspects of the experience: engagement, immersion and enjoyment/satisfaction. For 28 of these pairs, we also collect inter-beat interval (IBI) traces. We then apply seven approaches for measuring the synchrony of physiological signals and we contextualize these measures using metrics derived from the self-reports. We find that physiological synchrony -- measured using the Dynamic Time Warping algorithm -- can be used as a proxy to quantify participants' agreement on self-reported engagement. Our findings can be used to provide automated presenter-audience feedback in a conference setting and may be applicable in other scenarios, including education (teacher-student), arts (performer-audience), or meetings (presenter-audience).},
	articleno    = 13,
	numpages     = 19,
	keywords     = {Sensors, Presenter, Electrodermal Activity, Wearable, Physiological Synchrony, Audience}
}
@article{10.1145/3314397,
	title        = {Perils of Zero-Interaction Security in the Internet of Things},
	author       = {Fomichev, Mikhail and Maass, Max and Almon, Lars and Molina, Alejandro and Hollick, Matthias},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314397},
	url          = {https://doi.org/10.1145/3314397},
	issue_date   = {March 2019},
	abstract     = {The Internet of Things (IoT) demands authentication systems which can provide both security and usability. Recent research utilizes the rich sensing capabilities of smart devices to build security schemes operating without human interaction, such as zero-interaction pairing (ZIP) and zero-interaction authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and reported promising results. However, those schemes were often evaluated under conditions which do not reflect realistic IoT scenarios. In addition, drawing any comparison among the existing schemes is impossible due to the lack of a common public dataset and unavailability of scheme implementations.In this paper, we address these challenges by conducting the first large-scale comparative study of ZIP and ZIA schemes, carried out under realistic conditions. We collect and release the most comprehensive dataset in the domain to date, containing over 4250 hours of audio recordings and 1 billion sensor readings from three different scenarios, and evaluate five state-of-the-art schemes based on these data. Our study reveals that the effectiveness of the existing proposals is highly dependent on the scenario they are used in. In particular, we show that these schemes are subject to error rates between 0.6% and 52.8%.},
	articleno    = 10,
	numpages     = 38,
	keywords     = {Authentication, Secure Device Pairing, Internet-of-Things, Context-based Security}
}
@article{10.1145/3287075,
	title        = {DeepType: On-Device Deep Learning for Input Personalization Service with Minimal Privacy Concern},
	author       = {Xu, Mengwei and Qian, Feng and Mei, Qiaozhu and Huang, Kang and Liu, Xuanzhe},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287075},
	url          = {https://doi.org/10.1145/3287075},
	issue_date   = {December 2018},
	abstract     = {Mobile users spend an extensive amount of time on typing. A more efficient text input instrument brings a significant enhancement of user experience. Deep learning techniques have been recently applied to suggesting the next words of input, but to achieve more accurate predictions, these models should be customized for individual users. Personalization is often at the expense of privacy concerns. Existing solutions require users to upload the historical logs of their input text to the cloud so that a deep learning predictor can be trained. In this work, we propose a novel approach, called DeepType, to personalize text input with better privacy. The basic idea is intuitive: training deep learning predictors on the device instead of on the cloud, so that the model makes personalized and private data never leaves the device to externals. With DeepType, a global model is first trained on the cloud using massive public corpora, and our personalization is done by incrementally customizing the global model with data on individual devices. We further propose a set of techniques that effectively reduce the computation cost of training deep learning models on mobile devices at the cost of negligible accuracy loss. Experiments using real-world text input from millions of users demonstrate that DeepType significantly improves the input efficiency for individual users, and its incurred computation and energy costs are within the performance and battery restrictions of typical COTS mobile devices.},
	articleno    = 197,
	numpages     = 26,
	keywords     = {Personalization, Mobile Computing, Deep Learning}
}
@article{10.1145/3287067,
	title        = {Combining Low and Mid-Level Gaze Features for Desktop Activity Recognition},
	author       = {Srivastava, Namrata and Newn, Joshua and Velloso, Eduardo},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287067},
	url          = {https://doi.org/10.1145/3287067},
	issue_date   = {December 2018},
	abstract     = {Human activity recognition (HAR) is an important research area due to its potential for building context-aware interactive systems. Though movement-based activity recognition is an established area of research, recognising sedentary activities remains an open research question. Previous works have explored eye-based activity recognition as a potential approach for this challenge, focusing on statistical measures derived from eye movement properties---low-level gaze features---or some knowledge of the Areas-of-Interest (AOI) of the stimulus---high-level gaze features. In this paper, we extend this body of work by employing the addition of mid-level gaze features; features that add a level of abstraction over low-level features with some knowledge of the activity, but not of the stimulus. We evaluated our approach on a dataset collected from 24 participants performing eight desktop computing activities. We trained a classifier extending 26 low-level features derived from existing literature with the addition of 24 novel candidate mid-level gaze features. Our results show an overall classification performance of 0.72 (F1-Score), with up to 4% increase in accuracy when adding our mid-level gaze features. Finally, we discuss the implications of combining low- and mid-level gaze features, as well as the future directions for eye-based activity recognition.},
	articleno    = 189,
	numpages     = 27,
	keywords     = {activity recognition, Eye tracking, gaze features}
}
@article{10.1145/3287053,
	title        = {Interrupting Drivers for Interactions: Predicting Opportune Moments for In-Vehicle Proactive Auditory-Verbal Tasks},
	author       = {Kim, Auk and Choi, Woohyeok and Park, Jungmi and Kim, Kyeyoon and Lee, Uichin},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287053},
	url          = {https://doi.org/10.1145/3287053},
	issue_date   = {December 2018},
	abstract     = {Auditory-verbal interactions with in-vehicle information systems have become increasingly popular for improving driver safety because they obviate the need for distractive visual-manual operations. This opens up new possibilities for enabling proactive auditory-verbal services where intelligent agents proactively provide contextualized recommendations and interactive decision-making. However, prior studies have warned that such interactions may consume considerable attentional resources, thus negatively affecting driving performance. This work aims to develop a machine learning model that can find opportune moments for the driver to engage in proactive auditory-verbal tasks by using the vehicle and environment sensor data. Given that there is a lack of definition about what constitutes interruptibility for auditory-verbal tasks, we first define interruptible moments by considering multiple dimensions and then iteratively develop the experimental framework through an extensive literature review and four pilot studies. We integrate our framework into OsmAnd, an open-source navigation service, and perform a real-road field study with 29 drivers to collect sensor data and user responses. Our machine learning analysis shows that opportune moments for interruption can be conservatively inferred with an accuracy of 0.74. We discuss how our experimental framework and machine learning models can be used to design intelligent auditory-verbal services in practical deployment contexts.},
	articleno    = 175,
	numpages     = 28,
	keywords     = {Human-vehicle interaction, In-vehicle information system, Speech-based interaction, Interruptibility, Auditory-verbal interface}
}
@article{10.1145/3287040,
	title        = {Painting an Apple with an Apple: A Tangible Tabletop Interface for Painting with Physical Objects},
	author       = {Fujinami, Kaori and Kosaka, Mami and Indurkhya, Bipin},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287040},
	url          = {https://doi.org/10.1145/3287040},
	issue_date   = {December 2018},
	abstract     = {We introduce UnicrePaint, a digital painting system that allows the user to paint with physical objects by acquiring three parameters from the interacting object: the form, the color pattern and the contact pressure. The design of the system is motivated by a hypothesis that integrating direct input from physical objects with digital painting offers unique creative experiences to the user. A major technical challenge in implementing UnicrePaint is to resolve the conflict between input and output, i.e., to be able to capture the form and color pattern of contacting objects from a camera, while at the same time be able to present the captured data using a projector. We present a solution for this problem. We implemented a prototype and carried out a user study with fifteen novice users. Additionally, five professional users with art-related backgrounds participated in a user study to obtain insights into how professionals might view our system. The results show that UnicrePaint offers unique experiences with painting in a creative manner. Also, its potentials beyond mere artwork are suggested.},
	articleno    = 162,
	numpages     = 22,
	keywords     = {Tangible User Interfaces (TUI), Digital painting system, Tabletop interface, Frustrated Total Internal Reflection (FTIR), Creativity support system}
}
@article{10.1145/3287038,
	title        = {Detecting Door Events Using a Smartphone via Active Sound Sensing},
	author       = {Dissanayake, Thilina and Maekawa, Takuya and Amagata, Daichi and Hara, Takahiro},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287038},
	url          = {https://doi.org/10.1145/3287038},
	issue_date   = {December 2018},
	abstract     = {Event detection of indoor objects, including doors, has a wide variety of applications, including intruder detection, HVAC control, and surveillance of independently living elderly people. Hence, this has been the focus of multiple research projects in the UbiComp research community. Herein, we propose a method to accurately detect door events in an indoor environment, without the installation and maintenance costs of using distributed ubiquitous sensors. In particular, we recognize the events of multiple doors existing in the environment via active sound probing using a disused smartphone installed in the environment. We perform event recognition by fusing the analysis of the Doppler shift caused by the moving doors with the acoustic characteristics describing the open/close states of the doors acquired via impulse response. To accurately distinguish between the events of different doors via sound probing, our method employs the time-series analysis of the Doppler shift as well as the active sound probing using directional high-frequency sine waves and stereo sound recording. In addition, by incorporating prior knowledge about the state transitions of a door object into a recognition model, we attempt to improve the accuracy of event recognition. Moreover, our method is capable of recognizing walking activities of a person related to door events in the environment, which are necessary information for applications such as HVAC control that require information about both door events and human presence.},
	articleno    = 160,
	numpages     = 26,
	keywords     = {Indoor context recognition, active sound sensing, pattern recognition, open/close event}
}
@article{10.1145/3287037,
	title        = {CrowdX: Enhancing Automatic Construction of Indoor Floorplan with Opportunistic Encounters},
	author       = {Chen, Huijie and Li, Fan and Hei, Xiaojun and Wang, Yu},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287037},
	url          = {https://doi.org/10.1145/3287037},
	issue_date   = {December 2018},
	abstract     = {The lack of floorplan limits the spread of pervasive indoor location-based services. Existing crowdsourcing based approaches mostly rely on identifying, locating landmarks in the environment and utilizing the spatial relationship between the landmarks and traces for efficiently constructing fine-grained floorplan. However, these methods are always restricted by the sparse landmark distribution or may cause privacy leakage. In this paper, we propose CrowdX, a crowdsourcing system for accurate, low-cost indoor floorplan construction enhanced with opportunistic encounters among mobile users. The key insight is that the spatial relation (i.e., the displacement of each user and the distance between each other during the encounter) will be extracted from the audio and inertia data, which are aligned by the proposed vibration event-based method. Such information can be used to calibrate the drift of encounter position. The calibrated encounter position is beneficial to most of the floorplan generation steps, such as trace drift elimination, landmark positioning, hallway assembling, and room area estimation. Our experiments in three shopping malls show that CrowdX achieves an average F-measure around 89.4%. In addition, the average estimated room area error within about 20%. The evaluation results demonstrate a significant improvement of accuracy enhanced with opportunistic encounters.},
	articleno    = 159,
	numpages     = 21,
	keywords     = {Mobile CrowdSensing, Automatic Floorplan Construction, Peer Assisted Localization}
}
@article{10.1145/3287032,
	title        = {SoundSignaling: Realtime, Stylistic Modification of a Personal Music Corpus for Information Delivery},
	author       = {Ananthabhotla, Ishwarya and Paradiso, Joseph A.},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287032},
	url          = {https://doi.org/10.1145/3287032},
	issue_date   = {December 2018},
	abstract     = {Drawing inspiration from the notion of cognitive incongruence associated with Stroop's famous experiment, from musical principles, and from the observation that music consumption on an individual basis is becoming increasingly ubiquitous, we present the SoundSignaling system -- a software platform designed to make real-time, stylistically relevant modifications to a personal corpus of music as a means of conveying information or notifications. In this work, we discuss in detail the system's technical implementation and its motivation from a musical perspective, and validate these design choices through a crowd-sourced signal identification experiment consisting of 200 independent tasks performed by 50 online participants. We then qualitatively discuss the potential implications of such a system from the standpoint of switch cost, cognitive load, and listening behavior by considering the anecdotal outcomes of a small-scale, in-the-wild experiment consisting of over 180 hours of usage from 6 participants. Through this work, we suggest a re-evaluation of the age-old paradigm of binary audio notifications in favor of a system designed to operate upon the relatively unexplored medium of a user's musical preferences.},
	articleno    = 154,
	numpages     = 23,
	keywords     = {SoundSignaling, music, modification, audio, signal processing, attention, notifications, switch cost, genre, sonification, cognitive load}
}
@article{10.1145/3266002,
	title        = {Authenticating On-Body Backscatter by Exploiting Propagation Signatures},
	author       = {Luo, Zhiqing and Wang, Wei and Xiao, Jiang and Huang, Qianyi and jiang, Tao and Zhang, Qian},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3266002},
	url          = {https://doi.org/10.1145/3266002},
	issue_date   = {September 2018},
	abstract     = {The vision of battery-free communication has made backscatter a compelling technology for on-body wearable and implantable devices. Recent advances have facilitated the communication between backscatter tags and on-body smart devices. These studies have focused on the communication dimension, while the security dimension remains vulnerable. It has been demonstrated that wireless connectivity can be exploited to send unauthorized commands or fake messages that result in device malfunctioning. The key challenge in defending these attacks stems from the minimalist design in backscatter. Thus, in this paper, we explore the feasibility of authenticating an on-body backscatter tag without modifying its signal or protocol. We present SecureScatter, a physical-layer solution that delegates the security of backscatter to an on-body smart device. To this end, we profile the on-body propagation paths of backscatter links, and construct highly sensitive propagation signatures to identify on-body backscatter links. We implement our design in a software radio and evaluate it with different backscatter tags that work at 2.4 GHz and 900 MHz. Results show that our system can identify on-body devices at 93.23% average true positive rate and 3.18% average false positive rate.},
	articleno    = 123,
	numpages     = 22,
	keywords     = {Wearable computing, On-body authentication, Backscatter}
}
@article{10.1145/3264961,
	title        = {Location Privacy-Preserving Data Recovery for Mobile Crowdsensing},
	author       = {Zhou, Tongqing and Cai, Zhiping and Xiao, Bin and Wang, Leye and Xu, Ming and Chen, Yueyue},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264961},
	url          = {https://doi.org/10.1145/3264961},
	issue_date   = {September 2018},
	abstract     = {Data recovery techniques such as compressive sensing are commonly used in mobile crowdsensing (MCS) applications to infer the information of unsensed regions based on data from nearby participants. However, the participants' locations are exposed when they report geo-tagged data to an application server. While there are considerable location protection approaches for MCS, they fail to maintain the correlation of sensory data, leading to the existence of unrecoverable data. None of the previous approaches can achieve both data recovery and data privacy preservation. We propose a novel location privacy-preserving data recovery method in this paper. Based on our discovery that the adjacency relations of non-zero elements are key to the missing data recovery in a crowdsensing data matrix, we design a correlation-preserving location obfuscation scheme to hide the participants' locations under effective camouflage. We also design an encrypted data recovery scheme based on the homomorphic encryption in order to avoid location privacy leakage from sensory data. Location obfuscation and data encryption preserve the participants' privacy, while the correlation-preserving and homomorphic properties of our method ensure data recovery accuracy. Evaluations of real-world datasets show that our privacy-preserving method can effectively obfuscate locations (e.g., yielding an average location distortion of 1.7km in a 2.4km x 4km area for successful location hiding), and it can efficiently achieve similar data recovery accuracy to compressive sensing (which has no privacy protection).},
	articleno    = 151,
	numpages     = 23,
	keywords     = {data recovery, location privacy, mobile crowdsensing, compressive sensing}
}
@article{10.1145/3264958,
	title        = {FullBreathe: Full Human Respiration Detection Exploiting Complementarity of CSI Phase and Amplitude of WiFi Signals},
	author       = {Zeng, Youwei and Wu, Dan and Gao, Ruiyang and Gu, Tao and Zhang, Daqing},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264958},
	url          = {https://doi.org/10.1145/3264958},
	issue_date   = {September 2018},
	abstract     = {Human respiration detection based on Wi-Fi signals does not require users to carry any device, hence it has drawn a lot of attention due to better user acceptance and great potential for real-world deployment. However, recent studies show that respiration sensing performance varies in different locations due to the nature of Wi-Fi radio wave propagation in indoor environments, i.e., respiration detection may experience poor performance at certain locations which we call "blind spots". In this paper, we aim to address the blind spot problem to ensure full coverage of respiration detection. Basically, the amplitude and phase of Wi-Fi channel state information (CSI) are orthogonal and complementary to each other, so they can be combined to eliminate the blind spots. However, accurate CSI phase cannot be obtained from commodity Wi-Fi due to the clock-unsynchronized transceivers. Thus, we apply conjugate multiplication (CM) of CSI between two antennas to remove the phase offset and construct two orthogonal signals--new "amplitude and phase" which are still complementary to each other. In this way, we can ensure full human respiration detection. Based on these ideas, We design and implement a real-time respiration detection system with commodity Wi-Fi devices. We conduct extensive experiments to validate our model and design. The results show that, with only one transceiver pair and without leveraging multiple sub-carriers, our system enables full location coverage with no blind spot, showing great potential for real deployment.},
	articleno    = 148,
	numpages     = 19,
	keywords     = {Wi-Fi, Channel state information (CSI), Respiration Sensing}
}
@article{10.1145/3264947,
	title        = {RF-Based Fall Monitoring Using Convolutional Neural Networks},
	author       = {Tian, Yonglong and Lee, Guang-He and He, Hao and Hsu, Chen-Yu and Katabi, Dina},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264947},
	url          = {https://doi.org/10.1145/3264947},
	issue_date   = {September 2018},
	abstract     = {Falls are the top reason for fatal and non-fatal injuries among seniors. Existing solutions are based on wearable fall-alert sensors, but medical research has shown that they are ineffective, mostly because seniors do not wear them. These revelations have led to new passive sensors that infer falls by analyzing Radio Frequency (RF) signals in homes. Seniors can go about their lives as usual without the need to wear any device. While passive monitoring has made major advances, current approaches still cannot deal with the complexities of real-world scenarios. They typically train and test their classifiers on the same people in the same environments, and cannot generalize to new people or new environments. Further, they cannot separate motions from different people and can easily miss a fall in the presence of other motions.To overcome these limitations, we introduce Aryokee, an RF-based fall detection system that uses convolutional neural networks governed by a state machine. Aryokee works with new people and environments unseen in the training set. It also separates different sources of motion to increase robustness. Results from testing Aryokee with over 140 people performing 40 types of activities in 57 different environments show a recall of 94% and a precision of 92% in detecting falls.},
	articleno    = 137,
	numpages     = 24,
	keywords     = {Deep learning, Device--free, Fall Detection}
}
@article{10.1145/3264941,
	title        = {Variability in Reactions to Instructional Guidance during Smartphone-Based Assisted Navigation of Blind Users},
	author       = {Ohn-Bar, Eshed and Guerreiro, Jo\~{a}o and Kitani, Kris and Asakawa, Chieko},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264941},
	url          = {https://doi.org/10.1145/3264941},
	issue_date   = {September 2018},
	abstract     = {'Turn slightly to the left' the navigational system announces, with the aim of directing a blind user to merge into a corridor. Yet, due to long reaction time, the user turns too late and proceeds into the wrong hallway. Observations of such user behavior in real-world navigation settings motivate us to study the manner in which blind users react to the instructional feedback of a turn-by-turn guidance system. We found little previous work analyzing the extent of the variability among blind users in reaction to different instructional guidance during assisted navigation. To gain insight into how navigational interfaces can be better designed to accommodate the information needs of different users, we conduct a data-driven analysis of reaction variability as defined by motion and timing measures. Based on continuously tracked user motion during real-world navigation with a deployed system, we find significant variability between users in their reaction characteristics. Specifically, the statistical analysis reveals significant variability during the crucial elements of the navigation (e.g., turning and encountering obstacles). With the end-user experience in mind, we identify the need to not only adjust interface timing and content to each user's personal walking pace, but also their individual navigation skill and style. The design implications of our study inform the development of assistive systems which consider such user-specific behavior to ensure successful navigation.},
	articleno    = 131,
	numpages     = 25,
	keywords     = {clustering motion patterns, turn-by-turn navigation, navigation task performance, reaction time, Indoor navigation, accessibility, blind users, task timing, motion analysis}
}
@article{10.1145/3264935,
	title        = {SAW: Wristband-Based Authentication for Desktop Computers},
	author       = {Mare, Shrirang and Rawassizadeh, Reza and Peterson, Ronald and Kotz, David},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264935},
	url          = {https://doi.org/10.1145/3264935},
	issue_date   = {September 2018},
	abstract     = {Token-based proximity authentication methods that authenticate users based on physical proximity are effortless, but lack explicit user intentionality, which may result in accidental logins. For example, a user may get logged in when she is near a computer or just passing by, even if she does not intend to use that computer. Lack of user intentionality in proximity-based methods makes them less suitable for multi-user shared computer environments, despite their desired usability benefits over passwords. We present an authentication method for desktops called Seamless Authentication using Wristbands (SAW), which addresses the lack of intentionality limitation of proximity-based methods. SAW uses a low-effort user input step for explicitly conveying user intentionality, while keeping the overall usability of the method better than password-based methods. In SAW, a user wears a wristband that acts as the user's identity token, and to authenticate to a desktop, the user provides a low-effort input by tapping a key on the keyboard multiple times or wiggling the mouse with the wristband hand. This input to the desktop conveys that someone wishes to log in to the desktop, and SAW verifies the user who wishes to log in by confirming the user's proximity and correlating the received keyboard or mouse inputs with the user's wrist movement, as measured by the wristband. In our feasibility user study (n=17), SAW proved quick to authenticate (within two seconds), with a low false-negative rate of 2.5% and worst-case false-positive rate of 1.8%. In our user perception study (n=16), a majority of the participants rated it as more usable than passwords.},
	articleno    = 125,
	numpages     = 29,
	keywords     = {Authentication, Privacy, Wearable, Security}
}
@article{10.1145/3264932,
	title        = {PAvessel: Practical 3D Vessel Structure Sensing through Photoacoustic Effects with Its Applications in Palm Biometrics},
	author       = {Li, Zhengxiong and Wang, Yuehang and Rathore, Aditya Singh and Song, Chen and Nyayapathi, Nikhila and Vu, Tri and Xia, Jun and Xu, Wenyao},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264932},
	url          = {https://doi.org/10.1145/3264932},
	issue_date   = {September 2018},
	abstract     = {The blood vessels are the most critical part of the human circulatory system. Information acquired on the structure and status of blood vessels drives the development of numerous medical and biometric applications. Therefore, it is of paramount importance to find an effective way to sense vessel structures. Traditional methods, including infrared and Doppler sensing modalities, are limited by optical diffusion and ultrasonic scattering that are not good at vessel sensing with high performance. In comparison, we argue photoacoustic (PA) sensing is an emerging technique that can image 3D vessel structure deep in tissue with high-resolution visualization, maintaining the advantages of both optical and ultrasound methods. In this work, we propose and develop PAvessel, a practical 3D vessel structure sensing system based on PA effects. The entire sensing system comprises two key components, PA sensing hardware and PA sensing software. Specifically, the hardware mainly consists of a linear ultrasound transducer array, an ultrasound data acquisition system, and a neodymium-doped yttrium aluminum garnet (Nd:YAG) laser. After receiving the PA raw data, we use the advanced image reconstruction and 3D photoacoustic vein model to establish the 3D vessel structure model. We validated its effectiveness, cost-effectiveness and high resolution of PAvessel in the evaluation. The system achieves 52% higher signal-to-noise ratio (SNR) compared to the other methods. Furthermore, considering the 3D palm vein contains high dimensional human features and is almost impossible to forge, we also explored its applications in palm biometrics. In a pilot study with 10 participants, PAvessel, combined with a 3D vessel structure matching algorithm (EMD-VT), has proven to possess high accuracy and robustness as a biometric. PAvessel achieves the precision and recall of 98.33% and 97.37%, respectively.},
	articleno    = 122,
	numpages     = 24,
	keywords     = {Biometrics, 3D Vessel Structure, Photoacoustic Sensing}
}
@article{10.1145/3264928,
	title        = {Experiencing Electrical Muscle Stimulation},
	author       = {Knibbe, J. and Alsmith, A. and Hornb\ae{}k, K.},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264928},
	url          = {https://doi.org/10.1145/3264928},
	issue_date   = {September 2018},
	abstract     = {Electrical Muscle Stimulation (EMS) offers rich opportunities for interaction. By varying stimulation parameters (amplitudes, pulse widths and frequencies), EMS can be used to either trigger muscle contractions, and so convey object affordances or guide user movements, or provide rich haptic feedback. However, the way users' experience changes with these parameters, and EMS in general, is poorly understood. Using a phenomenologically inspired interview technique, the explicitation interview, we study fifteen users' experience of EMS across 48 combinations of stimulation parameters. We synthesize the descriptions of EMS and relate stimulation parameters to categories of experience, such as 'temperature', 'motion', and 'sensitivity'. From the interviews, we explore more general topics in body-based interfaces, including the experience of control, metaphors for having your body actuated, and the relation between EMS parameters and perceived depth and location of sensations. These findings provide a vocabulary of EMS experience, and an insight into the relationship between specific parameters and associated sensations. In turn, this can help designers consider the user experience of EMS when developing interfaces.},
	articleno    = 118,
	numpages     = 14,
	keywords     = {EMS, User Experience, Electric Muscle Stimulation}
}
@article{10.1145/3264924,
	title        = {On Being Told How We Feel: How Algorithmic Sensor Feedback Influences Emotion Perception},
	author       = {Hollis, Victoria and Pekurovsky, Alon and Wu, Eunika and Whittaker, Steve},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264924},
	url          = {https://doi.org/10.1145/3264924},
	issue_date   = {September 2018},
	abstract     = {Algorithms and sensors are increasingly deployed for highly personal aspects of our everyday lives. Recent work suggests people have imperfect understanding of system outputs, often assuming sophisticated capabilities and deferring to feedback. We explore how people construe algorithmic interpretations of emotional data in personal informatics systems. A survey (n=188) showed strong interest in automatic stress and emotion tracking, but many respondents expected these systems to provide objective measurements for their emotional experiences. A second study examined how algorithmic sensor feedback influences emotional self-judgments, by comparing three system framings of physiological ElectroDermal Activity data (EDA): Positive ("alert and engaged"), Negative ("stressed"), and Control (no frame) in a mixed-methods study with 64 participants. Despite users reporting strategies to test system outputs, users still deferred to feedback and their perceived emotions were significantly influenced by feedback frames. Some users overrode personal judgments, believing the system had access to privileged information about their emotions. Based on these findings, we explore design implications for personal informatics including risks of users trusting systems that seemingly "unlock" hidden aspects of the self. We propose design approaches that provide opportunities for future emotion-monitoring systems to exploit these framing effects, and for users to more actively construe emotional states.},
	articleno    = 114,
	numpages     = 31,
	keywords     = {Stress, Affective Computing, Personal Informatics, Feedback, Anxiety, Health, Algorithms, Deference, Emotion Regulation, Trust, Framing, Emotion}
}
@article{10.1145/3264915,
	title        = {Online Deep Ensemble Learning for Predicting Citywide Human Mobility},
	author       = {Fan, Zipei and Song, Xuan and Xia, Tianqi and Jiang, Renhe and Shibasaki, Ryosuke and Sakuramachi, Ritsu},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264915},
	url          = {https://doi.org/10.1145/3264915},
	issue_date   = {September 2018},
	abstract     = {Predicting citywide human mobility is critical to an effective management and regulation of city governance, especially during a rare event (e.g. large event such as New Year's celebration or Comiket). Classical models can effectively predict routine human mobility, but irregular mobility during a rare event (precedented or unprecedented), which is much more difficult to model, has not drawn sufficient attention. Moreover, the complexity and non-linearity of human mobility hinders a simple model from making an accurate prediction. Bearing these facts in mind, we propose a novel online gating neural network framework with two phases. In the offline training phase, we train a gated recurrent unit-based human mobility predictor for each day in our training set, while in the online predicting phase, we construct an online adaptive human mobility predictor as well as a gating neural network that switches among the pre-trained predictors and the online adaptive human predictor. Our approach was evaluated using a real-world GPS-log dataset from Tokyo and Osaka and achieved a higher prediction accuracy than baseline models.},
	articleno    = 105,
	numpages     = 21,
	keywords     = {ensemble learning, deep learning, intelligent surveillance, human mobility modeling, urban computing}
}
@article{10.1145/3264908,
	title        = {SleepGuard: Capturing Rich Sleep Information Using Smartwatch Sensing Data},
	author       = {Chang, Liqiong and Lu, Jiaqi and Wang, Ju and Chen, Xiaojiang and Fang, Dingyi and Tang, Zhanyong and Nurmi, Petteri and Wang, Zheng},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264908},
	url          = {https://doi.org/10.1145/3264908},
	issue_date   = {September 2018},
	abstract     = {Sleep is an important part of our daily routine -- we spend about one-third of our time doing it. By tracking sleep-related events and activities, sleep monitoring provides decision support to help us understand sleep quality and causes of poor sleep. Wearable devices provide a new way for sleep monitoring, allowing us to monitor sleep from the comfort of our own home. However, existing solutions do not take full advantage of the rich sensor data provided by these devices. In this paper, we present the design and development of SleepGuard, a novel approach to track a wide range of sleep-related events using smartwatches. We show that using merely a single smartwatch, it is possible to capture a rich amount of information about sleep events and sleeping context, including body posture and movements, acoustic events, and illumination conditions. We demonstrate that through these events it is possible to estimate sleep quality and identify factors affecting it most. We evaluate our approach by conducting extensive experiments involved fifteen users across a 2-week period. Our experimental results show that our approach can track a richer set of sleep events, provide better decision support for evaluating sleep quality, and help to identify causes for sleep problems compared to prior work.},
	articleno    = 98,
	numpages     = 34,
	keywords     = {mobile sensing, Smartwatch sensing, sleep monitoring, sleep events}
}
@article{10.1145/3264904,
	title        = {FingerReader2.0: Designing and Evaluating a Wearable Finger-Worn Camera to Assist People with Visual Impairments While Shopping},
	author       = {Boldu, Roger and Dancu, Alexandru and Matthies, Denys J.C. and Buddhika, Thisum and Siriwardhana, Shamane and Nanayakkara, Suranga},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264904},
	url          = {https://doi.org/10.1145/3264904},
	issue_date   = {September 2018},
	abstract     = {People with Visual Impairments (PVI) experience greater difficulties with daily tasks, such as supermarket shopping. Identifying and purchasing an item proves challenging for PVI. Using a user-centered design process, we understand the difficulties PVI encounter in their daily routines. Consequently, the previous FingerReader model was elevated to a new level. In contrast, FingerReader2.0 incorporates a highly integrated hardware design, as it is standalone, wearable, and not tethered to a computer. Software-wise, the prototype utilizes a deep learning system, relying on a hybrid, an on-board and a cloud-based model. The advanced design significantly extends the range of mobile assistive technology, particularly for shopping purposes. This paper presents the findings from interviews, several iterative studies, and a field study in supermarkets to demonstrate the FingerReader2.0's enhanced capabilities for those with varied levels of visual impairment.},
	articleno    = 94,
	numpages     = 19,
	keywords     = {Hybrid Deep Learning, Accessibility, Low vision, Supermarket Shopping, Assistive Technology, Thumb-to-finger Interaction, Finger-worn camera, Wearable technology}
}
@article{10.1145/3214286,
	title        = {My Smartphone Recognizes Genuine QR Codes! Practical Unclonable QR Code via 3D Printing},
	author       = {Song, Chen and Li, Zhengxiong and Xu, Wenyao and Zhou, Chi and Jin, Zhanpeng and Ren, Kui},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214286},
	url          = {https://doi.org/10.1145/3214286},
	issue_date   = {June 2018},
	abstract     = {Additive manufacturing, or 3D printing, has been widely applied in product manufacturing. However, the emerging unauthorized access of 3D printing data, as well as the growth in the pervasiveness and capability of 3D printing devices have raised serious concerns about 3D printing product anti-counterfeit. Electronic product tags are the current standard for authentication purposes; however, often this technology is neither secure nor cost-effective, and fails to take advantage of other unique 3D printing features. Considering the great usability of the QR code, we are motivated to enhance the QR code for the practical and cost-effective 3D printing product identification. Particularly, we bring up the all-in-one design, all-in-one manufacturing concept incorporating the QR code in the complete 3D printing paradigm. In detail, we explore the possibility of leveraging the random and uncontrollable process variations in the 3D printing system to generate a unique fingerprint for the integrated QR code. To this end, we present an end-to-end 3D-printed QR code verification framework, which does not change the original QR protocol and functionality. The entire solution can be implemented with commodity 3D printers and smartphones. Specifically, we first investigate the inevitable and random process variations in the 3D printing mechanism and analyze the causality between the variations and detectable geometric deformation. We further develop a fingerprint extraction algorithm taking into account both the QR code property and the 3D printer characteristics. The system evaluation indicates that our solution is secure and robust in multiple scenarios.},
	articleno    = 83,
	numpages     = 20,
	keywords     = {Embedded Systems, Authentication, Hardware Security, 3D Printing}
}
@article{10.1145/3214280,
	title        = {Remotion: A Motion-Based Capture and Replay Platform of Mobile Device Interaction for Remote Usability Testing},
	author       = {Qian, Jing and Chapin, Arielle and Papoutsaki, Alexandra and Yang, Fumeng and Nelissen, Klaas and Huang, Jeff},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214280},
	url          = {https://doi.org/10.1145/3214280},
	issue_date   = {June 2018},
	abstract     = {Remotion is an end-to-end system for capturing and replaying rich mobile device interactions, comprising both on-screen video and physical device motions. The blueprints and software provided here allow an interface to be instrumented with Remotion's capture and visualization system. Remotion is able to mimic mobile device motion through a software 3D graphical visualization and a robotic mount that replicates the movements of a mobile device from afar. Deployed together, experimenters can emulate the mobile device postures of a remote user as if they were in the room. This is important since many usability studies are carried remotely and the contribution and scale of those studies are irreplaceable. We compared how HCI experts ("analysts") observed remote users behavioral data across three replay platforms: a traditional live time-series of motion, Remotion's software visualization, and Remotion's hardware visualization. We found that Remotion can assist analysts to infer the user's attention, emotional state, habits, and active hand posture; Remotion also has a reduced effect on mental demand for analysts when analyzing the remote user's contextual information.},
	articleno    = 77,
	numpages     = 18,
	keywords     = {mobile sensing, remote studies, fabrication, motion visualization, smartphone behavior}
}
@article{10.1145/3214279,
	title        = {PrivacyShield: A Mobile System for Supporting Subtle Just-in-Time Privacy Provisioning through Off-Screen-Based Touch Gestures},
	author       = {Pushp, Saumay and Liu, Yunxin and Xu, Mengwei and Koh, Changyoung and Song, Junehwa},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214279},
	url          = {https://doi.org/10.1145/3214279},
	issue_date   = {June 2018},
	abstract     = {Current in-situ privacy solution approaches are inadequate in protecting sensitive information. They either require extra configuration effort or lack the ability to configure user desired privacy settings. Based on in-depth discussions during a design workshop, we propose PrivacyShield, a mobile system for providing subtle just-in-time privacy provisioning. PrivacyShield leverages the screen I/O device (screen digitizer) of smartphones to recognize gesture commands, even when the phone's screen is turned off. Based on gesture command inputs, various privacy-protection policies can be configured on-the-fly. We develop a novel stroke-based approach to address the challenges in segmenting and recognizing gesture command inputs, which helps the system in achieving good usability and performance. PrivacyShield also provides developers with APIs to enable just-in-time privacy provisioning in their applications. We have implemented an energy efficient PrivacyShield prototype on the Android platform, including smartphones with and without a low-power co-processor. Evaluation results show that our gesture segmentation algorithm is fast enough for real-time performance (introducing less than 200ms processing latency) and accurate (achieving an accuracy of 95% for single-character gestures and 89% for even three-character gestures). We also build a non-touch-screen-based just-in-time privacy provisioning prototype called the wrist gesture method. We compare the performance of the two prototypes by doing a 6-week field study with 12 participants and show why a simplistic solution falls short in providing privacy configurations. We also report the participants' perceptions and reactions after the field study.},
	articleno    = 76,
	numpages     = 38,
	keywords     = {In-situ privacy, Just-in-time privacy provisioning, Gesture recognition, In-situ usability, Mobile Systems, Gesture segmentation}
}
@article{10.1145/3214278,
	title        = {Smartphone-Based Acoustic Indoor Space Mapping},
	author       = {Pradhan, Swadhin and Baig, Ghufran and Mao, Wenguang and Qiu, Lili and Chen, Guohai and Yang, Bo},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214278},
	url          = {https://doi.org/10.1145/3214278},
	issue_date   = {June 2018},
	abstract     = {Constructing a map of indoor space has many important applications, such as indoor navigation, VR/AR, construction, safety, facility management, and network condition prediction. Existing indoor space mapping requires special hardware (e.g., indoor LiDAR equipment) and well-trained operators. In this paper, we develop a smartphone-based indoor space mapping system that lets a regular user quickly map an indoor space by simply walking around while holding a phone in his/her hand. Our system accurately measures the distance to nearby reflectors, estimates the user's trajectory, and pairs different reflectors the user encounters during the walk to automatically construct the contour. Using extensive evaluation, we show our contour construction is accurate: the median errors are 1.5 cm for a single wall and 6 cm for multiple walls (due to longer trajectory and the higher number of walls). We show that our system provides a median error of 30 cm and a 90-percentile error of 1 m, which is significantly better than the state-of-the-art smartphone acoustic mapping system BatMapper [64], whose corresponding errors are 60 cm and 2.5 m respectively, even after multiple walks. We further show that the constructed indoor contour can be used to predict wireless received signal strength (RSS).},
	articleno    = 75,
	numpages     = 26,
	keywords     = {Smartphone, Acoustic Sensing, Mapping, FMCW}
}
@article{10.1145/3214277,
	title        = {AROMA: A Deep Multi-Task Learning Based Simple and Complex Human Activity Recognition Method Using Wearable Sensors},
	author       = {Peng, Liangying and Chen, Ling and Ye, Zhenan and Zhang, Yi},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214277},
	url          = {https://doi.org/10.1145/3214277},
	issue_date   = {June 2018},
	abstract     = {Human activity recognition (HAR) is a promising research issue in ubiquitous and wearable computing. However, there are some problems existing in traditional methods: 1) They treat HAR as a single label classification task, and ignore the information from other related tasks, which is helpful for the original task. 2) They need to predesign features artificially, which are heuristic and not tightly related to HAR task. To address these problems, we propose AROMA (human activity recognition using deep multi-task learning). Human activities can be divided into simple and complex activities. They are closely linked. Simple and complex activity recognitions are two related tasks in AROMA. For simple activity recognition task, AROMA utilizes a convolutional neural network (CNN) to extract deep features, which are task dependent and non-handcrafted. For complex activity recognition task, AROMA applies a long short-term memory (LSTM) network to learn the temporal context of activity data. In addition, there is a shared structure between the two tasks, and the object functions of these two tasks are optimized jointly. We evaluate AROMA on two public datasets, and the experimental results show that AROMA is able to yield a competitive performance in both simple and complex activity recognitions.},
	articleno    = 74,
	numpages     = 16,
	keywords     = {Human activity recognition, deep learning, LSTM, multi-task learning}
}
@article{10.1145/3214274,
	title        = {Rethinking the Future of Wireless Emergency Alerts: A Comprehensive Study of Technical and Conceptual Improvements},
	author       = {Kumar, Sumeet and Erdogmus, Hakan and Iannucci, Bob and Griss, Martin and Falc\~{a}o, Jo\~{a}o Diogo},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214274},
	url          = {https://doi.org/10.1145/3214274},
	issue_date   = {June 2018},
	abstract     = {The Wireless Emergency Alerting (WEA) service is a standards-based transport and presentation channel used nationwide in the United States. The service can deliver short text warnings to wireless subscribers through a cell broadcast mechanism. For emergency situations in which a broadcast modality and a single, short text message are sufficient to convey information, the WEA service can be efficient and effective. However, the content to be delivered may necessitate more than a single, unchanging short message. In this research, we first examine the WEA service from the perspective of alert originators. We then use the insights gained to explore the efficacy of a range of potential extensions to the service. The extensions mainly address the importance of user context and the ability to create awareness through careful attention to the integrity of the vital information. We evaluated these extensions using a WEA emulation testbed in two public usability trials. We present an analysis of the broad range of improvements as a basis for further research into improving the service. We conclude that (1) precise geo-targeting augmented with location information and maps is an important aspect of capturing users' context, and (2) presenting information in a digested form can markedly improve the actionability and the accuracy of interpretation.},
	articleno    = 71,
	numpages     = 33,
	keywords     = {Context awareness, Mobile emergency systems, Emergency alerting, Wireless Emergency Alerts}
}
@article{10.1145/3214273,
	title        = {Reflection Companion: A Conversational System for Engaging Users in Reflection on Physical Activity},
	author       = {Kocielnik, Rafal and Xiao, Lillian and Avrahami, Daniel and Hsieh, Gary},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214273},
	url          = {https://doi.org/10.1145/3214273},
	issue_date   = {June 2018},
	abstract     = {Mobile, wearable and other connected devices allow people to collect and explore large amounts of data about their own activities, behavior, and well-being. Yet, learning from-, and acting upon such data remain a challenge. The process of reflection has been identified as a key component of such learning. However, most tools do not explicitly design for reflection, carrying an implicit assumption that providing access to self-tracking data is sufficient. In this paper, we present Reflection Companion, a mobile conversational system that supports engaging reflection on personal sensed data, specifically physical activity data collected with fitness trackers. Reflection Companion delivers daily adaptive mini-dialogues and graphs to users' mobile phones to promote reflection. To generate our system's mini dialogues, we conducted a set of workshops with fitness trackers users, producing a diverse corpus of 275 reflection questions synthesized into a set of 25 reflection mini dialogues. In a 2-week field deployment with 33 active Fitbit users, we examined our system's ability to engage users in reflection through dialog. Results suggest that the mini-dialogues were successful in triggering reflection and that this reflection led to increased motivation, empowerment, and adoption of new behaviors. As a strong indicator of our system's value, 16 of the 33 participants elected to continue using the system for two additional weeks without compensation. We present our findings and describe implications for the design of technology-supported dialog systems for reflection on data.},
	articleno    = 70,
	numpages     = 26,
	keywords     = {reflection, self-learning, behavior change, Conversational design, conversational AI, physical activity, reflective dialogues}
}
@article{10.1145/3214260,
	title        = {EyeSpyVR: Interactive Eye Sensing Using Off-the-Shelf, Smartphone-Based VR Headsets},
	author       = {Ahuja, Karan and Islam, Rahul and Parashar, Varun and Dey, Kuntal and Harrison, Chris and Goel, Mayank},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214260},
	url          = {https://doi.org/10.1145/3214260},
	issue_date   = {June 2018},
	abstract     = {Low cost virtual reality (VR) headsets powered by smartphones are becoming ubiquitous. Their unique position on the user's face opens interesting opportunities for interactive sensing. In this paper, we describe EyeSpyVR, a software-only eye sensing approach for smartphone-based VR, which uses a phone's front facing camera as a sensor and its display as a passive illuminator. Our proof-of-concept system, using a commodity Apple iPhone, enables four sensing modalities: detecting when the VR head set is worn, detecting blinks, recognizing the wearer's identity, and coarse gaze tracking - features typically found in high-end or specialty VR headsets. We demonstrate the utility and accuracy of EyeSpyVR in a series of studies with 70 participants, finding a worn detection of 100%, blink detection rate of 95.3%, family user identification accuracy of 81.4%, and mean gaze tracking error of 10.8° when calibrated to the wearer (12.9° without calibration). These sensing abilities can be used by developers to enable new interactive features and more immersive VR experiences on existing, off-the-shelf hardware.},
	articleno    = 57,
	numpages     = 10,
	keywords     = {VR, eye tracking, periocular biometrics, user identification, personalized service delivery on VR, blink detection, gaze tracking}
}
@article{10.1145/3191788,
	title        = {FinDroidHR: Smartwatch Gesture Input with Optical Heartrate Monitor},
	author       = {Zhang, Yu and Gu, Tao and Luo, Chu and Kostakos, Vassilis and Seneviratne, Aruna},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191788},
	url          = {https://doi.org/10.1145/3191788},
	issue_date   = {March 2018},
	abstract     = {We present FinDroidHR, a novel gesture input technique for off-the-shelf smartwatches. Our technique is designed to detect 10 hand gestures on the hand wearing a smartwatch. The technique is enabled by analysing features of the Photoplethysmography (PPG) signal that optical heart-rate sensors capture. In a study with 20 participants, we show that FinDroidHR achieves 90.55% accuracy and 90.73% recall. Our work is the first study to explore the feasibility of using optical sensors on the off-the-shelf wearable devices to recognise gestures. Without requiring bespoke hardware, FinDroidHR can be readily used on existing smartwatches.},
	articleno    = 56,
	numpages     = 42,
	keywords     = {Gesture Interaction, Wearable Computing, Machine Learning, Mobile Sensors}
}
@article{10.1145/3191782,
	title        = {Accurate and Efficient Indoor Location by Dynamic Warping in Sequence-Type Radio-Map},
	author       = {Ye, Xuehan and Wang, Yongcai and Guo, Yuhe and Hu, Wei and Li, Deying},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191782},
	url          = {https://doi.org/10.1145/3191782},
	issue_date   = {March 2018},
	abstract     = {An efficient way to overcome the calibration challenge and RSS dynamics in radio-map-based indoor localization is to collect radio signal strength (RSS) along indoor paths and conduct localization by sequence matching. But such sequence-based indoor localization suffers problems including indoor path combinational explosion, random RSS miss-of-detection during user movement, and user moving speed disparity in online and offline phases. To address these problems, this paper proposes an undirected graph model, called WarpMap to efficiently calibrate and store the sequence-type radio-map. It reduces RSS sequence signature storage complexity from O(2N) to O(N) where N is the number of path crosses. An efficient on-line candidate path extraction algorithm is developed in it to find a set of the most possible candidate paths for matching with the on-line collected RSS sequence. Then, to determine the user's exact location, a sub-sequence dynamic time warping (SDTW) algorithm is proposed, which matches the online collected RSS sequence with the sequential RSS signatures of the candidate paths. We show the SDTW algorithm is highly efficient and adaptive, which localizes user without backtracking of warping path. Extensive experiments in office environments verified the efficiency and accuracy of WarpMap, which can be calibrated within thirty minutes by one person for 1100m2 area and provides overall nearly 20% accuracy improvements than the state-of-the-art of radio-map method.},
	articleno    = 50,
	numpages     = 22,
	keywords     = {Indoor Location, Dynamic Time Warping, Sequence-type radio-map}
}
@article{10.1145/3191776,
	title        = {BRAVO: Improving the Rebalancing Operation in Bike Sharing with Rebalancing Range Prediction},
	author       = {Wang, Shuai and He, Tian and Zhang, Desheng and Shu, Yuanchao and Liu, Yunhuai and Gu, Yu and Liu, Cong and Lee, Haengju and Son, Sang H.},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191776},
	url          = {https://doi.org/10.1145/3191776},
	issue_date   = {March 2018},
	abstract     = {Bike sharing systems, which provide a convenient commute choice for short trips, have emerged rapidly in many cities. While bike sharing has greatly facilitated people's commutes, those systems are facing a costly maintenance issue -- rebalancing bikes among stations. We observe that existing systems frequently suffer situations such as no-bike-to-borrow (empty) or no-dock-to-return (full) due to existing ad hoc rebalancing practice. To address this issue, we provide systematic analysis on user trip data, station status data, rebalancing data, and meteorology data, and propose BRAVO - the first practical data-driven bike rebalancing app for operators to improve bike sharing service while reducing the maintenance cost. Specifically, leveraging experiences from two-round round-the-clock field studies and comprehensive information from four data sets, a data-driven model is proposed to capture and predict the safe rebalancing range for each station. Based on this safe rebalancing range, BRAVO computes the optimal rebalancing amounts for the full and empty stations to minimize the rebalancing cost. BRAVO is evaluated with 24-month data from Capital, Hangzhou and NiceRide bikeshare systems. The experiment results show that given the same user demand, BRAVO reduces 28% of the station visits and 37% of the rebalancing amounts.},
	articleno    = 44,
	numpages     = 22,
	keywords     = {bike rebalancing system, urban data, Bike sharing, demand prediction}
}
@article{10.1145/3191767,
	title        = {RuleSelector: Selecting Conditional Action Rules from User Behavior Patterns},
	author       = {Srinivasan, Vijay and Koehler, Christian and Jin, Hongxia},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191767},
	url          = {https://doi.org/10.1145/3191767},
	issue_date   = {March 2018},
	abstract     = {Modern smartphones and ubiquitous computing systems collect a wealth of context data from users. Conditional action rules, as popularized by the IFTTT (If-This-Then-That) platform, are a popular way for users to automate frequently repeated tasks or receive smart reminders, due to the intelligibility and control that rules provide to users. A key drawback of IFTTT systems is that they place the burden of manually specifying action rules on the user. While multiple rule mining algorithms have been proposed in existing work to automatically discover action rules, they generate hundreds of action rules, and the problem of how to present a small subset of rules to smartphone users and allow them to interactively select action rules remains unsolved. In this work, we take the first step towards solving this problem by designing and implementing RuleSelector, the first interactive rule selection tool to allow smartphone users to browse, modify, and select action rules from a small set of summarized rules presented to the user. We propose novel rule selection metrics to address the needs of smartphone users, and analyze the performance of RuleSelector using data from 200 users. We also perform a qualitative user study in order to evaluate how users use the RuleSelector tool and perceive the selected rules, and present the insights gained and design recommendations for future rule selection systems. Our users rated the selected rules from useful to very useful, and an important finding of our study is that users prefer an interactive rule selection system such as RuleSelector that automatically suggests rules, but allows users to select and modify the suggested rules. Finally, we examine the promise of RuleSelector in other ubiquitous computing systems such as smart homes and smart TVs by applying our tool to public context datasets from these domains.},
	articleno    = 35,
	numpages     = 34,
	keywords     = {Context Recognition, Pattern Mining, Smart Homes, Digital Health, Smartphones, Rule Summarization}
}
@article{10.1145/3191766,
	title        = {Riskalyzer: Inferring Individual Risk-Taking Propensity Using Phone Metadata},
	author       = {Singh, Vivek K. and Goyal, Rushil and Wu, Shenghao},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191766},
	url          = {https://doi.org/10.1145/3191766},
	issue_date   = {March 2018},
	abstract     = {An individual's risk-taking propensity is "the stable tendency to choose options with a lower probability of success, but greater rewards". This risk propensity plays a central role in decision making by customers as well as managers, and is a mediator in behavior associated with security, privacy, health, finance, and well-being. Most common approach to understanding an individual's risk propensity remain lab-based games and surveys. Administering such surveys and games is a manual, time, and money intensive process that is also fraught with multiple biases. Recently, smartphones are increasingly seen as large-scale sensors of human activity, recording data related to physical and social aspects of people's lives. Building on this trend we investigate the potential of passive phone-based data for automatically inferring an individual's risk propensity. Specifically, we describe a novel approach to model an individual's risk propensity based on her mobile phone usage. Based on a 10-week field + lab study involving 50 participants, we report that: (1) multiple phone-based features (e.g., average gyradius) are intricately associated with participants' risk propensity; and (2) a phone-based model outperforms demography-based models by 39% in terms of accuracy of predicting risk propensity. In organizational terms, a better understanding of risk behavior could contribute significantly to risk management programs. At the same time, such results could open doors for more nuanced understanding of the underlying human risk phenomena and their interconnections with social and mobility behavior.},
	articleno    = 34,
	numpages     = 21,
	keywords     = {Phoneotype, Risk propensity, Behavioral Informatics, Phone metadata, Mobile Sensing, Risk}
}
@article{10.1145/3191764,
	title        = {Employing Consumer Wearables to Detect Office Workers' Cognitive Load for Interruption Management},
	author       = {Schaule, Florian and Johanssen, Jan Ole and Bruegge, Bernd and Loftness, Vivian},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191764},
	url          = {https://doi.org/10.1145/3191764},
	issue_date   = {March 2018},
	abstract     = {Office workers' productivity and well-being are reduced by interruptions, especially if they occur during an inconvenient moment. Interruptions in phases of high cognitive load are more disruptive than in phases of low cognitive load. Based on an explorative study, we suppose the presence of social codes that signal office workers' interruptibility. We propose a system that utilizes the cognitive load of an office worker to indicate situations suitable for interruptions. The cognitive load is inferred from office workers' physiological state measured by a consumer smartwatch. The system adapts an externally mounted smart device to indicate if the office worker is interruptible. To predict the cognitive load, we trained a classifier with ten office workers and achieved an accuracy between 66% and 86%. In order to validate the classifier's accurateness in an office setting, we performed a verification study with five office workers: We systematically triggered interruptions for each subject over an interval of half a day of office work. The classifier was able to infer the level of cognitive load for three office workers. This result supports our hypotheses that inferring cognitive load using a consumer smartwatch is a viable concept.},
	articleno    = 32,
	numpages     = 20,
	keywords     = {Social Code, Explorative Study, Cognitive Load, Office Environment, Wearable Devices, Interruption Management System, Productivity, Automatic Interruptibility Measurement}
}
@article{10.1145/3191757,
	title        = {Measuring Interaction Proxemics with Wearable Light Tags},
	author       = {Montanari, Alessandro and Tian, Zhao and Francu, Elena and Lucas, Benjamin and Jones, Brian and Zhou, Xia and Mascolo, Cecilia},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191757},
	url          = {https://doi.org/10.1145/3191757},
	issue_date   = {March 2018},
	abstract     = {The proxemics of social interactions (e.g., body distance, relative orientation) influences many aspects of our everyday life: from patients' reactions to interaction with physicians, successes in job interviews, to effective teamwork. Traditionally, interaction proxemics has been studied via questionnaires and participant observations, imposing high burden on users, low scalability and precision, and often biases.In this paper we present Protractor, a novel wearable technology for measuring interaction proxemics as part of non-verbal behavior cues with fine granularity. Protractor employs near-infrared light to monitor both the distance and relative body orientation of interacting users. We leverage the characteristics of near-infrared light (i.e., line-of-sight propagation) to accurately and reliably identify interactions; a pair of collocated photodiodes aid the inference of relative interaction angle and distance. We achieve robustness against temporary blockage of the light channel (e.g., by the user's hand or clothes) by designing sensor fusion algorithms that exploit inertial sensors to obviate the absence of light tracking results.We fabricated Protractor tags and conducted real-world experiments. Results show its accuracy in tracking body distances and relative angles. The framework achieves less than 6° error 95% of the time for measuring relative body orientation and 2.3-cm - 4.9-cm mean error in estimating interaction distance. We deployed Protractor tags to track user's non-verbal behaviors when conducting collaborative group tasks. Results with 64 participants show that distance and angle data from Protractor tags can help assess individual's task role with 84.9% accuracy, and identify task timeline with 93.2% accuracy.},
	articleno    = 25,
	numpages     = 30,
	keywords     = {non-verbal behaviors, light sensing, Face-to-face interactions}
}
@article{10.1145/3191749,
	title        = {An Energy-Efficient and Lightweight Indoor Localization System for Internet-of-Things (IoT) Environments},
	author       = {Kwak, Myeongcheol and Park, Youngmong and Kim, Junyoung and Han, Jinyoung and Kwon, Taekyoung},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191749},
	url          = {https://doi.org/10.1145/3191749},
	issue_date   = {March 2018},
	abstract     = {Each and every spatial point in an indoor space has its own distinct and stable fingerprint, which arises owing to the distortion of the magnetic field induced by the surrounding steel and iron structures. This phenomenon makes many indoor positioning techniques rely on the magnetic field as an important source of localization. Most of the existing studies, however, have leveraged smartphones that have a relatively high computational power and many sensors. Thus, their algorithmic complexity is usually high, especially for commercial location-based services. In this paper, we present an energy-efficient and lightweight system that utilizes the magnetic field for indoor positioning in Internet of Things (IoT) environments. We propose a new hardware design of an IoT device that has a BLE interface and two sensors (magnetometer and accelerometer), with the lifetime of one year when using a coin-size battery. We further propose an augmented particle filter framework that features a robust motion model and a localization heuristic with small sensory data. The prototype-based evaluation shows that the proposed system achieves a median accuracy of 1.62 m for an office building, while exhibiting low computational complexity and high energy efficiency.},
	articleno    = 17,
	numpages     = 28,
	keywords     = {Internet-of-Things, Magnetic Field, Indoor Localization, Wireless Communication, BLE, Particle Filter}
}
@article{10.1145/3191746,
	title        = {Deep ROI-Based Modeling for Urban Human Mobility Prediction},
	author       = {Jiang, Renhe and Song, Xuan and Fan, Zipei and Xia, Tianqi and Chen, Quanjun and Chen, Qi and Shibasaki, Ryosuke},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191746},
	url          = {https://doi.org/10.1145/3191746},
	issue_date   = {March 2018},
	abstract     = {Rapidly developing location acquisition technologies have provided us with big GPS trajectory data, which offers a new means of understanding people's daily behaviors as well as urban dynamics. With such data, predicting human mobility at the city level will be of great significance for transportation scheduling, urban regulation, and emergency management. In particular, most urban human behaviors are related to a small number of important regions, referred to as Regions-of-Interest (ROIs). Therefore, in this study, a deep ROI-based modeling approach is proposed for effectively predicting urban human mobility. Urban ROIs are first discovered from historical trajectory data, and urban human mobility is designated using two types of ROI labels (ISROI and WHICHROI). Then, urban mobility prediction is modeled as a sequence classification problem for each type of label. Finally, a deep-learning architecture built with recurrent neural networks is designed as an effective sequence classifier. Experimental results demonstrate that the superior performance of our proposed approach to the baseline models and several real-world practices show the applicability of our approach to real-world urban computing problems.},
	articleno    = 14,
	numpages     = 29,
	keywords     = {big data, urban computing, deep learning, human mobility}
}
@article{10.1145/3161415,
	title        = {WiStep: Device-Free Step Counting with WiFi Signals},
	author       = {Xu, Yang and Yang, Wei and Wang, Jianxin and Zhou, Xing and Li, Hong and Huang, Liusheng},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161415},
	url          = {https://doi.org/10.1145/3161415},
	issue_date   = {December 2017},
	abstract     = {Inspired by the emerging WiFi-based applications, in this paper, we leverage ubiquitous WiFi signals and propose a device-free step counting system, called WiStep. Based on the multipath propagation model, when a person is walking, her torso and limbs move at different speeds, which modulates wireless signals to the propagation paths with different lengths and thus introduces different frequency components into the received Channel State Information (CSI). To count walking steps, we first utilize time-frequency analysis techniques to segment and recognize the walking movement, and then dynamically select the sensitive subcarriers with largest amplitude variances from multiple CSI streams. Wavelet decomposition is applied to extract the detail coefficients corresponding to the frequencies induced by feet or legs, and compress the data so as to improve computing speed. Short-time energy of the coefficients is then calculated as the metric for step counting. Finally, we combine the results derived from the selected subcarriers to produce a reliable step count estimation. In contrast to counting steps based on the torso frequency analysis, WiStep can count the steps of in-place walking even when the person's torso speed is null. We implement WiStep on commodity WiFi devices in two different indoor scenarios, and various influence factors are taken into consideration when evaluating the performance of WiStep. The experimental results demonstrate that WiStep can realize overall step counting accuracies of 90.2% and 87.59% respectively in these two scenarios, and it is resilient to the change of scenarios.},
	articleno    = 172,
	numpages     = 23,
	keywords     = {WiFi Signals, Device-free Step Counting, Channel State Information (CSI)}
}
@article{10.1145/3161412,
	title        = {SHOW: Smart Handwriting on Watches},
	author       = {Lin, Xinye and Chen, Yixin and Chang, Xiao-Wen and Liu, Xue and Wang, Xiaodong},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161412},
	url          = {https://doi.org/10.1145/3161412},
	issue_date   = {December 2017},
	abstract     = {Smart watch is becoming a new gateway through which people stay connected and track everyday activities, and text-entry on it is becoming a frequent need. With the two de facto solutions: tap-on-screen and voice input, text-entry on the watch remains a tedious task because 1. Tap-on-screen is error prone due to the small screen; 2. Voice input is strongly constrained by the surroundings and suffers from privacy leak. In this paper, we propose SHOW, which enables the user to input as they handwrite on horizontal surfaces, and the only requirement is to use the elbow as the support point. SHOW captures the gyroscope and accelerometer traces and deduces the user's handwriting thereafter. SHOW differs from previous work of gesture recognition in that: 1. it employs a novel rotation injection technique to substantially reduce the effort of data collection; 2. it does not require whole-arm posture, hence is better suited to space-limited places (e.g. vehicles). Our experiments show that SHOW can effectively generate 60 traces from one real handwriting trace and achieve high accuracy at 99.9% when recognizing the 62 different characters written by 10 volunteers. Furthermore, having more screen space after removing the virtual keyboard, SHOW can display 4x candidate words for autocompletion. Aided by the tolerance of character ambiguity and accurate character recognition, SHOW achieves over 70% lower mis-recognition-rate, 43% lower no-response-rate in both daily and general purposed text-entry scenarios, and 33.3% higher word suggestion coverage than the tap-on-screen method using a virtual QWERTY keyboard.},
	articleno    = 151,
	numpages     = 23,
	keywords     = {input method, virtual keyboard, handwriting, accelerometer, text entry, gyroscope, n-gram, smart watches}
}
@article{10.1145/3161200,
	title        = {Understanding Group Event Scheduling via the OutWithFriendz Mobile Application},
	author       = {Zhang, Shuo and Alanezi, Khaled and Gartrell, Mike and Han, Richard and Lv, Qin and Mishra, Shivakant},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161200},
	url          = {https://doi.org/10.1145/3161200},
	issue_date   = {December 2017},
	abstract     = {The wide adoption of smartphones and mobile applications has brought significant changes to not only how individuals behave in the real world, but also how groups of users interact with each other when organizing group events. Understanding how users make event decisions as a group and identifying the contributing factors can offer important insights for social group studies and more effective system and application design for group event scheduling.In this work, we have designed a new mobile application called OutWithFriendz, which enables users of our mobile app to organize group events, invite friends, suggest and vote on event time and venue. We have deployed OutWithFriendz at both Apple App Store and Google Play, and conducted a large-scale user study spanning over 500 users and 300 group events. Our analysis has revealed several important observations regarding group event planning process including the importance of user mobility, individual preferences, host preferences, and group voting process.},
	articleno    = 175,
	numpages     = 19,
	keywords     = {Group Event Scheduling, Group Decision Making, Collaboration, OutWithFriendz}
}
@article{10.1145/3161199,
	title        = {Towards Wearable Everyday Body-Frame Tracking Using Passive RFIDs},
	author       = {Jin, Haojian and Yang, Zhijian and Kumar, Swarun and Hong, Jason I.},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161199},
	url          = {https://doi.org/10.1145/3161199},
	issue_date   = {December 2017},
	abstract     = {We introduce RF-Wear, an accurate and wearable solution to track movements of a user's body using passive RFIDs embedded in their clothing. RF-Wear processes wireless signals reflected off these tags to a compact single-antenna RFID reader in the user's pocket. In doing so, RF-Wear enables a first-of-its-kind body-frame tracking mechanism that is lightweight and convenient for day-to-day use, without relying on external infrastructure. At the heart of RF-Wear is a novel primitive that computes angles between different parts of the user's body using the RFID tags attached to them. RF-Wear achieves this by treating groups of RFID tags as an array of antennas whose orientation can be computed accurately relative to the handheld reader. By computing the orientation of individual body parts, we demonstrate how RF-Wear reconstructs the real-time posture of the user's entire body frame. Our solution overcomes multiple challenges owing to the interactions of wireless signals with the body, the 3-D nature of human joints and the flexibility of fabric on which RFIDs are placed. We implement and evaluate a prototype of RF-Wear on commercial RFID readers and tags and demonstrate its performance in body-frame tracking. Our results reveal a mean error of 8--12° in tracking angles at joints that rotate along one degree-of-freedom, and 21°- azimuth, 8°- elevation for joints supporting two degrees-of-freedom.},
	articleno    = 145,
	numpages     = 23,
	keywords     = {antenna arrays, human sensing, wireless, wearable computing, Smart fabric, gesture-based interfaces, RFIDs, skeleton tracking}
}
@article{10.1145/3161190,
	title        = {Mitigating Bystander Privacy Concerns in Egocentric Activity Recognition with Deep Learning and Intentional Image Degradation},
	author       = {Dimiccoli, Mariella and Mar\'{\i}n, Juan and Thomaz, Edison},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161190},
	url          = {https://doi.org/10.1145/3161190},
	issue_date   = {December 2017},
	abstract     = {Recent advances in wearable camera technology and computer vision algorithms have greatly enhanced the automatic capture and recognition of human activities in real-world settings. While the appeal and utility of wearable camera devices for human-behavior understanding is indisputable, privacy concerns have limited the broader adoption of this method. To mitigate this problem, we propose a deep learning-based approach that recognizes everyday activities in egocentric photos that have been intentionally degraded in quality to preserve the privacy of bystanders. An evaluation on 2 annotated datasets collected in the field with a combined total of 84,078 egocentric photos showed activity recognition performance with accuracy between 79% and 88% across 17 and 21 activity classes when the images were subjected to blurring (mean filter k=20). To confirm that image degradation does indeed raise the perception of bystander privacy, we conducted a crowd sourced validation study with 640 participants; it showed a statistically significant positive relationship between the amount of image degradation and participants' willingness to be captured by wearable cameras. This work contributes to the field of privacy-sensitive activity recognition with egocentric photos by highlighting the trade-off between perceived bystander privacy protection and activity recognition performance.},
	articleno    = 132,
	numpages     = 18,
	keywords     = {Privacy, Activity Recognition, Wearable Cameras, Egocentric Vision, Image Degradation}
}
@article{10.1145/3161187,
	title        = {Comparing Speech and Keyboard Text Entry for Short Messages in Two Languages on Touchscreen Phones},
	author       = {Ruan, Sherry and Wobbrock, Jacob O. and Liou, Kenny and Ng, Andrew and Landay, James A.},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161187},
	url          = {https://doi.org/10.1145/3161187},
	issue_date   = {December 2017},
	abstract     = {With the ubiquity of mobile touchscreen devices like smartphones, two widely used text entry methods have emerged: small touch-based keyboards and speech recognition. Although speech recognition has been available on desktop computers for years, it has continued to improve at a rapid pace, and it is currently unknown how today's modern speech recognizers compare to state-of-the-art mobile touch keyboards, which also have improved considerably since their inception. To discover both methods' “upper-bound performance,” we evaluated them in English and Mandarin Chinese on an Apple iPhone 6 Plus in a laboratory setting. Our experiment was carried out using Baidu's Deep Speech 2, a deep learning-based speech recognition system, and the built-in Qwerty (English) or Pinyin (Mandarin) Apple iOS keyboards. We found that with speech recognition, the English input rate was 2.93 times faster (153 vs. 52 WPM), and the Mandarin Chinese input rate was 2.87 times faster (123 vs. 43 WPM) than the keyboard for short message transcription under laboratory conditions for both methods. Furthermore, although speech made fewer errors during entry (5.30% vs. 11.22% corrected error rate), it left slightly more errors in the final transcribed text (1.30% vs. 0.79% uncorrected error rate). Our results show that comparatively, under ideal conditions for both methods, upper-bound speech recognition performance has greatly improved compared to prior systems, and might see greater uptake in the future, although further study is required to quantify performance in non-laboratory settings for both methods.},
	articleno    = 159,
	numpages     = 23,
	keywords     = {touch keyboards, speech recognition, smartphones, Mobile phones, text entry, text input}
}
@article{10.1145/3161180,
	title        = {Enabling Interactive Infrastructure with Body Channel Communication},
	author       = {Varga, Virag and Vakulya, Gergely and Sample, Alanson and Gross, Thomas R.},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161180},
	url          = {https://doi.org/10.1145/3161180},
	issue_date   = {December 2017},
	abstract     = {Body channel communication (BCC) uses the human body to carry signals, and therefore provides communication and localization that are directly tied to human presence and actions. Previous BCC systems were expensive, could operate only in a laboratory, or only focused on special use cases. We present here an end-to-end BCC system that is designed for ambient intelligence. We introduce the BCC infrastructure that consists of portable devices (e.g., a simple sphere), mobile devices (e.g., a smartwatch-like wristband), and stationary devices (e.g., floor/wall tiles). We also describe the core technology that is used in each of these units. The TouchCom hardware-software platform is a simple transceiver with software-centered processing. The focus on software (even the implementation of the physical layer is based on software) allows the adaptivity that is necessary to operate a BCC-based system in practice. The paper describes the design and a prototype implementation of the TouchCom-based interactive infrastructure and provides evidence that this BCC infrastructure works for different persons and different setups. The system provides moderate bandwidth (about 3.5 kb/s) that is suitable for several usage scenarios like games, localization, and identification. The implemented demonstrations illustrate the benefits these applications gain when touching an object is tied to communication.},
	articleno    = 169,
	numpages     = 29,
	keywords     = {wearable devices, human-computer interaction, body channel communication, smart floor, capacitive coupling}
}
@article{10.1145/3161168,
	title        = {EyePACT: Eye-Based Parallax Correction on Touch-Enabled Interactive Displays},
	author       = {Khamis, Mohamed and Buschek, Daniel and Thieron, Tobias and Alt, Florian and Bulling, Andreas},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161168},
	url          = {https://doi.org/10.1145/3161168},
	issue_date   = {December 2017},
	abstract     = {The parallax effect describes the displacement between the perceived and detected touch locations on a touch-enabled surface. Parallax is a key usability challenge for interactive displays, particularly for those that require thick layers of glass between the screen and the touch surface to protect them from vandalism. To address this challenge, we present EyePACT, a method that compensates for input error caused by parallax on public displays. Our method uses a display-mounted depth camera to detect the user's 3D eye position in front of the display and the detected touch location to predict the perceived touch location on the surface. We evaluate our method in two user studies in terms of parallax correction performance as well as multi-user support. Our evaluations demonstrate that EyePACT (1) significantly improves accuracy even with varying gap distances between the touch surface and the display, (2) adapts to different levels of parallax by resulting in significantly larger corrections with larger gap distances, and (3) maintains a significantly large distance between two users' fingers when interacting with the same object. These findings are promising for the development of future parallax-free interactive displays.},
	articleno    = 146,
	numpages     = 18,
	keywords     = {Touch screens, Parallax, Public Displays, Gaze}
}
@article{10.1145/3161167,
	title        = {ITour: Making Tourist Maps GPS-Enabled},
	author       = {Hsu, Chih-Hsiang and Ku, Chia-Lun and Chang, Yung-Ju and Wang, Yu-Shuen and Tr\^{a}n, Uyn-Dinh and Cheng, Wen-Hao and Yang, Chu-Yuan and Hsieh, Ching-Yu and Lin, Chun-Cheng},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161167},
	url          = {https://doi.org/10.1145/3161167},
	issue_date   = {December 2017},
	abstract     = {Although tourist maps are useful resources for people to visit scenic areas, they are also commonly distorted and omit details according to the purposes and functions of a map. In this paper, we present iTour, a semi-automatic system that turns tourist maps into digital maps. By involving users in matching the road network of a tourist map and the paired standard map, our system computes road network correspondence between the two maps. By doing so, users can navigate on such GPS-enabled tourist maps using mobile devices. This transformation creates the possibility of augmenting a large number of tourist maps with digital map features. To evaluate the performance of matching road networks, we compared the presented semi-automatic interface to a manual interface. The results showed that the semi-automatic interface saved participants significant effort in generating correspondence and was perceived to require significantly less time by the participants. In addition, we conducted a field study of the iTour in comparison to using a tourist map and Google Maps together. Our results showed that iTour helped participants find their way during travel. The participants provided positive feedback on the combination of tourist maps and GPS location because of its highlights of important landmarks, showing users' locations relative to those landmarks, and saving the effort of switching tourist maps and Google Maps.},
	articleno    = 139,
	numpages     = 27,
	keywords     = {tourist map, GPS navigation, space warping, road network correspondence}
}
@article{10.1145/3161166,
	title        = {HEYEbrid: A Hybrid Approach for Mobile Calibration-Free Gaze Estimation},
	author       = {Lander, Christian and l\"{o}chtefeld, Markus and Kr\"{u}ger, Antonio},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161166},
	url          = {https://doi.org/10.1145/3161166},
	issue_date   = {December 2017},
	abstract     = {We introduce hEYEbrid, a calibration-free method for spontaneous and long-term eye gaze tracking, with competitive gaze estimation. It is based on a hybrid concept that combines infrared eye images with corneal imaging. For this, two eye cameras are mounted on a glasses frame. In this way, the pupil can be tracked quickly with high precision. This information is translated into the corneal image, which is used to create a connection to the environment, acting like a scene camera. In a user study with 20 participants, we evaluated our approach against an extended version of the system, called 3C-hEYEbrid, and a state-of-the-art head-mounted Pupil Labs eye tracker. We show that hEYEbrid provides accurate gaze estimation in unconstrained environments and is robust against calibration drift (e.g. caused by taking off and putting on the device). In addition, we present a mobile and wearable implementation of hEYEbrid and 3C-hEYEbrid, that is also usable with a monocular Pupil Labs eye tracker. It connects the head-mounted device to a mobile phone, enabling gaze estimation in real time. hEYEbrid represents a significant step towards pervasive gaze-based interfaces.},
	articleno    = 149,
	numpages     = 29,
	keywords     = {Eye Tracking, Corneal Imaging, accuracy, Gaze Estimation}
}
@article{10.1145/3131903,
	title        = {Cost-Sensitive Semi-Supervised Personalized Semantic Place Label Recognition Using Multi-Context Data},
	author       = {Wu, Xiaojie and Chen, Ling and Lv, Mingqi and Han, Mingrui and Chen, Gencai},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131903},
	url          = {https://doi.org/10.1145/3131903},
	issue_date   = {September 2017},
	abstract     = {Personalized semantic place label recognition is a promising research issue in ubiquitous computing. However, there are some problems existed in the current methods: 1) They use trajectory data to learn recognition model and ignore other context data that could reflect human behaviors over places. 2) They tend to maximize accuracy and ignore the imbalanced costs caused by the similarity between semantic place labels. 3) They use supervised learning, which cannot achieve a good performance when the number of labeled places is limited. In this paper, we exploit multi-context data to construct effective features and propose a method called CEMENT (Cost-sensitive sEmi-supervised personalized seMantic place labEl recogNiTion). On one hand, CEMENT is cost-sensitive, and it calculates misclassification costs according to the semantic similarity between place labels. On the other hand, CEMENT utilizes ensemble semi-supervised learning to leverage the unlabeled data to improve the performance. Experimental results show that the proposed method has superiority over state-of-the-art methods.},
	articleno    = 116,
	numpages     = 14,
	keywords     = {Semantic place label recognition, multi-context data, semi-supervised learning, cost-sensitive learning}
}
@article{10.1145/3131902,
	title        = {Detecting Gaze Towards Eyes in Natural Social Interactions and Its Use in Child Assessment},
	author       = {Chong, Eunji and Chanda, Katha and Ye, Zhefan and Southerland, Audrey and Ruiz, Nataniel and Jones, Rebecca M. and Rozga, Agata and Rehg, James M.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131902},
	url          = {https://doi.org/10.1145/3131902},
	issue_date   = {September 2017},
	abstract     = {Eye contact is a crucial element of non-verbal communication that signifies interest, attention, and participation in social interactions. As a result, measures of eye contact arise in a variety of applications such as the assessment of the social communication skills of children at risk for developmental disorders such as autism, or the analysis of turn-taking and social roles during group meetings. However, the automated measurement of visual attention during naturalistic social interactions is challenging due to the difficulty of estimating a subject’s looking direction from video. This paper proposes a novel approach to eye contact detection during adult-child social interactions in which the adult wears a point-of-view camera which captures an egocentric view of the child’s behavior. By analyzing the child’s face regions and inferring their head pose we can accurately identify the onset and duration of the child’s looks to their social partner’s eyes. We introduce the Pose-Implicit CNN, a novel deep learning architecture that predicts eye contact while implicitly estimating the head pose. We present a fully automated system for eye contact detection that solves the sub-problems of end-to-end feature learning and pose estimation using deep neural networks. To train our models, we use a dataset comprising 22 hours of 156 play session videos from over 100 children, half of whom are diagnosed with Autism Spectrum Disorder. We report an overall precision of 0.76, recall of 0.80, and an area under the precision-recall curve of 0.79, all of which are significant improvements over existing methods.},
	articleno    = 43,
	numpages     = 20,
	keywords     = {eye contact, autism spectrum disorder, assessment, Wearable camera, gaze classification, computer vision, machine learning, deep learning}
}
@article{10.1145/3130975,
	title        = {XRec: Behavior-Based User Recognition Across Mobile Devices},
	author       = {Wang, Xiao and Yu, Tong and Zeng, Ming and Tague, Patrick},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130975},
	url          = {https://doi.org/10.1145/3130975},
	issue_date   = {September 2017},
	abstract     = {As smartphones and tablets become increasingly prevalent, more customers have multiple devices. The multi-user, multi-device interactions inspire many problems worthy of investigation, among which recognizing users across devices has significant implications on recommendation, advertising and user experience. Unlike the binary classification problem in user identification on a single device, cross-device user recognition is essentially a set partition problem. The app back-end aims to divide user activities on devices hosting the app into groups each associated with one user. In this paper, we present XRec which leverages user behavioral patterns, namely when, where and how a user uses the app, to achieve the recognition. To address the user-device partition problem, we propose a classification-plus-refinement algorithm. To validate our approach, we conduct a field study with an Android app. We instrument the app to collect usage data from real users. We provide proof-of-concept experimental results to demonstrate how XRec can provide added value to mobile apps, with the ability to correctly match a user across multiple devices with 70% recall and 90% precision.},
	articleno    = 111,
	numpages     = 26
}
@article{10.1145/3130963,
	title        = {Sensing Cold-Induced Situational Impairments in Mobile Interaction Using Battery Temperature},
	author       = {Sarsenbayeva, Zhanna and van Berkel, Niels and Visuri, Aku and Rissanen, Sirkka and Rintamaki, Hannu and Kostakos, Vassilis and Goncalves, Jorge},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130963},
	url          = {https://doi.org/10.1145/3130963},
	issue_date   = {September 2017},
	abstract     = {Previous work has highlighted the detrimental effect of cold ambience on fine-motor skills during interaction with mobile devices. In this work, we develop a method to infer changes in finger temperature of smartphone users without the need for specialised hardware. Specifically, we demonstrate that smartphone battery temperature is a reliable gauge for determining changes to finger temperature. In addition, we show that the behaviour of smartphone battery temperature in cold settings is consistent across different smartphone models and battery configurations. Our method can be used to determine cold-induced situational impairments, and trigger interface adaptations during mobile interaction.},
	articleno    = 98,
	numpages     = 9,
	keywords     = {battery temperature, Smartphones, situational impairments, cold chamber, finger temperature, ambient temperature}
}
@article{10.1145/3130956,
	title        = {Beyond Interruptibility: Predicting Opportune Moments to Engage Mobile Phone Users},
	author       = {Pielot, Martin and Cardoso, Bruno and Katevas, Kleomenis and Serr\`{a}, Joan and Matic, Aleksandar and Oliver, Nuria},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130956},
	url          = {https://doi.org/10.1145/3130956},
	issue_date   = {September 2017},
	abstract     = {Many of today's mobile products and services engage their users proactively via push notifications. However, such notifications are not always delivered at the right moment, therefore not meeting products' and users' expectations. To address this challenge, we aim at developing an intelligent mobile system that automatically infers moments in which users are open to engage with suggested content. To inform the development of such a system, we carried out a field study with 337 mobile phone users. For 4 weeks, participants ran a study application on their primary phones. They were tasked to frequently report their current mood via a notification-administered experience-sampling questionnaire. In this study, however, we analyze whether they voluntarily engaged with content that we offered at the bottom of that questionnaire. In addition, the study app logged a wide range of data related to their phone use. Based on 120 Million phone-use events and 78,930 questionnaire notifications, we build a machine-learning model that before delivering a notification predicts whether a participant will click on the notification and subsequently engage with the offered content. When compared to a na\"{\i}ve baseline, which emulates current non-intelligent engagement strategies, our model achieves 66.6% higher success rate in its predictions. If the model also considers the user's past behavior, predictions improve 5-fold over the baseline. Based on these findings, we discuss the implications for building an intelligent service that identifies opportune moments for proactive user engagement, while, at the same time, reduces the number of undesirable interruptions.},
	articleno    = 91,
	numpages     = 25,
	keywords     = {Proactive Recommendations, Mobile Devices, Conversion, Push Notifications}
}
@article{10.1145/3130951,
	title        = {Detecting Emerging Activity-Based Working Traits through Wearable Technology},
	author       = {Montanari, Alessandro and Mascolo, Cecilia and Sailer, Kerstin and Nawaz, Sarfraz},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130951},
	url          = {https://doi.org/10.1145/3130951},
	issue_date   = {September 2017},
	abstract     = {A recent trend in corporate real-estate is Activity-Based Working (ABW). The ABW concept removes designated desks but offers different work settings designed to support typical work activities. In this context there is still a need for objective data to understand the implications of these design decisions. We aim to contribute by using automated data collection to study how ABW’s principles impact office usage and dynamics.To this aim we analyse team dynamics and employees’ tie strength in relation to space usage and organisational hierarchy using data collected with wearable devices in a company adopting ABW principles. Our findings show that the office fosters interactions across team boundaries and among the lower levels of the hierarchy suggesting a strong lateral communication. Employees also tend to have low space exploration on a daily basis which is instead more prevalent during an average week and strong social clusters seem to be resisting the ABW principles of space dynamics. With the availability of two additional data sets about social encounters in traditional offices we highlight traits emerging from the application of ABW’s principles. In particular, we observe how the absence of designated desks might be responsible for more rapid dynamics inside the office.In more general terms, this work opens the door to new and scalable technology-based methodologies to study dynamic office usage and social interactions.},
	articleno    = 86,
	numpages     = 24,
	keywords     = {Bluetooth Low Energy, Actvity-Based Working, Office Analytics, Mobile Sensing}
}
@article{10.1145/3130943,
	title        = {Supporting Social Interactions with an Expressive Heart Rate Sharing Application},
	author       = {Liu, Fannie and Dabbish, Laura and Kaufman, Geoff},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130943},
	url          = {https://doi.org/10.1145/3130943},
	issue_date   = {September 2017},
	abstract     = {The present work explores the social dynamics of expressive biosignals: leveraging wearable technologies to introduce sensed physiological data as a means of clarifying the emotional or psychological processes underlying our subjective experiences. We developed an Android application that linked to a wearable heart rate sensor and allowed for the direct sharing and real-time broadcasting of users’ heart rate via text messaging. We deployed this application in a two-week field study to investigate the contextual triggers, perceptions, and consequences of users’ sharing behaviors. The study (N=13) utilized a combination of Experience Sampling Methodology (ESM) and qualitative interviews to discover the situations in which users were more or less likely to share their heart rate with contacts, and the subsequent interactions that occurred after sharing. The results revealed that participants used heart rate sharing as a means to express emotions and provide daily updates, as well as simply a novel and playful form of communication. They reported a variety of communicative consequences of their sharing as well as specific logistical and psychological barriers to sharing. The implications of these results for the design of expressive biosignal sharing systems for supporting positive social interactions are discussed.},
	articleno    = 77,
	numpages     = 26,
	keywords     = {psychological states, Wearable sensors, experience sampling, heart rate, interpersonal communication, physiological signals}
}
@article{10.1145/3130938,
	title        = {Auto++: Detecting Cars Using Embedded Microphones in Real-Time},
	author       = {Li, Sugang and Fan, Xiaoran and Zhang, Yanyong and Trappe, Wade and Lindqvist, Janne and Howard, Richard E.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130938},
	url          = {https://doi.org/10.1145/3130938},
	issue_date   = {September 2017},
	abstract     = {In this work, we propose a system that detects approaching cars for smartphone users. In addition to detecting the presence of a vehicle, it can also estimate the vehicle’s driving direction, as well as count the number of cars around the user. We achieve these goals by processing the acoustic signal captured by microphones embedded in the user’s mobile phone. The largest challenge we faced involved addressing the fact that vehicular noise is predominantly due to tire-road friction, and therefore lacked strong (frequency) formant or temporal structure. Additionally, outdoor environments have complex acoustic noise characteristics, which are made worse when the signal is captured by non-professional grade microphones embedded in smartphones. We address these challenges by monitoring a new feature: maximal frequency component that crosses a threshold. We extract this feature with a blurred edge detector. Through detailed experiments, we found our system to be robust across different vehicles and environmental conditions, and thereby support unsupervised car detection and counting. We evaluated our system using audio tracks recorded from seven different models of cars, including SUVs, medium-sized sedans, compact cars, and electric cars. We also tested our system with the user walking in various outdoor environments including parking lots, campus roads, residential areas, and shopping centers. Our results show that we can accurately and robustly detect cars with low CPU and memory requirements.},
	articleno    = 70,
	numpages     = 20
}
@article{10.1145/3130920,
	title        = {CrowdStory: Fine-Grained Event Storyline Generation by Fusion of Multi-Modal Crowdsourced Data},
	author       = {Guo, Bin and Ouyang, Yi and Zhang, Cheng and Zhang, Jiafan and Yu, Zhiwen and Wu, Di and Wang, Yu},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130920},
	url          = {https://doi.org/10.1145/3130920},
	issue_date   = {September 2017},
	abstract     = {Event summarization based on crowdsourced microblog data is a promising research area, and several researchers have recently focused on this field. However, these previous works fail to characterize the fine-grained evolution of an event and the rich correlations among posts. The semantic associations among the multi-modal data in posts are also not investigated as a means to enhance the summarization performance. To address these issues, this study presents CrowdStory, which aims to characterize an event as a fine-grained, evolutionary, and correlation-rich storyline. A crowd-powered event model and a generic event storyline generation framework are first proposed, based on which a multi-clue--based approach to fine-grained event summarization is presented. The implicit human intelligence (HI) extracted from visual contents and community interactions is then used to identify inter-clue associations. Finally, a cross-media mining approach to selective visual story presentation is proposed. The experiment results indicate that, compared with the state-of-the-art methods, CrowdStory enables fine-grained event summarization (e.g., dynamic evolution) and correctly identifies up to 60% strong correlations (e.g., causality) of clues. The cross-media approach shows diversity and relevancy in visual data selection.},
	articleno    = 55,
	numpages     = 19,
	keywords     = {Mobile Crowdsourcing, Fine-grained, Event Sensing, Correlation, Storyline}
}
@article{10.1145/3130911,
	title        = {Lessons Learned from Two Cohorts of Personal Informatics Self-Experiments},
	author       = {Daskalova, Nediyana and Desingh, Karthik and Papoutsaki, Alexandra and Schulze, Diane and Sha, Han and Huang, Jeff},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130911},
	url          = {https://doi.org/10.1145/3130911},
	issue_date   = {September 2017},
	abstract     = {Self-experiments allow people to investigate their own individual outcomes from behavior change, often with the aid of personal tracking devices. The challenge is to design scientifically valid self-experiments that can reach conclusive results. In this paper, we aim to understand how novices run self-experiments when they are provided with a structured lesson in experimental design. We conducted a study on self-experimentation with two cohorts of students, where a total of 34 students performed a self-experiment of their choice. In the first cohort, students were given only two restrictions: a specific number of variables to track and a set duration for the study. The findings from this cohort helped us generate concrete guidelines for running a self-experiment, and use them as the format for the next cohort. A second cohort of students used these guidelines to conduct their own self-experiments in a more structured manner. Based on the findings from both cohorts, we propose a set of guidelines for running successful self-experiments that address the pitfalls encountered by students in the study, such as inadequate study design and analysis methods. We also discuss broader implications for future self-experimenters and designers of tools for self-experimentation.},
	articleno    = 46,
	numpages     = 22,
	keywords     = {quantified self, personal informatics, self-tracking}
}
@article{10.1145/3090085,
	title        = {BiliScreen: Smartphone-Based Scleral Jaundice Monitoring for Liver and Pancreatic Disorders},
	author       = {Mariakakis, Alex and Banks, Megan A. and Phillipi, Lauren and Yu, Lei and Taylor, James and Patel, Shwetak N.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090085},
	url          = {https://doi.org/10.1145/3090085},
	issue_date   = {June 2017},
	abstract     = {Pancreatic cancer has one of the worst survival rates amongst all forms of cancer because its symptoms manifest later into the progression of the disease. One of those symptoms is jaundice, the yellow discoloration of the skin and sclera due to the buildup of bilirubin in the blood. Jaundice is only recognizable to the naked eye in severe stages, but a ubiquitous test using computer vision and machine learning can detect milder forms of jaundice. We propose BiliScreen, a smartphone app that captures pictures of the eye and produces an estimate of a person's bilirubin level, even at levels normally undetectable by the human eye. We test two low-cost accessories that reduce the effects of external lighting: (1) a 3D-printed box that controls the eyes' exposure to light and (2) paper glasses with colored squares for calibration. In a 70-person clinical study, we found that BiliScreen with the box achieves a Pearson correlation coefficient of 0.89 and a mean error of -0.09 ± 2.76 mg/dl in predicting a person's bilirubin level. As a screening tool, BiliScreen identifies cases of concern with a sensitivity of 89.7% and a specificity of 96.8% with the box accessory.},
	articleno    = 20,
	numpages     = 26,
	keywords     = {image processing, Health sensing, jaundice, smartphones, bilirubin}
}
@article{10.1145/3090083,
	title        = {BlindType: Eyes-Free Text Entry on Handheld Touchpad by Leveraging Thumb's Muscle Memory},
	author       = {Lu, Yiqin and Yu, Chun and Yi, Xin and Shi, Yuanchun and Zhao, Shengdong},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090083},
	url          = {https://doi.org/10.1145/3090083},
	issue_date   = {June 2017},
	abstract     = {Eyes-free input is desirable for ubiquitous computing, since interacting with mobile and wearable devices often competes for visual attention with other devices and tasks. In this paper, we explore eyes-free typing on a touchpad using one thumb, wherein a user taps on an imaginary QWERTY keyboard while receiving text feedback on a separate screen. Our hypothesis is that users can transfer their typing ability obtained from visible keyboards to eyes-free use. We propose two statistical decoding algorithms to infer users’ eyes-free input: the absolute algorithm and the relative algorithm. The absolute algorithm infers user input based on the absolute position of touch endpoints, while the relative algorithm infers based on the vectors between successive touch endpoints. Evaluation results showed users could achieve satisfying performance with both algorithms. Text entry rate was 17-23 WPM (words per minute) depending on the algorithm used. In comparison, a baseline cursor-based text entry method yielded only 7.66 WPM. In conclusion, our research demonstrates for the first time the feasibility of thumb-based eyes-free typing, which provides a new possibility for text entry on ubiquitous computing platforms such as smart TVs and HMDs.},
	articleno    = 18,
	numpages     = 24,
	keywords     = {text entry, smart TV interaction, Eyes-free input, HMDs}
}
@article{10.1145/3090076,
	title        = {Ensembles of Deep LSTM Learners for Activity Recognition Using Wearables},
	author       = {Guan, Yu and Pl\"{o}tz, Thomas},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090076},
	url          = {https://doi.org/10.1145/3090076},
	issue_date   = {June 2017},
	abstract     = {Recently, deep learning (DL) methods have been introduced very successfully into human activity recognition (HAR) scenarios in ubiquitous and wearable computing. Especially the prospect of overcoming the need for manual feature design combined with superior classification capabilities render deep neural networks very attractive for real-life HAR applications. Even though DL-based approaches now outperform the state-of-the-art in a number of recognition tasks, still substantial challenges remain. Most prominently, issues with real-life datasets, typically including imbalanced datasets and problematic data quality, still limit the effectiveness of activity recognition using wearables. In this paper we tackle such challenges through Ensembles of deep Long Short Term Memory (LSTM) networks. LSTM networks currently represent the state-of-the-art with superior classification performance on relevant HAR benchmark datasets. We have developed modified training procedures for LSTM networks and combine sets of diverse LSTM learners into classifier collectives. We demonstrate that Ensembles of deep LSTM learners outperform individual LSTM networks and thus push the state-of-the-art in human activity recognition using wearables. Through an extensive experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition capabilities of our approach and its potential for real-life applications of human activity recognition.},
	articleno    = 11,
	numpages     = 28,
	keywords     = {LSTM, ensembles, activity recognition, deep learning}
}
@article{10.1145/3090075,
	title        = {Augmenting Audits: Exploring the Role of Sensor Toolkits in Sustainable Buildings Management},
	author       = {Finnigan, S. Mitchell and Clear, A. K. and Farr-Wharton, G. and Ladha, K. and Comber, R.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090075},
	url          = {https://doi.org/10.1145/3090075},
	issue_date   = {June 2017},
	abstract     = {Audits are commonly carried out by facilities managers (FMs) to quantify the sustainability and performance of the buildings they manage, informing improvements to infrastructure for resource and cost savings, and assessing compliance with standards and legislation. The scope for what can be audited is limited by available infrastructure. In this article, we investigate the utility of a flexible sensor toolkit to enhance existing energy auditing practices. We present findings from a qualitative study with FM and student auditor participants from 3 organisations. Our study covers how these toolkits were used and integrated into auditing practices within these organisations, and the opportunities and issues for resource management that arose as a result. We conclude with design implications for toolkits to support sensor-augmented audits, make recommendations towards a deployment protocol for sensor toolkits used in this context, and develop broader considerations for how future standards and policies might be adapted to leverage this potential.},
	articleno    = 10,
	numpages     = 19,
	keywords     = {Sensor Toolkits, Audits, Data, Practices, Sustainability, Professionals, Building Management, Energy}
}
@article{10.1145/3090054,
	title        = {Mago: Mode of Transport Inference Using the Hall-Effect Magnetic Sensor and Accelerometer},
	author       = {Chen, Ke-Yu and Shah, Rahul C. and Huang, Jonathan and Nachman, Lama},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090054},
	url          = {https://doi.org/10.1145/3090054},
	issue_date   = {June 2017},
	abstract     = {In this paper, we introduce Mago, a novel system that can infer a person's mode of transport (MOT) using the Hall-effect magnetic sensor and accelerometer present in most smart devices. When a vehicle is moving, the motions of its mechanical components such as the wheels, transmission and the differential distort the earth's magnetic field. The magnetic field is distorted corresponding to the vehicle structure (e.g., bike chain or car transmission system), which manifests itself as a strong signal for sensing a person's transportation modality. We utilize this magnetic signal combined with the accelerometer and design a robust algorithm for the MOT detection. In particular, our system extracts frame-based features from the sensor data and can run in nearly real-time with only a few seconds of delay. We evaluated Mago using over 70 hours of daily commute data from 7 participants and the leave-one-out analysis of our cross-user, cross-device model reports an average accuracy of 94.4% among seven classes (stationary, bus, bike, car, train, light rail and scooter). Besides MOT, our system is able to reliably differentiate the phone's in-car position at an average accuracy of 92.9%. We believe Mago could potentially benefit many contextually-aware applications that require MOT detection such as a digital personal assistant or a life coaching application.},
	articleno    = 8,
	numpages     = 23,
	keywords     = {driver, ubiquitous computing, sensing, mobile, Magnetic-field sensing, magnetic field, wearables, MOT, detection, passenger, accelerometer, mode of transport, in-car}
}
@article{10.1145/3075960,
	title        = {Editorial},
	author       = {Abowd, Gregory D. and Kostakos, Vassilis and Santini, Silvia and Scott, James and Yatani, Koji},
	year         = 2017,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 1,
	doi          = {10.1145/3075960},
	url          = {https://doi.org/10.1145/3075960},
	issue_date   = {March 2017},
	numpages     = 1
}
@article{10.1145/3053330,
	title        = {DualBlink: A Wearable Device to Continuously Detect, Track, and Actuate Blinking For Alleviating Dry Eyes and Computer Vision Syndrome},
	author       = {Dementyev, Artem and Holz, Christian},
	year         = 2017,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 1,
	doi          = {10.1145/3053330},
	url          = {https://doi.org/10.1145/3053330},
	issue_date   = {March 2017},
	abstract     = {Increased visual attention, such as during computer use leads to less blinking, which can cause dry eyes—the leading cause of computer vision syndrome. As people spend more time looking at screens on mobile and desktop devices, computer vision syndrome is becoming epidemic in today's population, leading to blurry vision, fatigue, and a reduced quality of life.One way to alleviate dry eyes is increased blinking. In this paper, we present a series of glasses-mounted devices that track the wearer's blink rate and, upon absent blinks, trigger blinks through actuation: light flashes, physical taps, and small puffs of air near the eye. We conducted a user study to evaluate the effectiveness of our devices and found that air puff and physical tap actuations result in a 36% increase in participants’ average blink rate. Air puff thereby struck the best compromise between effective blink actuations and low distraction ratings from participants. In a follow-up study, we found that high intensity, short puffs near the eye were most effective in triggering blinks while receiving only low-rated distraction and invasiveness ratings from participants. We conclude this paper with two miniaturized and self-contained DualBlink prototypes, one integrated into the frame of a pair of glasses and the other one as a clip-on for existing glasses. We believe that DualBlink can serve as an always-available and viable option to treat computer vision syndrome in the future.},
	articleno    = 1,
	numpages     = 19,
	keywords     = {Computer vision syndrome, Dry eyes, well-being, CVS}
}
@article{10.1145/3130977,
	title        = {SuperpowerGlass: A Wearable Aid for the At-Home Therapy of Children with Autism},
	author       = {Washington, Peter and Voss, Catalin and Kline, Aaron and Haber, Nick and Daniels, Jena and Fazel, Azar and De, Titas and Feinstein, Carl and Winograd, Terry and Wall, Dennis},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130977},
	url          = {https://doi.org/10.1145/3130977},
	issue_date   = {September 2017},
	abstract     = {We have developed a system for automatic facial expression recognition running on Google Glass, delivering real-time social cues to children with Autism Spectrum Disorder (ASD). The system includes multiple mechanisms to engage children and their parents, who administer this technology within the home. We completed an at-home design trial with 14 families that used the learning aid over a 3-month period. We found that children with ASD generally respond well to wearing the system at home and opt for the most expressive feedback choice. We further evaluated app usage, facial engagement, and model accuracy. We found that the device can act as a powerful training aid when used periodically in the home, that interactive video content from wearable therapy sessions should be augmented with sufficient context about the content to produce long-term engagement, and that the design of wearable systems for children with ASD should be heavily dependent on the functioning level of the child. We contribute general design implications for developing wearable aids used by children with ASD and other behavioral disorders as well as their parents during at-home parent-administered therapy sessions.},
	articleno    = 112,
	numpages     = 22,
	keywords     = {Wearable Computing, Autism, Behavior Therapy}
}
@article{10.1145/3130966,
	title        = {Running with Technology: Evaluating the Impact of Interacting with Wearable Devices on Running Movement},
	author       = {Seuter, Matthias and Pfeiffer, Max and Bauer, Gernot and Zentgraf, Karen and Kray, Christian},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130966},
	url          = {https://doi.org/10.1145/3130966},
	issue_date   = {September 2017},
	abstract     = {The use of wearable devices during running has become commonplace. Although there is ongoing research on interaction techniques for use while running, the effects of the resulting interactions on the natural movement patterns have received little attention so far. While previous studies on pedestrians reported increased task load and reduced walking speed while interacting, running movement further restricts interaction and requires minimizing interferences, e.g. to avoid injuries and maximize comfort. In this paper, we aim to shed light on how interacting with wearable devices affects running movement. We present results from a motion-tracking study (N=12) evaluating changes in movement and task load when users interact with a smartphone, a smartwatch, or a pair of smartglasses while running. In our study, smartwatches required less effort than smartglasses when using swipe input, resulted in less interference with the running movement and were preferred overall. From our results, we infer a number of guidelines regarding interaction design targeting runners.},
	articleno    = 101,
	numpages     = 17,
	keywords     = {wearables, evaluation, movement, running, interaction, motion capture}
}
@article{10.1145/3130937,
	title        = {Reconstructing Hand Poses Using Visible Light},
	author       = {Li, Tianxing and Xiong, Xi and Xie, Yifei and Hito, George and Yang, Xing-Dong and Zhou, Xia},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130937},
	url          = {https://doi.org/10.1145/3130937},
	issue_date   = {September 2017},
	abstract     = {Free-hand gestural input is essential for emerging user interactions. We present Aili, a table lamp reconstructing a 3D hand skeleton in real time, requiring neither cameras nor on-body sensing devices. Aili consists of an LED panel in a lampshade and a few low-cost photodiodes embedded in the lamp base. To reconstruct a hand skeleton, Aili combines 2D binary blockage maps from vantage points of different photodiodes, which describe whether a hand blocks light rays from individual LEDs to all photodiodes. Empowering a table lamp with sensing capability, Aili can be seamlessly integrated into the existing environment. Relying on such low-level cues, Aili entails lightweight computation and is inherently privacy-preserving. We build and evaluate an Aili prototype. Results show that Aili’s algorithm reconstructs a hand pose within 7.2 ms on average, with 10.2° mean angular deviation and 2.5-mm mean translation deviation in comparison to Leap Motion. We also conduct user studies to examine the privacy issues of Leap Motion and solicit feedback on Aili’s privacy protection. We conclude by demonstrating various interaction applications Aili enables.},
	articleno    = 71,
	numpages     = 20,
	keywords     = {Gestural input, 3D hand reconstruction, visible light sensing}
}
@article{10.1145/3130927,
	title        = {Activity Recognition for Quality Assessment of Batting Shots in Cricket Using a Hierarchical Representation},
	author       = {Khan, Aftab and Nicholson, James and Pl\"{o}tz, Thomas},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130927},
	url          = {https://doi.org/10.1145/3130927},
	issue_date   = {September 2017},
	abstract     = {Quality assessment in cricket is a complex task that is performed by understanding the combination of individual activities a player is able to perform and by assessing how well these activities are performed. We present a framework for inexpensive and accessible, automated recognition of cricketing shots. By means of body-worn inertial measurement units, movements of batsmen are recorded, which are then analysed using a parallelised, hierarchical recognition system that automatically classifies relevant categories of shots as required for assessing batting quality. Our system then generates meaningful visualisations of key performance parameters, including feet positions, attack/defence, and distribution of shots around the ground. These visualisations are the basis for objective skill assessment thereby focusing on specific personal improvement points as identified through our system. We evaluated our framework through a deployment study where 6 players engaged in batting exercises. Based on the recorded movement data we could automatically identify 20 classes of unique batting shot components with an average F1-score greater than 88%. This analysis is the basis for our detailed analysis of our study participants’ skills. Our system has the potential to rival expensive vision-based systems but at a fraction of the cost.},
	articleno    = 62,
	numpages     = 31,
	keywords     = {Hierarchical models, Activity Recognition, Skill Assessment, Sports}
}
@article{10.1145/3130917,
	title        = {Intelligent Interruption Management Using Electro Dermal Activity Based Physiological Sensor for Collaborative Sensemaking},
	author       = {Goyal, Nitesh and Fussell, Susan R.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130917},
	url          = {https://doi.org/10.1145/3130917},
	issue_date   = {September 2017},
	abstract     = {Sensemaking tasks are difficult to accomplish with limited time and attentional resources because analysts are faced with a constant stream of new information. While this information is often important, the timing of the interruptions may detract from analyst's work. In an ideal world, there would be no interruptions. But that is not the case in real world sensemaking tasks. So, in this study, we explore the value of timing interruptions based on an analyst's state of arousal as detected by Electrodermal activity derived form galvanic skin response (EDA). In a laboratory study, we compared performance when interruptions were timed to occur during increasing arousal, decreasing arousal, at random intervals or not at all. Analysts performed significantly better when interruptions occurred during periods of increasing arousal than when they were random. Further, analysts rated process component of team experience significantly higher also during periods of increasing arousal than when they were random. Self-reported workload was not impacted by interruptions timing. We discuss how system designs could leverage inexpensive off-the-shelf wrist sensors to improve interruption timing.},
	articleno    = 52,
	numpages     = 21,
	keywords     = {Sensemaking, EDA, interruption, Collaborative Sensemaking, interface design, Analytics, Galvanic Skin Response, notification}
}
@article{10.1145/3130899,
	title        = {Drone Near Me: Exploring Touch-Based Human-Drone Interaction},
	author       = {Abtahi, Parastoo and Zhao, David Y. and E., Jane L. and Landay, James A.},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130899},
	url          = {https://doi.org/10.1145/3130899},
	issue_date   = {September 2017},
	abstract     = {Personal drones are becoming more mainstream and are used for a variety of tasks, such as delivery and photography. The exposed blades in conventional drones raise serious safety concerns. To address this, commercial drones have been moving towards a safe-to-touch design or have increased safety by adding propeller guards. The affordances of safe-to-touch drones enable new types of touch-based human-drone interaction. Various applications have been explored, such as augmented sports and haptic feedback in virtual reality; however, it is unclear if individuals feel comfortable using direct touch and manipulation when interacting with safe-to-touch drones. A previous elicitation study showed how users naturally interact with drones. We replicated this study with an unsafe and a safe-to-touch drone, to find out if participants will instinctively use touch as a means of interacting with the safe-to-touch drone. We found that 58% of the participants used touch, and across all tasks 39% of interactions were touch-based. The proposed touch interactions were in agreement for 67% of the tasks, and users reported that interacting with the safe-to-touch drone was significantly less mentally demanding than the unsafe drone.},
	articleno    = 34,
	numpages     = 8,
	keywords     = {UAV, quadcopter, human-drone interaction, Drone, elicitation study, touch interaction}
}
@article{10.1145/3090086,
	title        = {Can Less Be More? Contrasting Limited, Unlimited, and Automatic Picture Capture for Augmenting Memory Recall},
	author       = {Niforatos, Evangelos and Cinel, Caterina and Mack, Cathleen Cortis and Langheinrich, Marc and Ward, Geoff},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090086},
	url          = {https://doi.org/10.1145/3090086},
	issue_date   = {June 2017},
	abstract     = {Today's abundance1 of cheap digital storage in the form of tiny memory cards put literally no bounds on the number of images one can capture with one's digital camera or smartphone during an event. However, prior work has shown that taking many pictures may actually make us remember less of a particular event. Does automated picture taking (lifelogging) help avoid this, yet still offer to capture meaningful pictures? In this work, we investigate the effect of capture modality (i.e., limited, unlimited, automatic, and no capture) on people's ability to recall a past event – with and without the support of the pictures captured through these modalities. Our results from a field experiment with 83 participants show that capturing fewer pictures does not necessarily lead to the capture of more relevant pictures. However, when controlling for number of pictures taken, our results show that having a limited number of pictures to capture may lead to pictures with increased memory value. At the same time, automated capture failed to produce pictures that would help remember the past experience better.},
	articleno    = 21,
	numpages     = 22,
	keywords     = {Memory Recall, Human Memory Augmentation, Lifelogging, Picture Capture, Picture Review}
}
@article{10.1145/3090080,
	title        = {Beautiful…but at What Cost? An Examination of Externalities in Geographic Vehicle Routing},
	author       = {Johnson, I. and Henderson, J. and Perry, C. and Sch\"{o}ning, J. and Hecht, B.},
	year         = 2017,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 2,
	doi          = {10.1145/3090080},
	url          = {https://doi.org/10.1145/3090080},
	issue_date   = {June 2017},
	abstract     = {Millions of people use platforms such as Google Maps to search for routes to their desired destinations. Recently, researchers and mapping platforms have shown growing interest in optimizing routes for criteria other than travel time, e.g. simplicity, safety, and beauty. However, despite the ubiquity of algorithmic routing and its potential to define how millions of people move around the world, very little is known about the externalities that arise when adopting these new optimization criteria, e.g. potential redistribution of traffic to certain neighborhoods and increased route complexity (with its associated risks). In this paper, we undertake the first controlled examination of these externalities, doing so across multiple mapping platforms, alternative optimizations, and cities. We find, for example, that scenic routing (i.e. “beauty”-optimized routing) would remove vehicles from highways, greatly increase traffic around parks, and, in certain cases, do the same for high-income areas. Our results also highlight that the interaction between routing criteria and urban structure is complex and effects vary from city to city, an important consideration for the growing literature on alternative routing strategies. Finally, to address the lack of open implementations of alternative routing algorithms and controlled routing evaluation frameworks, we are releasing our alternative routing and evaluation platform with this paper.},
	articleno    = 15,
	numpages     = 21,
	keywords     = {Geography, alternative routing, urban structure, Vehicle routing, externalities}
}
@article{10.1145/3534616,
	title        = {Sensurfaces: A Novel Approach for Embedded Touch Sensing on Everyday Surfaces},
	author       = {Parilusyan, Brice and Teyssier, Marc and Martinez-Missir, Valentin and Duhart, Cl\'{e}ment and Serrano, Marcos},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534616},
	url          = {https://doi.org/10.1145/3534616},
	issue_date   = {July 2022},
	abstract     = {Ubiquitous touch sensing surfaces are largely influenced by touchscreens' look and feel and fail to express the physical richness of existing surrounding materials. We introduce Sensurfaces, a plug-and-play electronic module that allows to rapidly experiment with touch-sensitive surfaces while preserving the original appearance of materials. Sensurfaces is composed of plug-and-play modules that can be connected together to expand the size and number of materials composing a sensitive surface. The combination of Sensurfaces modules allows the creation of small or large multi-material sensitive surfaces that can detect multi-touch but also body proximity, pose, pass, or even human steps. In this paper, we present the design and implementation of Sensurfaces. We propose a design space describing the factors of Sensurfaces interfaces. Then, through a series of technical evaluations, we demonstrate the capabilities of our system. Finally, we report on two workshops validating the usability of our system.},
	articleno    = 67,
	numpages     = 19,
	keywords     = {WiFi, multi-touch interface, modularity, multi-material, capacitive, scalable}
}
@article{10.1145/3534607,
	title        = {SafeGait: Safeguarding Gait-Based Key Generation against Vision-Based Side Channel Attack Using Generative Adversarial Network},
	author       = {Wu, Yuezhong and Hassan, Mahbub and Hu, Wen},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534607},
	url          = {https://doi.org/10.1145/3534607},
	issue_date   = {July 2022},
	abstract     = {Recent works have shown that wearable or implanted devices attached at different locations of the body can generate an identical security key from their independent measurements of the same gait. This has created an opportunity to realize highly secured data exchange to and from critical implanted devices. In this paper, we first demonstrate that vision can be used to easily attack such gait-based key generations; an attacker with a commodity camera can measure the gait from a distance and generate a security key with any target wearable or implanted device faster than other legitimate devices worn at different locations of the subject's body. To counter the attack, we propose a firewall to stop video-based gait measurements to proceed with key generation, but letting measurements from inertial measurement units (IMUs) that are widely used in wearable devices to measure the gait accelerations from the body to proceed. We implement the firewall concept with an IMU-vs-Video binary classifier that combines InceptionTime, an ensemble of deep Convolutional Neural Network (CNN) models for effective feature extraction from gait measurements, to a Generative Adversarial Network (GAN) that can generalize the classifier across subjects. Comprehensive evaluation with a real-world dataset shows that our proposed classifier can perform with an accuracy of 97.82%. Given that an attacker has to fool the classifier for multiple consecutive gait cycles to generate the complete key, the high single-cycle classification accuracy results in an extremely low probability for a video attacker to successfully pair with a target wearable device. More precisely, a video attacker would have one in a billion chance to successfully generate a 128-bit key, which would require the attacker to observe the subject for thousands of years.},
	articleno    = 80,
	numpages     = 27,
	keywords     = {Side-channel Attack, Key Generation, Gait, Deep Learning}
}
@article{10.1145/3534599,
	title        = {Wet-Ra: Monitoring Diapers Wetness with Wireless Signals},
	author       = {Xue, Meng and Chen, Yanjiao and Gong, Xueluan and Zhang, Jian and Fan, Chunkai},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534599},
	url          = {https://doi.org/10.1145/3534599},
	issue_date   = {July 2022},
	abstract     = {Diaper wetness monitoring is essential in various situations (e.g., babies and patients) to guarantee hygiene and avoid embarrassment. Existing diaper wetness monitoring methods include indicator lines, special sensors, and RFID, which require modifications on every diaper piece and cannot be easily checked under visual occlusions (e.g., trousers). In this paper, we introduce Wet-Ra, a contactless, ubiquitous, and user-friendly diaper wetness monitoring system based on RF signals. To extract informative features for wetness detection from RF signals, we construct Continuous-Radio-Snapshot and build corresponding signal representations that capture the distinct patterns of diapers of different wetness levels. We refine the signal representation by eliminating multi-path interference from the environment and mitigating the smearing effect with wavelet multisynchrosqueezing transform. To expand the usability of Wet-Ra, we build a transferable model that yields robust detection results in diversified environments and for new users. We conduct extensive experiments to evaluate Wet-Ra with 47 volunteers in 7 different rooms with three off-the-shelf diaper brands. Experiment results confirm that Wet-Ra can accurately identify diaper wetness in the real environment.},
	articleno    = 84,
	numpages     = 26,
	keywords     = {Contactless Sensing, Wetness monitoring, Wireless Sensing, Diaper}
}
@article{10.1145/3534592,
	title        = {Wavesdropper: Through-Wall Word Detection of Human Speech via Commercial MmWave Devices},
	author       = {Wang, Chao and Lin, Feng and Ba, Zhongjie and Zhang, Fan and Xu, Wenyao and Ren, Kui},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534592},
	url          = {https://doi.org/10.1145/3534592},
	issue_date   = {July 2022},
	abstract     = {Most existing eavesdropping attacks leverage propagating sound waves for speech retrieval. However, soundproof materials are widely deployed in speech-sensitive scenes (e.g., a meeting room). In this paper, we reveal that human speech protected by an isolated room can be compromised by portable and commercial off-the-shelf mmWave devices. To achieve this goal, we develop Wavesdropper, a word detection system that utilizes a mmWave probe to sense the targeted speaker's throat vibration and recover speech contents in the obstructed condition. We proposed a CEEMD-based method to suppress dynamic clutters (e.g., human movements) in the room and a wavelet-based processing method to extract the delicate vocal vibration information from the hybrid signals. To recover speech contents from mmWave signals related to the vocal vibration, we designed a neural network to infer the speech contents. Moreover, we explored word detection on a conversation with multiple (two) probes and reveal that the adversary can detect words on multiple people simultaneously with only one mmWave device. We performed extensive experiments to evaluate the system performance with over 60,000 pronunciations. The experimental results indicate that Wavesdropper can achieve 91.3% accuracy for 57-word recognition on 23 volunteers.},
	articleno    = 77,
	numpages     = 26,
	keywords     = {through walls, mmWave sensing, word detection}
}
@article{10.1145/3534587,
	title        = {Learning on the Rings: Self-Supervised 3D Finger Motion Tracking Using Wearable Sensors},
	author       = {Zhou, Hao and Lu, Taiting and Liu, Yilin and Zhang, Shijia and Gowda, Mahanth},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534587},
	url          = {https://doi.org/10.1145/3534587},
	issue_date   = {July 2022},
	abstract     = {This paper presents ssLOTR (self-supervised learning on the rings), a system that shows the feasibility of designing self-supervised learning based techniques for 3D finger motion tracking using a custom-designed wearable inertial measurement unit (IMU) sensor with a minimal overhead of labeled training data. Ubiquitous finger motion tracking enables a number of applications in augmented and virtual reality, sign language recognition, rehabilitation healthcare, sports analytics, etc. However, unlike vision, there are no large-scale training datasets for developing robust machine learning (ML) models on wearable devices. ssLOTR designs ML models based on data augmentation and self-supervised learning to first extract efficient representations from raw IMU data without the need for any training labels. The extracted representations are further trained with small-scale labeled training data. In comparison to fully supervised learning, we show that only 15% of labeled training data is sufficient with self-supervised learning to achieve similar accuracy. Our sensor device is designed using a two-layer printed circuit board (PCB) to minimize the footprint and uses a combination of Polylactic acid (PLA) and Thermoplastic polyurethane (TPU) as housing materials for sturdiness and flexibility. It incorporates a system-on-chip (SoC) microcontroller with integrated WiFi/Bluetooth Low Energy (BLE) modules for real-time wireless communication, portability, and ubiquity. In contrast to gloves, our device is worn like rings on fingers, and therefore, does not impede dexterous finger motion. Extensive evaluation with 12 users depicts a 3D joint angle tracking accuracy of 9.07° (joint position accuracy of 6.55mm) with robustness to natural variation in sensor positions, wrist motion, etc, with low overhead in latency and power consumption on embedded platforms.},
	articleno    = 90,
	numpages     = 31,
	keywords     = {Wearable, Self-supervised learning, IoT, Finger motion tracking}
}
@article{10.1145/3534577,
	title        = {Combating False Data Injection Attacks on Human-Centric Sensing Applications},
	author       = {Xin, Jingyu and Phoha, Vir V. and Salekin, Asif},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534577},
	url          = {https://doi.org/10.1145/3534577},
	issue_date   = {July 2022},
	abstract     = {The recent prevalence of machine learning-based techniques and smart device embedded sensors has enabled widespread human-centric sensing applications. However, these applications are vulnerable to false data injection attacks (FDIA) that alter a portion of the victim's sensory signal with forged data comprising a targeted trait. Such a mixture of forged and valid signals successfully deceives the continuous authentication system (CAS) to accept it as an authentic signal. Simultaneously, introducing a targeted trait in the signal misleads human-centric applications to generate specific targeted inference; that may cause adverse outcomes. This paper evaluates the FDIA's deception efficacy on sensor-based authentication and human-centric sensing applications simultaneously using two modalities - accelerometer, blood volume pulse signals. We identify variations of the FDIA such as different forged signal ratios, smoothed and non-smoothed attack samples. Notably, we present a novel attack detection framework named Siamese-MIL that leverages the Siamese neural networks' generalizable discriminative capability and multiple instance learning paradigms through a unique sensor data representation. Our exhaustive evaluation demonstrates Siamese-MIL's real-time execution capability and high efficacy in different attack variations, sensors, and applications.},
	articleno    = 83,
	numpages     = 22,
	keywords     = {Sensor Attack, Multiple Instance Learning, Authentication, Injection Attack, Mobile, False Data Injection Attack, Wearable, Defense, Siamese Network, Deep Learning}
}
@article{10.1145/3534576,
	title        = {VoLearn: A Cross-Modal Operable Motion-Learning System Combined with Virtual Avatar and Auditory Feedback},
	author       = {Xia, Chengshuo and Fang, Xinrui and Arakawa, Riku and Sugiura, Yuta},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534576},
	url          = {https://doi.org/10.1145/3534576},
	issue_date   = {July 2022},
	abstract     = {Conventional motion tutorials rely mainly on a predefined motion and vision-based feedback that normally limits the application scenario and requires professional devices. In this paper, we propose VoLearn, a cross-modal system that provides operability for user-defined motion learning. The system supports the ability to import a desired motion from RGB video and animates the motion in a 3D virtual environment. We built an interface to operate on the input motion, such as controlling the speed, and the amplitude of limbs for the respective directions. With exporting of virtual rotation data, a user can employ a daily device (i.e., smartphone) as a wearable device to train and practice the desired motion according to comprehensive auditory feedback, which is able to provide both temporal and amplitude assessment. The user study demonstrated that the system helps reduce the amplitude and time errors of motion learning. The developed motion-learning system maintains the characteristics of high user accessibility, flexibility, and ubiquity in its application.},
	articleno    = 81,
	numpages     = 26,
	keywords     = {virtual avatar, feedback, motion learning, Cross-modality}
}
@article{10.1145/3517259,
	title        = {Contextual Biases in Microinteraction Ecological Momentary Assessment (ΜEMA) Non-Response},
	author       = {Ponnada, Aditya and Li, Jixin and Wang, Shirlene and Wang, Wei-Lin and Do, Bridgette and Dunton, Genevieve F. and Intille, Stephen S.},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517259},
	url          = {https://doi.org/10.1145/3517259},
	issue_date   = {March 2022},
	abstract     = {Ecological momentary assessment (EMA) is used to gather in-situ self-report on behaviors using mobile devices. Microinteraction EMA (μEMA), is a type of EMA where each survey is only one single question that can be answered with a glanceable microinteraction on a smartwatch. Prior work shows that even when μEMA interrupts far more frequently than smartphone-EMA, μEMA yields higher response rates with lower burden. We examined the contextual biases associated with non-response of μEMA prompts on a smartwatch. Based on prior work on EMA non-response and smartwatch use, we identified 10 potential contextual biases from three categories: temporal (time of the day, parts of waking day, day of the week, and days in study), device use (screen state, charging status, battery mode, and phone usage), and activity (wrist motion and location). We used data from a longitudinal study where 131 participants (Mean age 22.9 years, SD = 3.0) responded to μEMA surveys on a smartwatch for at least six months. Using mixed-effects logistic regression, we found that all temporal, activity/mobility, and device use variables had a statistically significant (p&lt;0.001) association with momentary μEMA non-response. We discuss the implication of these results for future use of context-aware μEMA methodology.},
	articleno    = 26,
	numpages     = 24,
	keywords     = {context, experience sampling, smartwatch, ecological momentary assessment, Microinteraction}
}
@article{10.1145/3517244,
	title        = {Toward Reliable Non-Line-of-Sight Localization Using Multipath Reflections},
	author       = {Zhang, Xianan and Chen, Lieke and Feng, Mingjie and Jiang, Tao},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517244},
	url          = {https://doi.org/10.1145/3517244},
	issue_date   = {March 2022},
	abstract     = {The past decade's research in RF indoor localization has led to technologies with decimeter-level accuracy under controlled experimental settings. However, existing solutions are not reliable in challenging environments with rich multipath and various occlusions. The errors can be 3-5 times compared to settings with clear LoS paths. In addition, when the direct path is completely blocked, such approaches would generate wrong location estimates. In this paper, we present NLoc, a reliable non-line-of-sight localization system that overcomes the above limitations. The key innovation of NLoc is to convert multipath reflections to virtual direct paths to enhance the localization performance. To this end, NLoc first extracts reliable multi-dimensional parameters by characterizing phase variations. Then, it models the relation between the target location and the geometric features of multipath reflections to obtain virtual direct paths. Finally, it incorporates novel algorithms to remove random ToF offsets due to lack of synchronization and compensate target orientation that determines the geometric features, for accurate location estimates. We implement NLoc on commercial off-the-shelf WiFi devices. Our experiments in multipath challenged environments with dozens of obstacles and occlusions demonstrate that NLoc outperforms state-of-the-art approaches by 44% at the median and 200% at 90% percentile.},
	articleno    = 36,
	numpages     = 25,
	keywords     = {WiFi, NLoS Localization, Multipath}
}
@article{10.1145/3517240,
	title        = {EyeQoE: A Novel QoE Assessment Model for 360-Degree Videos Using Ocular Behaviors},
	author       = {Zhu, Huadi and Li, Tianhao and Wang, Chaowei and Jin, Wenqiang and Murali, Srinivasan and Xiao, Mingyan and Ye, Dongqing and Li, Ming},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517240},
	url          = {https://doi.org/10.1145/3517240},
	issue_date   = {March 2022},
	abstract     = {As virtual reality (VR) offers an unprecedented experience than any existing multimedia technologies, VR videos, or called 360-degree videos, have attracted considerable attention from academia and industry. How to quantify and model end users' perceived quality in watching 360-degree videos, or called QoE, resides the center for high-quality provisioning of these multimedia services. In this work, we present EyeQoE, a novel QoE assessment model for 360-degree videos using ocular behaviors. Unlike prior approaches, which mostly rely on objective factors, EyeQoE leverages the new ocular sensing modality to comprehensively capture both subjective and objective impact factors for QoE modeling. We propose a novel method that models eye-based cues into graphs and develop a GCN-based classifier to produce QoE assessment by extracting intrinsic features from graph-structured data. We further exploit the Siamese network to eliminate the impact from subjects and visual stimuli heterogeneity. A domain adaptation scheme named MADA is also devised to generalize our model to a vast range of unseen 360-degree videos. Extensive tests are carried out with our collected dataset. Results show that EyeQoE achieves the best prediction accuracy at 92.9%, which outperforms state-of-the-art approaches. As another contribution of this work, we have publicized our dataset on https://github.com/MobiSec-CSE-UTA/EyeQoE_Dataset.git.},
	articleno    = 39,
	numpages     = 26,
	keywords     = {graph learning, eye-based cues, QoE assessment}
}
@article{10.1145/3517231,
	title        = {MTransSee: Enabling Environment-Independent MmWave Sensing Based Gesture Recognition via Transfer Learning},
	author       = {Liu, Haipeng and Cui, Kening and Hu, Kaiyuan and Wang, Yuheng and Zhou, Anfu and Liu, Liang and Ma, Huadong},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517231},
	url          = {https://doi.org/10.1145/3517231},
	issue_date   = {March 2022},
	abstract     = {Gesture recognition using millimeter-wave radios facilitates natural human-computer interactions, but existing works require a consistent environment, i.e., the neural networks for recognition are trained and tested for the same users and at some fixed positions. In this case, their performance will decrease rapidly when they enter into a new environment. To make the model applicable in different environments, a straightforward approach is to collect and re-train the model for the gesture samples on every possible position upon each new user. However, it may ask the users to spend unacceptable time to accomplish such adaptation, which makes it difficult to be widely used in practice. In this paper, we first collect an abundant mmWave gesture dataset containing 59,280 samples as a benchmark to investigate the impact of the environment changes quantitatively. Then we propose a novel transfer-learning approach called mTransSee, which can serve the gestures in practice using pre-learned experience by least adaptation, i.e., retraining using only 8 samples per gesture for the same accuracy. mTransSee reduces dozens of workloads for the environment adaptation. We implement mTransSee on a commodity mmWave sensor and make a user study to compare the advance of mTransSee over the state-of-the-art solution in terms of user experience during adaptation.},
	articleno    = 23,
	numpages     = 28,
	keywords     = {Millimeter Wave Radar, Smart Home, Human-Computer Interaction, Gesture Recognition, Transfer Learning}
}
@article{10.1145/3517224,
	title        = {DeXAR: Deep Explainable Sensor-Based Activity Recognition in Smart-Home Environments},
	author       = {Arrotta, Luca and Civitarese, Gabriele and Bettini, Claudio},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517224},
	url          = {https://doi.org/10.1145/3517224},
	issue_date   = {March 2022},
	abstract     = {The sensor-based recognition of Activities of Daily Living (ADLs) in smart-home environments is an active research area, with relevant applications in healthcare and ambient assisted living. The application of Explainable Artificial Intelligence (XAI) to ADLs recognition has the potential of making this process trusted, transparent and understandable. The few works that investigated this problem considered only interpretable machine learning models. In this work, we propose DeXAR, a novel methodology to transform sensor data into semantic images to take advantage of XAI methods based on Convolutional Neural Networks (CNN). We apply different XAI approaches for deep learning and, from the resulting heat maps, we generate explanations in natural language. In order to identify the most effective XAI method, we performed extensive experiments on two different datasets, with both a common-knowledge and a user-based evaluation. The results of a user study show that the white-box XAI method based on prototypes is the most effective.},
	articleno    = 1,
	numpages     = 30,
	keywords     = {activity recognition, explainable artificial intelligence, deep learning, smart-home}
}
@article{10.1145/3495003,
	title        = {Spatio-Temporal Graph Attention Embedding for Joint Crowd Flow and Transition Predictions: A Wi-Fi-Based Mobility Case Study},
	author       = {Yang, Xi and He, Suining and Wang, Bing and Tabatabaie, Mahan},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3495003},
	url          = {https://doi.org/10.1145/3495003},
	issue_date   = {Dec 2021},
	abstract     = {Crowd mobility prediction, in particular, forecasting flows at and transitions across different locations, is essential for crowd analytics and management in spacious environments featured with large gathering. We propose GAEFT, a novel crowd mobility analytics system based on the multi-task graph attention neural network to forecast crowd flows (inflows/outflows) and transitions. Specifically, we leverage the collective and sanitized campus Wi-Fi association data provided by our university information technology service and conduct a relatable case study. Our comprehensive data analysis reveals the important challenges of sparsity and skewness, as well as the complex spatio-temporal variations within the crowd mobility data. Therefore, we design a novel spatio-temporal clustering method to group Wi-Fi access points (APs) with similar transition features, and obtain more regular mobility features for model inputs. We then propose an attention-based graph embedding design to capture the correlations among the crowd flows and transitions, and jointly predict the AP-level flows as well as transitions across buildings and clusters through a multi-task formulation. Extensive experimental studies using more than 28 million association records collected during 2020-2021 academic year validate the excellent accuracy of GAEFT in forecasting dynamic and complex crowd mobility.},
	articleno    = 187,
	numpages     = 24,
	keywords     = {Wi-Fi association data, Graph attention, crowd flow, transition, prediction}
}
@article{10.1145/3495001,
	title        = {Auditable Augmented/Mixed/Virtual Reality: The Practicalities of Mobile System Transparency},
	author       = {Cloete, Richard and Norval, Chris and Singh, Jatinder},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3495001},
	url          = {https://doi.org/10.1145/3495001},
	issue_date   = {Dec 2021},
	abstract     = {Virtual, Augmented and Mixed Reality (XR) technologies are becoming increasingly pervasive. However, the contextual nature of XR, and its tight coupling of the digital and physical environments, brings real propensity for loss and harm.This means that auditability---the ability to inspect how a system operates---will be crucial for dealing with incidents as they occur, by providing the information enabling rectification, repair and recourse. However, supporting audit in XR brings considerations, as the process of capturing audit data itself has implications and challenges, both for the application (e.g., overheads) and more broadly.This paper explores the practicalities of auditing XR systems, characterises the tensions between audit and other considerations, and argues the need for flexible tools enabling the management of such. In doing so, we introduce Droiditor, a configurable open-source Android toolkit that enables the runtime capture of audit-relevant data from mobile applications. We use Droiditor as a means to indicate some potential implications of audit data capture, demonstrate how greater configurability can assist in managing audit-related concerns, and discuss the potential considerations that result. Given the societal demands for more transparent and accountable systems, our broader aim is to draw attention to auditability, highlighting tangible ways forward and areas for future work.},
	articleno    = 149,
	numpages     = 24,
	keywords     = {mobile/pervasive systems, accountability, augmented/mixed/virtual reality, audit tooling, transparency, Android, audit}
}
@article{10.1145/3494979,
	title        = {Exploring Multiple Antennas for Long-Range WiFi Sensing},
	author       = {Zeng, Youwei and Liu, Jinyi and Xiong, Jie and Liu, Zhaopeng and Wu, Dan and Zhang, Daqing},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494979},
	url          = {https://doi.org/10.1145/3494979},
	issue_date   = {Dec 2021},
	abstract     = {Despite extensive research effort on contactless WiFi sensing over the past few years, there are still significant barriers hindering its wide application. One key issue is the limited sensing range due to the intrinsic nature of employing the weak target-reflected signal for sensing and therefore the sensing range is much smaller than the communication range. In this work, we address this challenging issue, moving WiFi sensing one step closer to real-world adoption. The key idea is to effectively utilize the multiple antennas widely available on commodity WiFi access points to simultaneously strengthen the target-reflected signal and reduce the noise. Although traditional beamforming schemes can help increase the signal strength, they are designed for communication and can not be directly applied to benefit sensing. To effectively increase the WiFi sensing range using multiple antennas, we first propose a new metric that quantifies the signal sensing capability. We then propose novel signal processing methods, which lay the theoretical foundation to support beamforming-based long-range WiFi sensing. To validate the proposed idea, we develop two sensing applications: fine-grained human respiration monitoring and coarse-grained human walking tracking. Extensive experiments show that: (i) the human respiration sensing range is significantly increased from the state-of-the-art 6-8 m to 11 m;1 and (ii) human walking can be accurately tracked even when the target is 18 m away from the WiFi transceivers, outperforming the sensing range of the state-of-the-art by 50%.},
	articleno    = 190,
	numpages     = 30,
	keywords     = {WiFi Sensing, Channel State Information (CSI), Sensing-signal-to-noise Ratio (SSNR)}
}
@article{10.1145/3494972,
	title        = {SmartLOC: Indoor Localization with Smartphone Anchors for On-Demand Delivery},
	author       = {Ding, Yi and Jiang, Dongzhe and Liu, Yunhuai and Zhang, Desheng and He, Tian},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494972},
	url          = {https://doi.org/10.1145/3494972},
	issue_date   = {Dec 2021},
	abstract     = {On-demand delivery is a rapidly developing business worldwide, where meals and groceries are delivered door to door from merchants to customers by the couriers. Couriers' real-time localization plays a key role in on-demand delivery for all parties like the platform's order dispatching, merchants' order preparing, couriers' navigation, and customers' shopping experience. Although GPS has well solved outdoor localization, indoor localization is still challenging due to the lack of large-coverage, low-cost anchors. Given the high penetration of smartphones in merchants and frequent rendezvous between merchants and couriers, we employ merchants' smartphones as indoor anchors for a new sensing opportunity. In this paper, we design, implement and evaluate SmartLOC, a map-free localization system that employs merchants' smartphones as anchors to obtain couriers' real-time locations. Specifically, we design a rendezvous detection module based on Bluetooth Low Energy (BLE), build indoor shop graphs for each mall, and adopt graph embedding to extract indoor shops' topology. To guarantee anchors' accuracy and privacy, we build a mutual localization module to iteratively infer merchants' state (in-shop or not) and couriers' locations with transformer models. We implement SmartLOC in a large on-demand delivery platform and deploy the system in 566 malls in Shanghai, China. We evaluate SmartLOC in two multi-floor malls in Shanghai and show that it can improve the accuracy of couriers' travel time estimation by 24%, 43%, 70%, and 76% compared with a straightforward graph solution, GPS, Wi-Fi, and TransLoc.},
	articleno    = 153,
	numpages     = 24,
	keywords     = {Indoor Localization, Graph Learning, On-Demand Delivery}
}
@article{10.1145/3494958,
	title        = {Adaptive Computation Offloading for Mobile Augmented Reality},
	author       = {Ren, Jie and Gao, Ling and Wang, Xiaoming and Ma, Miao and Qiu, Guoyong and Wang, Hai and Zheng, Jie and Wang, Zheng},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494958},
	url          = {https://doi.org/10.1145/3494958},
	issue_date   = {Dec 2021},
	abstract     = {Augmented reality (AR) underpins many emerging mobile applications, but it increasingly requires more computation power for better machine understanding and user experience. While computation offloading promises a solution for high-quality and interactive mobile AR, existing methods work best for high-definition videos but cannot meet the real-time requirement for emerging 4K videos due to the long uploading latency. We introduce ACTOR, a novel computation-offloading framework for 4K mobile AR. To reduce the uploading latency, ACTOR dynamically and judiciously downscales the mobile video feed to be sent to the remote server. On the server-side, it leverages image super-resolution technology to scale back the received video so that high-quality object detection, tracking and rendering can be performed on the full 4K resolution. ACTOR employs machine learning to predict which of the downscaling resolutions and super-resolution configurations should be used, by taking into account the video content, server processing delay, and user expected latency. We evaluate ACTOR by applying it to over 2,000 4K video clips across two typical WiFi network settings. Extensive experimental results show that ACTOR consistently and significantly outperforms competitive methods for simultaneously meeting the latency and user-perceived video quality requirements.},
	articleno    = 175,
	numpages     = 30,
	keywords     = {Mobile Augmented reality, Adaptive Computation Offloading, 4K Video Processing, Super-resolution}
}
@article{10.1145/3478124,
	title        = {ULoc: Low-Power, Scalable and Cm-Accurate UWB-Tag Localization and Tracking for Indoor Applications},
	author       = {Zhao, Minghui and Chang, Tyler and Arun, Aditya and Ayyalasomayajula, Roshan and Zhang, Chi and Bharadia, Dinesh},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478124},
	url          = {https://doi.org/10.1145/3478124},
	issue_date   = {Sept 2021},
	abstract     = {A myriad of IoT applications, ranging from tracking assets in hospitals, logistics, and construction industries to indoor tracking in large indoor spaces, demand centimeter-accurate localization that is robust to blockages from hands, furniture, or other occlusions in the environment. With this need, in the recent past, Ultra Wide Band (UWB) based localization and tracking has become popular. Its popularity is driven by its proposed high bandwidth and protocol specifically designed for localization of specialized "tags". This high bandwidth of UWB provides a fine resolution of the time-of-travel of the signal that can be translated to the location of the tag with centimeter-grade accuracy in a controlled environment. Unfortunately, we find that high latency and high-power consumption of these time-of-travel methods are the major culprits which prevent such a system from deploying multiple tags in the environment. Thus, we developed ULoc, a scalable, low-power, and cm-accurate UWB localization and tracking system. In ULoc, we custom build a multi-antenna UWB anchor that enables azimuth and polar angle of arrival (henceforth shortened to '3D-AoA') measurements, with just the reception of a single packet from the tag. By combining multiple UWB anchors, ULoc can localize the tag in 3D space. The single-packet location estimation reduces the latency of the entire system by at least 3\texttimes{}, as compared with state of art multi-packet UWB localization protocols, making UWB based localization scalable. ULoc's design also reduces the power consumption per location estimate at the tag by 9\texttimes{}, as compared to state-of-art time-of-travel algorithms. We further develop a novel 3D-AoA based 3D localization that shows a stationary localization accuracy of 3.6 cm which is 1.8\texttimes{} better than the state-of-the-art two-way ranging (TWR) systems. We further developed a temporal tracking system that achieves a tracking accuracy of 10 cm in mobile conditions which is 4.3\texttimes{} better than the state-of-the-art TWR systems.},
	articleno    = 140,
	numpages     = 31,
	keywords     = {Anchor Board, L-shaped, cm-Accurate, 3D-Localization,3D-tracking, Low-power, Hardware, Ultra-Wideband, Angle of Arrival, Real-time}
}
@article{10.1145/3478110,
	title        = {Reducing Muscle Activity When Playing Tremolo by Using Electrical Muscle Stimulation to Learn Efficient Motor Skills},
	author       = {Niijima, Arinobu and Takeda, Toki and Tanaka, Kentaro and Aoki, Ryosuke and Koike, Yukio},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478110},
	url          = {https://doi.org/10.1145/3478110},
	issue_date   = {Sept 2021},
	abstract     = {When beginners play the piano, the activity of the forearm muscles tends to be greater than that of experts because beginners move their fingers with more force than necessary. Reducing forearm muscle activity is important for pianists to prevent fatigue and injury. However, it is difficult for beginners to learn how to do so by themselves. We propose using electrical muscle stimulation (EMS) to teach beginners how to reduce this muscle activity while playing a tremolo: a rapid alternation between two notes. Since experts use wrist rotation efficiently when playing tremolos, we propose an EMS-based support system that applies EMS not to muscles that are relevant to moving the fingers but to the supinator and pronator teres muscles, which are involved in wrist rotation. We conducted a user study with 16 beginners to investigate how the forearm muscle activity on the extensor pollicis longus and digitorum muscles changed when using our EMS-based support system. We divided the participants into two groups: an experimental group who practiced by themselves with EMS and a control group who practiced by themselves without EMS and then practiced with instruction. When practicing by themselves, practicing with EMS was more effective than that without EMS; the activity levels of the extensor pollicis longus and digitorum muscles were significantly lower with EMS, and the participants felt less fatigue when playing tremolos. By comparing the improvement in reducing muscle activity between practicing with EMS and practicing with instruction, there was no significant difference. The results suggest that our EMS-based support system can reduce target muscle activity by applying EMS to other muscles to teach beginners how to move limbs efficiently.},
	articleno    = 123,
	numpages     = 17,
	keywords     = {tremolo, electrical muscle stimulation, muscle activity, electromyography, piano}
}
@article{10.1145/3478102,
	title        = {Audio Keyword Reconstruction from On-Device Motion Sensor Signals via Neural Frequency Unfolding},
	author       = {Wang, Tianshi and Yao, Shuochao and Liu, Shengzhong and Li, Jinyang and Liu, Dongxin and Shao, Huajie and Wang, Ruijie and Abdelzaher, Tarek},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478102},
	url          = {https://doi.org/10.1145/3478102},
	issue_date   = {Sept 2021},
	abstract     = {In this paper, we present a novel deep neural network architecture that reconstructs the high-frequency audio of selected spoken human words from low-sampling-rate signals of (ego-)motion sensors, such as accelerometer and gyroscope data, recorded on everyday mobile devices. As the sampling rate of such motion sensors is much lower than the Nyquist rate of ordinary human voice (around 6kHz+), these motion sensor recordings suffer from a significant frequency aliasing effect. In order to recover the original high-frequency audio signal, our neural network introduces a novel layer, called the alias unfolding layer, specialized in expanding the bandwidth of an aliased signal by reversing the frequency folding process in the time-frequency domain. While perfect unfolding is known to be unrealizable, we leverage the sparsity of the original signal to arrive at a sufficiently accurate statistical approximation. Comprehensive experiments show that our neural network significantly outperforms the state of the art in audio reconstruction from motion sensor data, effectively reconstructing a pre-trained set of spoken keywords from low-frequency motion sensor signals (with a sampling rate of 100-400 Hz). The approach demonstrates the potential risk of information leakage from motion sensors in smart mobile devices.},
	articleno    = 132,
	numpages     = 29,
	keywords     = {Motion sensors, Deep learning, Time frequency analysis}
}
@article{10.1145/3478079,
	title        = {Captivates: A Smart Eyeglass Platform for Across-Context Physiological Measurement},
	author       = {Chwalek, Patrick and Ramsay, David and Paradiso, Joseph A.},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478079},
	url          = {https://doi.org/10.1145/3478079},
	issue_date   = {Sept 2021},
	abstract     = {We present Captivates, an open-source smartglasses system designed for long-term, in-the-wild psychophysiological monitoring at scale. Captivates integrate many underutilized physiological sensors in a streamlined package, including temple and nose temperature measurement, blink detection, head motion tracking, activity classification, 3D localization, and head pose estimation. Captivates were designed with an emphasis on: (1) manufacturing and scalability, so we can easily support large scale user studies for ourselves and offer the platform as a generalized tool for ambulatory psychophysiology research; (2) robustness and battery life, so long-term studies result in trustworthy data individual's entire day in natural environments without supervision or recharge; and (3) aesthetics and comfort, so people can wear them in their normal daily contexts without self-consciousness or changes in behavior.Captivates are intended to enable large scale data collection without altering user behavior. We validate that our sensors capture useful data robustly for a small set of beta testers. We also show that our additional effort on aesthetics was imperative to meet our goals; namely, earlier versions of our prototype make people uncomfortable to interact naturally in public, and our additional design and miniaturization effort has made a significant impact in preserving natural behavior.There is tremendous promise in translating psychophysiological laboratory techniques into real-world insight. Captivates serve as an open-source bridge to this end. Paired with an accurate underlying model, Captivates will be able to quantify the long-term psychological impact of our design decisions and provide real-time feedback for technologists interested in actuating a cognitively adaptive, user-aligned future.},
	articleno    = 93,
	numpages     = 32,
	keywords     = {robust ambulatory sensing, smart eyeglass, localization, face temperature, blink sensing}
}
@article{10.1145/3463517,
	title        = {Robust Inertial Motion Tracking through Deep Sensor Fusion across Smart Earbuds and Smartphone},
	author       = {Gong, Jian and Zhang, Xinyu and Huang, Yuanjun and Ren, Ju and Zhang, Yaoxue},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463517},
	url          = {https://doi.org/10.1145/3463517},
	issue_date   = {June 2021},
	abstract     = {IMU based inertial tracking plays an indispensable role in many mobility centric tasks, such as robotic control, indoor navigation and virtual reality gaming. Despite its mature application in rigid machine mobility (e.g., robot and aircraft), tracking human users via mobile devices remains a fundamental challenge due to the intractable gait/posture patterns. Recent data-driven models have tackled sensor drifting, one key issue that plagues inertial tracking. However, these systems still assume the devices are held or attached to the user body with a relatively fixed posture. In practice, natural body activities may rotate/translate the device which may be mistaken as whole body movement. Such motion artifacts remain as the dominating factor that fails existing inertial tracing systems in practical uncontrolled settings.Inspired by the observation that human heads induces far less intensive movement relative to the body during walking, compared to other parts, we propose a novel multi-stage sensor fusion pipeline called DeepIT, which realizes inertial tracking by synthesizing the IMU measurements from a smartphone and an associated earbud. DeepIT introduces a data-driven reliability aware attention model, which assesses the reliability of each IMU and opportunistically synthesizes their data to mitigate the impacts of motion noise. Furthermore, DeepIT uses a reliability aware magnetometer compensation scheme to combat the angular drifting problem caused by unrestricted motion artifacts. We validate DeepIT on the first large-scale inertial navigation dataset involving both smartphone and earbud IMUs. The evaluation results show that DeepIT achieves multiple folds of accuracy improvement on the challenging uncontrolled natural walking scenarios, compared with state-of-the-art closed-form and data-driven models.},
	articleno    = 62,
	numpages     = 26,
	keywords     = {sensor fusion, smartphone, smart earbuds, deep learning}
}
@article{10.1145/3463515,
	title        = {IriTrack: Face Presentation Attack Detection Using Iris Tracking},
	author       = {Shen, Meng and Wei, Yaqian and Liao, Zelin and Zhu, Liehuang},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463515},
	url          = {https://doi.org/10.1145/3463515},
	issue_date   = {June 2021},
	abstract     = {With a growing adoption of face authentication systems in various application scenarios, face Presentation Attack Detection (PAD) has become of great importance to withstand artefacts. Existing methods of face PAD generally focus on designing intelligent classifiers or customized hardware to differentiate between the image or video samples of a real legitimate user and the imitated ones. Although effective, they can be resource-consuming and suffer from performance degradation due to environmental changes.In this paper, we propose IriTrack, which is a simple and efficient PAD system that takes iris movement as a significant evidence to identify face artefacts. More concretely, users are required to move their eyes along with a randomly generated poly-line, where the resulting trajectories of their irises are used as an evidence for PAD i.e., a presentation attack will be identified if the deviation of one's actual iris trajectory from the given poly-line exceeds a threshold. The threshold is carefully selected to balance the latency and accuracy of PAD. We have implemented a prototype and conducted extensive experiments to evaluate the performance of the proposed system. The results show that IriTrack can defend against artefacts with moderate time and memory overheads.},
	articleno    = 78,
	numpages     = 21,
	keywords     = {face recognition, Presentation attack detection, facial biometric artefact, iris tracking, authentication}
}
@article{10.1145/3463499,
	title        = {CoolMoves: User Motion Accentuation in Virtual Reality},
	author       = {Ahuja, Karan and Ofek, Eyal and Gonzalez-Franco, Mar and Holz, Christian and Wilson, Andrew D.},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463499},
	url          = {https://doi.org/10.1145/3463499},
	issue_date   = {June 2021},
	abstract     = {Current Virtual Reality (VR) systems are bereft of stylization and embellishment of the user's motion - concepts that have been well explored in animations for games and movies. We present CooIMoves, a system for expressive and accentuated full-body motion synthesis of a user's virtual avatar in real-time, from the limited input cues afforded by current consumer-grade VR systems, specifically headset and hand positions. We make use of existing motion capture databases as a template motion repository to draw from. We match similar spatio-temporal motions present in the database and then interpolate between them using a weighted distance metric. Joint prediction probability is then used to temporally smooth the synthesized motion, using human motion dynamics as a prior. This allows our system to work well even with very sparse motion databases (e.g., with only 3-5 motions per action). We validate our system with four experiments: a technical evaluation of our quantitative pose reconstruction and three additional user studies to evaluate the motion quality, embodiment and agency.},
	articleno    = 52,
	numpages     = 23,
	keywords     = {Motion Embellishment, Virtual Reality, Pose Tracking, Motion Stylization}
}
@article{10.1145/3448124,
	title        = {Listen2Cough: Leveraging End-to-End Deep Learning Cough Detection Model to Enhance Lung Health Assessment Using Passively Sensed Audio},
	author       = {Xu, Xuhai and Nemati, Ebrahim and Vatanparvar, Korosh and Nathan, Viswam and Ahmed, Tousif and Rahman, Md Mahbubur and McCaffrey, Daniel and Kuang, Jilong and Gao, Jun Alex},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448124},
	url          = {https://doi.org/10.1145/3448124},
	issue_date   = {March 2021},
	abstract     = {The prevalence of ubiquitous computing enables new opportunities for lung health monitoring and assessment. In the past few years, there have been extensive studies on cough detection using passively sensed audio signals. However, the generalizability of a cough detection model when applied to external datasets, especially in real-world implementation, is questionable and not explored adequately. Beyond detecting coughs, researchers have looked into how cough sounds can be used in assessing lung health. However, due to the challenges in collecting both cough sounds and lung health condition ground truth, previous studies have been hindered by the limited datasets. In this paper, we propose Listen2Cough to address these gaps. We first build an end-to-end deep learning architecture using public cough sound datasets to detect coughs within raw audio recordings. We employ a pre-trained MobileNet and integrate a number of augmentation techniques to improve the generalizability of our model. Without additional fine-tuning, our model is able to achieve an F1 score of 0.948 when tested against a new clean dataset, and 0.884 on another in-the-wild noisy dataset, leading to an advantage of 5.8% and 8.4% on average over the best baseline model, respectively. Then, to mitigate the issue of limited lung health data, we propose to transform the cough detection task to lung health assessment tasks so that the rich cough data can be leveraged. Our hypothesis is that these tasks extract and utilize similar effective representation from cough sounds. We embed the cough detection model into a multi-instance learning framework with the attention mechanism and further tune the model for lung health assessment tasks. Our final model achieves an F1-score of 0.912 on healthy v.s. unhealthy, 0.870 on obstructive v.s. non-obstructive, and 0.813 on COPD v.s. asthma classification, outperforming the baseline by 10.7%, 6.3%, and 3.7%, respectively. Moreover, the weight value in the attention layer can be used to identify important coughs highly correlated with lung health, which can potentially provide interpretability for expert diagnosis in the future.},
	articleno    = 42,
	numpages     = 22,
	keywords     = {Multi-instance learning, Lung health assessment, Cough detection}
}
@article{10.1145/3448117,
	title        = {Person-Centered Predictions of Psychological Constructs with Social Media Contextualized by Multimodal Sensing},
	author       = {Saha, Koustuv and Grover, Ted and Mattingly, Stephen M. and swain, Vedant Das and Gupta, Pranshu and Martinez, Gonzalo J. and Robles-Granda, Pablo and Mark, Gloria and Striegel, Aaron and De Choudhury, Munmun},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448117},
	url          = {https://doi.org/10.1145/3448117},
	issue_date   = {March 2021},
	abstract     = {Personalized predictions have shown promises in various disciplines but they are fundamentally constrained in their ability to generalize across individuals. These models are often trained on limited datasets which do not represent the fluidity of human functioning. In contrast, generalized models capture normative behaviors between individuals but lack precision in predicting individual outcomes. This paper aims to balance the tradeoff between one-for-each and one-for-all models by clustering individuals on mutable behaviors and conducting cluster-specific predictions of psychological constructs in a multimodal sensing dataset of 754 individuals. Specifically, we situate our modeling on social media that has exhibited capability in inferring psychosocial attributes. We hypothesize that complementing social media data with offline sensor data can help to personalize and improve predictions. We cluster individuals on physical behaviors captured via Bluetooth, wearables, and smartphone sensors. We build contextualized models predicting psychological constructs trained on each cluster's social media data and compare their performance against generalized models trained on all individuals' data. The comparison reveals no difference in predicting affect and a decline in predicting cognitive ability, but an improvement in predicting personality, anxiety, and sleep quality. We construe that our approach improves predicting psychological constructs sharing theoretical associations with physical behavior. We also find how social media language associates with offline behavioral contextualization. Our work bears implications in understanding the nuanced strengths and weaknesses of personalized predictions, and how the effectiveness may vary by multiple factors. This work reveals the importance of taking a critical stance on evaluating the effectiveness before investing efforts in personalization.},
	articleno    = 32,
	numpages     = 32,
	keywords     = {social media, personality traits, person-centered, sleep, clustering, personalization, affect, multimodal sensing, language, machine learning, cognitive ability}
}
@article{10.1145/3448111,
	title        = {Objective Measures of Cognitive Load Using Deep Multi-Modal Learning: A Use-Case in Aviation},
	author       = {Wilson, Justin C. and Nair, Suku and Scielzo, Sandro and Larson, Eric C.},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448111},
	url          = {https://doi.org/10.1145/3448111},
	issue_date   = {March 2021},
	abstract     = {The capability of measuring human performance objectively is hard to overstate, especially in the context of the instructor and student relationship within the process of learning. In this work, we investigate the automated classification of cognitive load leveraging the aviation domain as a surrogate for complex task workload induction. We use a mixed virtual and physical flight environment, given a suite of biometric sensors utilizing the HTC Vive Pro Eye and the E4 Empatica. We create and evaluate multiple models. And we have taken advantage of advancements in deep learning such as generative learning, multi-modal learning, multi-task learning, and x-vector architectures to classify multiple tasks across 40 subjects inclusive of three subject types --- pilots, operators, and novices. Our cognitive load model can automate the evaluation of cognitive load agnostic to subject, subject type, and flight maneuver (task) with an accuracy of over 80%. Further, this approach is validated with real-flight data from five test pilots collected over two test and evaluation flights on a C-17 aircraft.},
	articleno    = 40,
	numpages     = 35,
	keywords     = {deep learning, workload, cognitive load classification, virtual reality}
}
@article{10.1145/3448107,
	title        = {Leveraging Collaborative-Filtering for Personalized Behavior Modeling: A Case Study of Depression Detection among College Students},
	author       = {Xu, Xuhai and Chikersal, Prerna and Dutcher, Janine M. and Sefidgar, Yasaman S. and Seo, Woosuk and Tumminia, Michael J. and Villalba, Daniella K. and Cohen, Sheldon and Creswell, Kasey G. and Creswell, J. David and Doryab, Afsaneh and Nurius, Paula S. and Riskin, Eve and Dey, Anind K. and Mankoff, Jennifer},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448107},
	url          = {https://doi.org/10.1145/3448107},
	issue_date   = {March 2021},
	abstract     = {The prevalence of mobile phones and wearable devices enables the passive capturing and modeling of human behavior at an unprecedented resolution and scale. Past research has demonstrated the capability of mobile sensing to model aspects of physical health, mental health, education, and work performance, etc. However, most of the algorithms and models proposed in previous work follow a one-size-fits-all (i.e., population modeling) approach that looks for common behaviors amongst all users, disregarding the fact that individuals can behave very differently, resulting in reduced model performance. Further, black-box models are often used that do not allow for interpretability and human behavior understanding. We present a new method to address the problems of personalized behavior classification and interpretability, and apply it to depression detection among college students. Inspired by the idea of collaborative-filtering, our method is a type of memory-based learning algorithm. It leverages the relevance of mobile-sensed behavior features among individuals to calculate personalized relevance weights, which are used to impute missing data and select features according to a specific modeling goal (e.g., whether the student has depressive symptoms) in different time epochs, i.e., times of the day and days of the week. It then compiles features from epochs using majority voting to obtain the final prediction. We apply our algorithm on a depression detection dataset collected from first-year college students with low data-missing rates and show that our method outperforms the state-of-the-art machine learning model by 5.1% in accuracy and 5.5% in F1 score. We further verify the pipeline-level generalizability of our approach by achieving similar results on a second dataset, with an average improvement of 3.4% across performance metrics. Beyond achieving better classification performance, our novel approach is further able to generate personalized interpretations of the models for each individual. These interpretations are supported by existing depression-related literature and can potentially inspire automated and personalized depression intervention design in the future.},
	articleno    = 41,
	numpages     = 27,
	keywords     = {Behavior mining, Depression detection, Passive sensing, Personalization}
}
@article{10.1145/3448097,
	title        = {User Expectations and Mental Models for Communicating Emotions through Compressive &amp; Warm Affective Garment Actuation},
	author       = {Foo, Esther W. and Dunne, Lucy E. and Holschuh, Brad},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448097},
	url          = {https://doi.org/10.1145/3448097},
	issue_date   = {March 2021},
	abstract     = {Wearable haptic garments for communicating emotions have great potential in various applications, including supporting social interactions, improving immersive experiences in entertainment, or simply as a research tool. Shape-memory alloys (SMAs) are an emerging and interesting actuation scheme for affective haptic garments since they provide coupled warmth and compressive sensations in a single actuation---potentially acting as a proxy for human touch. However, SMAs are underutilized in current research and there are many unknowns regarding their design/use. The goal of this work is to map the design space for SMA-based garment-mediated emotional communication through warm, compressive actuation (termed 'warm touch'). Two online surveys were deployed to gather user expectations in using varying 'warm touch' parameters (body location, intensity, pattern) to communicate 7 distinct emotions. Further, we also investigated mental models used by participants during the haptic strategy selection process. The findings show 5 major categories of mental models, including representation of body sensations, replication of typical social touch strategies, metaphorical representation of emotions, symbolic representation of physical actions, and mimicry of objects or tasks; the frequency of use of each of these mental frameworks in relation to the selected 'warm touch' parameters in the communication of emotions are presented. These gathered insights can inform more intuitive and consistent haptic garment design approaches for emotional communication.},
	articleno    = 10,
	numpages     = 25,
	keywords     = {Emotion Elicitation, Haptic Garments, Compression Feedback, Affective Haptics, Mediated Social Touch, Emotion Communication, Shape Memory Alloys, Wearable Technology}
}
@article{10.1145/3448095,
	title        = {Outliers in Smartphone Sensor Data Reveal Outliers in Daily Happiness},
	author       = {Buda, Teodora Sandra and Khwaja, Mohammed and Matic, Aleksandar},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448095},
	url          = {https://doi.org/10.1145/3448095},
	issue_date   = {March 2021},
	abstract     = {Enabling smartphones to understand our emotional well-being provides the potential to create personalised applications and highly responsive interfaces. However, this is by no means a trivial task - subjectivity in reporting emotions impacts the reliability of ground-truth information whereas smartphones, unlike specialised wearables, have limited sensing capabilities. In this paper, we propose a new approach that advances emotional state prediction by extracting outlier-based features and by mitigating the subjectivity in capturing ground-truth information. We utilised this approach in a distinctive and challenging use case - happiness detection - and we demonstrated prediction performance improvements of up to 13% in AUC and 27% in F-score compared to the traditional modelling approaches. The results indicate that extreme values (i.e. outliers) of sensor readings mirror extreme values in the reported happiness levels. Furthermore, we showed that this approach is more robust in replicating the prediction model in completely new experimental settings.},
	articleno    = 5,
	numpages     = 19,
	keywords     = {Machine Learning, Smartphone Sensing, Mobile Health, Mental Health}
}
@article{10.1145/3448091,
	title        = {Towards Finding the Optimum Position in the Visual Field for a Head Worn Display Used for Task Guidance with Non-Registered Graphics},
	author       = {Lin, Georgianna and Haynes, Malcolm and Srinivas, Sarthak and Kotipalli, Pramod and Starner, Thad},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448091},
	url          = {https://doi.org/10.1145/3448091},
	issue_date   = {March 2021},
	abstract     = {Where should a HWD be placed in a user's visual field? We present two studies that compare comfort, preference, task efficiency and accuracy for various HWD positions. The first study offsets a 9.2° horizontal field-of-view (FOV) display temporally (toward the ear) from 0° to 30° in 10° steps. 30° proves too uncomfortable while 10° is the most preferred position for a simple button-pushing game, corroborating results from previous single-task reading experiments. The second experiment uses a Magic Leap One to compare 10° x 10° FOV interfaces centered at line-of-sight, temporally offset 15° (center-right), inferiorly offset 15° (bottom-center), and offset in both directions (bottom-right) for an order picking task. The bottom-right position proved worst in terms of accuracy and several subjective metrics when compared to the line-of-sight position.},
	articleno    = 22,
	numpages     = 26,
	keywords     = {head worn displays, head mounted displays, head-up displays, mobile systems, wearable computing}
}
@article{10.1145/3448085,
	title        = {S3: Side-Channel Attack on Stylus Pencil through Sensors},
	author       = {Farrukh, Habiba and Yang, Tinghan and Xu, Hanwen and Yin, Yuxuan and Wang, He and Celik, Z. Berkay},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448085},
	url          = {https://doi.org/10.1145/3448085},
	issue_date   = {March 2021},
	abstract     = {With smart devices being an essential part of our everyday lives, unsupervised access to the mobile sensors' data can result in a multitude of side-channel attacks. In this paper, we study potential data leaks from Apple Pencil (2nd generation) supported by the Apple iPad Pro, the latest stylus pen which attaches to the iPad body magnetically for charging. We observe that the Pencil's body affects the magnetic readings sensed by the iPad's magnetometer when a user is using the Pencil. Therefore, we ask: Can we infer what a user is writing on the iPad screen with the Apple Pencil, given access to only the iPad's motion sensors' data? To answer this question, we present Side-channel attack on Stylus pencil through Sensors (S3), a system that identifies what a user is writing from motion sensor readings. We first use the sharp fluctuations in the motion sensors' data to determine when a user is writing on the iPad. We then introduce a high-dimensional particle filter to track the location and orientation of the Pencil during usage. Lastly, to guide particles, we build the Pencil's magnetic map serving as a bridge between the measured magnetic data and the Pencil location and orientation. We evaluate S3 with 10 subjects and demonstrate that we correctly identify 93.9%, 96%, 97.9%, and 93.33% of the letters, numbers, shapes, and words by only having access to the motion sensors' data.},
	articleno    = 8,
	numpages     = 25,
	keywords     = {stylus pencils, side-channel attack, user privacy}
}
@article{10.1145/3448084,
	title        = {WiFiTrace: Network-Based Contact Tracing for Infectious Diseases Using Passive WiFi Sensing},
	author       = {Trivedi, Amee and Zakaria, Camellia and Balan, Rajesh and Becker, Ann and Corey, George and Shenoy, Prashant},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448084},
	url          = {https://doi.org/10.1145/3448084},
	issue_date   = {March 2021},
	abstract     = {Contact tracing is a well-established and effective approach for the containment of the spread of infectious diseases. While Bluetooth-based contact tracing method using phones has become popular recently, these approaches suffer from the need for a critical mass adoption to be effective. In this paper, we present WiFiTrace, a network-centric approach for contact tracing that relies on passive WiFi sensing with no client-side involvement. Our approach exploits WiFi network logs gathered by enterprise networks for performance and security monitoring, and utilizes them for reconstructing device trajectories for contact tracing. Our approach is specifically designed to enhance the efficacy of traditional methods, rather than to supplant them with new technology. We designed an efficient graph algorithm to scale our approach to large networks with tens of thousands of users. The graph-based approach outperforms an indexed PostgresSQL in memory by at least 4.5X without any index update overheads or blocking. We have implemented a full prototype of our system and deployed it on two large university campuses. We validated our approach and demonstrate its efficacy using case studies and detailed experiments using real-world WiFi datasets.},
	articleno    = 37,
	numpages     = 26,
	keywords     = {Digital Contact Tracing, WiFi, Syslogs, Passive sensing, Access Point}
}
@article{10.1145/3397310,
	title        = {OptiStructures: Fabrication of Room-Scale Interactive Structures with Embedded Fiber Bragg Grating Optical Sensors and Displays},
	author       = {Swaminathan, Saiganesh and Fagert, Jonathon and Rivera, Michael and Cao, Andrew and Laput, Gierad and Noh, Hae Young and Hudson, Scott E.},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397310},
	url          = {https://doi.org/10.1145/3397310},
	issue_date   = {June 2020},
	abstract     = {A recent topic of considerable interest in the "smart building" community involves building interactive devices using sensors, and rapidly creating these objects using new fabrication methods. However, much of this work has been done at what might be called hand scale, with less attention paid to larger objects and structures (at furniture or room scales) despite the fact that we are very often literally surrounded by such objects. In this work, we present a new set of techniques for creating interactive objects at these scales. We demonstrate fabrication of both input sensors and displays directly into cast materials -those formed from a liquid or paste which solidifies in a mold; including, for example: concrete, plaster, polymer resins, and composites.Through our novel set of sensing and fabrication techniques, we enable human activity recognition at room scale and across a variety of materials. Our techniques create objects that appear the same as typical passive objects, but contain internal fiber optics for both input sensing and simple displays. We use a new fabrication device to inject optical fibers into CNC milled molds. Fiber Bragg Grating optical sensors configured as very sensitive vibration sensors are embedded in these objects. These require no internal power, can be placed at multiple locations along a single fiber, and can be interrogated from the end of the fiber. We evaluate the performance of our system by creating two full-scale application prototypes: an interactive wall, and an interactive table. With these prototypes, we demonstrate the ability of our system to sense a variety of human activities across eight different users. Our tests show that with suitable materials these sensors can detect and classify both direct interactions (such as tapping) and more subtle vibrations caused by activities such as walking across the floor nearby.},
	articleno    = 50,
	numpages     = 21,
	keywords     = {embedded sensing, optical fibers, interactive structures}
}
@article{10.1145/3397309,
	title        = {Ready, Steady, Touch! Sensing Physical Contact with a Finger-Mounted IMU},
	author       = {Shi, Yilei and Zhang, Haimo and Zhao, Kaixing and Cao, Jiashuo and Sun, Mengmeng and Nanayakkara, Suranga},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397309},
	url          = {https://doi.org/10.1145/3397309},
	issue_date   = {June 2020},
	abstract     = {A finger held in the air exhibits microvibrations, which are reduced when it touches a static object. When a finger moves along a surface, the friction between them produces vibrations, which can not be produced with a free-moving finger in the air. With an inertial measurement unit (IMU) capturing such motion characteristics, we demonstrate the feasibility to detect contact between the finger and static objects. We call our technique ActualTouch. Studies show that a single nail-mounted IMU on the index finger provides sufficient data to train a binary touch status classifier (i.e., touch vs. no-touch), with an accuracy above 95%, generalised across users. This model, trained on a rigid tabletop surface, was found to retain an average accuracy of 96% for 7 other types of everyday surfaces with varying rigidity, and in walking and sitting scenarios where no touch occurred. ActualTouch can be combined with other interaction techniques, such as in a uni-stroke gesture recogniser on arbitrary surfaces, where touch status from ActualTouch is used to delimit the motion gesture data that feed into the recogniser. We demonstrate the potential of ActualTouch in a range of scenarios, such as interaction for augmented reality applications, and leveraging daily surfaces and objects for ad-hoc interactions.},
	articleno    = 59,
	numpages     = 25,
	keywords     = {Finger Touch, Additional Key Words and Phrases:, Everyday Surface, Touch Interaction}
}
@article{10.1145/3397308,
	title        = {Keep the Phone in Your Pocket: Enabling Smartphone Operation with an IMU Ring for Visually Impaired People},
	author       = {Liu, Guanhong and Gu, Yizheng and Yin, Yiwen and Yu, Chun and Wang, Yuntao and Mi, Haipeng and Shi, Yuanchun},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397308},
	url          = {https://doi.org/10.1145/3397308},
	issue_date   = {June 2020},
	abstract     = {Previous studies have shown that visually impaired users face a unique set of pain points in smartphone interaction including locating and removing the phone from a pocket, two-handed interaction while holding a cane, and keeping personal data private in a public setting. In this paper, we present a ring-based input interaction that enables in-pocket smartphone operation. By wearing a ring with an Inertial Measurement Unit on the index finger, users can perform gestures on any surface (e.g., tables, thighs) using subtle, one-handed gestures and receive auditory feedback via earphones. We conducted participatory studies to obtain a set of versatile commands and corresponding gestures. We subsequently trained an SVM model to recognize these gestures and achieved a mean accuracy of 95.5% on 15 classifications. Evaluation results showed that our ring interaction is more efficient than some baseline phone interactions and is easy, private, and fun to use.},
	articleno    = 58,
	numpages     = 23,
	keywords     = {gestural input, accessibility}
}
@article{10.1145/3381009,
	title        = {Interruptibility for In-Vehicle Multitasking: Influence of Voice Task Demands and Adaptive Behaviors},
	author       = {Kim, Auk and Park, Jung-Mi and Lee, Uichin},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381009},
	url          = {https://doi.org/10.1145/3381009},
	issue_date   = {March 2020},
	abstract     = {As a countermeasure to visual-manual distractions, auditory-verbal (voice) interfaces are becoming increasingly popular for in-vehicle systems. This opens up new opportunities for drivers to receive proactive personalized services from various service domains. However, prior studies warned that such interactions can cause cognitive distractions due to the nature of concurrent multitasking with a limited amount of cognitive resources. In this study, we examined (1) how the varying demands of proactive voice tasks under diverse driving situations impact driver interruptibility, and (2) how drivers adapt their concurrent multitasking of driving and proactive voice tasks, and how the adaptive behaviors are related to driver interruptibility. Our quantitative and qualitative analyses showed that in addition to the driving-task demand, the voice-task demand and adaptive behaviors are also significantly related to driver interruptibility. Additionally, we discuss how our findings can be used to design and realize three types of flow-control mechanisms for voice interactions that can improve driver interruptibility.},
	articleno    = 14,
	numpages     = 22,
	keywords     = {Auditory-verbal interface, Interruptibility, In-vehicle information system, Human-vehicle interaction}
}
@article{10.1145/3381005,
	title        = {D3P: Data-Driven Demand Prediction for Fast Expanding Electric Vehicle Sharing Systems},
	author       = {Luo, Man and Du, Bowen and Klemmer, Konstantin and Zhu, Hongming and Ferhatosmanoglu, Hakan and Wen, Hongkai},
	year         = 2020,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 1,
	doi          = {10.1145/3381005},
	url          = {https://doi.org/10.1145/3381005},
	issue_date   = {March 2020},
	abstract     = {The future of urban mobility is expected to be shared and electric. It is not only a more sustainable paradigm that can reduce emissions, but can also bring societal benefits by offering a more affordable on-demand mobility option to the general public. Many car sharing service providers as well as automobile manufacturers are entering the competition by expanding both their EV fleets and renting/returning station networks, aiming to seize a share of the market and to bring car sharing to the zero emissions level. During their fast expansion, one determinant for success is the ability of predicting the demand of stations as the entire system is growing continuously. There are several challenges in this demand prediction problem: First, unlike most of the existing work which predicts demand only for static systems or at few stages of expansion, in the real world we often need to predict the demand as or even before stations are being deployed or closed, to provide information and decision support. Second, for the new stations to be deployed, there is no historical data available to help the prediction of their demand. Finally, the impact of deploying/closing stations on the other stations in the system can be complex. To address these challenges, we formulate the demand prediction problem in the context of fast expanding electric vehicle sharing systems, and propose a data-driven demand prediction approach which aims to model the expansion dynamics directly from the data. We use a local temporal encoding process to handle the historical data for each existing station, and a dynamic spatial encoding process to take correlations between stations into account with Graph Convolutional Neural Networks (GCN). The encoded features are fed to a multi-scale predictor, which forecasts both the long-term expected demand of the stations and their instant demand in the near future. We evaluate the proposed approach with real-world data collected from a major EV sharing platform for one year. Experimental results demonstrate that our approach significantly outperforms the state of the art, showing up to three-fold performance gain in predicting demand for the expanding EV sharing systems.},
	articleno    = 21,
	numpages     = 21,
	keywords     = {Demand Prediction, System Expansion, Data-driven, Electric Vehicle Sharing}
}
@article{10.1145/3351276,
	title        = {ProxiTalk: Activate Speech Input by Bringing Smartphone to the Mouth},
	author       = {Yang, Zhican and Yu, Chun and Zheng, Fengshi and Shi, Yuanchun},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351276},
	url          = {https://doi.org/10.1145/3351276},
	issue_date   = {September 2019},
	abstract     = {Speech input, such as voice assistant and voice message, is an attractive interaction option for mobile users today. However, despite its popularity, there is a use limitation for smartphone speech input: users need to press a button or say a wake word to activate it before use, which is not very convenient. To address it, we match the motion that brings the phone to mouth with the user's intention to use voice input. In this paper, we present ProxiTalk, an interaction technique that allows users to enable smartphone speech input by simply moving it close to their mouths. We study how users use ProxiTalk and systematically investigate the recognition abilities of various data sources (e.g., using a front camera to detect facial features, using two microphones to estimate the distance between phone and mouth). Results show that it is feasible to utilize the smartphone's built-in sensors and instruments to detect ProxiTalk use and classify gestures. An evaluation study shows that users can quickly acquire ProxiTalk and are willing to use it. In conclusion, our work provides the empirical support that ProxiTalk is a practical and promising option to enable smartphone speech input, which coexists with current trigger mechanisms.},
	articleno    = 118,
	numpages     = 25,
	keywords     = {voice input, inertial sensors, smartphone, activity recognition, mobile interaction}
}
@article{10.1145/3351272,
	title        = {IVR: Integrated Vision and Radio Localization with Zero Human Effort},
	author       = {Xu, Jingao and Chen, Hengjie and Qian, Kun and Dong, Erqun and Sun, Min and Wu, Chenshu and Zhang, Li and Yang, Zheng},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351272},
	url          = {https://doi.org/10.1145/3351272},
	issue_date   = {September 2019},
	abstract     = {Smartphone localization is essential to a wide range of applications in shopping malls, museums, office buildings, and other public places. Existing solutions relying on radio fingerprints and/or inertial sensors suffer from large location errors and considerable deployment efforts. We observe an opportunity in the recent trend of increasing numbers of security surveillance cameras installed in indoor spaces to overcome these limitations and revisit the problem of smartphone localization with a fresh perspective. However, fusing vision-based and radio-based systems is non-trivial due to the absence of absolute location, incorrespondence of identification and looseness of sensor fusion. This study proposes iVR, an integrated vision and radio localization system that achieves sub-meter accuracy with indoor semantic maps automatically generated from only two surveillance cameras, superior to precedent systems that require manual map construction or plentiful captured images. iVR employs a particle filter to fuse raw estimates from multiple systems, including vision, radio, and inertial sensor systems. By doing so, iVR outputs enhanced accuracy with zero start-up costs, while overcoming the respective drawbacks of each individual sub-system. We implement iVR on commodity smartphones and validate its performance in five different scenarios. The results show that iVR achieves a remarkable localization accuracy of 0.7m, outperforming the state-of-the-art systems by &gt;70%.},
	articleno    = 114,
	numpages     = 22,
	keywords     = {Indoor Localization, Wireless, Pedestrian Tracking, Computer Vision}
}
@article{10.1145/3351270,
	title        = {MilliBack: Real-Time Plug-n-Play Millimeter Level Tracking Using Wireless Backscattering},
	author       = {Xiao, Ning and Yang, Panlong and Li, Xiang-Yang and Zhang, Yanyong and Yan, Yubo and Zhou, Hao},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351270},
	url          = {https://doi.org/10.1145/3351270},
	issue_date   = {September 2019},
	abstract     = {Real-time handwriting tracking is important for many emerging applications such as Artificial Intelligence assisted education and healthcare. Existing movement tracking systems, including those based on vision, ultrasound or wireless technologies, fail to offer high tracking accuracy, no learning/training/calibration process, low tracking latency, low cost and easy to deploy at the same time. In this work, we design and evaluate a wireless backscattering based handwriting tracking system, called MilliBack, that satisfies all these requirements. At the heart of MilliBack are two Phase Differential Iterative (PDI) schemes that can infer the position of the backscatter tag (which is attached to a writing tool) from the change in the signal phase. By adopting carefully-designed differential techniques in an iterative manner, we can take the diversity of devices out of the equation. The resulting position calculation has a linear complexity with the number of samples, ensuring fast and accurate tracking.We have put together a MilliBack prototype and conducted comprehensive experiments. We show that our system can track various handwriting traces accurately, in some testings it achieve a median error of 4.9 mm. We can accurately track and reconstruct arbitrary writing/drawing trajectories such as equations, Chinese characters or just random shapes. We also show that MilliBack can support relatively high writing speed and smoothly adapt to the changes of working environment.},
	articleno    = 112,
	numpages     = 23,
	keywords     = {Wireless Tracking, Battery Free, Wireless Backscattering}
}
@article{10.1145/3351258,
	title        = {Combating Replay Attacks Against Voice Assistants},
	author       = {Pradhan, Swadhin and Sun, Wei and Baig, Ghufran and Qiu, Lili},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351258},
	url          = {https://doi.org/10.1145/3351258},
	issue_date   = {September 2019},
	abstract     = {Recently, there has been a surge in the popularity of voice-first devices, such as Amazon Echo, Google Home, etc. While these devices make our life more convenient, they are vulnerable to new attacks, such as voice replay. We develop an end-to-end system to detect replay attacks without requiring a user to wear any wearable device. Our system, called REVOLT, has several distinct features: (i) it intelligently exploits the inherent differences between the spectral characteristics of the original and replayed voice signals, (ii) it exploits both acoustic and WiFi channels in tandem, (iii) it utilizes unique breathing rate extracted from WiFi signal while speaking to test the liveness of human voice. After extensive evaluation, our voice component yields Equal Error Rate (EER) of 0.88% and 10.32% in our dataset and ASV2017 dataset, respectively; and WiFi based breathing detection achieves Breaths Per Minute (BPM) error of 1.8 up to 3m distance. We further combine WiFi and voice based detection and show the overall system offers low false positive and false negative when evaluated against a range of attacks.},
	articleno    = 100,
	numpages     = 26,
	keywords     = {Replay attack, Spectrogram, Authentication, Voice, FFT, Breathing}
}
@article{10.1145/3351256,
	title        = {Light Ears: Information Leakage via Smart Lights},
	author       = {Maiti, Anindya and Jadliwala, Murtuza},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351256},
	url          = {https://doi.org/10.1145/3351256},
	issue_date   = {September 2019},
	abstract     = {Modern Internet-enabled smart lights promise energy efficiency and many additional capabilities over traditional lamps. However, these connected lights also create a new attack surface, which can be maliciously used to violate users' privacy and security. In this paper, we design and evaluate novel attacks that take advantage of light emitted by modern smart bulbs, in order to infer users' private data and preferences. The first two attacks are designed to infer users' audio and video playback by a systematic observation and analysis of the multimedia-visualization functionality of smart light bulbs. The third attack utilizes the infrared capabilities of such smart light bulbs to create a covert-channel, which can be used as a gateway to exfiltrate user's private data out of their secured home or office network. A comprehensive evaluation of these attacks in various real-life settings confirms their feasibility and affirms the need for new privacy protection mechanisms.},
	articleno    = 98,
	numpages     = 27,
	keywords     = {Smart Bulb, Multimedia, Data Exfiltration, Security, Smart Light, Privacy}
}
@article{10.1145/3351250,
	title        = {GeoLifecycle: User Engagement of Geographical Exploration and Churn Prediction in LBSNs},
	author       = {Kwon, Young D. and Chatzopoulos, Dimitris and ul Haq, Ehsan and Wong, Raymond Chi-Wing and Hui, Pan},
	year         = 2019,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 3,
	doi          = {10.1145/3351250},
	url          = {https://doi.org/10.1145/3351250},
	issue_date   = {September 2019},
	abstract     = {As Location-Based Social Networks (LBSNs) have become widely used by users, understanding user engagement and predicting user churn are essential to the maintainability of the services. In this work, we conduct a quantitative analysis to understand user engagement patterns exhibited both offline and online in LBSNs. We employ two large-scale datasets which consist of 1.3 million and 62 million users with 5.3 million reviews and 19 million tips in Yelp and Foursquare, respectively. We discover that users keep traveling to diverse locations where they have not reviewed before, which is in contrast to "human life" analogy in real life, an initial exploration followed by exploitation of existing preferences. Interestingly, we find users who eventually leave the community show distinct engagement patterns even with their first ten reviews in various facets, e.g., geographical, venue-specific, linguistic, and social aspects. Based on these observations, we construct predictive models to detect potential churners. We then demonstrate the effectiveness of our proposed features in the churn prediction. Our findings of geographical exploration and online interactions of users enhance our understanding of human mobility based on reviews, and provide important implications for venue recommendations and churn prediction.},
	articleno    = 92,
	numpages     = 29,
	keywords     = {Foursquare, User Engagement, Applied Machine Learning, Churn Prediction, Yelp, Location Based Social Networks, Swarm}
}
@article{10.1145/3328933,
	title        = {Input, Output and Construction Methods for Custom Fabrication of Room-Scale Deployable Pneumatic Structures},
	author       = {Swaminathan, Saiganesh and Rivera, Michael and Kang, Runchang and Luo, Zheng and Ozutemiz, Kadri Bugra and Hudson, Scott E.},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328933},
	url          = {https://doi.org/10.1145/3328933},
	issue_date   = {June 2019},
	abstract     = {In this paper, we examine the future of designing room-scale deployable pneumatic structures that can be fabricated with interactive capabilities and thus be responsive to human input and environments. While there have been recent advances in fabrication methods for creating large-scale structures, they have mainly focused around creating passive structures. Hence in this work, we collectively tackle three main challenges that need to be solved for designing room-scale interactive deployable structures namely -- the input, output (actuation) and construction methods. First, we explore three types of sensing methods --- acoustic, capacitive and pressure --- in order to embed input into these structures. These sensing methods enable users to perform gestures such as knock, squeeze and swipe with specific parts of our fabricated structure such as doors, windows, etc. and make them interactive. Second, we explore three types of actuation mechanisms -- inflatable tendon drive, twisted tendon drive and roll bending actuator -- that are implemented at structural scale and can be embedded into our structures to enable a variety of responsive actuation. Finally, we provide a construction method to custom fabricate and assemble inter-connected pneumatic trusses with embedded sensing and actuation capability to prototype interactions with room-scale deployable structures. To further illustrate the collective (input, output and construction) usage of the system, we fabricated three exemplar interactive deployable structures -- a responsive canopy, an interactive geodesic dome and a portable table (Figures 1 and 2). These can be deployed from a compact deflated state to a much larger inflated state which takes on a desired form while offering interactivity.},
	articleno    = 62,
	numpages     = 17,
	keywords     = {deployable pneumatic structures, embedded interactivity, digital fabrication}
}
@article{10.1145/3328926,
	title        = {Exploring the Efficacy of Sparse, General-Purpose Sensor Constellations for Wide-Area Activity Sensing},
	author       = {Laput, Gierad and Harrison, Chris},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328926},
	url          = {https://doi.org/10.1145/3328926},
	issue_date   = {June 2019},
	abstract     = {Future smart homes, offices, stores and many other environments will increasingly be monitored by distributed sensors, supporting rich, context-sensitive applications. There are two opposing instrumentation approaches. On one end is full sensor saturation, where every object of interest is tagged with a sensor. On the other end, we can imagine a hypothetical, omniscient sensor capable of detecting events throughout an entire building from one location. Neither approach is currently practical, and thus we explore the middle ground between these two extremes: a sparse constellation of sensors working together to provide the benefits of full saturation, but without the social, aesthetic, maintenance and financial drawbacks. More specifically, we target a density of one sensor per room (and less), which means the average home could achieve full coverage with perhaps ten sensors. We quantify and characterize the performance of sparse sensor constellations through deployments across three environments and 67 unique activities. Our results illuminate accuracy implications across key spatial configurations important for enabling more practical, wide-area activity sensing.},
	articleno    = 55,
	numpages     = 19,
	keywords     = {Smart Environments, Internet-of-Things, Ubiquitous sensing, IoT}
}
@article{10.1145/3328911,
	title        = {BoostMeUp: Improving Cognitive Performance in the Moment by Unobtrusively Regulating Emotions with a Smartwatch},
	author       = {Costa, Jean and Guimbreti\`{e}re, Fran\c{c}ois and Jung, Malte F. and Choudhury, Tanzeem},
	year         = 2019,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 2,
	doi          = {10.1145/3328911},
	url          = {https://doi.org/10.1145/3328911},
	issue_date   = {June 2019},
	abstract     = {A person's emotional state can strongly influence their ability to achieve optimal task performance. Aiming to help individuals manage their feelings, different emotion regulation technologies have been proposed. However, despite the well-known influence that emotions have on task performance, no study to date has shown if an emotion regulation technology can also enhance user's cognitive performance in the moment. In this paper, we present BoostMeUp, a smartwatch intervention designed to improve user's cognitive performance by regulating their emotions unobtrusively. Based on studies that show that people tend to associate external signals that resemble heart rates as their own, the intervention provides personalized haptic feedback simulating a different heart rate. Users can focus on their tasks and the intervention acts upon them in parallel, without requiring any additional action. The intervention was evaluated in an experiment with 72 participants, in which they had to do math tests under high pressure. Participants who were exposed to slow haptic feedback during the tests decreased their anxiety, increased their heart rate variability and performed better in the math tests, while fast haptic feedback led to the opposite effects. These results indicate that the BoostMeUp intervention can lead to positive cognitive, physiological and behavioral changes.},
	articleno    = 40,
	numpages     = 23,
	keywords     = {anxiety, heart rate, heart rate variability, cognition, emotion regulation, emotion, apple watch, arousal, unobtrusive, cognitive enhancement, wearable, math, cognitive, intervention, mobile, experiment, mood, self-regulation, feedback loop, feeling, HRV, cognitive performance, mindless, haptic, feedback, smartwatch, distraction, stress}
}
@article{10.1145/3314420,
	title        = {Towards a Diffraction-Based Sensing Approach on Human Activity Recognition},
	author       = {Zhang, Fusang and Niu, Kai and Xiong, Jie and Jin, Beihong and Gu, Tao and Jiang, Yuhang and Zhang, Daqing},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314420},
	url          = {https://doi.org/10.1145/3314420},
	issue_date   = {March 2019},
	abstract     = {In recent years, wireless sensing has been exploited as a promising research direction for contactless human activity recognition. However, one major issue hindering the real deployment of these systems is that the signal variation patterns induced by the human activities with different devices and environmental settings are neither stable nor consistent, resulting in unstable system performance. The existing machine learning based methods usually take the "black box" approach and fails to achieve consistent performance. In this paper, we argue that a deep understanding of radio signal propagation in wireless sensing is needed, and it may be possible to develop a deterministic sensing model to make the signal variation patterns predictable.With this intuition, in this paper we investigate: 1) how wireless signals are affected by human activities taking transceiver location and environment settings into consideration; 2) a new deterministic sensing approach to model the received signal variation patterns for different human activities; 3) a proof-of-concept prototype to demonstrate our approach and a case study to detect diverse activities. In particular, we propose a diffraction-based sensing model to quantitatively determine the signal change with respect to a target's motions, which eventually links signal variation patterns with motions, and hence can be used to recognize human activities. Through our case study, we demonstrate that the diffraction-based sensing model is effective and robust in recognizing exercises and daily activities. In addition, we demonstrate that the proposed model improves the recognition accuracy of existing machine learning systems by above 10%.},
	articleno    = 33,
	numpages     = 25,
	keywords     = {Human activity sensing, Wireless sensing, Fresnel diffraction model, Wi-Fi}
}
@article{10.1145/3314407,
	title        = {Understanding Cycling Trip Purpose and Route Choice Using GPS Traces and Open Data},
	author       = {Nair, Suraj and Javkar, Kiran and Wu, Jiahui and Frias-Martinez, Vanessa},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314407},
	url          = {https://doi.org/10.1145/3314407},
	issue_date   = {March 2019},
	abstract     = {Many mobile applications such as Strava or Mapmyride allow cyclists to collect detailed GPS traces of their trips for health or route sharing purposes. However, cycling GPS traces also have a lot of potential from an urban planning perspective. In this paper, we focus on two important issues to characterize urban cyclist behavior: trip purpose and route choice. Cycling trip purpose has been typically analyzed using survey data. Here, we present a method to automatically infer the purpose of a cycling trip using cyclists' personal data, GPS traces and a variety of built-in and social environment features extracted from open datasets characterizing the streets cycled. We evaluate the proposed method using GPS traces from over 7, 000 cycling routes in the city of Philadelphia and report F1 scores of up to 86% when four trip purposes are considered. On the other hand, we also present a novel statistical method to identify the role that certain variables characterizing the built-in and social environment play in the selection of a specific cycling route. Our results show that cyclists in Philadelphia tend to favor routes with green areas, safety and centrality.},
	articleno    = 20,
	numpages     = 26,
	keywords     = {trip purpose classification, statistical analysis of route choice, spatio-temporal cycling traces}
}
@article{10.1145/3314392,
	title        = {Understanding Parents' Perspectives on Mealtime Technology},
	author       = {Chen, Ying-Yu and Li, Ziyue and Rosner, Daniela and Hiniker, Alexis},
	year         = 2019,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/3314392},
	url          = {https://doi.org/10.1145/3314392},
	issue_date   = {March 2019},
	abstract     = {For young children, family meals are an enjoyable and developmentally useful part of daily life. Although prior work has shown that ubiquitous computing solutions can enhance children's eating habits and mealtime experiences in valuable ways, other work demonstrates that many families are hesitant to use technology in this context. This paper examines adoption barriers for technology for family meals to understand with more nuance what parents value and resist in this space. Using mixed methods, we first observed family dinnertime experiences and then surveyed 122 parents with children from two to six years old. We found that parents prefer screen-based technology over voice interfaces and smart objects, because parents perceive the latter two systems to intrude on their relationship with children. The pervasiveness of smart objects embedded at meals led parents to worry about distraction and technology dependence, while the anthropomorphization of voice interfaces led parents to worry that this technology could displace parenting relationships or disrupt interpersonal interactions among family members. Parents mindlessly applied social scripts to voice interfaces, suggesting families may be more likely to apply concerns from interpersonal interactions to voice interfaces than to other technologies. We discuss the ways different form factors appeal to and worry parents, providing designers with insights about the likelihood of adoption and acceptance.},
	articleno    = 5,
	numpages     = 19,
	keywords     = {parents, Internet of things, children, smart devices, speculative design, family, Mealtime, conversational agents}
}
@article{10.1145/3287050,
	title        = {Predicting Episodes of Non-Conformant Mobility in Indoor Environments},
	author       = {Jayarajah, Kasthuri and Misra, Archan},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287050},
	url          = {https://doi.org/10.1145/3287050},
	issue_date   = {December 2018},
	abstract     = {Traditional mobility prediction literature focuses primarily on improved methods to extract latent patterns from individual-specific movement data. When such predictions are incorrect, we ascribe it to 'random' or 'unpredictable' changes in a user's movement behavior. Our hypothesis, however, is that such apparently-random deviations from daily movement patterns can, in fact, of ten be anticipated. In particular, we develop a methodology for predicting Likelihood of Future Non-Conformance (LFNC), based on two central hypotheses: (a) the likelihood of future deviations in movement behavior is positively correlated to the intensity of such trajectory deviations observed in the user's recent past, and (b) the likelihood of such future deviations increases if the user's strong-ties have also recently exhibited such non-conformant movement behavior. We use extensive longitudinal indoor location data (spanning 4+ months) from an urban university campus to validate these hypotheses, and then show that these features can be used to build an accurate non-conformance predictor: it can predict non-conformant mobility behavior two hours in advance with an AUC ≥ 0.85, significantly outperforming the baseline. We also show that this prediction methodology holds for a representative outdoor public-transport based mobility dataset. Finally, we use a real-world mobile crowd-sourcing application to show the practical impact of such non-conformance: failure to identify such likely anomalous movement behavior causes workers to suffer a noticeable drop in task completion rates and reduces the spatial spread of successfully completed tasks.},
	articleno    = 172,
	numpages     = 24,
	keywords     = {predictability, Indoor mobility, crowdtasking}
}
@article{10.1145/3287043,
	title        = {Device-Free Personalized Fitness Assistant Using WiFi},
	author       = {Guo, Xiaonan and Liu, Jian and Shi, Cong and Liu, Hongbo and Chen, Yingying and Chuah, Mooi Choo},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287043},
	url          = {https://doi.org/10.1145/3287043},
	issue_date   = {December 2018},
	abstract     = {There is a growing trend for people to perform regular workouts in home/office environments because work-at-home people or office workers can barely squeeze in time to go to dedicated exercise places (e.g., gym). To provide personalized fitness assistance in home/office environments, traditional solutions, e.g., hiring personal coaches incur extra cost and are not always available, while new trends requiring wearing smart devices around the clock are cumbersome. In order to overcome these limitations, we develop a device-free fitness assistant system in home/office environments using existing WiFi infrastructure. Our system aims to provide personalized fitness assistance by differentiating individuals, automatically recording fine-grained workout statistics, and assessing workout dynamics. In particular, our system performs individual identification via deep learning techniques on top of workout interpretation. It further assesses the workout by analyzing both short and long-term workout quality, and provides workout reviews for users to improve their daily exercises. Additionally, our system adopts a spectrogram-based workout detection algorithm along with a Cumulative Short Time Energy (CSTE)-based workout segmentation method to ensure its robustness. Extensive experiments involving 20 participants demonstrate that our system can achieve a 93% accuracy on workout recognition and a 97% accuracy for individual identification.},
	articleno    = 165,
	numpages     = 23,
	keywords     = {personalized fitness assistant, device-free, channel state information}
}
@article{10.1145/3287041,
	title        = {Detecting Conversing Groups Using Social Dynamics from Wearable Acceleration: Group Size Awareness},
	author       = {Gedik, Ekin and Hung, Hayley},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287041},
	url          = {https://doi.org/10.1145/3287041},
	issue_date   = {December 2018},
	abstract     = {In this paper, we propose a method for detecting conversing groups. More specifically, we detect pairwise F-formation membership using a single worn accelerometer. We focus on crowded real life scenarios, specifically mingling events, where groups of different sizes naturally occur and evolve over time. Our method uses the dynamics of interaction, derived from people's coordinated social actions and movements. The social actions, speaking, head and hand gesturing, are inferred from wearable acceleration with a transfer learning approach. These automatically labeled actions, together with the raw acceleration, are used to define joint representations of interaction between people through the extraction of pairwise features. We present a new feature set based on the overlap patterns of social actions and utilize some others that were previously proposed in other domains. Our approach considers various interaction patterns of different sized groups by training multiple classifiers with respect to cardinality. The final estimation is then dynamically performed by meta-classifier learning using the local neighborhood of the current test sample. We experimentally show that the proposed method outperforms state of the art approaches. Finally, we show how the accuracy of the social action detection affects group detection performance, analyze the effectiveness of features for different group sizes in detail, discuss how different types of features contribute to the final performance and evaluate the effects of using the local neighborhood for meta-classifier learning.},
	articleno    = 163,
	numpages     = 24,
	keywords     = {conversing group detection, dynamic ensemble selection, F-formation detection, wearable acceleration}
}
@article{10.1145/3287031,
	title        = {Is More Always Better? Discovering Incentivized MHealth Intervention Engagement Related to Health Behavior Trends},
	author       = {Alshurafa, Nabil and Jain, Jayalakshmi and Alharbi, Rawan and Iakovlev, Gleb and Spring, Bonnie and Pfammatter, Angela},
	year         = 2018,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 4,
	doi          = {10.1145/3287031},
	url          = {https://doi.org/10.1145/3287031},
	issue_date   = {December 2018},
	abstract     = {Behavioral medicine is devoting increasing attention to the topic of participant engagement and its role in effective mobile health (mHealth) behavioral interventions. Several definitions of the term "engagement" have been proposed and discussed, especially in the context of digital health behavioral interventions. We consider that engagement refers to specific interaction and use patterns with the mHealth tools such as smartphone applications for intervention, whereas adherence refers to compliance with the directives of the health intervention, independent of the mHealth tools. Through our analysis of participant interaction and self-reported behavioral data in a college student health study with incentives, we demonstrate an example of measuring "effective engagement" as engagement behaviors that can be linked to the goals of the desired intervention. We demonstrate how clustering of one year of weekly health behavior self-reports generate four interpretable clusters related to participants' adherence to the desired health behaviors: healthy and steady, unhealthy and steady, decliners, and improvers. Based on the intervention goals of this study (health promotion and behavioral change), we show that not all app usage metrics are indicative of the desired outcomes that create effective engagement. As such, mHealth intervention design might consider eliciting not just more engagement or use overall, but rather, effective engagement defined by use patterns related to the desired behavioral outcome.},
	articleno    = 153,
	numpages     = 26,
	keywords     = {engagement, multiple health behaviors, college students, mHealth, adherence, longitudinal study, clustering algorithm}
}
@article{10.1145/3264957,
	title        = {Inferring Mobility Relationship via Graph Embedding},
	author       = {Yu, Yanwei and Wang, Hongjian and Li, Zhenhui},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264957},
	url          = {https://doi.org/10.1145/3264957},
	issue_date   = {September 2018},
	abstract     = {Inferring social relationships from user location data has become increasingly important for real-world applications, such as recommendation, advertisement targeting, and transportation scheduling. Most existing mobility relationship measures are based on pairwise meeting frequency, that it, the more frequently two users meet (i.e., co-locate at the same time), the more likely that they are friends. However, such frequency-based methods suffer greatly from data sparsity challenge. Due to data collection limitation and bias in the real world (e.g., check-in data), the observed meeting events between two users might be very few. On the other hand, existing methods focus too much on the interactions between two users, but fail to incorporate the whole social network structure. For example, the relationship propagation is not well utilized in existing methods. In this paper, we propose to construct a user graph based on their spatial-temporal interactions and employ graph embedding technique to learn user representations from such a graph. The similarity measure of such representations can well describe mobility relationship and it is particularly useful to describe the similarity for user pairs with low or even zero meeting frequency. Furthermore, we introduce semantic information on meeting events by using point-of-interest (POI) categorical information. Additionally, when part of the social graph is available as friendship ground truth, we can easily encode such online social network information through a joint graph embedding. Experiments on two real-world datasets demonstrate the effectiveness of our proposed method.},
	articleno    = 147,
	numpages     = 21,
	keywords     = {social computing, spatio-temporal, relationship strength, mobility, Data mining}
}
@article{10.1145/3264954,
	title        = {SenseGAN: Enabling Deep Learning for Internet of Things with a Semi-Supervised Framework},
	author       = {Yao, Shuochao and Zhao, Yiran and Shao, Huajie and Zhang, Chao and Zhang, Aston and Hu, Shaohan and Liu, Dongxin and Liu, Shengzhong and Su, Lu and Abdelzaher, Tarek},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264954},
	url          = {https://doi.org/10.1145/3264954},
	issue_date   = {September 2018},
	abstract     = {Recent proliferation of Internet of Things (IoT) devices with enhanced computing and sensing capabilities has revolutionized our everyday life. The massive data from these ubiquitous devices motivate the creation of intelligent IoT systems that can collectively learn. However, labelling data for learning purposes is extremely time-consuming, which greatly hinders deployment. In this paper, we describe a semi-supervised deep learning framework, called SenseGAN, that can leverage abundant unlabelled sensing data thereby minimizing the need for labelling effort. SenseGAN jointly trains three components with an adversarial game: (i) a classifier for predicting labels of input sensing data; (ii) a generator for generating sensing data samples based on the input labels; and (iii) a discriminator for differentiating the joint data/label distribution between real samples and partially generated samples from either the classifier or the generator. The classifier and the generator try to generate fake data/labels that can fool the discriminator. The adversarial game among the three components can mutually boost their performance, which helps the classifier learn to predict correct labels with unlabelled data in return. SenseGAN can effectively handle multimodal sensing inputs and easily stabilize the adversarial training process, which helps improve the performance of the classifier. Experiments on three IoT applications demonstrate the substantial improvements in accuracy and F1 score under SenseGAN, compared with supervised counterparts trained only on the labelled portion of the data, as well as other supervised and semi-supervised baselines. For these three applications, SenseGAN requires only 10% of the originally labelled data, to attain nearly the same accuracy as a deep learning classifier trained on the fully labelled dataset.},
	articleno    = 144,
	numpages     = 21,
	keywords     = {Internet-of-Things, Semi-Supervised Learning, Deep Learning, Mobile Computing, GAN}
}
@article{10.1145/3264949,
	title        = {Data and Expert Models for Sleep Timing and Chronotype Estimation from Smartphone Context Data and Simulations},
	author       = {Wahl, Florian and Amft, Oliver},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264949},
	url          = {https://doi.org/10.1145/3264949},
	issue_date   = {September 2018},
	abstract     = {We present a sleep timing estimation approach that combines data-driven estimators with an expert model and uses smartphone context data. Our data-driven methodology comprises a classifier trained on features from smartphone sensors. Another classifier uses time as input. Expert knowledge is incorporated via the human circadian and homeostatic two process model. We investigate the two process model as output filter on classifier results and as fusion method to combine sensor and time classifiers. We analyse sleep timing estimation performance, in data from a two-week free-living study of 13 participants and sensor data simulations of arbitrary sleep schedules, amounting to 98280 nights. Five intuitive sleep parameters were derived to control the simulation. Moreover, we investigate model personalisation, by retraining classifiers based on participant feedback. The joint data and expert model yields an average relative estimation error of -2±62 min for sleep onset and -5±70 min for wake (absolute errors 40±48 min and 42±57 min, mean median absolute deviation 22 min and 15 min), which significantly outperforms data-driven methods. Moreover, the data and expert models combination remains robust under varying sleep schedules. Personalising data models with user feedback from the last two days showed the largest performance gain of 57% for sleep onset and 59% for wake up. Our power-efficient smartphone app makes convenient everyday sleep monitoring finally realistic.},
	articleno    = 139,
	numpages     = 28,
	keywords     = {personalisation, machine learning, smartphone sensors, domain expert knowledge, sleep detection}
}
@article{10.1145/3264945,
	title        = {Deep Room Recognition Using Inaudible Echos},
	author       = {Song, Qun and Gu, Chaojie and Tan, Rui},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264945},
	url          = {https://doi.org/10.1145/3264945},
	issue_date   = {September 2018},
	abstract     = {Recent years have seen the increasing need of location awareness by mobile applications. This paper presents a room-level indoor localization approach based on the measured room's echos in response to a two-millisecond single-tone inaudible chirp emitted by a smartphone's loudspeaker. Different from other acoustics-based room recognition systems that record full-spectrum audio for up to ten seconds, our approach records audio in a narrow inaudible band for 0.1 seconds only to preserve the user's privacy. However, the short-time and narrowband audio signal carries limited information about the room's characteristics, presenting challenges to accurate room recognition. This paper applies deep learning to effectively capture the subtle fingerprints in the rooms' acoustic responses. Our extensive experiments show that a two-layer convolutional neural network fed with the spectrogram of the inaudible echos achieve the best performance, compared with alternative designs using other raw data formats and deep models. Based on this result, we design a RoomRecognize cloud service and its mobile client library that enable the mobile application developers to readily implement the room recognition functionality without resorting to any existing infrastructures and add-on hardware. Extensive evaluation shows that RoomRecognize achieves 99.7%, 97.7%, 99%, and 89% accuracy in differentiating 22 and 50 residential/office rooms, 19 spots in a quiet museum, and 15 spots in a crowded museum, respectively. Compared with the state-of-the-art approaches based on support vector machine, RoomRecognize significantly improves the Pareto frontier of recognition accuracy versus robustness against interfering sounds (e.g., ambient music).},
	articleno    = 135,
	numpages     = 28,
	keywords     = {smartphone, Room recognition, inaudible sound}
}
@article{10.1145/3264942,
	title        = {Heed: Exploring the Design of Situated Self-Reporting Devices},
	author       = {Paruthi, Gaurav and Raj, Shriti and Baek, Seungjoo and Wang, Chuyao and Huang, Chuan-che and Chang, Yung-Ju and Newman, Mark W.},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264942},
	url          = {https://doi.org/10.1145/3264942},
	issue_date   = {September 2018},
	abstract     = {In-situ self-reporting is a widely used data collection technique for understanding people's behavior in context. Characteristics of smartphones such as their high proliferation, close proximity to their users, and heavy use have made them a popular choice for applications that require frequent self-reporting. Newer device categories such as wearables and voice assistants offer their own advantages, providing an opportunity to explore a wider range of self-reporting approaches. In this paper, we focus on exploring the design space of Situated Self-Reporting (SSR) devices. We present the Heed system, consisting of simple, low-cost, and low-power SSR devices that are distributed in the environment of the user and can be appropriated for reporting measures such as stress, sleepiness, and activities. In two real-world studies with 10 and 7 users, we compared and analyzed the use of smartphone and Heed devices to uncover differences in their use due to the influence of factors such as situational and social context, notification types, and physical design. Our findings show that Heed devices complemented smartphones in the coverage of activities, locations and interaction preferences. While the advantage of Heed was its single-purpose and dedicated location, smartphones provided mobility and flexibility of use.},
	articleno    = 132,
	numpages     = 21,
	keywords     = {EMA, real-world study, Experience Sampling, Context-aware systems, Self-reporting devices, qualitative study, ESM}
}
@article{10.1145/3264934,
	title        = {HMC: Robust Privacy Protection of Mobility Data against Multiple Re-Identification Attacks},
	author       = {Maouche, Mohamed and Ben Mokhtar, Sonia and Bouchenak, Sara},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264934},
	url          = {https://doi.org/10.1145/3264934},
	issue_date   = {September 2018},
	abstract     = {With the wide propagation of handheld devices, more and more mobile sensors are being used by end users on a daily basis. Those sensors could be leveraged to gather useful mobility data for city planners, business analysts and researches. However, gathering and exploiting mobility data raises many privacy threats. Sensitive information such as one's home or work place, hobbies, religious beliefs, political or sexual preferences can be inferred from the gathered data. In the last decade, Location Privacy Protection Mechanisms (LPPMs) have been proposed to protect user data privacy. However existing LPPMs fail at effectively protecting the users as most of them reason on local mobility features: micro-mobility (e.g., individual geographical coordinates) while ignoring higher level mobility features, which may allow attackers to discriminate between users. In this paper we propose HMC the first LPPM that reasons on the overall user mobility abstracted using heat maps. We evaluate HMC using four real mobility traces and multiple privacy and utility metrics. The results show that with HMC, across all the datasets 87% of mobile users are successfully protected against re-identification attacks, while others LPPMs only achieve a protection ranging from 43% to 79%. By considering only users protected with a high utility, the proportion of users stays high for HMC with 75%, while for others LPPMs it goes down to proportions between 4% and 43%.},
	articleno    = 124,
	numpages     = 25,
	keywords     = {Mobility Data, Location Privacy, Protection Mechanism, Re-identification Attack, Utility}
}
@article{10.1145/3264920,
	title        = {CapHarvester: A Stick-on Capacitive Energy Harvester Using Stray Electric Field from AC Power Lines},
	author       = {Gulati, Manoj and Parizi, Farshid Salemi and Whitmire, Eric and Gupta, Sidhant and Ram, Shobha Sundar and Singh, Amarjeet and Patel, Shwetak N.},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264920},
	url          = {https://doi.org/10.1145/3264920},
	issue_date   = {September 2018},
	abstract     = {Internet of Things (IoT) applications and platforms are becoming increasingly prevalent. Alongside this growth of smart devices comes added costs for deployment, maintenance, and the need to manage power consumption so as to reduce recurrent costs of replacing batteries. To alleviate recurrent battery replacement and maintenance, we propose a novel battery-free, stick-on capacitive energy harvester that harvests the stray electric field generated around AC power lines (110 V/230 V) without an ohmic connection to earth ground reference, thereby obviating the need for cumbersome scraping of paint on concrete walls or digging a earth ground plate. Furthermore, our harvester does not require any appliance or load to be operating on the power line and can continuously harvest power after deployment. In effect, end-users are expected to simply stick the proposed harvester onto any existing power-line cord in order to power a sensing platform. Our controlled lab measurements and real-world deployments demonstrate that our device can harvest 270.6 μJ of energy from a 14 cm long interface in 12 min. We also demonstrate several applications, such as distributed temperature monitoring, appliance state monitoring, and environmental parameter logging for indoor farming.},
	articleno    = 110,
	numpages     = 20,
	keywords     = {Ultra-low power, Internet of Things, Power harvesting}
}
@article{10.1145/3264910,
	title        = {The Role of Urban Mobility in Retail Business Survival},
	author       = {D'Silva, Krittika and Jayarajah, Kasthuri and Noulas, Anastasios and Mascolo, Cecilia and Misra, Archan},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264910},
	url          = {https://doi.org/10.1145/3264910},
	issue_date   = {September 2018},
	abstract     = {Economic and urban planning agencies have strong interest in tackling the hard problem of predicting the odds of survival of individual retail businesses. In this work, we tap urban mobility data available both from a location-based intelligence platform, Foursquare, and from public transportation agencies, and investigate whether mobility-derived features can help foretell the failure of such retail businesses, over a 6 month horizon, across 10 distinct cities spanning the globe. We hypothesise that the survival of such a retail outlet is correlated with not only venue-specific characteristics but also broader neighbourhood-level effects. Through careful statistical analysis of Foursquare and taxi mobility data, we uncover a set of discriminative features, belonging to the neighbourhood's static characteristics, the venue-specific customer visit dynamics, and the neighbourhood's mobility dynamics. We demonstrate that classifiers trained on such features can predict such survival with high accuracy, achieving approximately 80% precision and recall across the cities. We also show that the impact of such features varies across new and established venues and across different cities. Besides achieving a significant improvement over past work on business vitality prediction, our work demonstrates the vital role that mobility dynamics plays in the economic evolution of a city.},
	articleno    = 100,
	numpages     = 22,
	keywords     = {location-based services, spatio-temporal patterns, Urban computing, predictive modeling}
}
@article{10.1145/3264909,
	title        = {ParkLoc: Light-Weight Graph-Based Vehicular Localization in Parking Garages},
	author       = {Cherian, Jim and Luo, Jun and Ho, Shen-Shyang},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264909},
	url          = {https://doi.org/10.1145/3264909},
	issue_date   = {September 2018},
	abstract     = {Locating a vehicle indoors (e.g., underground parking garages) has been a difficult problem to tackle, due to the unavailability of GPS and/or Wi-Fi signals. Current GPS-free indoor localization efforts often rely on infrastructure supports such as Wi-Fi or BLE beacons, whereas the smartphone-only proposals mostly require significant data collection and training efforts per garage. In this context, we propose ParkLoc, a novel lightweight smartphone-only solution for vehicular localization in GPS/Wi-Fi-deprived environments such as indoor parking garages. ParkLoc exploits the inherent planar graph structure of the navigable paths in parking facilities, in order to match a vehicle trajectory onto a sub-section of the map, by modeling these as sparse directed graphs. Exploiting an approximate graph matching method, ParkLoc is able to track a vehicle in real-time with a median error of 4.8m and localize a parked vehicle with a median error of 2m from the nearest parking space. Furthermore, ParkLoc adopts the popular GraphSLAM algorithm from robotics research; it learns the map graph from the observed trajectory graphs and a given set of bootstrap (seed) landmark nodes, in a semi-supervised manner. A key benefit of our approach is that ParkLoc works off-the-shelf without any expensive on-site training or sensor data collection per garage. A comprehensive evaluation of ParkLoc through extensive experiments performed in 4 different parking facilities reveals the promising performance of our graph-based approach for both localization and mapping.},
	articleno    = 99,
	numpages     = 23,
	keywords     = {graph algorithms, SLAM, mobile sensing, Positioning and tracking}
}
@article{10.1145/3264905,
	title        = {Controlling Fine-Grain Sharing in Natural Language with a Virtual Assistant},
	author       = {Campagna, Giovanni and Xu, Silei and Ramesh, Rakesh and Fischer, Michael and Lam, Monica S.},
	year         = 2018,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 3,
	doi          = {10.1145/3264905},
	url          = {https://doi.org/10.1145/3264905},
	issue_date   = {September 2018},
	abstract     = {This paper proposes a novel approach to let consumers share data from their existing web accounts and devices easily, securely, and with fine granularity of control. Our proposal is to have our personal virtual assistant be responsible for sharing our digital assets. The owner can specify fine-grain access control in natural language; the virtual assistant executes access requests on behalf of the requesters and returns the results, if the requests conform to the owner's access control policies.Specifically, we allow a virtual assistant to share any ThingTalk command--an event-driven task composed of skills drawn from Thingpedia, a crowdsourced repository with over 200 functions currently. Access control in natural language is translated into TACL, a formal language we introduce to let users express for whom, what, when, where, and how ThingTalk commands can be executed. TACL policies are in turn translated into SMT (Satisfiability Modulo Theories) formulas and enforced using a provably correct algorithm. Our Distributed ThingTalk Protocol lets users access their own and others' data through their own virtual assistant, while enabling sharing without disclosing information to a third party.The proposed ideas have been incorporated and released in the open-source Almond virtual assistant. 18 of the 20 users in a study say that they like the concept proposed, and 14 like the prototype. We show that users are more willing to share their data given the ability to impose TACL constraints, that 90% of enforceable use cases suggested by 60 users are supported by TACL, and that static and dynamic conformance of policies can be enforced efficiently.},
	articleno    = 95,
	numpages     = 28,
	keywords     = {natural language interfaces, Internet of Things, remote program execution, Web APIs, usable security}
}
@article{10.1145/3214288,
	title        = {RF-ECG: Heart Rate Variability Assessment Based on COTS RFID Tag Array},
	author       = {Wang, Chuyu and Xie, Lei and Wang, Wei and Chen, Yingying and Bu, Yanling and Lu, Sanglu},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214288},
	url          = {https://doi.org/10.1145/3214288},
	issue_date   = {June 2018},
	abstract     = {As an important indicator of autonomic regulation for circulatory function, Heart Rate Variability (HRV) is widely used for general health evaluation. Apart from using dedicated devices (e.g, ECG) in a wired manner, current methods search for a ubiquitous manner by either using wearable devices, which suffer from low accuracy and limited battery life, or applying wireless techniques (e.g., FMCW), which usually utilize dedicated devices (e.g., USRP) for the measurement. To address these issues, we present RF-ECG based on Commercial-Off-The-Shelf (COTS) RFID, a wireless approach to sense the human heartbeat through an RFID tag array attached on the chest area in the clothes. In particular, as the RFID reader continuously interrogates the tag array, two main effects are captured by the tag array: the reflection effect representing the RF-signal reflected from the heart movement due to heartbeat; the moving effect representing the tag movement caused by chest movement due to respiration. To extract the reflection signal from the noisy RF-signals, we develop a mechanism to capture the RF-signal variation of the tag array caused by the moving effect, aiming to eliminate the signals related to respiration. To estimate the HRV from the reflection signal, we propose a signal reflection model to depict the relationship between the RF-signal variation from the tag array and the reflection effect associated with the heartbeat. A fusing technique is developed to combine multiple reflection signals from the tag array for accurate estimation of HRV. Experiments with 15 volunteers show that RF-ECG can achieve a median error of 3% of Inter-Beat Interval (IBI), which is comparable to existing wired techniques.},
	articleno    = 85,
	numpages     = 26,
	keywords     = {Heart rate, HRV, RFID}
}
@article{10.1145/3214285,
	title        = {Effect of Distinct Ambient Noise Types on Mobile Interaction},
	author       = {Sarsenbayeva, Zhanna and van Berkel, Niels and Velloso, Eduardo and Kostakos, Vassilis and Goncalves, Jorge},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214285},
	url          = {https://doi.org/10.1145/3214285},
	issue_date   = {June 2018},
	abstract     = {The adverse effect of ambient noise on humans has been extensively studied in fields like cognitive science, indicating a significant impact on cognitive performance, behaviour, and emotional state. Surprisingly, the effect of ambient noise has not been studied in the context of mobile interaction. As smartphones are ubiquitous by design, smartphone users are exposed to a wide variety of ambient noises while interacting with their devices. In this paper, we present a structured analysis of the effect of six distinct ambient noise types on typical smartphone usage tasks. The evaluated ambient noise types include variants of music, urban noise and speech. We analyse task completion time and errors, and find that different ambient noises affect users differently. For example, while speech and urban noise slow down text entry, being exposed to music reduces completion time in target acquisition tasks. Our study contributes to the growing research area on situational impairments, and we compare our results to previous work on the effect of cold-induced situational impairments. Our results can be used to support smartphone users through adaptive interfaces which respond to the ongoing context of the user.},
	articleno    = 82,
	numpages     = 23,
	keywords     = {mobile interaction, ambient noise, Smartphones, situational impairments, performance}
}
@article{10.1145/3214266,
	title        = {Enabling Public Cameras to Talk to the Public},
	author       = {Cao, Siyuan and Wang, He},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214266},
	url          = {https://doi.org/10.1145/3214266},
	issue_date   = {June 2018},
	abstract     = {This paper asks: Is it possible for cameras in public areas, say ceiling cameras in a museum, to send personalized messages to people without knowing any addresses of their phones? We define this kind of problem as Private Human Addressing and develop a real-time end-to-end system called PHADE to solve it. Unlike traditional data transmission protocols that need to first learn the destination's address, our cameras rely on viewing user's motion patterns, and use the uniqueness of these patterns as the address for communication. Once receiving the wireless broadcast from the cameras, the user's phone can locally compare the "motion address" of the packet against its own motion sensor data, and accept the packet upon a "good" match.In addition to requiring no data from users, our system transforms the motion patterns into low-dimensional codes to prevent leakage of user's walking behaviors. Thus, a hacker who collects all the broadcast messages would still not be able to infer the motion patterns of users. Real-world experiments show that PHADE discriminates 2, 4, 6, 8, 10 people with 98%, 95%, 90%, 90%, 87% correctness and about 3 seconds constant delay. Since abundant and accurate information can be extracted from videos, PHADE would find applications in various contexts. Extended to localization system and audio guide, PHADE achieves a median error of 0.19m and 99.7% matching correctness, respectively. PHADE can also deliver messages based on human gestures. There is no need to deploy any extra infrastructures or to require users to rent any dedicated device.},
	articleno    = 63,
	numpages     = 20,
	keywords     = {camera, Human addressing, principal component analysis, motion features, communication}
}
@article{10.1145/3191781,
	title        = {Lightweight Display-to-Device Communication Using Electromagnetic Radiation and FM Radio},
	author       = {Yang, Zhice and Zhang, Jiansong and Wang, Zeyu and Zhang, Qian},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191781},
	url          = {https://doi.org/10.1145/3191781},
	issue_date   = {March 2018},
	abstract     = {This paper presents Shadow, a novel display-to-device communication system working in radio frequency. It leverages Electromagnetic Radiation (EMR) signals emanating from displays to transmit information. Specifically, Shadow modulates the high-frequency electric signals flowing in the display interface and makes the leaked EMR signals fall into the FM band. In this way, nearby mobile devices can receive information from the display through FM receivers. Compared with other display-to-device communication approaches, Shadow does not rely on cameras, and is thus more lightweight and requires fewer user actions. Furthermore, Shadow's transmissions do not incur any degradation in the display quality, as they only take place in the Blanking Interval, which will not be shown on the display panel. Shadow requires no modification to existing hardware. The prototype is implemented with commodity display systems and mobile devices. Results show that it can achieve 1.5 kbps at distances of up to 20 cm from the display panel.},
	articleno    = 49,
	numpages     = 19,
	keywords     = {Mobile Devices, Electromagnetic Radiation, Display, Blanking Interval}
}
@article{10.1145/3191748,
	title        = {Obfuscation At-Source: Privacy in Context-Aware Mobile Crowd-Sourcing},
	author       = {Kandappu, Thivya and Misra, Archan and Cheng, Shih-Fen and Tandriansyah, Randy and Lau, Hoong Chuin},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191748},
	url          = {https://doi.org/10.1145/3191748},
	issue_date   = {March 2018},
	abstract     = {By effectively reaching out to and engaging larger population of mobile users, mobile crowd-sourcing has become a strategy to perform large amount of urban tasks. The recent empirical studies have shown that compared to the pull-based approach, which expects the users to browse through the list of tasks to perform, the push-based approach that actively recommends tasks can greatly improve the overall system performance. As the efficiency of the push-based approach is achieved by incorporating worker's mobility traces, privacy is naturally a concern. In this paper, we propose a novel, 2-stage and user-controlled obfuscation technique that provides a trade off-amenable framework that caters to multi-attribute privacy measures (considering the per-user sensitivity and global uniqueness of locations). We demonstrate the effectiveness of our approach by testing it using the real-world data collected from the well-established TA$Ker platform. More specifically, we show that one can increase its location entropy by 23% with only modest changes to the real trajectories while imposing an additional 24% (&lt; 1 min) of detour overhead on average. Finally, we present insights derived by carefully inspecting various parameters that control the whole obfuscation process.},
	articleno    = 16,
	numpages     = 24,
	keywords     = {trajectory, Mobile Crowd-sourcing platforms, obfuscation, context-aware, Privacy}
}
@article{10.1145/3191735,
	title        = {Students' Experiences with Ecological Momentary Assessment Tools to Report on Emotional Well-Being},
	author       = {Chan, Larry and Swain, Vedant Das and Kelley, Christina and de Barbaro, Kaya and Abowd, Gregory D. and Wilcox, Lauren},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191735},
	url          = {https://doi.org/10.1145/3191735},
	issue_date   = {March 2018},
	abstract     = {Ecological Momentary Assessment (EMA) methods have emerged as an approach that enhances the ecological validity of data collected for the study of human behavior and experience. In particular, EMA methods are used to capture individuals' experiences (e.g., symptoms, affect, and behaviors) in real-world contexts and in near-real time. However, work investigating participants' experiences in EMA studies and in particular, how these experiences may influence the collected data, is limited. We conducted in-depth focus groups with 32 participants following an EMA study on mental well-being in college students. In doing so, we probed how the elicitation of high-quality, reflective responses is related to the design of EMA interactions. Through our study, we distilled three primary considerations for designing EMA interactions, based on observations of 1) response strategies to repeated questions, 2) the perceived burden of EMA prompts, and 3) challenges to the validity and robustness of EMA data. We present these considerations in the context of two microinteraction-based EMA approaches that we tested: lock-screen EMA and image-based question prompts. We conclude by characterizing design tensions in the presentation and delivery of EMA prompts, and outline directions for future work to address these tensions.},
	articleno    = 3,
	numpages     = 20,
	keywords     = {Qualitative Study, User Interface Design, Experience Sampling Method, Focus Groups, Ecological Momentary Assessment, Emotion}
}
@article{10.1145/3161601,
	title        = {Wordometer Systems for Everyday Life},
	author       = {Augereau, Olivier and Sanches, Charles Lima and Kise, Koichi and Kunze, Kai},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161601},
	url          = {https://doi.org/10.1145/3161601},
	issue_date   = {December 2017},
	abstract     = {We present in this paper a detailed comparison of different algorithms and devices to determine the number of words read in everyday life. We call our system the “Wordometer”. We used three kinds of eye tracking systems in our experiment: mobile video-oculography (MVoG); stationary video-oculography (SVoG); and electro-oculography (EoG). By analyzing the movement of the eyes we were able to estimate the number of words that a user read. Recently, inexpensive eye trackers have appeared on the market. Thus, we undertook a large-scale experiment that compared three devices that can be used for daily reading on a screen: the Tobii Eye X SVoG; the JINS MEME EoG; and the Pupil MVoG. We found that the accuracy of the everyday life devices and professional devices was similar when used with the Wordometer. We analyzed the robustness of the systems for special reading behaviors: rereading and skipping.With the MVoG, SVoG and EoG systems, we obtained estimation errors respectively, 7.2%, 13.0%, and 10.6% in our main experiment. In all our experiments, we obtained 300 recordings by 14 participants, which amounted to 109,097 read words.},
	articleno    = 123,
	numpages     = 21,
	keywords     = {electrooculography, wordometer, machine learning, reading analysis, Eye tracking}
}
@article{10.1145/3161416,
	title        = {TouchCam: Realtime Recognition of Location-Specific On-Body Gestures to Support Users with Visual Impairments},
	author       = {Stearns, Lee and Oh, Uran and Findlater, Leah and Froehlich, Jon E.},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161416},
	url          = {https://doi.org/10.1145/3161416},
	issue_date   = {December 2017},
	abstract     = {On-body interaction, which employs the user's own body as an interactive surface, offers several advantages over existing touchscreen devices: always-available control, an expanded input space, and additional proprioceptive and tactile cues that support non-visual use. While past work has explored a variety of approaches such as wearable depth cameras, bio-acoustics, and infrared reflectance (IR) sensors, these systems do not instrument the gesturing finger, do not easily support multiple body locations, and have not been evaluated with visually impaired users (our target). In this paper, we introduce TouchCam, a finger wearable to support location-specific, on-body interaction. TouchCam combines data from infrared sensors, inertial measurement units, and a small camera to classify body locations and gestures using supervised learning. We empirically evaluate TouchCam's performance through a series of offline experiments followed by a realtime interactive user study with 12 blind and visually impaired participants. In our offline experiments, we achieve high accuracy (&gt;96%) at recognizing coarse-grained touch locations (e.g., palm, fingers) and location-specific gestures (e.g., tap on wrist, left swipe on thigh). The follow-up user study validated our real-time system and helped reveal tradeoffs between various on-body interface designs (e.g., accuracy, convenience, social acceptability). Our findings also highlight challenges to robust input sensing for visually impaired users and suggest directions for the design of future on-body interaction systems.},
	articleno    = 164,
	numpages     = 23,
	keywords     = {Gesture Recognition, Wearable sensors, Skin Texture Classification, Accessibility, Computer Vision Applications, Blind and Low-Vision Users, On-body input}
}
@article{10.1145/3161408,
	title        = {MORP: Data-Driven Multi-Objective Route Planning and Optimization for Electric Vehicles},
	author       = {Sarker, Ankur and Shen, Haiying and Stankovic, John A.},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161408},
	url          = {https://doi.org/10.1145/3161408},
	issue_date   = {December 2017},
	abstract     = {The Wireless Power Transfer (WPT) system that enables in-motion charging (or wireless charging) for Electric Vehicles (EVs) has been introduced to resolve battery-related issues (such as long charging time, high cost, and short driving range) and increase the wide-acceptance of EVs. In this paper, we study the WPT system with the objectives of minimizing energy consumption, travel time, charging monetary cost on the way, and range anxiety for online EVs. Specifically, we propose the Multi-Objective Route Planner system (MORP) to guide EVs for the multi-objective routing. MORP incorporates two components: traffic state prediction and optimal route determination. For the traffic state prediction, we conducted analysis on a traffic dataset and observed spatial-temporal features of traffic patterns. Accordingly, we introduce the horizontal space-time Autoregressive Integrated Moving Average (ARIMA) model to predict vehicle counts (i.e., traffic volume) for locations with available historical traffic data. And, we use the spatial-temporal ordinary kriging method to predict vehicle counts for locations without historical traffic data. Based on vehicle counts, we use the non-parametric kernel regression method to predict velocity of road sections, which is used to predict travel time and then, energy consumption of a route of an EV with the help of the proposed energy consumption model. We also estimate charging monetary cost and EV related range anxiety based on unit energy cost, predicted travel time and energy consumption, and current onboard energy. We design four different cost functions (travel time, energy consumption, charging monetary cost, and range anxiety) of routing and formulate a multi-objective routing optimization problem. We use the predicted parameters as inputs of the optimization problem and find the optimal route using the adaptive epsilon constraint method. We evaluate our proposed MORP system in four different aspects (including traffic prediction, velocity prediction, energy consumption prediction, and EV routing). From the experimental studies, we find the effectiveness of the proposed MORP system in different aspects of the online EV routing system.},
	articleno    = 162,
	numpages     = 35,
	keywords     = {Wireless power transfer system, electric vehicle routing, multi-objective optimization, spatial-temporal traffic analysis}
}
@article{10.1145/3161189,
	title        = {Auth ‘n’ Scan: Opportunistic Photoplethysmography in Mobile Fingerprint Authentication},
	author       = {Hashizume, Takahiro and Arizono, Takuya and Yatani, Koji},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161189},
	url          = {https://doi.org/10.1145/3161189},
	issue_date   = {December 2017},
	abstract     = {Recent commodity smartphones have biometric sensing capabilities, allowing their daily use for authentication and identification. This increasing use of biometric systems motivates us to design an opportunistic way to sense user's additional physiological or behavioral data. We define this concurrent physiological or behavioral data sensing during biometric authentication or identification as dual-purpose biometrics. As an instance of dual-purpose biometrics, we develop photoplethysmography (PPG) sensing during mobile fingerprint authentication, called Auth ‘n’ Scan. Our system opportunistically extracts cardiovascular information, such as a heart rate and its variability, while users perform phone unlock of a smartphone. To achieve this sensing, our Auth ‘n’ Scan system attaches four PPG units around a fingerprint sensor. The system also performs noise removal and signal selection to accurately estimate cardiovascular information. This paper presents the hardware implementation and signal processing algorithm of our Auth ‘n’ Scan prototype. We also report our system evaluations with 10 participants, showing that, despite a little low precision (a standard deviation of 3--7), estimation of heart rates with high accuracy (under a mean error of 1) is possible from PPG data of five seconds and longer if their baseline information is given. We discuss the feasibility of opportunistic PPG sensing in mobile fingerprint authentication.},
	articleno    = 137,
	numpages     = 27,
	keywords     = {fingerprint authentication, cardiovascular sensing, Dual-purpose biometrics, unobtrusive sensing, photoplethysmography}
}
@article{10.1145/3161186,
	title        = {CoCo: Collaboration Coach for Understanding Team Dynamics during Video Conferencing},
	author       = {Samrose, Samiha and Zhao, Ru and White, Jeffery and Li, Vivian and Nova, Luis and Lu, Yichen and Ali, Mohammad Rafayet and Hoque, Mohammed Ehsan},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161186},
	url          = {https://doi.org/10.1145/3161186},
	issue_date   = {December 2017},
	abstract     = {We present and discuss a fully-automated collaboration system, CoCo, that allows multiple participants to video chat and receive feedback through custom video conferencing software. After a conferencing session, a virtual feedback assistant provides insights on the conversation to participants. CoCo automatically pulls audial and visual data during conversations and analyzes the extracted streams for affective features, including smiles, engagement, attention, as well as speech overlap and turn-taking. We validated CoCo with 39 participants split into 10 groups. Participants played two back-to-back team-building games, Lost at Sea and Survival on the Moon, with the system providing feedback between the two. With feedback, we found a statistically significant change in balanced participation---that is, everyone spoke for an equal amount of time. There was also statistically significant improvement in participants' self-evaluations of conversational skills awareness, including how often they let others speak, as well as of teammates' conversational skills. The entire framework is available at https://github.com/ROC-HCI/CollaborationCoach_PostFeedback.},
	articleno    = 160,
	numpages     = 24,
	keywords     = {Team dynamics, Video conferencing, Virtual feedback system, Group discussion}
}
@article{10.1145/3161178,
	title        = {Automated Dyadic Data Recorder (ADDR) Framework and Analysis of Facial Cues in Deceptive Communication},
	author       = {Sen, Taylan and Hasan, Md Kamrul and Teicher, Zach and Hoque, Mohammed Ehsan},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161178},
	url          = {https://doi.org/10.1145/3161178},
	issue_date   = {December 2017},
	abstract     = {We developed an online framework that can automatically pair two crowd-sourced participants, prompt them to follow a research protocol, and record their audio and video on a remote server. The framework comprises two web applications: an Automatic Quality Gatekeeper for ensuring only high quality crowd-sourced participants are recruited for the study, and a Session Controller which directs participants to play a research protocol, such as an interrogation game. This framework was used to run a research study for analyzing facial expressions during honest and deceptive communication using a novel interrogation protocol. The protocol gathers two sets of nonverbal facial cues in participants: features expressed during questions relating to the interrogation topic and features expressed during control questions. The framework and protocol were used to gather 151 dyadic conversations (1.3 million video frames). Interrogators who were lied to expressed the smile-related lip corner puller cue more often than interrogators who were being told the truth, suggesting that facial cues from interrogators may be useful in evaluating the honesty of witnesses in some contexts. Overall, these results demonstrate that this framework is capable of gathering high quality data which can identify statistically significant results in a communication study.},
	articleno    = 163,
	numpages     = 22,
	keywords     = {web framework, video conferencing, interrogation, facial expression analysis, Deception detection}
}
@article{10.1145/3161161,
	title        = {Bites‘n’Bits: Inferring Eating Behavior from Contextual Mobile Data},
	author       = {Biel, Joan-Isaac and Martin, Nathalie and Labbe, David and Gatica-Perez, Daniel},
	year         = 2018,
	month        = {jan},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 4,
	doi          = {10.1145/3161161},
	url          = {https://doi.org/10.1145/3161161},
	issue_date   = {December 2017},
	abstract     = {We collect and analyze mobile data about everyday eating occasions to study eating behavior in relation to its context (time, location, social context, related activities and physical activity). Our contributions are three-fold. First, we deployed a data collection campaign with 122 Swiss university students, resulting in 1208 days of food data, 3414 meal occasions, 1034 snacking occasions, 5097 photos, and 998 days of physical activity. Second, we analyzed the collected data and report findings associated to the compliance, snacks vs. meals patterns, physical activity, and contextual differences between snacks and meals. Third, we addressed a novel ubicomp task, namely the classification of eating occasions (meals vs. snacks) in everyday life. We show that a machine learning method using time of day, time since last intake, and location is able to discriminate eating occasions with 84% accuracy, which significantly outperforms a baseline method based only on time.},
	articleno    = 125,
	numpages     = 33,
	keywords     = {Physical Activity, Snack and Meal, Eating Behavior, Mobile Crowdsensing, Machine Learning}
}
@article{10.1145/3132030,
	title        = {FiDO: A Community-Based Web Browsing Agent and CDN for Challenged Network Environments},
	author       = {Vigil-Hayes, Morgan and Belding, Elizabeth and Zegura, Ellen},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3132030},
	url          = {https://doi.org/10.1145/3132030},
	issue_date   = {September 2017},
	abstract     = {Homes located on tribal lands, particularly in rural areas of the United States, continue to lack access to broadband Internet and cellular connectivity [19]. Inspired by previous observations of community content similarity in tribal networks, we propose FiDO, a community-based Web browsing and content delivery system that takes advantage of user mobility, opportunistic connectivity, and collaborative filtering to provide relevant Web content to members of disconnected households via opportunistic contact with cellular base stations during a daily commute. We evaluate FiDO using trace-driven simulations with network usage data collected from a tribal-operated ISP that serves the Coeur d’Alene Indian Reservation in Western Idaho. By collecting data about household Web preferences and applying a collaborative filtering technique based on the Web usage patterns of the surrounding reservation community, we are able to opportunistically browse the Web on behalf of members of disconnected households, providing an average of 69.4 Web pages (all content from a specific URL, e.g., “http://gis.cdatribe-nsn.gov/LandBuyBack/”) crawled from 73% of their top 10 most visited Web domains (e.g., “cdatribe-nsn.gov” or “cnn.com/”) per day. Moreover, this content is able to be fetched and pushed to users even when the opportunistic data rate is limited to an average of only 0.99 Mbps (σ = 0.24 Mbps) and the daily opportunistic connection time is an average of 45.9 minutes (σ = 2.3 minutes). Additionally, we demonstrate a hybrid “search and browse” approach that allocates a percentage of opportunistic resources to the download of user-specified content. By dedicating only 10% of opportunistic windows of connectivity to the download of social media content, 51% of households were able to receive all of their daily expected social media content in addition to an average of 55.3 Web pages browsed on their behalf from an average of 4 different Web domains. Critically, we demonstrate the feasibility of a collaborative and community-based Web browsing model that extends access to Web content across the last mile(s) using existing infrastructure and rural patterns of mobility.},
	articleno    = 108,
	numpages     = 25
}
@article{10.1145/3131894,
	title        = {Recognizing Eating from Body-Worn Sensors: Combining Free-Living and Laboratory Data},
	author       = {Mirtchouk, Mark and Lustig, Drew and Smith, Alexandra and Ching, Ivan and Zheng, Min and Kleinberg, Samantha},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3131894},
	url          = {https://doi.org/10.1145/3131894},
	issue_date   = {September 2017},
	abstract     = {Automated dietary monitoring solutions that can find when, what, and how much individuals consume are needed for many applications such as providing feedback to individuals with chronic disease. Advances in body-worn sensors have led to systems with high accuracy for finding meals and even which foods are consumed in each bite. However, most tests are done in controlled lab settings with restricted meal choices, little background noise, and subjects focused on eating. For these systems to be adopted by users it is critical that they work well in realistic situations and be able to handle confounding factors such as background noise, shared meals, and multi-tasking. Work in realistic environments usually has lower accuracy, but has challenges in determining ground truth. Most critically, there has been a significant gap between lab and free-living environments. This is compounded by data usually being collected for different individuals in each setting, making it difficult to determine how the accuracy gap can be closed. We present a multi-modality study on eating recognition, using body-worn motion (head, wrists) and audio (earbud microphone) sensors for 12 participants (6 from the lab study, 6 new to test generalizability). In contrast to the lab, where audio alone has the highest accuracy, we find now that a combination of sensing modalities (audio, motion) is needed; yet sensor placement (head vs. wrist) is not critical. We further find that lab data does generalize to other participants, but while personal free-living data improves accuracy, more data from others can actually lead to worse performance.},
	articleno    = 85,
	numpages     = 20,
	keywords     = {Nutrition, Eating recognition, Acoustic and motion sensing}
}
@article{10.1145/3130982,
	title        = {CityFlowFragility: Measuring the Fragility of People Flow in Cities to Disasters Using GPS Data Collected from Smartphones},
	author       = {Yabe, Takahiro and Tsubouchi, Kota and Sekimoto, Yoshihide},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130982},
	url          = {https://doi.org/10.1145/3130982},
	issue_date   = {September 2017},
	abstract     = {Economic loss caused by natural disasters is increasing in many cities around the world. There is an increasing demand for a method that effectively measures the fragility of people flow to appropriately plan the future investment into infrastructure. Conventional methods measure the fragility of urban systems using infrastructure data such as the road and railway networks. However, these methods are costly to perform, cannot directly measure the disruption on human activities caused by disasters, nor can they be applied for individual disasters. Here, we propose a novel method that quantifies the fragility of cities through detecting the delay in commuting activities using GPS data collected from smartphones. Because commuting activities are daily routines for many people, commuting flow has little day-to-day fluctuation, which makes it an appropriate metric for detecting anomalies and disruption in urban systems. This method can be utilized in any city in the world regardless of differences in network structures or population distribution, as long as people commute on a daily basis. We validate our method in various cities for snowfall and typhoons using real datasets in Japan, and show that intuitive results can be obtained. Our method's reliability is clarified by comparing the results with conventional metrics. We also present useful analyses and applications of CityFlowFragility for urban planning and disaster management.},
	articleno    = 117,
	numpages     = 17,
	keywords     = {mobile phone GPS data, disaster fragility, Urban human dynamics}
}
@article{10.1145/3130981,
	title        = {CLSTERS: A General System for Reducing Errors of Trajectories Under Challenging Localization Situations},
	author       = {Wu, Hao and Sun, Weiwei and Zheng, Baihua and Yang, Li and Zhou, Wei},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130981},
	url          = {https://doi.org/10.1145/3130981},
	issue_date   = {September 2017},
	abstract     = {Trajectory data generated by outdoor activities have great potential for location based services. However, depending on the localization technique used, certain trajectory data could contain large errors. For example, the error of trajectories generated by cellular-based localization techniques is around 100m which is ten times larger than that of GPS-based trajectories. Hence, enhancing the utility of those large-error trajectories becomes a challenge. In this paper we show how to improve the quality of trajectory data having large errors. Some existing works reduce the error through hardware which requires information such as the time of arrival (TOA), received signal strength indication (RSSI), the position of cell towers, etc. Moreover, different positioning techniques will result in different hardware-based solutions and different data formats, which limit the generalizablity. Other works study a related but different problem, i.e., map matching, with the aid of road network information, to reduce the uncertainty and the noise of trajectory data. However, most of these approaches are designed for the GPS-sampled data, and hence they might not be able to achieve a similar performance when applied directly to trajectories with large errors. Motivated by this, we propose a general error reduction system namely CLSTERS for trajectories with large scale of errors. Our system is hardware independent and only requires the coordinates and the time stamp of each sample point which makes it general and ubiquitous. We present results from experiments using three real-world datasets in three different cities generated by two different localization techniques and the results show that our approach outperforms existing solutions.},
	articleno    = 115,
	numpages     = 28,
	keywords     = {Localization, map matching, error reduction, cellular-based trajectory}
}
@article{10.1145/3130979,
	title        = {From Intermittent to Ubiquitous: Enhancing Mobile Access to Online Social Networks with Opportunistic Optimization},
	author       = {Wu, Di and Arkhipov, Dmitri I. and Przepiorka, Thomas and Li, Yong and Guo, Bin and Liu, Qiang},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130979},
	url          = {https://doi.org/10.1145/3130979},
	issue_date   = {September 2017},
	abstract     = {Accessing online social networks in situations with intermittent Internet connectivity is a challenge. We have designed a context-aware mobile system to enable efficient offline access to online social media by prefetching, caching and disseminating content opportunistically when signal availability is detected. This system can measure, crowdsense and predict network characteristics, and then use these predictions of mobile network signal to schedule cellular access or device-to-device (D2D) communication. We propose several opportunistic optimization schemes to enhance controlled crowdsensing, resource constrained mobile prefetch, and D2D transmissions impacted by individual selfishness. Realistic tests and large-scale trace analysis show our system can achieve a significant improvement over existing approaches in situations where users experience intermittent cellular service or disrupted network connection.},
	articleno    = 114,
	numpages     = 32,
	keywords     = {mobile crowdsensing, mobile social networks, opportunistic networking, D2D communication, Mobile access}
}
@article{10.1145/3130974,
	title        = {Watching inside the Screen: Digital Activity Monitoring for Task Recognition and Proactive Information Retrieval},
	author       = {Vuong, Tung and Jacucci, Giulio and Ruotsalo, Tuukka},
	year         = 2017,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 1,
	number       = 3,
	doi          = {10.1145/3130974},
	url          = {https://doi.org/10.1145/3130974},
	issue_date   = {September 2017},
	abstract     = {We investigate to what extent it is possible to infer a user’s work tasks by digital activity monitoring and use the task models for proactive information retrieval. Ten participants volunteered for the study, in which their computer screen was monitored and related logs were recorded for 14 days. Corresponding diary entries were collected to provide ground truth to the task detection method. We report two experiments using this data. The unsupervised task detection experiment was conducted to detect tasks using unsupervised topic modeling. The results show an average task detection accuracy of more than 70% by using rich screen monitoring data. The single-trial task detection and retrieval experiment utilized unseen user inputs in order to detect related work tasks and retrieve task-relevant information on-line. We report an average task detection accuracy of 95%, and the corresponding model-based document retrieval with Normalized Discounted Cumulative Gain of 98%. We discuss and provide insights regarding the types of digital tasks occurring in the data, the accuracy of task detection on different task types, and the role of using different data input such as application names, extracted keywords, and bag-of-words representations in the task detection process. We also discuss the implications of our results for ubiquitous user modeling and privacy.},
	articleno    = 109,
	numpages     = 23,
	keywords     = {Activity recognition, screen scraping, user modeling, digital activity monitoring, task detection}
}
@article{10.1145/3448119,
	title        = {ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch},
	author       = {Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448119},
	url          = {https://doi.org/10.1145/3448119},
	issue_date   = {March 2021},
	abstract     = {Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.},
	articleno    = 45,
	numpages     = 25,
	keywords     = {vibration intelligence, micro finger writing, wearable devices, text input}
}
@article{10.1145/3448115,
	title        = {DronePrint: Acoustic Signatures for Open-Set Drone Detection and Identification with Online Data},
	author       = {Kolamunna, Harini and Dahanayaka, Thilini and Li, Junye and Seneviratne, Suranga and Thilakaratne, Kanchana and Zomaya, Albert Y. and Seneviratne, Aruna},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448115},
	url          = {https://doi.org/10.1145/3448115},
	issue_date   = {March 2021},
	abstract     = {With the ubiquitous availability of drones, they are adopted benignly in multiple applications such as cinematography, surveying, and legal goods delivery. Nonetheless, they are also being used for reconnaissance, invading personal or secure spaces, harming targeted individuals, smuggling drugs and contraband, or creating public disturbances. These malicious or improper use of drones can pose significant privacy and security threats in both civilian and military settings. Therefore, it is vital to identify drones in different environments to assist the decisions on whether or not to contain unknown drones. While there are several methods proposed for detecting the presence of a drone, they have limitations when it comes to low visibility, limited access, or hostile environments. In this paper, we propose DronePrint that uses drone acoustic signatures to detect the presence of a drone and identify the make and the model of the drone. We address the shortage of drone acoustic data by relying on audio components of online videos. In drone detection, we achieved 96% accuracy in a closed-set scenario, and 86% accuracy in a more challenging open-set scenario. Our proposed method of cascaded drone identification, where a drone is identified for its 'make' followed by the 'model' of the drone achieves 90% overall accuracy. In this work, we cover 13 commonly used commercial and consumer drone models, which is to the best of understanding is the most comprehensive such study to date. Finally, we demonstrate the robustness of DronePrint to drone hardware modifications, Doppler effect, varying SNR conditions, and in realistic open-set acoustic scenes.},
	articleno    = 20,
	numpages     = 31,
	keywords     = {Drones, Acoustic fingerprinting, LSTM, Drone Audio Dataset}
}
@article{10.1145/3448106,
	title        = {The Design and Evaluation of a Mobile System for Rapid Diagnostic Test Interpretation},
	author       = {Park, Chunjong and Ngo, Hung and Lavitt, Libby Rose and Karuri, Vincent and Bhatt, Shiven and Lubell-Doughtie, Peter and Shankar, Anuraj H. and Ndwiga, Leonard and Osoti, Victor and Wambua, Juliana K. and Bejon, Philip and Ochola-Oyier, Lynette Isabella and Chilver, Monique and Stocks, Nigel and Lyon, Victoria and Lutz, Barry R. and Thompson, Matthew and Mariakakis, Alex and Patel, Shwetak},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448106},
	url          = {https://doi.org/10.1145/3448106},
	issue_date   = {March 2021},
	abstract     = {Rapid diagnostic tests (RDTs) provide point-of-care medical screening without the need for expensive laboratory equipment. RDTs are theoretically straightforward to use, yet their analog colorimetric output leaves room for diagnostic uncertainty and error. Furthermore, RDT results within a community are kept isolated unless they are aggregated by healthcare workers, limiting the potential that RDTs can have in supporting public health efforts. In light of these issues, we present a system called RDTScan for detecting and interpreting lateral flow RDTs with a smartphone. RDTScan provides real-time guidance for clear RDT image capture and automatic interpretation for accurate diagnostic decisions. RDTScan is structured to be quickly configurable to new RDT designs by requiring only a template image and some metadata about how the RDT is supposed to be read, making it easier to extend than a data-driven approach. Through a controlled lab study, we demonstrate that RDTScan's limit-of-detection can match, and even exceed, the performance of expert readers who are interpreting the physical RDTs themselves. We then present two field evaluations of smartphone apps built on the RDTScan system: (1) at-home influenza testing in Australia and (2) malaria testing by community healthcare workers in Kenya. RDTScan achieved 97.5% and 96.3% accuracy compared to RDT interpretation by experts in the Australia Flu Study and the Kenya Malaria Study, respectively.},
	articleno    = 29,
	numpages     = 26,
	keywords     = {malaria, image processing, mobile health, rapid diagnostic tests (RDTs), image quality control, influenza}
}
@article{10.1145/3448104,
	title        = {SplitSR: An End-to-End Approach to Super-Resolution on Mobile Devices},
	author       = {Liu, Xin and Li, Yuang and Fromm, Josh and Wang, Yuntao and Jiang, Ziheng and Mariakakis, Alex and Patel, Shwetak},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448104},
	url          = {https://doi.org/10.1145/3448104},
	issue_date   = {March 2021},
	abstract     = {Super-resolution (SR) is a coveted image processing technique for mobile apps ranging from the basic camera apps to mobile health. Existing SR algorithms rely on deep learning models with significant memory requirements, so they have yet to be deployed on mobile devices and instead operate in the cloud to achieve feasible inference time. This shortcoming prevents existing SR methods from being used in applications that require near real-time latency. In this work, we demonstrate state-of-the-art latency and accuracy for on-device super-resolution using a novel hybrid architecture called SplitSR and a novel lightweight residual block called SplitSRBlock. The SplitSRBlock supports channel-splitting, allowing the residual blocks to retain spatial information while reducing the computation in the channel dimension. SplitSR has a hybrid design consisting of standard convolutional blocks and lightweight residual blocks, allowing people to tune SplitSR for their computational budget. We evaluate our system on a low-end ARM CPU, demonstrating both higher accuracy and up to 5\texttimes{} faster inference than previous approaches. We then deploy our model onto a smartphone in an app called ZoomSR to demonstrate the first-ever instance of on-device, deep learning-based SR. We conducted a user study with 15 participants to have them assess the perceived quality of images that were post-processed by SplitSR. Relative to bilinear interpolation --- the existing standard for on-device SR --- participants showed a statistically significant preference when looking at both images (Z=-9.270, p&lt;0.01) and text (Z=-6.486, p&lt;0.01).},
	articleno    = 25,
	numpages     = 20,
	keywords     = {edge computing, mobile computing, on-device machine learning, image super-resolution}
}
@article{10.1145/3448096,
	title        = {11 Years with Wearables: Quantitative Analysis of Social Media, Academia, News Agencies, and Lead User Community from 2009-2020 on Wearable Technologies},
	author       = {Gan, Yanglei and Wang, Tianyi and Javaheri, Alireza and Momeni-Ortner, Elaheh and Dehghani, Milad and Hosseinzadeh, Mehdi and Rawassizadeh, Reza},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448096},
	url          = {https://doi.org/10.1145/3448096},
	issue_date   = {March 2021},
	abstract     = {The role of wearable technology in our daily lives is rapidly growing and many users are cumulatively becoming dependent on it. To provide insight into the future of wearable technologies and various community attitudes towards them, we implemented an in-depth quantitative investigation of opinions from academic texts (DBLP and PubMed), social media (Twitter), news media (Google News and Bing News), and entrepreneurship communities (Kickstarter and Indiegogo) over a 10-year period. Our results indicate that unlike academia, the news media, entrepreneurship communities, and social media all hold overall positive attitudes towards wearable technologies. Secondly, there are diverse perspectives towards various wearable products across different platforms. Specifically, "XR" technologies received the most attention, while "Exoskeleton" ignited the most heated debates. Thirdly, we discovered that the lifetime of a hyped wearable technology lasts approximately three years. Furthermore, the news media and entrepreneurship community's attitudes towards wearable technologies did not have a strong impact on public opinion. Finally, among all types of wearable technologies, "fashion design" and "healthcare" products were the most enlightening for the market.},
	articleno    = 11,
	numpages     = 26,
	keywords     = {market analysis, text mining, sentiment analysis, wearable technology}
}
@article{10.1145/3432209,
	title        = {The Effect of Goal Moderation on the Achievement and Satisfaction of Physical Activity Goals},
	author       = {Alqahtani, Deemah and Jay, Caroline and Vigo, Markel},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432209},
	url          = {https://doi.org/10.1145/3432209},
	issue_date   = {December 2020},
	abstract     = {Many self-trackers lose interest in, disengage from and ultimately withdraw from tracking. Reasons for this include poor motivation, unmet expectations and difficulty in attaining daily goals. To support users in reflecting on their goals more realistically, we developed FitReflect, an app that moderates physical activity goals by factoring in users' confidence in achieving the goals. The app also encourages users to reflect on their goals regularly and think about the factors affecting their achievement. We conducted a 4-week field experiment where we trialled the app with fourteen Fitbit users. We found that, compared with a non-moderated goal condition, participants with moderated goals achieved their goals more often, got closer to them, and adjusted them more frequently. Crucially, they were also more satisfied with their physical activity. More frequent goal updates were key to align user goals with their confidence and capabilities in achieving them.},
	articleno    = 116,
	numpages     = 18,
	keywords     = {healthy adults, goal setting, physical activity, reflection, self-efficacy, self-tracking, wearable devices}
}
@article{10.1145/3432208,
	title        = {PMC: A Privacy-Preserving Deep Learning Model Customization Framework for Edge Computing},
	author       = {Liu, Bingyan and Li, Yuanchun and Liu, Yunxin and Guo, Yao and Chen, Xiangqun},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432208},
	url          = {https://doi.org/10.1145/3432208},
	issue_date   = {December 2020},
	abstract     = {Deep learning models have been deployed to a wide range of edge devices. Since the data distribution on edge devices may differ from the cloud where the model was trained, it is typically desirable to customize the model for each edge device to improve accuracy. However, such customization is hard because collecting data from edge devices is usually prohibited due to privacy concerns. In this paper, we propose PMC, a privacy-preserving model customization framework to effectively customize a CNN model from the cloud to edge devices without collecting raw data. Instead, we introduce a method to extract statistical information from the edge, which contains adequate domain-related knowledge for model customization. PMC uses Gaussian distribution parameters to describe the edge data distribution, reweights the cloud data based on the parameters, and uses the reweighted data to train a specialized model for the edge device. During this process, differential privacy can be enforced by adding computed noises to the Gaussian parameters. Experiments on public datasets show that PMC can improve model accuracy by a large margin through customization. Finally, a study on user-generated data demonstrates the effectiveness of PMC in real-world settings.},
	articleno    = 139,
	numpages     = 25,
	keywords     = {edge computing, model compression, differential privacy, neural networks, domain adaptation}
}
@article{10.1145/3432190,
	title        = {Prompto: Investigating Receptivity to Prompts Based on Cognitive Load from Memory Training Conversational Agent},
	author       = {Chan, Samantha W. T. and Sapkota, Shardul and Mathews, Rebecca and Zhang, Haimo and Nanayakkara, Suranga},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432190},
	url          = {https://doi.org/10.1145/3432190},
	issue_date   = {December 2020},
	abstract     = {Prospective memory lapses, which involve forgetting to perform intended actions, affect independent living in older adults. Although memory training using smartphone applications could address them, users are sometimes unaware of available times for training or forget about it, presenting a need for proactive prompts. Existing applications mostly provide time-based prompts and prompts based on users' cognitive contexts remain an under-explored area. We developed Prompto, a conversational memory coach that detects physiological signals to suggest training sessions when users are relaxed and potentially more receptive. Our study with 21 older adults showed that users were more receptive to prompts and memory training under low cognitive load than under high cognitive load. Interviews and an in-the-wild deployment of Prompto indicated that majority of users appreciated the concept, found it helpful and were likely to respond to its prompts. We contribute towards developing technologies with cognitive context-aware prompting based on users' physiological readings.},
	articleno    = 121,
	numpages     = 23,
	keywords     = {physiological sensing, cognitive load, context-aware notifications, conversational agent, memory, receptivity, older adults}
}
@article{10.1145/3369830,
	title        = {Decentralized Attention-Based Personalized Human Mobility Prediction},
	author       = {Fan, Zipei and Song, Xuan and Jiang, Renhe and Chen, Quanjun and Shibasaki, Ryosuke},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369830},
	url          = {https://doi.org/10.1145/3369830},
	issue_date   = {December 2019},
	abstract     = {Human mobility prediction is essential to a variety of human-centered computing applications achieved through upgrading of location-based services (LBS) to future-location-based services (FLBS). Previous studies on human mobility prediction have mainly focused on centralized human mobility prediction, where user mobility data are collected, trained and predicted at the cloud server side. However, such a centralized approach leads to a high risk of privacy issues, and a real-time centralized system for processing such a large volume of distributed data is extremely difficult to apply. Moreover, a large and dynamic set of users makes the predictive model extremely challenging to personalize. In this paper, we propose a novel decentralized attention-based human mobility predictor in which 1) no additional training procedure is required for personalized prediction, 2) no additional training procedure is required for incremental learning, and 3) the predictor can be trained and predicted in a decentralized way. We tested our method on big data of real-world mobile phone user GPS and on Android devices, and achieved a low-power consumption and a good prediction accuracy without collecting user data in the server or applying additional training on the user side.},
	articleno    = 133,
	numpages     = 26,
	keywords     = {neural networks, information retrieval, human mobility prediction}
}
@article{10.1145/3369820,
	title        = {Machine Learning for Phone-Based Relationship Estimation: The Need to Consider Population Heterogeneity},
	author       = {Liu, Tony and Nicholas, Jennifer and Theilig, Max M. and Guntuku, Sharath C. and Kording, Konrad and Mohr, David C. and Ungar, Lyle},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369820},
	url          = {https://doi.org/10.1145/3369820},
	issue_date   = {December 2019},
	abstract     = {Estimating the category and quality of interpersonal relationships from ubiquitous phone sensor data matters for studying mental well-being and social support. Prior work focused on using communication volume to estimate broad relationship categories, often with small samples. Here we contextualize communications by combining phone logs with demographic and location data to predict interpersonal relationship roles on a varied sample population using automated machine learning methods, producing better performance (F1 = 0.68) than using communication features alone (F1 = 0.62). We also explore the effect of age variation in the underlying training sample on interpersonal relationship prediction and find that models trained on younger subgroups, which is popular in the field via student participation and recruitment, generalize poorly to the wider population. Our results not only illustrate the value of using data across demographics, communication patterns and semantic locations for relationship prediction, but also underscore the importance of considering population heterogeneity in phone-based personal sensing studies.},
	articleno    = 145,
	numpages     = 23,
	keywords     = {population heterogeneity, semantic location-based features, social relationship prediction, automated machine learning}
}
@article{10.1145/3369816,
	title        = {Privacy Adversarial Network: Representation Learning for Mobile Data Privacy},
	author       = {Liu, Sicong and Du, Junzhao and Shrivastava, Anshumali and Zhong, Lin},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369816},
	url          = {https://doi.org/10.1145/3369816},
	issue_date   = {December 2019},
	abstract     = {The remarkable success of machine learning has fostered a growing number of cloud-based intelligent services for mobile users. Such a service requires a user to send data, e.g. image, voice and video, to the provider, which presents a serious challenge to user privacy. To address this, prior works either obfuscate the data, e.g. add noise and remove identity information, or send representations extracted from the data, e.g. anonymized features. They struggle to balance between the service utility and data privacy because obfuscated data reduces utility and extracted representation may still reveal sensitive information.This work departs from prior works in methodology: we leverage adversarial learning to better balance between privacy and utility. We design a representation encoder that generates the feature representations to optimize against the privacy disclosure risk of sensitive information (a measure of privacy) by the privacy adversaries, and concurrently optimize with the task inference accuracy (a measure of utility) by the utility discriminator. The result is the privacy adversarial network (PAN), a novel deep model with the new training algorithm, that can automatically learn representations from the raw data. And the trained encoder can be deployed on the user side to generate representations that satisfy the task-defined utility requirements and the user-specified/agnostic privacy budgets.Intuitively, PAN adversarially forces the extracted representations to only convey information required by the target task. Surprisingly, this constitutes an implicit regularization that actually improves task accuracy. As a result, PAN achieves better utility and better privacy at the same time! We report extensive experiments on six popular datasets, and demonstrate the superiority of PAN compared with alternative methods reported in prior work.},
	articleno    = 144,
	numpages     = 18
}
@article{10.1145/3214291,
	title        = {Watching the TV Watchers},
	author       = {Zhang, Yun C. and Rehg, James M.},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214291},
	url          = {https://doi.org/10.1145/3214291},
	issue_date   = {June 2018},
	abstract     = {Studies have linked excessive TV watching to obesity in adults and children. In addition, TV content represents an important source of visual exposure to cues which can effect a broad set of health-related behaviors. This paper presents a ubiquitous sensing system which can detect moments of screen-watching during daily life activities. We utilize machine learning techniques to analyze video captured by a head-mounted wearable camera. Although wearable cameras do not directly provide a measure of visual attention, we show that attention to screens can be reliably inferred by detecting and tracking the location of screens within the camera's field-of-view. We utilize a computational model of the head movements associated with TV watching to identify TV watching events. We have evaluated our method on 13 hours of TV watching videos recorded from 16 participants in a home environment. Our model achieves a precision of 0.917 and a recall of 0.945 in identifying attention to screens. We validated the third-person annotations used to determine accuracy and further evaluated our system in a multi-device environment using gold standard attention measurements obtained from a wearable eye-tracker. Finally, we tested our system in a natural environment. Our system achieves a precision of 0.87 and a recall of 0.82 on challenging videos capturing the daily life activities of participants.},
	articleno    = 88,
	numpages     = 27,
	keywords     = {Wearable eyetracker, Obesity, Mobile health, Wearable camera, Visual attention, mHealth, Eye gaze, Computer vision, Television watching, Behavioral medicine, Sedentary behavior, Screen time, Ambulatory assessment}
}
@article{10.1145/3214272,
	title        = {Assisted Medication Management in Elderly Care Using Miniaturised Near-Infrared Spectroscopy},
	author       = {Klakegg, Simon and Goncalves, Jorge and Luo, Chu and Visuri, Aku and Popov, Alexey and van Berkel, Niels and Sarsenbayeva, Zhanna and Kostakos, Vassilis and Hosio, Simo and Savage, Scott and Bykov, Alexander and Meglinski, Igor and Ferreira, Denzil},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214272},
	url          = {https://doi.org/10.1145/3214272},
	issue_date   = {June 2018},
	abstract     = {Near-infrared spectroscopy (NIRS) measures the light reflected from objects to infer highly detailed information about their molecular composition. Traditionally, NIRS has been an instrument reserved for laboratory usage, but recently affordable and smaller devices for NIRS have proliferated. Pairing this technology with the ubiquitous smartphone opens up a plethora of new use cases. In this paper, we explore one such use case, namely medication management in a nursing home/elderly care centre. First, we conducted a qualitative user study with nurses working in an elderly care centre to examine the protocols and workflows involved in administering medication, and the nurses' perceptions on using this technology. Based on our findings, we identify the main impact areas that would benefit from introducing miniaturised NIRS. Finally, we demonstrate via a user study in a realistic scenario that miniaturised NIRS can be effectively used for medication management when leveraging appropriate machine learning techniques. Specifically, we assess the performance of multiple pre-processing and classification algorithms for a selected set of pharmaceuticals. In addition, we compare our solution with currently used methods for pharmaceutical identification in a local care centre. We hope that our reflection on the multiple aspects associated with the introduction of this device in an elderly care setting can help both academics and practitioners working on related problems.},
	articleno    = 69,
	numpages     = 24,
	keywords     = {user study, elderly care, machine learning, Near-infrared spectroscopy, preprocessing, medication management}
}
@article{10.1145/3214267,
	title        = {MuscleIO: Muscle-Based Input and Output for Casual Notifications},
	author       = {Duente, Tim and Schulte, Justin and Pfeiffer, Max and Rohs, Michael},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214267},
	url          = {https://doi.org/10.1145/3214267},
	issue_date   = {June 2018},
	abstract     = {Receiving and reacting to notifications on mobile devices can be cumbersome. We propose MuscleIO, the use of electrical muscle stimulation (EMS) for notification output and electromyography (EMG) for reacting to notifications. Our approach provides a one-handed, eyes-free, and low-effort way of dealing with notifications. We built a prototype that interleaves muscle input and muscle output signals using the same electrodes. EMS and EMG alternate such that the EMG input signal is measured in the gaps of the EMS output signal, so voluntary muscle contraction is measured during muscle stimulation.Notifications are represented as EMS signals and are accepted or refused either by a directional or a time-based EMG response. A lab user study with 12 participants shows that the directional EMG response is superior to the time-based response in terms of reaction time, error rate, and user preference. Furthermore, the directional approach is the fastest and the most intuitive for users compared to a button-based smartwatch interface as a baseline.},
	articleno    = 64,
	numpages     = 21,
	keywords     = {muscle input, electromyography, Notifications, wearable, EMG, muscle output, electrical muscle stimulation, EMS}
}
@article{10.1145/3214263,
	title        = {SATURN: A Thin and Flexible Self-Powered Microphone Leveraging Triboelectric Nanogenerator},
	author       = {Arora, Nivedita and Zhang, Steven L. and Shahmiri, Fereshteh and Osorio, Diego and Wang, Yi-Cheng and Gupta, Mohit and Wang, Zhengjun and Starner, Thad and Wang, Zhong Lin and Abowd, Gregory D.},
	year         = 2018,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 2,
	doi          = {10.1145/3214263},
	url          = {https://doi.org/10.1145/3214263},
	issue_date   = {June 2018},
	abstract     = {We demonstrate the design, fabrication, evaluation, and use of a self-powered microphone that is thin, flexible, and easily manufactured. Our technology is referred to as a Self-powered Audio Triboelectric Ultra-thin Rollable Nanogenerator (SATURN) microphone. This acoustic sensor takes advantage of the triboelectric nanogenerator (TENG) to transform vibrations into an electric signal without applying an external power source. The sound quality of the SATURN mic, in terms of acoustic sensitivity, frequency response, and directivity, is affected by a set of design parameters that we explore based on both theoretical simulation and empirical evaluation. The major advantage of this audio material sensor is that it can be manufactured simply and deployed easily to convert every-day objects and physical surfaces into microphones which can sense audio. We explore the space of potential applications for such a material as part of a self-sustainable interactive system.},
	articleno    = 60,
	numpages     = 28,
	keywords     = {flexible electronics, TENG (Triboelectric Nanogenerator), applications, Triboelectic effect, passive microphone}
}
@article{10.1145/3191779,
	title        = {Employing Opportunistic Charging for Electric Taxicabs to Reduce Idle Time},
	author       = {Yan, Li and Shen, Haiying and Li, Zhuozhao and Sarker, Ankur and Stankovic, John A. and Qiu, Chenxi and Zhao, Juanjuan and Xu, Chengzhong},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191779},
	url          = {https://doi.org/10.1145/3191779},
	issue_date   = {March 2018},
	abstract     = {For electric taxicabs, the idle time spent on cruising for passengers, seeking chargers, and charging is wasteful. Previous works can only save cruising time through better routing, or charger seeking and charging time through proper charger deployment, but not for both. With the advancement of wireless charging techniques, efficient opportunistic charging of electric vehicles at their parked positions becomes possible. This enables a taxicab to get charged while waiting for the next passenger. In this paper, we present an opportunistic wireless charger deployment scheme in a city, which both maximizes the taxicabs' opportunity of picking up passengers at the chargers and supports the taxicabs' continuous operability on roads, while minimizing the total deployment cost. We studied a metropolitan-scale taxicab dataset on several factors important for deploying wireless chargers and determining the numbers of the chargers in the regions: the number of passengers, the functionalities of buildings, and the frequency of passenger appearance in a region, and taxicab traffic flows in a city. Then, we formulate a multi-objective optimization problem and find the solution. Our trace-driven experiments demonstrate the superior performance of our scheme over other representative methods in terms of reducing idle time and supporting the operability of the taxicabs.},
	articleno    = 47,
	numpages     = 25,
	keywords     = {mobile data analysis, Vehicle wireless charging, charger deployment, kernel density estimation}
}
@article{10.1145/3191771,
	title        = {Smartwatch-Based Early Gesture Detection 8 Trajectory Tracking for Interactive Gesture-Driven Applications},
	author       = {Vu, Tran Huy and Misra, Archan and Roy, Quentin and Wei, Kenny Choo Tsu and Lee, Youngki},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191771},
	url          = {https://doi.org/10.1145/3191771},
	issue_date   = {March 2018},
	abstract     = {The paper explores the possibility of using wrist-worn devices (specifically, a smartwatch) to accurately track the hand movement and gestures for a new class of immersive, interactive gesture-driven applications. These interactive applications need two special features: (a) the ability to identify gestures from a continuous stream of sensor data early--i.e., even before the gesture is complete, and (b) the ability to precisely track the hand's trajectory, even though the underlying inertial sensor data is noisy. We develop a new approach that tackles these requirements by first building a HMM-based gesture recognition framework that does not need an explicit segmentation step, and then using a per-gesture trajectory tracking solution that tracks the hand movement only during these predefined gestures. Using an elaborate setup that allows us to realistically study the table-tennis related hand movements of users, we show that our approach works: (a) it can achieve 95% stroke recognition accuracy. Within 50% of gesture, it can achieve a recall value of 92% for 10 novice users and 93% for 15 experienced users from a continuous sensor stream; (b) it can track hand movement during such strokeplay with a median accuracy of 6.2 cm.},
	articleno    = 39,
	numpages     = 27,
	keywords     = {hand tracking, gesture recognition, wearable devices, low-latency, VR, immersive applications}
}
@article{10.1145/3191770,
	title        = {Takes Tutu to Ballet: Designing Visual and Verbal Feedback for Augmented Mirrors},
	author       = {Trajkova, Milka and Cafaro, Francesco},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191770},
	url          = {https://doi.org/10.1145/3191770},
	issue_date   = {March 2018},
	abstract     = {Mirrors have been a core feature in ballet studios for over five hundred years. While physical mirrors provide real-time feedback, they do not inform dancers of their errors. Thus, technologies such as motion tracking have been used to augment what a physical mirror can provide. Current augmented mirrors, however, only implement one mode of communication, usually visual, and do not provide a holistic feedback to dancers that includes all the feedback elements commonly used in ballet classes. We conducted a mixed-method study with 16 novices and 16 expert dancers in which we compared two different modes of communication (visual and verbal), two different types of feedback (value and corrective) and two levels of guidance (mirror, or no mirror). Participants' ballet technique scores were evaluated by a remote teacher on eight ballet combinations (tendue, adagio, pirouette, saute, pli\'{e}, degage, frappe and battement tendue). We report quantitative and qualitative results that show how the level of guidance, mode of communication, and type of feedback, needs to be tuned in different ways for novices and experts.},
	articleno    = 38,
	numpages     = 30,
	keywords     = {feedback, Augmented feedback, dance, dance education, augmented mirror, ballet, Kinect, verbal feedback, visual feedback, mirror, motion tracking, design}
}
@article{10.1145/3191760,
	title        = {Just Breathe: In-Car Interventions for Guided Slow Breathing},
	author       = {Paredes, Pablo E. and Zhou, Yijun and Hamdan, Nur Al-Huda and Balters, Stephanie and Murnane, Elizabeth and Ju, Wendy and Landay, James A.},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191760},
	url          = {https://doi.org/10.1145/3191760},
	issue_date   = {March 2018},
	abstract     = {Motivated by the idea that slow breathing practices could transform the automobile commute from a depleting, mindless activity into a calming, mindful experience, we introduce the first guided slow breathing intervention for drivers. We describe a controlled in-lab experiment (N=24) that contrasts the effectiveness and impact of haptic and voice guidance modalities at slowing drivers' breathing pace, which is a known modulator of stress. The experiment was conducted in two simulated driving environments (city, highway) while driving in one of two driving modes (autonomous, manual). Results show that both haptic and voice guidance systems can reduce drivers' breathing rate and provide a sustained post-intervention effect without affecting driving safety. Subjectively, most participants (19/24) preferred the haptic stimuli as they found it more natural to follow, less distracting, and easier to engage and disengage from, compared to the voice stimuli. Finally, while most participants found guided breathing to be a positive experience, a few participants in the autonomous driving condition found slow breathing to be an unusual activity inside the car. In this paper, we discuss such considerations, offer guidelines for designing in-car breathing interventions, and propose future research that extends our work to on-road studies. Altogether, this paper serves as foundational work on guided breathing interventions for automobile drivers.},
	articleno    = 28,
	numpages     = 23,
	keywords     = {Autonomous automobiles, Stress Management, Interventions, Mindfulness, Breathing, Commute, Health, Deep Breathing}
}
@article{10.1145/3191759,
	title        = {Xnavi: Travel Planning System Based on Experience Flows},
	author       = {Nomiyama, Masato and Takeuchi, Toshiki and Onimaru, Hiroyuki and Tanikawa, Tomohiro and Narumi, Takuji and Hirose, Michitaka},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191759},
	url          = {https://doi.org/10.1145/3191759},
	issue_date   = {March 2018},
	abstract     = {Though an increasing number of people is now involved in travel planning owing to the spread of the internet, it is still difficult for travelers to plan trips on their own. It is especially difficult for tourists using automobiles because they have several choices of accessible places. To make itineraries easily, travelers require a travel planning system that suggests two types of experiences: experiences characterizing the travel area and experiences stemming from a flow between the former experiences. Existing systems do not list specific spontaneous experiences of interest to travelers. In response, Xnavi, a travel planning system for drivers based on experience flows, is proposed, which provides these types of experiences. To recommend experience flows, Xnavi extracts experience keywords related to the travel area using natural language processing based on the TF-IDF method and also extracts flows of tourist attractions' attributes based on association analysis of driving histories. Trials of the proposed method and a user study were conducted. The results show that Xnavi is effective at suggesting experiences and satisfying tourists with their plans.},
	articleno    = 27,
	numpages     = 25,
	keywords     = {experience flows, automobile, association analysis, driving histories, UI, Travel planning}
}
@article{10.1145/3191756,
	title        = {W-Air: Enabling Personal Air Pollution Monitoring on Wearables},
	author       = {Maag, Balz and Zhou, Zimu and Thiele, Lothar},
	year         = 2018,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 2,
	number       = 1,
	doi          = {10.1145/3191756},
	url          = {https://doi.org/10.1145/3191756},
	issue_date   = {March 2018},
	abstract     = {Accurate, portable and personal air pollution sensing devices enable quantification of individual exposure to air pollution, personalized health advice and assistance applications. Wearables are promising (e.g., on wristbands, attached to belts or backpacks) to integrate commercial off-the-shelf gas sensors for personal air pollution sensing. Yet previous research lacks comprehensive investigations on the accuracies of air pollution sensing on wearables. In response, we proposed W-Air, an accurate personal multi-pollutant monitoring platform for wearables. We discovered that human emissions introduce non-linear interference when low-cost gas sensors are integrated into wearables, which is overlooked in existing studies. W-Air adopts a sensor-fusion calibration scheme to recover high-fidelity ambient pollutant concentrations from the human interference. It also leverages a neural network with shared hidden layers to boost calibration parameter training with fewer measurements and utilizes semi-supervised regression for calibration parameter updating with little user intervention. We prototyped W-Air on a wristband with low-cost gas sensors. Evaluations demonstrated that W-Air reports accurate measurements both with and without human interference and is able to automatically learn and adapt to new environments.},
	articleno    = 24,
	numpages     = 25,
	keywords     = {Wearables, Sensor Array, Air Pollution, Calibration}
}
@article{10.1145/3534623,
	title        = {Understanding Privacy Risks and Perceived Benefits in Open Dataset Collection for Mobile Affective Computing},
	author       = {Lee, Hyunsoo and Kang, Soowon and Lee, Uichin},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534623},
	url          = {https://doi.org/10.1145/3534623},
	issue_date   = {July 2022},
	abstract     = {Collecting large-scale mobile and wearable sensor datasets from daily contexts is essential in developing machine learning models for enabling everyday affective computing applications. However, there is a lack of knowledge on data contributors' perceived benefits and risks in participating in open dataset collection projects. To bridge this gap, we conducted an in-situ study on building an open dataset with mobile and wearable devices for affective computing research (N = 100, 4 weeks). Our study results showed that a mixture of financial and altruistic benefits was important in eliciting data contribution. Sensor-specific risks were largely associated with the revelation of personal traits and social behaviors. However, most of the participants were less concerned with open dataset collection and their perceived sensitivity of each sensor data did not change over time. We further discuss alternative approaches to promote data contributors' motivations and suggest design guidelines to alleviate potential privacy concerns in mobile open dataset collection.},
	articleno    = 61,
	numpages     = 26,
	keywords     = {Affective Computing, Risk-Benefit Assessment, Mobile and Wearable Computing, Open Dataset, Privacy}
}
@article{10.1145/3534621,
	title        = {EarIO: A Low-Power Acoustic Sensing Earable for Continuously Tracking Detailed Facial Movements},
	author       = {Li, Ke and Zhang, Ruidong and Liang, Bo and Guimbreti\`{e}re, Fran\c{c}ois and Zhang, Cheng},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534621},
	url          = {https://doi.org/10.1145/3534621},
	issue_date   = {July 2022},
	abstract     = {This paper presents EarIO, an AI-powered acoustic sensing technology that allows an earable (e.g., earphone) to continuously track facial expressions using two pairs of microphone and speaker (one on each side), which are widely available in commodity earphones. It emits acoustic signals from a speaker on an earable towards the face. Depending on facial expressions, the muscles, tissues, and skin around the ear would deform differently, resulting in unique echo profiles in the reflected signals captured by an on-device microphone. These received acoustic signals are processed and learned by a customized deep learning pipeline to continuously infer the full facial expressions represented by 52 parameters captured using a TruthDepth camera. Compared to similar technologies, it has significantly lower power consumption, as it can sample at 86 Hz with a power signature of 154 mW. A user study with 16 participants under three different scenarios, showed that EarIO can reliably estimate the detailed facial movements when the participants were sitting, walking or after remounting the device. Based on the encouraging results, we further discuss the potential opportunities and challenges on applying EarIO on future ear-mounted wearables.},
	articleno    = 62,
	numpages     = 24,
	keywords     = {Facial expression reconstruction, Deep learning, Acoustic sensing, Low-power}
}
@article{10.1145/3534619,
	title        = {MobiVQA: Efficient On-Device Visual Question Answering},
	author       = {Cao, Qingqing and Khanna, Prerna and Lane, Nicholas D. and Balasubramanian, Aruna},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534619},
	url          = {https://doi.org/10.1145/3534619},
	issue_date   = {July 2022},
	abstract     = {Visual Question Answering (VQA) is a relatively new task where a user can ask a natural question about an image and obtain an answer. VQA is useful for many applications and is widely popular for users with visual impairments. Our goal is to design a VQA application that works efficiently on mobile devices without requiring cloud support. Such a system will allow users to ask visual questions privately, without having to send their questions to the cloud, while also reduce cloud communication costs. However, existing VQA applications use deep learning models that significantly improve accuracy, but is computationally heavy. Unfortunately, existing techniques that optimize deep learning for mobile devices cannot be applied for VQA because the VQA task is multi-modal---it requires both processing vision and text data. Existing mobile optimizations that work for vision-only or text-only neural networks cannot be applied here because of the dependencies between the two modes. Instead, we design MobiVQA, a set of optimizations that leverage the multi-modal nature of VQA. We show using extensive evaluation on two VQA testbeds and two mobile platforms, that MobiVQA significantly improves latency and energy with minimal accuracy loss compared to state-of-the-art VQA models. For instance, MobiVQA can answer a visual question in 163 milliseconds on the phone, compared to over 20-second latency incurred by the most accurate state-of-the-art model, while incurring less than 1 point reduction in accuracy.},
	articleno    = 44,
	numpages     = 23,
	keywords     = {edge computing, visual question answering, on-device applications, mobile computing}
}
@article{10.1145/3534612,
	title        = {Ask the Users: A Case Study of Leveraging User-Centered Design for Designing Just-in-Time Adaptive Interventions (JITAIs)},
	author       = {Kabir, Kazi Sinthia and Kenfield, Stacey A. and Van Blarigan, Erin L. and Chan, June M. and Wiese, Jason},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534612},
	url          = {https://doi.org/10.1145/3534612},
	issue_date   = {July 2022},
	abstract     = {Just-in-Time Adaptive Interventions (JITAIs) are envisioned to harness rich data on users' contexts. However, many JITAIs fall short in leveraging the value of the data while sculpting the interventions. Investigating the literature reveals a lack of user-centered design (UCD) methods in designing JITAIs. Our case study of applying a UCD process revealed that even without deploying a JITAI, UCD could uncover user interactions that inform critical design decisions of a JITAI's components. We reflect on our experiences engaging in a user-centered JITAI design process and urge the broader human-computer interaction (HCI) community to devise concrete design guidelines for JITAIs. We take the first step toward that goal by proposing a checklist of key design considerations for future JITAIs. Together, this case study contributes insights on applying UCD to the design of all components of a JITAI, which can help capture users' needs and provide actionable interventions.},
	articleno    = 59,
	numpages     = 21,
	keywords     = {episodic tracking, Health behavior change, JITAI, user-centered design, intermittent tracking, just-in-time adaptive interventions, self-tracking, retrospective tracking}
}
@article{10.1145/3534600,
	title        = {M3Sense: Affect-Agnostic Multitask Representation Learning Using Multimodal Wearable Sensors},
	author       = {Samyoun, Sirat and Islam, Md Mofijul and Iqbal, Tariq and Stankovic, John},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534600},
	url          = {https://doi.org/10.1145/3534600},
	issue_date   = {July 2022},
	abstract     = {Modern smartwatches or wrist wearables having multiple physiological sensing modalities have emerged as a subtle way to detect different mental health conditions, such as anxiety, emotions, and stress. However, affect detection models depending on wrist sensors data often provide poor performance due to inconsistent or inaccurate signals and scarcity of labeled data representing a condition. Although learning representations based on the physiological similarities of the affective tasks offer a possibility to solve this problem, existing approaches fail to effectively generate representations that will work across these multiple tasks. Moreover, the problem becomes more challenging due to the large domain gap among these affective applications and the discrepancies among the multiple sensing modalities. We present M3Sense, a multi-task, multimodal representation learning framework that effectively learns the affect-agnostic physiological representations from limited labeled data and uses a novel domain alignment technique to utilize the unlabeled data from the other affective tasks to accurately detect these mental health conditions using wrist sensors only. We apply M3Sense to 3 mental health applications, and quantify the achieved performance boost compared to the state-of-the-art using extensive evaluations and ablation studies on publicly available and collected datasets. Moreover, we extensively investigate what combination of tasks and modalities aids in developing a robust Multitask Learning model for affect recognition. Our analysis shows that incorporating emotion detection in the learning models degrades the performance of anxiety and stress detection, whereas stress detection helps to boost the emotion detection performance. Our results also show that M3Sense provides consistent performance across all affective tasks and available modalities and also improves the performance of representation learning models on unseen affective tasks by 5% - 60%.},
	articleno    = 73,
	numpages     = 32,
	keywords     = {Wearable Sensors, Domain Adaptation, Representation Learning, Mental Health, Affect Recognition, Multimodal Learning, Multitask Learning, Health Informatics}
}
@article{10.1145/3534598,
	title        = {Reliable Digital Forensics in the Air: Exploring an RF-Based Drone Identification System},
	author       = {Li, Zhengxiong and Chen, Baicheng and Chen, Xingyu and Xu, Chenhan and Chen, Yuyang and Lin, Feng and Li, Changzhi and Dantu, Karthik and Ren, Kui and Xu, Wenyao},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534598},
	url          = {https://doi.org/10.1145/3534598},
	issue_date   = {July 2022},
	abstract     = {As the drone becomes widespread in numerous crucial applications with many powerful functionalities (e.g., reconnaissance and mechanical trigger), there are increasing cases related to misused drones for unethical even criminal activities. Therefore, it is of paramount importance to identify these malicious drones and track their origins using digital forensics. Traditional drone identification techniques for forensics (e.g., RF communication, ID landmarks using a camera, etc.) require high compliance of drones. However, malicious drones will not cooperate or even spoof these identification techniques. Therefore, we present an exploration for a reliable and passive identification approach based on unique hardware traits in drones directly (e.g., analogous to the fingerprint and iris in humans) for forensics purposes. Specifically, we investigate and model the behavior of the parasitic electronic elements under RF interrogation, a particular passive parasitic response modulated by an electronic system on drones, which is distinctive and unlikely to counterfeit. Based on this theory, we design and implement DroneTrace, an end-to-end reliable and passive identification system toward digital drone forensics. DroneTrace comprises a cost-effective millimeter-wave (mmWave) probe, a software framework to extract and process parasitic responses, and a customized deep neural network (DNN)-based algorithm to analyze and identify drones. We evaluate the performance of DroneTrace with 36 commodity drones. Results show that DroneTrace can identify drones with the accuracy of over 99% and an equal error rate (EER) of 0.009, under a 0.1-second sensing time budget. Moreover, we test the reliability, robustness, and performance variation under a set of real-world circumstances, where DroneTrace maintains accuracy of over 98%. DroneTrace is resilient to various attacks and maintains functionality. At its best, DroneTrace has the capacity to identify individual drones at the scale of 104 with less than 5% error.},
	articleno    = 63,
	numpages     = 25,
	keywords     = {Drone, Digital Forensics, Identification System}
}
@article{10.1145/3534594,
	title        = {TinyOdom: Hardware-Aware Efficient Neural Inertial Navigation},
	author       = {Saha, Swapnil Sayan and Sandha, Sandeep Singh and Garcia, Luis Antonio and Srivastava, Mani},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534594},
	url          = {https://doi.org/10.1145/3534594},
	issue_date   = {July 2022},
	abstract     = {Deep inertial sequence learning has shown promising odometric resolution over model-based approaches for trajectory estimation in GPS-denied environments. However, existing neural inertial dead-reckoning frameworks are not suitable for real-time deployment on ultra-resource-constrained (URC) devices due to substantial memory, power, and compute bounds. Current deep inertial odometry techniques also suffer from gravity pollution, high-frequency inertial disturbances, varying sensor orientation, heading rate singularity, and failure in altitude estimation. In this paper, we introduce TinyOdom, a framework for training and deploying neural inertial models on URC hardware. TinyOdom exploits hardware and quantization-aware Bayesian neural architecture search (NAS) and a temporal convolutional network (TCN) backbone to train lightweight models targetted towards URC devices. In addition, we propose a magnetometer, physics, and velocity-centric sequence learning formulation robust to preceding inertial perturbations. We also expand 2D sequence learning to 3D using a model-free barometric g-h filter robust to inertial and environmental variations. We evaluate TinyOdom for a wide spectrum of inertial odometry applications and target hardware against competing methods. Specifically, we consider four applications: pedestrian, animal, aerial, and underwater vehicle dead-reckoning. Across different applications, TinyOdom reduces the size of neural inertial models by 31\texttimes{} to 134\texttimes{} with 2.5m to 12m error in 60 seconds, enabling the direct deployment of models on URC devices while still maintaining or exceeding the localization resolution over the state-of-the-art. The proposed barometric filter tracks altitude within ±0.1m and is robust to inertial disturbances and ambient dynamics. Finally, our ablation study shows that the introduced magnetometer, physics, and velocity-centric sequence learning formulation significantly improve localization performance even with notably lightweight models.},
	articleno    = 71,
	numpages     = 32,
	keywords     = {machine-learning, neural architecture search, deep-learning, sequence-learning, dead-reckoning, resource-constrained devices, hardware-in-the-loop, tracking, inertial odometry}
}
@article{10.1145/3534591,
	title        = {Multi-Task Learning for Randomized Controlled Trials: A Case Study on Predicting Depression with Wearable Data},
	author       = {Dai, Ruixuan and Kannampallil, Thomas and Zhang, Jingwen and Lv, Nan and Ma, Jun and Lu, Chenyang},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534591},
	url          = {https://doi.org/10.1145/3534591},
	issue_date   = {July 2022},
	abstract     = {A randomized controlled trial (RCT) is used to study the safety and efficacy of new treatments, by comparing patient outcomes of an intervention group with a control group. Traditionally, RCTs rely on statistical analyses to assess the differences between the treatment and control groups. However, such statistical analyses are generally not designed to assess the impact of the intervention at an individual level. In this paper, we explore machine learning models in conjunction with an RCT for personalized predictions of a depression treatment intervention, where patients were longitudinally monitored with wearable devices. We formulate individual-level predictions in the intervention and control groups from an RCT as a multi-task learning (MTL) problem, and propose a novel MTL model specifically designed for RCTs. Instead of training separate models for the intervention and control groups, the proposed MTL model is trained on both groups, effectively enlarging the training dataset. We develop a hierarchical model architecture to aggregate data from different sources and different longitudinal stages of the trial, which allows the MTL model to exploit the commonalities and capture the differences between the two groups. We evaluated the MTL approach in an RCT involving 106 patients with depression, who were randomized to receive an integrated intervention treatment. Our proposed MTL model outperforms both single-task models and the traditional multi-task model in predictive performance, representing a promising step in utilizing data collected in RCTs to develop predictive models for precision medicine.},
	articleno    = 50,
	numpages     = 23,
	keywords     = {Multi-task Learning, Randomized Controlled Trial, Wearables, Depression}
}
@article{10.1145/3534590,
	title        = {Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention},
	author       = {Li, Zhipeng and Jiang, Yu and Zhu, Yihao and Chen, Ruijia and Wang, Ruolin and Wang, Yuntao and Yan, Yukang and Shi, Yuanchun},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534590},
	url          = {https://doi.org/10.1145/3534590},
	issue_date   = {July 2022},
	abstract     = {An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback [45] or input augmentation [37, 59], at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement.},
	articleno    = 64,
	numpages     = 26,
	keywords     = {Virtual Reality, visual illusion, user behavior modelling}
}
@article{10.1145/3534572,
	title        = {Acceleration-Based Activity Recognition of Repetitive Works with Lightweight Ordered-Work Segmentation Network},
	author       = {Yoshimura, Naoya and Maekawa, Takuya and Hara, Takahiro and Wada, Atsushi},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534572},
	url          = {https://doi.org/10.1145/3534572},
	issue_date   = {July 2022},
	abstract     = {This study presents a new neural network model for recognizing manual works using body-worn accelerometers in industrial settings, named Lightweight Ordered-work Segmentation Network (LOS-Net). In industrial domains, a human worker typically repetitively performs a set of predefined processes, with each process consisting of a sequence of activities in a predefined order. State-of-the-art activity recognition models, such as encoder-decoder models, have numerous trainable parameters, making their training difficult in industrial domains because of the consequent substantial cost for preparing a large amount of labeled data. In contrast, the LOS-Net is designed to be trained on a limited amount of training data. Specifically, the decoder in the LOS-Net has few trainable parameters and is designed to capture only the necessary information for precise recognition of ordered works. These are (i) the boundary information between consecutive activities, because a transition in the performed activities is generally associated with the trend change of the sensor data collected during the manual works and (ii) long-term context regarding the ordered works, e.g., information about the previous and next activity, which is useful for recognizing the current activity. This information is obtained by introducing a module that can collect it at distant time steps using few trainable parameters. Moreover, the LOS-Net can refine the activity estimation by the decoder by incorporating prior knowledge regarding the order of activities. We demonstrate the effectiveness of the LOS-Net using sensor data collected from workers in actual factories and a logistics center, and show that it can achieve state-of-the-art performance.},
	articleno    = 86,
	numpages     = 39,
	keywords     = {industrial domain, wearable sensor, Activity recognition}
}
@article{10.1145/3517262,
	title        = {Investigating Cross-Modal Approaches for Evaluating Error Acceptability of a Recognition-Based Input Technique},
	author       = {Henderson, Jay and Jonker, Tanya R. and Lank, Edward and Wigdor, Daniel and Lafreniere, Ben},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517262},
	url          = {https://doi.org/10.1145/3517262},
	issue_date   = {March 2022},
	abstract     = {Emerging input techniques that rely on sensing and recognition can misinterpret a user's intention, resulting in errors and, potentially, a negative user experience. To enhance the development of such input techniques, it is valuable to understand implications of these errors, but they can very costly to simulate. Through two controlled experiments, this work explores various low-cost methods for evaluating error acceptability of freehand mid-air gestural input in virtual reality. Using a gesture-driven game and a drawing application, the first experiment elicited error characteristics through text descriptions, video demonstrations, and a touchscreen-based interactive simulation. The results revealed that video effectively conveyed the dynamics of errors, whereas the interactive modalities effectively reproduced the user experience of effort and frustration. The second experiment contrasts the interactive touchscreen simulation with the target modality - a full VR simulation - and highlights the relative costs and benefits for assessment in an alternative, but still interactive, modality. These findings introduce a spectrum of low-cost methods for evaluating recognition-based errors in VR and a series of characteristics that can be understood in each.},
	articleno    = 15,
	numpages     = 24,
	keywords     = {Acceptability of accuracy, input techniques, error perception}
}
@article{10.1145/3517229,
	title        = {Recursive Sparse Representation for Identifying Multiple Concurrent Occupants Using Floor Vibration Sensing},
	author       = {Fagert, Jonathon and Mirshekari, Mostafa and Zhang, Pei and Noh, Hae Young},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517229},
	url          = {https://doi.org/10.1145/3517229},
	issue_date   = {March 2022},
	abstract     = {In this paper, we present a multiple concurrent occupant identification approach through footstep-induced floor vibration sensing. Identification of human occupants is useful in a variety of indoor smart structure scenarios, with applications in building security, space allocation, and healthcare. Existing approaches leverage sensing modalities such as vision, acoustic, RF, and wearables, but are limited due to deployment constraints such as line-of-sight requirements, sensitivity to noise, dense sensor deployment, and requiring each walker to wear/carry a device. To overcome these restrictions, we use footstep-induced structural vibration sensing. Footstep-induced signals contain information about the occupants' unique gait characteristics, and propagate through the structural medium, which enables sparse and passive identification of indoor occupants. The primary research challenge is that multiple-person footstep-induced vibration responses are a mixture of structurally-codependent overlapping individual responses with unknown timing, spectral content, and mixing ratios. As such, it is difficult to determine which part of the signal corresponds to each occupant. We overcome this challenge through a recursive sparse representation approach based on cosine distance that identifies each occupant in a footstep event in the order that their signals are generated, reconstructs their portion of the signal, and removes it from the mixed response. By leveraging sparse representation, our approach can simultaneously identify and separate mixed/overlapping responses, and the use of the cosine distance error function reduces the influence of structural codependency on the multiple walkers' signals. In this way, we isolate and identify each of the multiple occupants' footstep responses. We evaluate our approach by conducting real-world walking experiments with three concurrent walkers and achieve an average F1 score for identifying all persons of 0.89 (1.3x baseline improvement), and with a 10-person "hybrid" dataset (simulated combination of single-walker real-world data), we identify 2, 3, and 4 concurrent walkers with a trace-level accuracy of 100%, 93%, and 73%, respectively, and observe as much as a 2.9x error reduction over a naive baseline approach.},
	articleno    = 10,
	numpages     = 33,
	keywords     = {Structural Vibrations, Sparse Representation, Occupant Identification, Signal Separation, Smart Structures}
}
@article{10.1145/3494998,
	title        = {A CNN-Based Human Activity Recognition System Combining a Laser Feedback Interferometry Eye Movement Sensor and an IMU for Context-Aware Smart Glasses},
	author       = {Meyer, Johannes and Frank, Adrian and Schlebusch, Thomas and Kasneci, Enkeljeda},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494998},
	url          = {https://doi.org/10.1145/3494998},
	issue_date   = {Dec 2021},
	abstract     = {Smart glasses are considered the next breakthrough in wearables. As the successor of smart watches and smart ear wear, they promise to extend reality by immersive embedding of content in the user's field of view. While advancements in display technology seems to fulfill this promises, interaction concepts are derived from established wearable concepts like touch interaction or voice interaction, preventing full immersion as they require the user to frequently interact with the glasses. To minimize interactions, we propose to add context-awareness to smart glasses through human activity recognition (HAR) by combining head- and eye movement features to recognize a wide range of activities. To measure eye movements in unobtrusive way, we propose laser feedback interferometry (LFI) sensors. These tiny low power sensors are highly robust to ambient light. We combine LFI sensors and an IMU to collect eye and head movement features from 15 participants performing 7 cognitive and physical activities, leading to a unique data set. To recognize activities we propose a 1D-CNN model and apply transfer learning to personalize the classification, leading to an outstanding macro-F1 score of 88.15 % which outperforms state of the art methods. Finally, we discuss the applicability of the proposed system in a smart glasses setup.},
	articleno    = 172,
	numpages     = 24,
	keywords     = {head and eye movement, Human activity recognition, context awarness smart glasses, Laser Feedback Interferometry}
}
@article{10.1145/3494982,
	title        = {SmartKC: Smartphone-Based Corneal Topographer for Keratoconus Detection},
	author       = {Gairola, Siddhartha and Bohra, Murtuza and Shaheer, Nadeem and Jayaprakash, Navya and Joshi, Pallavi and Balasubramaniam, Anand and Murali, Kaushik and Kwatra, Nipun and Jain, Mohit},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494982},
	url          = {https://doi.org/10.1145/3494982},
	issue_date   = {Dec 2021},
	abstract     = {Keratoconus is a severe eye disease affecting the cornea (the clear, dome-shaped outer surface of the eye), causing it to become thin and develop a conical bulge. The diagnosis of keratoconus requires sophisticated ophthalmic devices which are non-portable and very expensive. This makes early detection of keratoconus inaccessible to large populations in low-and middle-income countries, making it a leading cause for partial/complete blindness among such populations. We propose SmartKC, a low-cost, smartphone-based keratoconus diagnosis system comprising of a 3D-printed placido's disc attachment, an LED light strip, and an intelligent smartphone app to capture the reflection of the placido rings on the cornea. An image processing pipeline analyzes the corneal image and uses the smartphone's camera parameters, the placido rings' 3D location, the pixel location of the reflected placido rings and the setup's working distance to construct the corneal surface, via the Arc-Step method and Zernike polynomials based surface fitting. In a clinical study with 101 distinct eyes, we found that SmartKC achieves a sensitivity of 87.8% and a specificity of 80.4%. Moreover, the quantitative curvature estimates (sim-K) strongly correlate with a gold-standard medical device (Pearson correlation coefficient = 0.77). Our results indicate that SmartKC has the potential to be used as a keratoconus screening tool under real-world medical settings.},
	articleno    = 155,
	numpages     = 27,
	keywords     = {placido, topography, low cost system, health sensing, diagnosis, keratometer, Keratron, optics, eye disease, corneal topography, smartphone, image processing, Keratoconus, mobile health, cornea, screening}
}
@article{10.1145/3494981,
	title        = {Context-Aware Compilation of DNN Training Pipelines across Edge and Cloud},
	author       = {Yao, Dixi and Xiang, Liyao and Wang, Zifan and Xu, Jiayu and Li, Chao and Wang, Xinbing},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494981},
	url          = {https://doi.org/10.1145/3494981},
	issue_date   = {Dec 2021},
	abstract     = {Empowered by machine learning, edge devices including smartphones, wearable, and IoT devices have become growingly intelligent, raising conflicts with the limited resource. On-device model personalization is particularly hard as training models on edge devices is highly resource-intensive. In this work, we propose a novel training pipeline across the edge and the cloud, by taking advantage of the powerful cloud while keeping data local at the edge. Highlights of the design incorporate the parallel execution enabled by our feature replay, reduced communication cost by our error-feedback feature compression, as well as the context-aware deployment decision engine. Working as an integrated system, the proposed pipeline training framework not only significantly speeds up training, but also incurs little accuracy loss or additional memory/energy overhead. We test our system in a variety of settings including WiFi, 5G, household IoT, and on different training tasks such as image/text classification, image generation, to demonstrate its advantage over the state-of-the-art. Experimental results show that our system not only adapts well to, but also draws on the varying contexts, delivering a practical and efficient solution to edge-cloud model training.},
	articleno    = 188,
	numpages     = 27,
	keywords     = {neural networks, edge computing, machine learning}
}
@article{10.1145/3494978,
	title        = {Janus: Dual-Radio Accurate and Energy-Efficient Proximity Detection},
	author       = {Istomin, Timofei and Leoni, Elia and Molteni, Davide and Murphy, Amy L. and Picco, Gian Pietro and Griva, Maurizio},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494978},
	url          = {https://doi.org/10.1145/3494978},
	issue_date   = {Dec 2021},
	abstract     = {Proximity detection is at the core of several mobile and ubiquitous computing applications. These include reactive use cases, e.g., alerting individuals of hazards or interaction opportunities, and others concerned only with logging proximity data, e.g., for offline analysis and modeling. Common approaches rely on Bluetooth Low Energy (BLE) or ultra-wideband (UWB) radios. Nevertheless, these strike opposite tradeoffs between the accuracy of distance estimates quantifying proximity and the energy efficiency affecting system lifetime, effectively forcing a choice between the two and ultimately constraining applicability.Janus reconciles these dimensions in a dual-radio protocol enabling accurate and energy-efficient proximity detection, where the energy-savvy BLE is exploited to discover devices and coordinate their distance measurements, acquired via the energy-hungry UWB. A model supports domain experts in configuring Janus for their use cases with predictable performance. The latency, reliability, and accuracy of Janus are evaluated experimentally, including realistic scenarios endowed with the mm-level ground truth provided by a motion capture system. Energy measurements show that Janus achieves weeks to months of autonomous operation, depending on the use case configuration. Finally, several large-scale campaigns exemplify its practical usefulness in real-world contexts.},
	articleno    = 162,
	numpages     = 33,
	keywords     = {ultra-wideband (UWB), proximity detection, Bluetooth}
}
@article{10.1145/3494968,
	title        = {SwiVR-Car-Seat: Exploring Vehicle Motion Effects on Interaction Quality in Virtual Reality Automated Driving Using a Motorized Swivel Seat},
	author       = {Colley, Mark and Jansen, Pascal and Rukzio, Enrico and Gugenheimer, Jan},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494968},
	url          = {https://doi.org/10.1145/3494968},
	issue_date   = {Dec 2021},
	abstract     = {Autonomous vehicles provide new input modalities to improve interaction with in-vehicle information systems. However, due to the road and driving conditions, the user input can be perturbed, resulting in reduced interaction quality. One challenge is assessing the vehicle motion effects on the interaction without an expensive high-fidelity simulator or a real vehicle. This work presents SwiVR-Car-Seat, a low-cost swivel seat to simulate vehicle motion using rotation. In an exploratory user study (N=18), participants sat in a virtual autonomous vehicle and performed interaction tasks using the input modalities touch, gesture, gaze, or speech. Results show that the simulation increased the perceived realism of vehicle motion in virtual reality and the feeling of presence. Task performance was not influenced uniformly across modalities; gesture and gaze were negatively affected while there was little impact on touch and speech. The findings can advise automotive user interface design to mitigate the adverse effects of vehicle motion on the interaction.},
	articleno    = 150,
	numpages     = 26,
	keywords     = {vehicle motion simulation, interaction quality, Autonomous vehicles, interface design}
}
@article{10.1145/3494967,
	title        = {RFTemp: Monitoring Microwave Oven Leakage to Estimate Food Temperature},
	author       = {Banerjee, Avishek and Srinivasan, Kannan},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494967},
	url          = {https://doi.org/10.1145/3494967},
	issue_date   = {Dec 2021},
	abstract     = {Microwave ovens have been widely used in recent years to heat food quickly and efficiently. Users estimate the time to heat the food by prior knowledge or by trial and error process. However, this often results in the food being over-heated or under-heated, destroying the nutrients. In this paper, we present RFTemp, a system that can monitor microwave oven leakage to estimate the temperature of the food that is being heated and thus estimate the accurate time when the food has reached the targeted temperature. To design such a system, we propose an innovative microwave leakage sensing procedure and a novel water-equivalent food model to estimate food temperature. To evaluate the real-world performance of RFTemp we build a prototype using software defined radios and conducted experiments on various food items using household microwave ovens. We show that RFTemp can estimate the temperature of the food with a mean error of 5°C, 2x improvement over contactless infrared thermometer and sensors.},
	articleno    = 144,
	numpages     = 25,
	keywords     = {Real-time, Food Temperature Estimation, Radio Frequency Sensing, Dielectric Property, Microwave Oven, Dielectric Heating}
}
@article{10.1145/3494961,
	title        = {Ubi-SleepNet: Advanced Multimodal Fusion Techniques for Three-Stage Sleep Classification Using Ubiquitous Sensing},
	author       = {Zhai, Bing and Guan, Yu and Catt, Michael and Pl\"{o}tz, Thomas},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494961},
	url          = {https://doi.org/10.1145/3494961},
	issue_date   = {Dec 2021},
	abstract     = {Sleep is a fundamental physiological process that is essential for sustaining a healthy body and mind. The gold standard for clinical sleep monitoring is polysomnography(PSG), based on which sleep can be categorized into five stages, including wake/rapid eye movement sleep (REM sleep)/Non-REM sleep 1 (N1)/Non-REM sleep 2 (N2)/Non-REM sleep 3 (N3). However, PSG is expensive, burdensome and not suitable for daily use. For long-term sleep monitoring, ubiquitous sensing may be a solution. Most recently, cardiac and movement sensing has become popular in classifying three-stage sleep, since both modalities can be easily acquired from research-grade or consumer-grade devices (e.g., Apple Watch). However, how best to fuse the data for greatest accuracy remains an open question. In this work, we comprehensively studied deep learning (DL)-based advanced fusion techniques consisting of three fusion strategies alongside three fusion methods for three-stage sleep classification based on two publicly available datasets. Experimental results demonstrate important evidences that three-stage sleep can be reliably classified by fusing cardiac/movement sensing modalities, which may potentially become a practical tool to conduct large-scale sleep stage assessment studies or long-term self-tracking on sleep. To accelerate the progression of sleep research in the ubiquitous/wearable computing community, we made this project open source, and the code can be found at: https://github.com/bzhai/Ubi-SleepNet.},
	articleno    = 191,
	numpages     = 33,
	keywords     = {Neural Networks, Sleep Monitoring, Apple Watch, Three Sleep Stages, Deep Learning, Ubiquitous Sensing, Wearable, Heart Rate, MESA, Heart Rate Variability, Multimodal Fusion}
}
@article{10.1145/3494960,
	title        = {Are Those Steps Worth Your Privacy? Fitness-Tracker Users' Perceptions of Privacy and Utility},
	author       = {Velykoivanenko, Lev and Niksirat, Kavous Salehzadeh and Zufferey, No\'{e} and Humbert, Mathias and Huguenin, K\'{e}vin and Cherubini, Mauro},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494960},
	url          = {https://doi.org/10.1145/3494960},
	issue_date   = {Dec 2021},
	abstract     = {Fitness trackers are increasingly popular. The data they collect provides substantial benefits to their users, but it also creates privacy risks. In this work, we investigate how fitness-tracker users perceive the utility of the features they provide and the associated privacy-inference risks. We conduct a longitudinal study composed of a four-month period of fitness-tracker use (N = 227), followed by an online survey (N = 227) and interviews (N = 19). We assess the users' knowledge of concrete privacy threats that fitness-tracker users are exposed to (as demonstrated by previous work), possible privacy-preserving actions users can take, and perceptions of utility of the features provided by the fitness trackers. We study the potential for data minimization and the users' mental models of how the fitness tracking ecosystem works. Our findings show that the participants are aware that some types of information might be inferred from the data collected by the fitness trackers. For instance, the participants correctly guessed that sexual activity could be inferred from heart-rate data. However, the participants did not realize that also the non-physiological information could be inferred from the data. Our findings demonstrate a high potential for data minimization, either by processing data locally or by decreasing the temporal granularity of the data sent to the service provider. Furthermore, we identify the participants' lack of understanding and common misconceptions about how the Fitbit ecosystem works.},
	articleno    = 181,
	numpages     = 41,
	keywords     = {fitness trackers, utility, privacy, wearable activity trackers, mental models}
}
@article{10.1145/3494956,
	title        = {Handwriting-Assistant: Reconstructing Continuous Strokes with Millimeter-Level Accuracy via Attachable Inertial Sensors},
	author       = {Bu, Yanling and Xie, Lei and Yin, Yafeng and Wang, Chuyu and Ning, Jingyi and Cao, Jiannong and Lu, Sanglu},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494956},
	url          = {https://doi.org/10.1145/3494956},
	issue_date   = {Dec 2021},
	abstract     = {Pen-based handwriting has become one of the major human-computer interaction methods. Traditional approaches either require writing on the specific supporting device like the touch screen, or limit the way of using the pen to pure rotation or translation. In this paper, we propose Handwriting-Assistant, to capture the free handwriting of ordinary pens on regular planes with mm-level accuracy. By attaching the inertial measurement unit (IMU) to the pen tail, we can infer the handwriting on the notebook, blackboard or other planes. Particularly, we build a generalized writing model to correlate the rotation and translation of IMU with the tip displacement comprehensively, thereby we can infer the tip trace accurately. Further, to display the effective handwriting during the continuous writing process, we leverage the principal component analysis (PCA) based method to detect the candidate writing plane, and then exploit the distance variation of each segment relative to the plane to distinguish on-plane strokes. Moreover, our solution can apply to other rigid bodies, enabling smart devices embedded with IMUs to act as handwriting tools. Experiment results show that our approach can capture the handwriting with high accuracy, e.g., the average tracking error is 1.84mm for letters with the size of about 2cmx1cm, and the average character recognition rate of recovered single letters achieves 98.2% accuracy of the ground-truth recorded by touch screen.},
	articleno    = 146,
	numpages     = 25,
	keywords     = {Millimeter-level Tracking, Inertial Sensor, Handwriting Reconstruction}
}
@article{10.1145/3494955,
	title        = {Be Consistent, Work the Program, Be Present Every Day: Exploring Technologies for Self-Tracking in Early Recovery},
	author       = {Jones, Jasmine and Yuan, Ye and Yarosh, Svetlana},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494955},
	url          = {https://doi.org/10.1145/3494955},
	issue_date   = {Dec 2021},
	abstract     = {Recovery from substance abuse disorders (SUDs) is a lifelong process of change. Self-tracking technologies have been proposed by the recovery community as a beneficial design space to support people adopting positive lifestyles and behaviors in their recovery. To explore the potential of this design space, we designed and deployed a technology probe consisting of a mobile app, wearable visualization, and ambient display to enable people to track and reflect on the activities they adopted in their recovery process. With this probe we conducted a four-week exploratory field study with 17 adults in early recovery to investigate 1) what activities people in recovery desire to track, 2) how people perceive self-tracking tools in relation to their recovery process, and 3) what digital resources self-tracking tools can provide to aid the recovery process. Our findings illustrate the array of activities that people track in their recovery, along with usage scenarios, preferences and design tensions that arose. We discuss implications for holistic self-tracking technologies and opportunities for future work in behavior change support for this context.},
	articleno    = 164,
	numpages     = 26,
	keywords     = {substance abuse disorder, self-tracking, patient-centered design, sobriety, self-care}
}
@article{10.1145/3494954,
	title        = {DAFI: WiFi-Based Device-Free Indoor Localization via Domain Adaptation},
	author       = {Li, Hang and Chen, Xi and Wang, Ju and Wu, Di and Liu, Xue},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494954},
	url          = {https://doi.org/10.1145/3494954},
	issue_date   = {Dec 2021},
	abstract     = {WiFi-based Device-free Passive (DfP) indoor localization systems liberate their users from carrying dedicated sensors or smartphones, and thus provide a non-intrusive and pleasant experience. Although existing fingerprint-based systems achieve sub-meter-level localization accuracy by training location classifiers/regressors on WiFi signal fingerprints, they are usually vulnerable to small variations in an environment. A daily change, e.g., displacement of a chair, may cause a big inconsistency between the recorded fingerprints and the real-time signals, leading to significant localization errors. In this paper, we introduce a Domain Adaptation WiFi (DAFI) localization approach to address the problem. DAFI formulates this fingerprint inconsistency issue as a domain adaptation problem, where the original environment is the source domain and the changed environment is the target domain. Directly applying existing domain adaptation methods to our specific problem is challenging, since it is generally hard to distinguish the variations in the different WiFi domains (i.e., signal changes caused by different environmental variations). DAFI embraces the following techniques to tackle this challenge. 1) DAFI aligns both marginal and conditional distributions of features in different domains. 2) Inside the target domain, DAFI squeezes the marginal distribution of every class to be more concentrated at its center. 3) Between two domains, DAFI conducts fine-grained alignment by forcing every target-domain class to better align with its source-domain counterpart. By doing these, DAFI outperforms the state of the art by up to 14.2% in real-world experiments.},
	articleno    = 167,
	numpages     = 21,
	keywords     = {WiFi, indoor localization, domain adaptation}
}
@article{10.1145/3478128,
	title        = {Overthere: A Simple and Intuitive Object Registration Method for an Absolute Mid-Air Pointing Interface},
	author       = {Seo, Hyunggoog and Kim, Jaedong and Seo, Kwanggyoon and Kim, Bumki and Noh, Junyong},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478128},
	url          = {https://doi.org/10.1145/3478128},
	issue_date   = {Sept 2021},
	abstract     = {An absolute mid-air pointing technique requires a preprocess called registration that makes the system remember the 3D positions and types of objects in advance. Previous studies have simply assumed that the information is already available because it requires a cumbersome process performed by an expert in a carefully calibrated environment. We introduce Overthere, which allows the user to intuitively register the objects in a smart environment by pointing to each target object a few times. To ensure accurate and coherent pointing gestures made by the user regardless of individual differences between them, we performed a user study and identified a desirable gesture motion for this purpose. In addition, we provide the user with various feedback to help them understand the current registration progress and adhere to required conditions, which will lead to accurate registration results. The user studies show that Overthere is sufficiently intuitive to be used by ordinary people.},
	articleno    = 127,
	numpages     = 24,
	keywords     = {absolute pointing interface, 3D positioning, object registration, freehand pointing}
}
@article{10.1145/3478119,
	title        = {SenseCollect: We Need Efficient Ways to Collect On-Body Sensor-Based Human Activity Data!},
	author       = {Chen, Wenqiang and Lin, Shupei and Thompson, Elizabeth and Stankovic, John},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478119},
	url          = {https://doi.org/10.1145/3478119},
	issue_date   = {Sept 2021},
	abstract     = {On-body sensor-based human activity recognition (HAR) lags behind other fields because it lacks large-scale, labeled datasets; this shortfall impedes progress in developing robust and generalized predictive models. To facilitate researchers in collecting more extensive datasets quickly and efficiently we developed SenseCollect. We did a survey and interviewed student researchers in this area to identify what barriers are making it difficult to collect on-body sensor-based HAR data from human subjects. Every interviewee identified data collection as the hardest part of their research, stating it was laborious, consuming and error-prone. To improve HAR data resources we need to address that barrier, but we need a better understanding of the complicating factors to overcome it. To that end we conducted a series of control variable experiments that tested several protocols to ascertain their impact on data collection. SenseCollect studied 240+ human subjects in total and presented the findings to develop a data collection guideline. We also implemented a system to collect data, created the two largest on-body sensor-based human activity datasets, and made them publicly available.},
	articleno    = 91,
	numpages     = 27,
	keywords     = {On-body sensors, Human activity recognition, Micro finger writing, Data collection}
}
@article{10.1145/3478100,
	title        = {Write, Attend and Spell: Streaming End-to-End Free-Style Handwriting Recognition Using Smartwatches},
	author       = {Zhang, Qian and Wang, Dong and Zhao, Run and Yu, Yinggang and Jing, JiaZhen},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478100},
	url          = {https://doi.org/10.1145/3478100},
	issue_date   = {Sept 2021},
	abstract     = {Text entry on a smartwatch is challenging due to its small form factor. Handwriting recognition using the built-in sensors of the watch (motion sensors, microphones, etc.) provides an efficient and natural solution to deal with this issue. However, prior works mainly focus on individual letter recognition rather than word recognition. Therefore, they need users to pause between adjacent letters for segmentation, which is counter-intuitive and significantly decreases the input speed. In this paper, we present 'Write, Attend and Spell' (WriteAS), a word-level text-entry system which enables free-style handwriting recognition using the motion signals of the smartwatch. First, we design a multimodal convolutional neural network (CNN) to abstract motion features across modalities. After that, a stacked dilated convolutional network with an encoder-decoder network is applied to get around letter segmentation and output words in an end-to-end way. More importantly, we leverage a multi-task sequence learning method to enable handwriting recognition in a streaming way. We construct the first sequence-to-sequence handwriting dataset using smartwatch. WriteAS can yield 9.3% character error rate (CER) on 250 words for new users and 3.8% CER for words unseen in the training set. In addition, WriteAS can handle various writing conditions very well. Given the promising performance, we envision that WriteAS can be a fast and accurate input tool for smartwatch.},
	articleno    = 138,
	numpages     = 25,
	keywords     = {text entry, end-to-end, smartwatch, handwriting}
}
@article{10.1145/3478098,
	title        = {Smartphone-Based Tapping Frequency as a Surrogate for Perceived Fatigue: An in-the-Wild Feasibility Study in Multiple Sclerosis Patients},
	author       = {Barrios, Liliana and Oldrati, Pietro and Hilty, Marc and Lindlbauer, David and Holz, Christian and Lutterotti, Andreas},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478098},
	url          = {https://doi.org/10.1145/3478098},
	issue_date   = {Sept 2021},
	abstract     = {Fatigue is a common symptom in various diseases, including multiple sclerosis (MS). The current standard method to assess fatigue is through questionnaires, which has several shortcomings; questionnaires are subjective, prone to recall bias, and potentially confounded by other symptoms like stress and depression. Thus, there is an unmet medical need to develop objective and reliable methods to evaluate fatigue. Our study seeks to develop an objective and ubiquitous monitoring tool for assessing fatigue. Leveraging a smartphone-based rapid tapping task, we conducted a two-week in-the-wild study with 35 MS patients. We explore the association between tapping derived metrics and perceived fatigue assessed with two standard clinical scales: fatigue severity scale (FSS) and fatigue scale for motor and cognitive function (FSMC). Our novel smartphone-based fatigue metric, mean tapping frequency, objectively ranks perceived fatigue with a mean AUCROC = .76, CI = [.71, .81] according to the FSMC, and a mean AUCROC = .81, CI = [.76, .86] according to the FSS. These results demonstrate that our approach is feasible and valid in uncontrolled environments. In this work, we provide a promising tool for objective fatigue monitoring to be used in clinical trials and routine medical care.},
	articleno    = 89,
	numpages     = 30,
	keywords     = {objective-measurement, fatigability, smartphones, impairment, multiple sclerosis, remote-monitoring, fatigue, mobile health}
}
@article{10.1145/3478094,
	title        = {Fall Detection via Inaudible Acoustic Sensing},
	author       = {Lian, Jie and Yuan, Xu and Li, Ming and Tzeng, Nian-Feng},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478094},
	url          = {https://doi.org/10.1145/3478094},
	issue_date   = {Sept 2021},
	abstract     = {The fall detection system is of critical importance in protecting elders through promptly discovering fall accidents to provide immediate medical assistance, potentially saving elders' lives. This paper aims to develop a novel and lightweight fall detection system by relying solely on a home audio device via inaudible acoustic sensing, to recognize fall occurrences for wide home deployment. In particular, we program the audio device to let its speaker emit 20kHz continuous wave, while utilizing a microphone to record reflected signals for capturing the Doppler shift caused by the fall. Considering interferences from different factors, we first develop a set of solutions for their removal to get clean spectrograms and then apply the power burst curve to locate the time points at which human motions happen. A set of effective features is then extracted from the spectrograms for representing the fall patterns, distinguishable from normal activities. We further apply the Singular Value Decomposition (SVD) and K-mean algorithms to reduce the data feature dimensions and to cluster the data, respectively, before input them to a Hidden Markov Model for training and classification. In the end, our system is implemented and deployed in various environments for evaluation. The experimental results demonstrate that our system can achieve superior performance for detecting fall accidents and is robust to environment changes, i.e., transferable to other environments after training in one environment.},
	articleno    = 114,
	numpages     = 21,
	keywords     = {Fall Detection, Device-free, Ultrasonic, Hidden Markov Model}
}
@article{10.1145/3478086,
	title        = {ISACS: In-Store Autonomous Checkout System for Retail},
	author       = {Falc\~{a}o, Jo\~{a}o Diogo and Ruiz, Carlos and Bannis, Adeola and Noh, Hae Young and Zhang, Pei},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478086},
	url          = {https://doi.org/10.1145/3478086},
	issue_date   = {Sept 2021},
	abstract     = {90% of retail sales occur in physical stores. In these physical stores 40% of shoppers leave the store based on the wait time. Autonomous stores can remove customer waiting time by providing a receipt without the need for scanning the items. Prior approaches use computer vision only, combine computer vision with weight sensors, or combine computer vision with sensors and human product recognition. These approaches, in general, suffer from low accuracy, up to hour long delays for receipt generation, or do not scale to store level deployments due to computation requirements and real-world multiple shopper scenarios.We present ISACS, which combines a physical store model (e.g. customers, shelves, and item interactions), multi-human 3D pose estimation, and live inventory monitoring to provide an accurate matching of multiple people to multiple products. ISACS utilizes only shelf weight sensors and does not require visual inventory monitoring which drastically reduces the computational requirements and thus is scalable to a store-level deployment. In addition, ISACS generates an instant receipt by not requiring human intervention during receipt generation. To fully evaluate the ISACS, we deployed and evaluated our approach in an operating convenience store covering 800 square feet with 1653 distinct products, and more than 20,000 items. Over the course of 13 months of operation, ISACS achieved a receipt daily accuracy of up to 96.4%. Which translates to a 3.5x reduction in error compared to self-checkout stations.},
	articleno    = 99,
	numpages     = 26,
	keywords     = {inventory monitoring, human tracking, instant receipt, sensor fusion, autonomous checkout, retail}
}
@article{10.1145/3478083,
	title        = {MetaTP: Traffic Prediction with Unevenly-Distributed Road Sensing Data via Fast Adaptation},
	author       = {Zhong, Weida and Suo, Qiuling and Gupta, Abhishek and Jia, Xiaowei and Qiao, Chunming and Su, Lu},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478083},
	url          = {https://doi.org/10.1145/3478083},
	issue_date   = {Sept 2021},
	abstract     = {With the popularity of smartphones, large-scale road sensing data is being collected to perform traffic prediction, which is an important task in modern society. Due to the nature of the roving sensors on smartphones, the collected traffic data which is in the form of multivariate time series, is often temporally sparse and unevenly distributed across regions. Moreover, different regions can have different traffic patterns, which makes it challenging to adapt models learned from regions with sufficient training data to target regions. Given that many regions may have very sparse data, it is also impossible to build individual models for each region separately. In this paper, we propose a meta-learning based framework named MetaTP to overcome these challenges. MetaTP has two key parts, i.e., basic traffic prediction network (base model) and meta-knowledge transfer. In base model, a two-layer interpolation network is employed to map original time series onto uniformly-spaced reference time points, so that temporal prediction can be effectively performed in the reference space. The meta-learning framework is employed to transfer knowledge from source regions with a large amount of data to target regions with a few data examples via fast adaptation, in order to improve model generalizability on target regions. Moreover, we use two memory networks to capture the global patterns of spatial and temporal information across regions. We evaluate the proposed framework on two real-world datasets, and experimental results show the effectiveness of the proposed framework.},
	articleno    = 141,
	numpages     = 28,
	keywords     = {meta learning, traffic prediction}
}
@article{10.1145/3478077,
	title        = {REHASH: A Flexible, Developer Focused, Heuristic Adaptation Platform for Intermittently Powered Computing},
	author       = {Bakar, Abu and Ross, Alexander G. and Yildirim, Kasim Sinan and Hester, Josiah},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478077},
	url          = {https://doi.org/10.1145/3478077},
	issue_date   = {Sept 2021},
	abstract     = {Battery-free sensing devices harvest energy from their surrounding environment to perform sensing, computation, and communication. This enables previously impossible applications in the Internet-of-Things. A core challenge for these devices is maintaining usefulness despite erratic, random or irregular energy availability; which causes inconsistent execution, loss of service and power failures. Adapting execution (degrading or upgrading) seems promising as a way to stave off power failures, meet deadlines, or increase throughput. However, because of constrained resources and limited local information, it is a challenge to decide when would be the best time to adapt, and how exactly to adapt execution. In this paper, we systematically explore the fundamental mechanisms of energy-aware adaptation, and propose heuristic adaptation as a method for modulating the performance of tasks to enable higher sensor coverage, completion rates, or throughput, depending on the application. We build a task based adaptive runtime system for intermittently powered sensors embodying this concept. We complement this runtime with a user facing simulator that enables programmers to conceptualize the tradeoffs they make when choosing what tasks to adapt, and how, relative to real world energy harvesting environment traces. While we target battery-free, intermittently powered sensors, we see general application to all energy harvesting devices. We explore heuristic adaptation with varied energy harvesting modalities and diverse applications: machine learning, activity recognition, and greenhouse monitoring, and find that the adaptive version of our ML app performs up to 46% more classifications with only a 5% drop in accuracy; the activity recognition app captures 76% more classifications with only nominal down-sampling; and find that heuristic adaptation leads to higher throughput versus non-adaptive in all cases.},
	articleno    = 87,
	numpages     = 42,
	keywords     = {Batteryless Platform, Adaptation, Intermittent Computing, Energy Harvesting}
}
@article{10.1145/3463528,
	title        = {Identifying Mobile Sensing Indicators of Stress-Resilience},
	author       = {Adler, Daniel A. and Tseng, Vincent W.-S. and Qi, Gengmo and Scarpa, Joseph and Sen, Srijan and Choudhury, Tanzeem},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463528},
	url          = {https://doi.org/10.1145/3463528},
	issue_date   = {June 2021},
	abstract     = {Resident physicians (residents) experiencing prolonged workplace stress are at risk of developing mental health symptoms. Creating novel, unobtrusive measures of resilience would provide an accessible approach to evaluate symptom susceptibility without the perceived stigma of formal mental health assessments. In this work, we created a system to find indicators of resilience using passive wearable sensors and smartphone-delivered ecological momentary assessment (EMA). This system identified indicators of resilience during a medical internship, the high stress first-year of a residency program. We then created density estimation approaches to predict these indicators before mental health changes occurred, and validated whether the predicted indicators were also associated with resilience. Our system identified resilience indicators associated with physical activity (step count), sleeping behavior, reduced heart rate, increased mood, and reduced mood variability. Density estimation models were able to replicate a subset of the associations between sleeping behavior, heart rate, and resilience. To the best of our knowledge, this work provides the first methodology to identify and predict indicators of resilience using passive sensing and EMA. Researchers studying resident mental health can apply this approach to design resilience-building interventions and prevent mental health symptom development.},
	articleno    = 51,
	numpages     = 32,
	keywords     = {mobile sensing, mental health, wearable technology, deep generative models}
}
@article{10.1145/3463519,
	title        = {SonicASL: An Acoustic-Based Sign Language Gesture Recognizer Using Earphones},
	author       = {Jin, Yincheng and Gao, Yang and Zhu, Yanjun and Wang, Wei and Li, Jiyang and Choi, Seokmin and Li, Zhangyu and Chauhan, Jagmohan and Dey, Anind K. and Jin, Zhanpeng},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463519},
	url          = {https://doi.org/10.1145/3463519},
	issue_date   = {June 2021},
	abstract     = {We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians.},
	articleno    = 67,
	numpages     = 30,
	keywords     = {Acoustic sensing, sign language gesture recognition, earphones}
}
@article{10.1145/3463509,
	title        = {MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-Capture},
	author       = {Chang, Yuhu and Zhao, Yingying and Dong, Mingzhi and Wang, Yujiang and Lu, Yutian and Lv, Qin and Dick, Robert P. and Lu, Tun and Gu, Ning and Shang, Li},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463509},
	url          = {https://doi.org/10.1145/3463509},
	issue_date   = {June 2021},
	abstract     = {This work presents MemX: a biologically-inspired attention-aware eyewear system developed with the goal of pursuing the long-awaited vision of a personalized visual Memex. MemX captures human visual attention on the fly, analyzes the salient visual content, and records moments of personal interest in the form of compact video snippets. Accurate attentive scene detection and analysis on resource-constrained platforms is challenging because these tasks are computation and energy intensive. We propose a new temporal visual attention network that unifies human visual attention tracking and salient visual content analysis. Attention tracking focuses computation-intensive video analysis on salient regions, while video analysis makes human attention detection and tracking more accurate. Using the YouTube-VIS dataset and 30 participants, we experimentally show that MemX significantly improves the attention tracking accuracy over the eye-tracking-alone method, while maintaining high system energy efficiency. We have also conducted 11 in-field pilot studies across a range of daily usage scenarios, which demonstrate the feasibility and potential benefits of MemX.},
	articleno    = 56,
	numpages     = 23,
	keywords     = {Smart glasses, energy-efficient, video instance segmentation, attention-aware, eye tracking}
}
@article{10.1145/3463503,
	title        = {Towards Early Detection and Burden Estimation of Atrial Fibrillation in an Ambulatory Free-Living Environment},
	author       = {Zhang, Hanbin and Zhu, Li and Nathan, Viswam and Kuang, Jilong and Kim, Jacob and Gao, Jun Alex and Olgin, Jeffrey},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463503},
	url          = {https://doi.org/10.1145/3463503},
	issue_date   = {June 2021},
	abstract     = {Early detection and accurate burden estimation of atrial fibrillation (AFib) can provide the foundation for effective physician treatment. New approaches to accomplish this have attracted tremendous attention in recent years. In this paper, we develop a novel passive smartwatch-based system to detect AFib episodes and estimate the AFib burden in an ambulatory free-living environment without user engagement. Our system leverages a built-in PPG sensor to collect heart rhythm without user engagement. Then, a data preprocessor module includes time-frequency (TF) analysis to augment features in both the time and frequency domain. Finally, a lightweight multi-view convolutional neural network consisting of 19 layers achieves the AFib detection. To validate our system, we carry out a research study that enrolls 53 participants across three months, where we collect and annotate more than 27,622 hours of data. Our system achieves an average of 91.6% accuracy, 93.0% specificity, and 90.8% sensitivity without dropping any data. Moreover, our system takes 0.51 million parameters and costs 5.18 ms per inference. These results reveal that our proposed system can provide a clinical assessment of AFib in daily living.},
	articleno    = 86,
	numpages     = 19,
	keywords     = {Atrial Fibrillation, Smartwatch, Mobile Health}
}
@article{10.1145/3463496,
	title        = {DropMonitor: Millimeter-Level Sensing for RFID-Based Infusion Drip Rate Monitoring},
	author       = {Lin, Yuancan and Xie, Lei and Wang, Chuyu and Bu, Yanling and Lu, Sanglu},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463496},
	url          = {https://doi.org/10.1145/3463496},
	issue_date   = {June 2021},
	abstract     = {As an important indicator of the infusion monitoring for clinical treatment, the drip rate is expected to be monitored in an accurate and real-time manner. However, state-of-the-art drip rate monitoring schemes either suffer from high maintenance or incur high hardware cost. In this paper, we propose DropMonitor, an RFID-based approach to perform the mm-level sensing for infusion drip rate monitoring. By attaching a pair of batteryless RFID tags on the drip chamber, we can estimate the drip rate by capturing the RF-signals reflected from the vibrating liquid surface caused by the falling droplets. Particularly, we use the sensing tag to perceive the liquid surface vibration in the drip chamber and further derive the drip rate for infusion monitoring. Moreover, to sufficiently mitigate the multi-path interference from the surrounding human activities, we use the reference tag to perceive the multi-path signals from the indoor environment. By computing the difference of RF-signals from tag pairs, we cancel the multi-path interference and extract the drip-rate-related signals. We have implemented a prototype system and evaluated its performance in real applications. The experiment results show that DropMonitor can accurately estimate the infusion drip rate, and the average relative error of drip rate estimation is below 1% for conventional cases. In this way, considering the essential sampling rates of each tag, DropMonitor is able to monitor the drip rate for over a dozen of infusion bottles/bags in parallel with one COTS RFID system.},
	articleno    = 72,
	numpages     = 22,
	keywords     = {Millimeter-level Sensing, RFID, Tag Pair, Drip Rate Monitoring}
}
@article{10.1145/3463495,
	title        = {UVLens: Urban Village Boundary Identification and Population Estimation Leveraging Open Government Data},
	author       = {Chen, Longbiao and Lu, Chenhui and Yuan, Fangxu and Jiang, Zhihan and Wang, Leye and Zhang, Daqing and Luo, Ruixiang and Fan, Xiaoliang and Wang, Cheng},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463495},
	url          = {https://doi.org/10.1145/3463495},
	issue_date   = {June 2021},
	abstract     = {Urban villages refer to the residential areas lagging behind the rapid urbanization process in many developing countries. These areas are usually with overcrowded buildings, high population density, and low living standards, bringing potential risks of public safety and hindering the urban development. Therefore, it is crucial for urban authorities to identify the boundaries of urban villages and estimate their resident and floating populations so as to better renovate and manage these areas. Traditional approaches, such as field surveys and demographic census, are time consuming and labor intensive, lacking a comprehensive understanding of urban villages. Against this background, we propose a two-phase framework for urban village boundary identification and population estimation. Specifically, based on heterogeneous open government data, the proposed framework can not only accurately identify the boundaries of urban villages from large-scale satellite imagery by fusing road networks guided patches with bike-sharing drop-off patterns, but also accurately estimate the resident and floating populations of urban villages with a proposed multi-view neural network model. We evaluate our method leveraging real-world datasets collected from Xiamen Island. Results show that our framework can accurately identify the urban village boundaries with an IoU of 0.827, and estimate the resident population and floating population with R2 of 0.92 and 0.94 respectively, outperforming the baseline methods. We also deploy our system on the Xiamen Open Government Data Platform to provide services to both urban authorities and citizens.},
	articleno    = 57,
	numpages     = 26,
	keywords     = {urban computing, population estimation, heterogeneous data, urban village}
}
@article{10.1145/3463492,
	title        = {Detecting Receptivity for MHealth Interventions in the Natural Environment},
	author       = {Mishra, Varun and K\"{u}nzler, Florian and Kramer, Jan-Niklas and Fleisch, Elgar and Kowatsch, Tobias and Kotz, David},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463492},
	url          = {https://doi.org/10.1145/3463492},
	issue_date   = {June 2021},
	abstract     = {Just-In-Time Adaptive Intervention (JITAI) is an emerging technique with great potential to support health behavior by providing the right type and amount of support at the right time. A crucial aspect of JITAIs is properly timing the delivery of interventions, to ensure that a user is receptive and ready to process and use the support provided. Some prior works have explored the association of context and some user-specific traits on receptivity, and have built post-study machine-learning models to detect receptivity. For effective intervention delivery, however, a JITAI system needs to make in-the-moment decisions about a user's receptivity. To this end, we conducted a study in which we deployed machine-learning models to detect receptivity in the natural environment, i.e., in free-living conditions.We leveraged prior work regarding receptivity to JITAIs and deployed a chatbot-based digital coach - Ally - that provided physical-activity interventions and motivated participants to achieve their step goals. We extended the original Ally app to include two types of machine-learning model that used contextual information about a person to predict when a person is receptive: a static model that was built before the study started and remained constant for all participants and an adaptive model that continuously learned the receptivity of individual participants and updated itself as the study progressed. For comparison, we included a control model that sent intervention messages at random times. The app randomly selected a delivery model for each intervention message. We observed that the machine-learning models led up to a 40% improvement in receptivity as compared to the control model. Further, we evaluated the temporal dynamics of the different models and observed that receptivity to messages from the adaptive model increased over the course of the study.},
	articleno    = 74,
	numpages     = 24,
	keywords     = {Interruption, Engagement, Mobile Health, Intervention, Receptivity}
}
@article{10.1145/3448118,
	title        = {LSVP: Towards Effective On-the-Go Video Learning Using Optical Head-Mounted Displays},
	author       = {Ram, Ashwin and Zhao, Shengdong},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448118},
	url          = {https://doi.org/10.1145/3448118},
	issue_date   = {March 2021},
	abstract     = {The ubiquity of mobile phones allows video content to be watched on the go. However, users' current on-the-go video learning experience on phones is encumbered by issues of toggling and managing attention between the video and surroundings, as informed by our initial qualitative study. To alleviate this, we explore how combining the emergent smart glasses (Optical Head-Mounted Display or OHMD) platform with a redesigned video presentation style can better distribute users' attention between learning and walking tasks. We evaluated three presentation techniques: highlighting, sequentiality, and data persistence to find that combining sequentiality and data persistence is highly effective, yielding a 56% higher immediate recall score compared to a static video presentation. We also compared the OHMD against smartphones to delineate the advantages of either platform for on-the-go video learning in the context of everyday mobility tasks. We found that OHMDs improved users' 7-day delayed recall scores by 17% while still allowing 5.6% faster walking speed, especially during complex mobility tasks. Based on the findings, we introduce Layered Serial Visual Presentation (LSVP) style, which incorporates sequentiality, strict data persistence, and transparent background, among other properties, for future OHMD-based on-the-go video learning.},
	articleno    = 30,
	numpages     = 27,
	keywords     = {dynamic information, Smart glasses}
}
@article{10.1145/3448116,
	title        = {When Do Drivers Interact with In-Vehicle Well-Being Interventions? An Exploratory Analysis of a Longitudinal Study on Public Roads},
	author       = {Koch, Kevin and Mishra, Varun and Liu, Shu and Berger, Thomas and Fleisch, Elgar and Kotz, David and Wortmann, Felix},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448116},
	url          = {https://doi.org/10.1145/3448116},
	issue_date   = {March 2021},
	abstract     = {Recent developments of novel in-vehicle interventions show the potential to transform the otherwise routine and mundane task of commuting into opportunities to improve the drivers' health and well-being. Prior research has explored the effectiveness of various in-vehicle interventions and has identified moments in which drivers could be interruptible to interventions. All the previous studies, however, were conducted in either simulated or constrained real-world driving scenarios on a pre-determined route. In this paper, we take a step forward and evaluate when drivers interact with in-vehicle interventions in unconstrained free-living conditions.To this end, we conducted a two-month longitudinal study with 10 participants, in which each participant was provided with a study car for their daily driving needs. We delivered two in-vehicle interventions - each aimed at improving affective well-being - and simultaneously recorded the participants' driving behavior. In our analysis, we found that several pre-trip characteristics (like trip length, traffic flow, and vehicle occupancy) and the pre-trip affective state of the participants had significant associations with whether the participants started an intervention or canceled a started intervention. Next, we found that several in-the-moment driving characteristics (like current road type, past average speed, and future brake behavior) showed significant associations with drivers' responsiveness to the intervention. Further, we identified several driving behaviors that "negated" the effectiveness of interventions and highlight the potential of using such "negative" driving characteristics to better inform intervention delivery. Finally, we compared trips with and without intervention and found that both interventions employed in our study did not have a negative effect on driving behavior. Based on our analyses, we provide solid recommendations on how to deliver interventions to maximize responsiveness and effectiveness and minimize the burden on the drivers.},
	articleno    = 19,
	numpages     = 30,
	keywords     = {Interruption; Interaction, Receptivity, In-Vehicle Intervention, Natural Driving, Field Study}
}
@article{10.1145/3448109,
	title        = {ViscoCam: Smartphone-Based Drink Viscosity Control Assistant for Dysphagia Patients},
	author       = {An, Kecheng and Zhang, Qian and Kwong, Elaine},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448109},
	url          = {https://doi.org/10.1145/3448109},
	issue_date   = {March 2021},
	abstract     = {Dysphagia patients need to carefully control their intake liquid's viscosity to reduce choking and aspiration risks. However, accurate liquid viscosity measurement requires expensive rheometers still unavailable in daily life. Though the existing approximate testing methods are low-cost, they are not convenient for everyday use as they require either tedious procedures or dedicated apparatus. This paper presents ViscoCam, the first liquid viscosity classification system for dysphagia patients or carers, which only requires a smartphone. It is easy to operate, widely deployable, and robust for daily use. ViscoCam classifies visually indistinguishable liquid of various viscosity levels by exploiting the fact that the sloshing motion of viscous liquid decays faster than thin liquid. To perform a measurement, the user shakes a cup of liquid and their smartphone to induce the liquid sloshing motion. Then, ViscoCam senses the cup's motion using the smartphone's built-in accelerometer or microphone and infers liquid viscosity from the fluid surface motion captured by flashlight camera. To combat changes in camera position, lighting conditions, and liquid sloshing motion, a 3D convolutional neural network is trained to extract reliable motion features for classification. We evaluate ViscoCam's performance in classifying three levels in the IDDSI standard, which is the most up-to-date and internationally adopted one for dysphagia patients. Results show that ViscoCam achieves an overall accuracy of 96.52% in controlled cases. It is robust to unseen liquid heights or container sizes, and &gt;81% accuracy is maintained under extreme testing cases.},
	articleno    = 3,
	numpages     = 25,
	keywords     = {mobile sensing, ubiquitous computing, dysphagia, liquid viscosity}
}
@article{10.1145/3448103,
	title        = {Unravelling Spatial Privacy Risks of Mobile Mixed Reality Data},
	author       = {Guzman, Jaybie Agullo de and Seneviratne, Aruna and Thilakarathna, Kanchana},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448103},
	url          = {https://doi.org/10.1145/3448103},
	issue_date   = {March 2021},
	abstract     = {Previously, 3D data---particularly, spatial data---have primarily been utilized in the field of geo-spatial analyses, or robot navigation (e.g. self-automated cars) as 3D representations of geographical or terrain data (usually extracted from lidar). Now, with the increasing user adoption of augmented, mixed, and virtual reality (AR/MR/VR; we collectively refer to as MR) technology on user mobile devices, spatial data has become more ubiquitous. However, this ubiquity also opens up a new threat vector for adversaries: aside from the traditional forms of mobile media such as images and video, spatial data poses additional and, potentially, latent risks to users of AR/MR/VR. Thus, in this work, we analyse MR spatial data using various spatial complexity metrics---including a cosine similarity-based, and a Euclidean distance-based metric---as heuristic or empirical measures that can signify the inference risk a captured space has. To demonstrate the risk, we utilise 3D shape recognition and classification algorithms for spatial inference attacks over various 3D spatial data captured using mobile MR platforms: i.e. Microsoft HoloLens, and Android with Google ARCore. Our experimental evaluation and investigation shows that the cosine similarity-based metric is a good spatial complexity measure of captured 3D spatial maps and can be utilised as an indicator of spatial inference risk.},
	articleno    = 14,
	numpages     = 26,
	keywords     = {and resource sharing, mixed or augmented reality, privacy, object detection}
}
@article{10.1145/3448090,
	title        = {Ok Google, What Am I Doing? Acoustic Activity Recognition Bounded by Conversational Assistant Interactions},
	author       = {Adaimi, Rebecca and Yong, Howard and Thomaz, Edison},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448090},
	url          = {https://doi.org/10.1145/3448090},
	issue_date   = {March 2021},
	abstract     = {Conversational assistants in the form of stand-alone devices such as Amazon Echo and Google Home have become popular and embraced by millions of people. By serving as a natural interface to services ranging from home automation to media players, conversational assistants help people perform many tasks with ease, such as setting timers, playing music and managing to-do lists. While these systems offer useful capabilities, they are largely passive and unaware of the human behavioral context in which they are used. In this work, we explore how off-the-shelf conversational assistants can be enhanced with acoustic-based human activity recognition by leveraging the short interval after a voice command is given to the device. Since always-on audio recording can pose privacy concerns, our method is unique in that it does not require capturing and analyzing any audio other than the speech-based interactions between people and their conversational assistants. In particular, we leverage background environmental sounds present in these short duration voice-based interactions to recognize activities of daily living. We conducted a study with 14 participants in 3 different locations in their own homes. We showed that our method can recognize 19 different activities of daily living with average precision of 84.85% and average recall of 85.67% in a leave-one-participant-out performance evaluation with 30-second audio clips bound by the voice interactions.},
	articleno    = 2,
	numpages     = 24,
	keywords     = {Environmental Sounds, Deep Learning, Activities of Daily Living, Voice Assistants, Smart Speaker, Smart Environment, Conversational Assistants, Audio Processing, Human Activity Recognition, Google Home}
}
@article{10.1145/3448076,
	title        = {Urban Map Inference by Pervasive Vehicular Sensing Systems with Complementary Mobility},
	author       = {Fang, Zhihan and Wang, Guang and Xie, Xiaoyang and Zhang, Fan and Zhang, Desheng},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448076},
	url          = {https://doi.org/10.1145/3448076},
	issue_date   = {March 2021},
	abstract     = {Accurate and up-to-date digital road maps are the foundation of many mobile applications, such as navigation and autonomous driving. A manually-created map suffers from the high cost for creation and maintenance due to constant road network updating. Recently, the ubiquity of GPS devices in vehicular systems has led to an unprecedented amount of vehicle sensing data for map inference. Unfortunately, accurate map inference based on vehicle GPS is challenging for two reasons. First, it is challenging to infer complete road structures due to the sensing deviation, sparse coverage, and low sampling rate of GPS of a fleet of vehicles with similar mobility patterns, e.g., taxis. Second, a road map requires various road properties such as road categories, which is challenging to be inferred by just GPS locations of vehicles. In this paper, we design a map inference system called coMap by considering multiple fleets of vehicles with Complementary Mobility Features. coMap has two key components: a graph-based map sketching component, a learning-based map painting component. We implement coMap with the data from four type-aware vehicular sensing systems in one city, which consists of 18 thousand taxis, 10 thousand private vehicles, 6 thousand trucks, and 14 thousand buses. We conduct a comprehensive evaluation of coMap with two state-of-the-art baselines along with ground truth based on OpenStreetMap and a commercial map provider, i.e., Baidu Maps. The results show that (i) for the map sketching, our work improves the performance by 15.9%; (ii) for the map painting, our work achieves 74.58% of average accuracy on road category classification.},
	articleno    = 47,
	numpages     = 24,
	keywords     = {map painting, heterogeneous vehicular fleets, GPS traces, map sketching}
}
@article{10.1145/3432222,
	title        = {CrowdAct: Achieving High-Quality Crowdsourced Datasets in Mobile Activity Recognition},
	author       = {Mairittha, Nattaya and Mairittha, Tittaya and Lago, Paula and Inoue, Sozo},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3432222},
	url          = {https://doi.org/10.1145/3432222},
	issue_date   = {March 2021},
	abstract     = {In this study, we propose novel gamified active learning and inaccuracy detection for crowdsourced data labeling for an activity recognition system using mobile sensing (CrowdAct). First, we exploit active learning to address the lack of accurate information. Second, we present the integration of gamification into active learning to overcome the lack of motivation and sustained engagement. Finally, we introduce an inaccuracy detection algorithm to minimize inaccurate data. To demonstrate the capability and feasibility of the proposed model in realistic settings, we developed and deployed the CrowdAct system to a crowdsourcing platform. For our experimental setup, we recruited 120 diverse workers. Additionally, we gathered 6,549 activity labels from 19 activity classes by using smartphone sensors and user engagement information. We empirically evaluated the quality of CrowdAct by comparing it with a baseline using techniques such as machine learning and descriptive and inferential statistics. Our results indicate that CrowdAct was effective in improving activity accuracy recognition, increasing worker engagement, and reducing inaccurate data in crowdsourced data labeling. Based on our findings, we highlight critical and promising future research directions regarding the design of efficient activity data collection with crowdsourcing.},
	articleno    = 50,
	numpages     = 32,
	keywords     = {gamified active learning, crowdsourced labeling, activity recognition, inaccuracy detection}
}
@article{10.1145/3432700,
	title        = {Uncovering Practical Security and Privacy Threats for Connected Glasses with Embedded Video Cameras},
	author       = {Opaschi, Octav and Vatavu, Radu-Daniel},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432700},
	url          = {https://doi.org/10.1145/3432700},
	issue_date   = {December 2020},
	abstract     = {We address in this work security and privacy threats for connected camera glasses, for which very few investigations have been conducted so far, despite the considerable attention to understanding security concerns for other wearables, such as smartwatches and fitness trackers. To raise awareness about such threats, we present the results of a case study involving a low-cost spy camera glasses device, readily available on the market, that can be used to record and stream live video, for which we demonstrate infringement of several privacy requirements (regarding the camera glasses device itself, the data collected by the embedded video camera, the wearer of the device, and for bystanders as well) that lead to corresponding security threats (e.g., data confidentiality, integrity, availability, and access control). To foster replicability and reproducibility of our empirical results, investigation method, and implementation of attacks, we describe our case study in the form of a detailed activity log and release full C++ code implementing our approach. Furthermore, we present our findings to three IT security experts and summarize their recommendations for designing more secure connected camera glasses.},
	articleno    = 167,
	numpages     = 26,
	keywords     = {Wi-Fi, Connected camera glasses, Spy cameras, Recommendations, Attacks, Security, Video, Privacy, Case study, Smartglasses}
}
@article{10.1145/3432227,
	title        = {Deriving Effective Human Activity Recognition Systems through Objective Task Complexity Assessment},
	author       = {Hiremath, Shruthi K. and Pl\"{o}tz, Thomas},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432227},
	url          = {https://doi.org/10.1145/3432227},
	issue_date   = {December 2020},
	abstract     = {Research in sensor based human activity recognition (HAR) has been a core concern of the mobile and ubiquitous computing community. Sophisticated systems have been developed with the main view on applications of HAR methods in research settings. This work addresses a related yet practically different problem that mainly focuses on users of HAR technology. We acknowledge that practitioners from outside the core HAR research community are motivated to employ HAR methods for practical deployments. Even though standard processing approaches exist, arguably, often times substantial modifications are necessary to derive effective analysis systems. It is not always clear a-priori how challenging a HAR task actually is and what dimensions of an analysis pipeline are crucial for successful automated assessments. In practice this can lead to disappointing results or disproportionate efforts that have to be invested into the optimization of data analysis pipelines, that were supposed to work "out of the box". We present a framework for the objective complexity assessment of HAR tasks that directly supports practitioners' decision making of whether and how to employ HAR for their deployments. We map a HAR task onto a vectorial representation that allows us to analyse the inherent challenges of the task and to draw conclusions through similarity analysis with regards to existing tasks. We validate our complexity assessment framework on 23 HAR datasets and derive a data-driven categorization of human activity recognition. We demonstrate how our objective analysis can be used to inform the deployment of HAR systems in practical scenarios.},
	articleno    = 131,
	numpages     = 24,
	keywords     = {data complexity, pattern recognition, human activity recognition}
}
@article{10.1145/3432217,
	title        = {BlinKey: A Two-Factor User Authentication Method for Virtual Reality Devices},
	author       = {Zhu, Huadi and Jin, Wenqiang and Xiao, Mingyan and Murali, Srinivasan and Li, Ming},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432217},
	url          = {https://doi.org/10.1145/3432217},
	issue_date   = {December 2020},
	abstract     = {Virtual Reality (VR) has shown promising potentials in many applications, such as e-business, healthcare, and social networking. Rich information regarding user's activities and their online accounts is stored in VR devices. If it is carelessly unattended, then attackers, including insiders, can make use of the stored information to, for example, perform in-app purchases at the legitimate owner's expenses. Current solutions, mostly following schemes designed for general personal devices, have been proved vulnerable to shoulder-surfing attacks due to the sight blocking caused by the headset. Although there have been efforts trying to fill this gap, they either rely on some highly advanced equipment, such as electrodes to read brainwaves, or introduce heavy cognitive load that has users perform a series of cumbersome authentication tasks. Therefore, an authentication method for VR devices that is robust and convenient is in dire need.In this paper, we present the design, implementation, and evaluation of a two-factor user authentication scheme, BlinKey, for VR devices that are equipped with an eye tracker. A user's secret passcode is a set of recorded rhythms when he/she blinks, together with the unique pupil size variation pattern. We call this passcode as a blinkey, which can be jointly characterized by knowledge-based and biometric features. To examine the performances, BlinKey is implemented on an HTC Vive Pro with a Pupil Labs eye tracker. Through extensive experimental evaluations with 52 participants, we show that our scheme can achieve the average EER as low as 4.0% with only 6 training samples. Besides, it is robust against various types of attacks. BlinKey also exhibits satisfactory usability in terms of login attempts, memorability, and impact of user motions. We also carry out questionnaire-based pre-/post-studies. The survey result indicates that BlinKey is well accepted as a user authentication scheme for VR devices.},
	articleno    = 164,
	numpages     = 29,
	keywords     = {pupil size variation, Two-factor authentication, blinking rhythm, VR device}
}
@article{10.1145/3432216,
	title        = {Acoussist: An Acoustic Assisting Tool for People with Visual Impairments to Cross Uncontrolled Streets},
	author       = {Jin, Wenqiang and Xiao, Mingyan and Zhu, Huadi and Deb, Shuchisnigdha and Kan, Chen and Li, Ming},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432216},
	url          = {https://doi.org/10.1145/3432216},
	issue_date   = {December 2020},
	abstract     = {To cross uncontrolled roadways, where no traffic-halting signal devices are present, pedestrians with visual impairments must rely on their other senses to detect oncoming vehicles and estimate the correct crossing interval in order to avoid potentially fatal collisions. To overcome the limitations of human auditory performance, which can be particularly impacted by weather or background noise, we develop an assisting tool called Acoussist, which relies on acoustic ranging to provide an additional layer of protection for pedestrian safety. The vision impaired can use the tool to double-confirm surrounding traffic conditions before they proceed through a non-signaled crosswalk.The Acoussist tool is composed of vehicle-mounted external speakers that emit acoustic chirps at a frequency range imperceptible by human ears, but detectable by smartphones operating the Acoussist app. This app would then communicate to the user when it is safe to cross the roadway. Several challenges exist when applying the acoustic ranging to traffic detection, including measuring multiple vehicles' instant velocities and directions with the presence many of them who emit homogeneous signals simultaneously. We address these challenges by leveraging insights from formal analysis on received signals' time-frequency (t-f) profiles. We implement a proof-of-concept of Acoussist using commercial off-the-shelf (COTS) portable speakers and smartphones. Extensive in-field experiments have been conducted to validate the effectiveness of Acoussist in improving mobility for people with visual impairments.},
	articleno    = 133,
	numpages     = 30,
	keywords     = {collision avoidance, Pedestrian safety, acoustic ranging}
}
@article{10.1145/3432212,
	title        = {Toward Lightweight In-Situ Self-Reporting: An Exploratory Study of Alternative Smartwatch Interface Designs in Context},
	author       = {Yan, Xinghui and Raj, Shriti and Huang, Bingjian and Park, Sun Young and Newman, Mark W.},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432212},
	url          = {https://doi.org/10.1145/3432212},
	issue_date   = {December 2020},
	abstract     = {In-situ self-reporting is an important measurement method used for capturing daily experience data right-in-the-moment in dynamic contexts. Research has been conducted to reduce the demand placed on users for manually reporting data in context. In this regard, smartwatches offer inherent benefits for making self-reporting more convenient and facilitate data gathering. However, self-reporting on the small touchscreen under various contextual conditions can be burdensome and challenging. In this study, to gain insights into designing smartwatch-based self-report interfaces, we conducted an exploratory user study with eight design probes and twenty-four participants under three simulated scenarios: walking, gaming, and social chatting. Findings showed that users' subjective perception of interface features (e.g., input methods and option layouts) varied with changes in context. Participants leveraged different features (e.g., hierarchical layout and discrete input) to micro-schedule self-report tasks (i.e., create one or multiple opportune moments) or to conduct eyes-free interaction with the assistance of smartwatch attributes (e.g., the physical frame of a smartwatch). We discuss implications for smartwatch-based self-report interface designs by considering context and designing interface features to support users' coping strategies.},
	articleno    = 158,
	numpages     = 22,
	keywords     = {In-situ self-reporting, interface design, exploratory study, smartwatch input}
}
@article{10.1145/3534611,
	title        = {DeepPCD: Enabling AutoCompletion of Indoor Point Clouds with Deep Learning},
	author       = {Cai, Pingping and Sur, Sanjib},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534611},
	url          = {https://doi.org/10.1145/3534611},
	issue_date   = {July 2022},
	abstract     = {3D Point Cloud Data (PCD) is an efficient machine representation for surrounding environments and has been used in many applications. But the measured PCD is often incomplete and sparse due to the sensor occlusion and poor lighting conditions. To automatically reconstruct complete PCD from the incomplete ones, we propose DeepPCD, a deep-learning-based system that reconstructs both geometric and color information for large indoor environments. For geometric reconstruction, DeepPCD uses a novel patch based technique that splits the PCD into multiple parts, approximates, extends, and independently reconstructs the parts by 3D planes, and then merges and refines them. For color reconstruction, DeepPCD uses a conditional Generative Adversarial Network to infer the missing color of the geometrically reconstructed PCD by using the color feature extracted from incomplete color PCD. We experimentally evaluate DeepPCD with several real PCD collected from large, diverse indoor environments and explore the feasibility of PCD autocompletion in various ubiquitous sensing applications.},
	articleno    = 43,
	numpages     = 29,
	keywords     = {Generative Adversarial Networks, Point Cloud Data, Graph Convolutions, Vision Transformer}
}
@article{10.1145/3534604,
	title        = {StudentSADD: Rapid Mobile Depression and Suicidal Ideation Screening of College Students during the Coronavirus Pandemic},
	author       = {Tlachac, ML and Flores, Ricardo and Reisch, Miranda and Kayastha, Rimsha and Taurich, Nina and Melican, Veronica and Bruneau, Connor and Caouette, Hunter and Lovering, Joshua and Toto, Ermal and Rundensteiner, Elke A.},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534604},
	url          = {https://doi.org/10.1145/3534604},
	issue_date   = {July 2022},
	abstract     = {The growing prevalence of depression and suicidal ideation among college students further exacerbated by the Coronavirus pandemic is alarming, highlighting the need for universal mental illness screening technology. With traditional screening questionnaires too burdensome to achieve universal screening in this population, data collected through mobile applications has the potential to rapidly identify at-risk students. While prior research has mostly focused on collecting passive smartphone modalities from students, smartphone sensors are also capable of capturing active modalities. The general public has demonstrated more willingness to share active than passive modalities through an app, yet no such dataset of active mobile modalities for mental illness screening exists for students. Knowing which active modalities hold strong screening capabilities for student populations is critical for developing targeted mental illness screening technology. Thus, we deployed a mobile application to over 300 students during the COVID-19 pandemic to collect the Student Suicidal Ideation and Depression Detection (StudentSADD) dataset. We report on a rich variety of machine learning models including cutting-edge multimodal pretrained deep learning classifiers on active text and voice replies to screen for depression and suicidal ideation. This unique StudentSADD dataset is a valuable resource for the community for developing mobile mental illness screening tools.},
	articleno    = 76,
	numpages     = 32,
	keywords     = {mental health assessment, mobile health, digital phenotype, voice recordings}
}
@article{10.1145/3534586,
	title        = {Auritus: An Open-Source Optimization Toolkit for Training and Development of Human Movement Models and Filters Using Earables},
	author       = {Saha, Swapnil Sayan and Sandha, Sandeep Singh and Pei, Siyou and Jain, Vivek and Wang, Ziqi and Li, Yuchen and Sarker, Ankur and Srivastava, Mani},
	year         = 2022,
	month        = {jul},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 2,
	doi          = {10.1145/3534586},
	url          = {https://doi.org/10.1145/3534586},
	issue_date   = {July 2022},
	abstract     = {Smart ear-worn devices (called earables) are being equipped with various onboard sensors and algorithms, transforming earphones from simple audio transducers to multi-modal interfaces making rich inferences about human motion and vital signals. However, developing sensory applications using earables is currently quite cumbersome with several barriers in the way. First, time-series data from earable sensors incorporate information about physical phenomena in complex settings, requiring machine-learning (ML) models learned from large-scale labeled data. This is challenging in the context of earables because large-scale open-source datasets are missing. Secondly, the small size and compute constraints of earable devices make on-device integration of many existing algorithms for tasks such as human activity and head-pose estimation difficult. To address these challenges, we introduce Auritus, an extendable and open-source optimization toolkit designed to enhance and replicate earable applications. Auritus serves two primary functions. Firstly, Auritus handles data collection, pre-processing, and labeling tasks for creating customized earable datasets using graphical tools. The system includes an open-source dataset with 2.43 million inertial samples related to head and full-body movements, consisting of 34 head poses and 9 activities from 45 volunteers. Secondly, Auritus provides a tightly-integrated hardware-in-the-loop (HIL) optimizer and TinyML interface to develop lightweight and real-time machine-learning (ML) models for activity detection and filters for head-pose tracking. To validate the utlity of Auritus, we showcase three sample applications, namely fall detection, spatial audio rendering, and augmented reality (AR) interfacing. Auritus recognizes activities with 91% leave 1-out test accuracy (98% test accuracy) using real-time models as small as 6-13 kB. Our models are 98-740x smaller and 3-6% more accurate over the state-of-the-art. We also estimate head pose with absolute errors as low as 5 degrees using 20kB filters, achieving up to 1.6x precision improvement over existing techniques. We make the entire system open-source so that researchers and developers can contribute to any layer of the system or rapidly prototype their applications using our dataset and algorithms.},
	articleno    = 70,
	numpages     = 34,
	keywords     = {neural networks, head-pose, TinyML, machine learning, earable, network architecture search, hardware-in-the-loop, datasets, optimization, human activity, filters}
}
@article{10.1145/3517254,
	title        = {AEROKEY: Using Ambient Electromagnetic Radiation for Secure and Usable Wireless Device Authentication},
	author       = {Lee, Kyuin and Yang, Yucheng and Prabhune, Omkar and Chithra, Aishwarya Lekshmi and West, Jack and Fawaz, Kassem and Klingensmith, Neil and Banerjee, Suman and Kim, Younghyun},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517254},
	url          = {https://doi.org/10.1145/3517254},
	issue_date   = {March 2022},
	abstract     = {Wireless connectivity is becoming common in increasingly diverse personal devices, enabling various interoperation- and Internet-based applications and services. More and more interconnected devices are simultaneously operated by a single user with short-lived connections, making usable device authentication methods imperative to ensure both high security and seamless user experience. Unfortunately, current authentication methods that heavily require human involvement, in addition to form factor and mobility constraints, make this balance hard to achieve, often forcing users to choose between security and convenience. In this work, we present a novel over-the-air device authentication scheme named AEROKEY that achieves both high security and high usability. With virtually no hardware overhead, AEROKEY leverages ubiquitously observable ambient electromagnetic radiation to autonomously generate spatiotemporally unique secret that can be derived only by devices that are closely located to each other. Devices can make use of this unique secret to form the basis of a symmetric key, making the authentication procedure more practical, secure and usable with no active human involvement. We propose and implement essential techniques to overcome challenges in realizing AEROKEY on low-cost microcontroller units, such as poor time synchronization, lack of precision analog front-end, and inconsistent sampling rates. Our real-world experiments demonstrate reliable authentication as well as its robustness against various realistic adversaries with low equal-error rates of 3.4% or less and usable authentication time of as low as 24 s.},
	articleno    = 20,
	numpages     = 29,
	keywords     = {EMR-based authentication, device authentication, key generation}
}
@article{10.1145/3517253,
	title        = {LASense: Pushing the Limits of Fine-Grained Activity Sensing Using Acoustic Signals},
	author       = {Li, Dong and Liu, Jialin and Lee, Sunghoon Ivan and Xiong, Jie},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517253},
	url          = {https://doi.org/10.1145/3517253},
	issue_date   = {March 2022},
	abstract     = {Acoustic signals have been widely adopted in sensing fine-grained human activities, including respiration monitoring, finger tracking, eye blink detection, etc. One major challenge for acoustic sensing is the extremely limited sensing range, which becomes even more severe when sensing fine-grained activities. Different from the prior efforts that adopt multiple microphones and/or advanced deep learning techniques for long sensing range, we propose a system called LASense, which can significantly increase the sensing range for fine-grained human activities using a single pair of speaker and microphone. To achieve this, LASense introduces a virtual transceiver idea that purely leverages delicate signal processing techniques in software. To demonstrate the effectiveness of LASense, we apply the proposed approach to three fine-grained human activities, i.e., respiration, finger tapping and eye blink. For respiration monitoring, we significantly increase the sensing range from the state-of-the-art 2 m to 6 m. For finer-grained finger tapping and eye blink detection, we increase the state-of-the-art sensing range by 150% and 80%, respectively.},
	articleno    = 21,
	numpages     = 27,
	keywords     = {contact-free sensing, fine-grained activity sensing, long-range acoustic sensing}
}
@article{10.1145/3517251,
	title        = {Layer by Layer, Patterned Valves Enable Programmable Soft Surfaces},
	author       = {Gonzalez, Jesse T. and Hudson, Scott E.},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517251},
	url          = {https://doi.org/10.1145/3517251},
	issue_date   = {March 2022},
	abstract     = {Programmable surfaces, which can be instructed to alter their shape or texture, may one day serve as a platform for tangible interfaces and adaptive environments. But so far, these structures have been constrained in scale by a challenging fabrication process, as the numerous constituent actuators must be built and assembled individually. We look towards emerging trends in mechanical engineering and consider an alternate framework --- layer-driven design, which enables the production of dynamic, discretely-actuated surfaces at multiple scales. By centering the construction around patterning and stacking, forgoing individual assembly in favor of bulk processes such as photo-etching and laser cutting, we avoid the need for multiple manufacturing steps that are repeated for each of the many actuators that compose the surface. As an instance of this layer-driven model, we build an array of electrostatic valves, and use this composite material (which we refer to as Stoma-Board) to drive four types of pneumatic transducers. We also show how this technique may be readily industrialized, through integration with the highly mature and automated manufacturing processes of modern electronics.},
	articleno    = 12,
	numpages     = 25,
	keywords     = {soft robotics, metamaterials, tangible interfaces, programmable materials}
}
@article{10.1145/3517242,
	title        = {Ultra Low-Latency Backscatter for Fast-Moving Location Tracking},
	author       = {Wang, Jingxian and Ranganathan, Vaishnavi and Lester, Jonathan and Kumar, Swarun},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517242},
	url          = {https://doi.org/10.1145/3517242},
	issue_date   = {March 2022},
	abstract     = {This paper explores building an ultra-low latency and high-accuracy location tracking solution using battery-free tags. While there is rich prior work on location tracking with battery-free RFID tags and backscatter devices, these systems typically face tradeoffs with accuracy, power consumption, and latency. Such limitations make these existing solutions unsuitable for emerging applications like industrial augmented reality which requires tracking fast-moving machinery; monitoring indoor sports activities that require real-time tracking of fast-moving objects with high precision and under stringent latency constraints.We propose and demonstrate FastLoc, a precision tracking system that locates tiny, battery-free analog backscatter tags at sub-millisecond latency and sub-centimeter accuracy. FastLoc is a hybrid system that simultaneously uses RF and optical signals to track tiny tags that can be attached to everyday objects. FastLoc leverages the RF channel responses from tags for estimating the coarse region where the tags may be located. It simultaneously uses the sensed optical information modulated on the backscatter signals to enable fine-grained location estimation within the coarse region. To achieve this, we design and fabricate a custom analog tag that consumes less than 150 uW and instantaneously converts incident optical signals to one-shot wideband harmonic RF responses at nanosecond latency. We then develop a static high-density distributed-frequency structured light pattern that can localize tags in the area of interest at a sub-centimeter accuracy and microsecond-scale latency. A detailed experimental evaluation of FastLoc shows a median accuracy of 0.7 cm in tag localization with a 0.51 ms effective localization latency.},
	articleno    = 30,
	numpages     = 22
}
@article{10.1145/3517236,
	title        = {Battery-Free MakeCode: Accessible Programming for Intermittent Computing},
	author       = {Kraemer, Christopher and Guo, Amy and Ahmed, Saad and Hester, Josiah},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517236},
	url          = {https://doi.org/10.1145/3517236},
	issue_date   = {March 2022},
	abstract     = {Hands-on computing has emerged as an exciting and accessible way to learn about computing and engineering in the physical world for students and makers of all ages. Current end-to-end approaches like Microsoft MakeCode require tethered or battery-powered devices like a micro:bit, limiting usefulness and applicability, as well as abdicating responsibility for teaching sustainable practices. Unfortunately, energy harvesting computing devices are usually only programmable by experts and require significant supporting toolchains and knowledge across multiple engineering and computing disciplines to work effectively. This paper bridges the gap between sustainable computing efforts, the maker movement, and novice-focused programming environments with MakeCode-Iceberg, a set of compiler extensions to Microsoft's open-source MakeCode project. The extensions automatically and invisibly transform user code in any language supported (Blocks, JavaScript, Python)into a version that can safely and correctly execute across intermittent power failures caused by unreliable energy harvesting. Determining where, when, and what to save in a checkpoint on limited energy, time, and hardware budget is challenging. We leverage the unique intermediate representation of the MakeCode source-to-source compiler to design and deploy various checkpointing techniques. Our approach allows us to provide, for the first time, a fully web-based and toolchain-free environment to program intermittent computing devices, making battery-free operation accessible to all. We demonstrate new use cases with multiple energy harvesters, peripherals, and application domains: including a Smart Terrarium, Step Counter, and Combination Lock. MakeCode-Iceberg provides sustainable hands-on computing opportunities to a broad audience of makers and learners, democratizing access to energy harvesting and battery-free embedded systems.},
	articleno    = 18,
	numpages     = 35,
	keywords     = {Battery-free, Block based programming, Energy Harvesting, Intermittent Computing}
}
@article{10.1145/3517233,
	title        = {Revisiting Reflection in HCI: Four Design Resources for Technologies That Support Reflection},
	author       = {Bentvelzen, Marit and Wo\'{z}niak, Pawe\l{} W. and Herbes, Pia S.F. and Stefanidi, Evropi and Niess, Jasmin},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517233},
	url          = {https://doi.org/10.1145/3517233},
	issue_date   = {March 2022},
	abstract     = {Reflection is a commonly addressed design goal in commercial systems and in Human-Computer Interaction (HCI) research. Yet, it is still unclear what tools are at the disposal of designers who want to build systems that support reflection. Understanding the design space of reflection support systems and the interaction techniques that can foster reflection is necessary to enable building technologies that contribute to the users' well-being. In order to gain additional insight into how interactive artefacts foster reflection, we investigated past research prototypes and reflection-supporting smartphone applications (apps). Through a structured literature review and an analysis of app reviews, we constructed four design resources for reflection: temporal perspective, conversation, comparison and discovery. We also identified design patterns in past digital artefacts that implement the resources. Our work constitutes intermediate-level knowledge that is intended to inspire future technologies that better support reflection.},
	articleno    = 2,
	numpages     = 27,
	keywords     = {technology-supported reflection, systematic review, artefacts}
}
@article{10.1145/3517228,
	title        = {Toward Understanding Playful Beverage-Based Gustosonic Experiences},
	author       = {Wang, Yan and Li, Zhuying and Khot, Rohit Ashok and Mueller, Florian 'Floyd'},
	year         = 2022,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 6,
	number       = 1,
	doi          = {10.1145/3517228},
	url          = {https://doi.org/10.1145/3517228},
	issue_date   = {March 2022},
	abstract     = {Recent advances in interactive technology are being used to enrich our interactions around food and drinks. Making use of sound to enrich dining - providing "gustosonic" experiences - has recently garnered interest as an exciting area of ubiquitous computing. However, these experiences have tended to focus on eating. In response to the lack of drinking-focused experiences, we explore the combining playful design and drinking activities so as to allow users to experience playful personalized sounds via drinking through "Sonic Straws". We present the findings of an in-the-wild study that highlights how our system supported self-expression via playful drinking actions, facilitated pleasurable social drinking moments, and promoted reflection on drinking practices. We discuss the implications of this work for designers of future gustosonic experiences, including how to amplify entertainment experiences around drinking/eating, how to highlight the joy coming from multisensory experiences, and how to facilitate mindful engagement with what one drinks. Ultimately, we aim to contribute to the enrichment of dining experiences through playful design.},
	articleno    = 33,
	numpages     = 23,
	keywords     = {Gustosonic experiences, Food play, Human-Food Interaction, Sound}
}
@article{10.1145/3494980,
	title        = {Understanding and Supporting Self-Tracking App Selection},
	author       = {Lee, Jong Ho and Schroeder, Jessica and Epstein, Daniel A.},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494980},
	url          = {https://doi.org/10.1145/3494980},
	issue_date   = {Dec 2021},
	abstract     = {People often face barriers to selecting self-tracking tools that support their goals and needs, resulting in tools not meeting their expectations and ultimately abandonment. We therefore examine how people approach selecting self-tracking apps and investigate how technology can better support the process. Drawing on past literature on how people select and perceive the features of commercial and research tracking tools, we surface seven attributes people consider during selection, and design a low-fidelity prototype of an app store that highlights these attributes. We then conduct semi-structured interviews with 18 participants to further investigate what people consider during selection, how people select self-tracking apps, and how surfacing tracking-related attributes could better support selection. We find that people often prioritize features related to self-tracking during selection, such as approaches to collecting and reflecting on data, and trial apps to determine whether they would suit their needs. Our results also show potential for technology surfacing how apps support tracking to reduce barriers to selection. We discuss future opportunities for improving self-tracking app selection, such as ways to enhance existing self-tracking app distribution platforms to enable people to filter and search apps by desirable features.},
	articleno    = 166,
	numpages     = 25,
	keywords     = {Selection, App Stores, Quantified Self, Personal Informatics, Self-Tracking}
}
@article{10.1145/3494966,
	title        = {DistFL: Distribution-Aware Federated Learning for Mobile Scenarios},
	author       = {Liu, Bingyan and Cai, Yifeng and Zhang, Ziqi and Li, Yuanchun and Wang, Leye and Li, Ding and Guo, Yao and Chen, Xiangqun},
	year         = 2022,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 4,
	doi          = {10.1145/3494966},
	url          = {https://doi.org/10.1145/3494966},
	issue_date   = {Dec 2021},
	abstract     = {Federated learning (FL) has emerged as an effective solution to decentralized and privacy-preserving machine learning for mobile clients. While traditional FL has demonstrated its superiority, it ignores the non-iid (independently identically distributed) situation, which widely exists in mobile scenarios. Failing to handle non-iid situations could cause problems such as performance decreasing and possible attacks. Previous studies focus on the "symptoms" directly, as they try to improve the accuracy or detect possible attacks by adding extra steps to conventional FL models. However, previous techniques overlook the root causes for the "symptoms": blindly aggregating models with the non-iid distributions. In this paper, we try to fundamentally address the issue by decomposing the overall non-iid situation into several iid clusters and conducting aggregation in each cluster. Specifically, we propose DistFL, a novel framework to achieve automated and accurate Distribution-aware Federated Learning in a cost-efficient way. DistFL achieves clustering via extracting and comparing the distribution knowledge from the uploaded models. With this framework, we are able to generate multiple personalized models with distinctive distributions and assign them to the corresponding clients. Extensive experiments on mobile scenarios with popular model architectures have demonstrated the effectiveness of DistFL.},
	articleno    = 168,
	numpages     = 26,
	keywords     = {privacy, neural networks, federated learning, distribution knowledge}
}
@article{10.1145/3478125,
	title        = {Learning When Agents Can Talk to Drivers Using the INAGT Dataset and Multisensor Fusion},
	author       = {Wu, Tong and Martelaro, Nikolas and Stent, Simon and Ortiz, Jorge and Ju, Wendy},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478125},
	url          = {https://doi.org/10.1145/3478125},
	issue_date   = {Sept 2021},
	abstract     = {This paper examines sensor fusion techniques for modeling opportunities for proactive speech-based in-car interfaces. We leverage the Is Now a Good Time (INAGT) dataset, which consists of automotive, physiological, and visual data collected from drivers who self-annotated responses to the question "Is now a good time?," indicating the opportunity to receive non-driving information during a 50-minute drive. We augment this original driver-annotated data with third-party annotations of perceived safety, in order to explore potential driver overconfidence. We show that fusing automotive, physiological, and visual data allows us to predict driver labels of availability, achieving an 0.874 F1-score by extracting statistically relevant features and training with our proposed deep neural network, PazNet. Using the same data and network, we achieve an 0.891 F1-score for predicting third-party labeled safe moments. We train these models to avoid false positives---determinations that it is a good time to interrupt when it is not---since false positives may cause driver distraction or service deactivation by the driver. Our analyses show that conservative models still leave many moments for interaction and show that most inopportune moments are short. This work lays a foundation for using sensor fusion models to predict when proactive speech systems should engage with drivers.},
	articleno    = 133,
	numpages     = 28,
	keywords     = {deep convolutional network, dataset, multi-modal learning, vehicle, interaction timing}
}
@article{10.1145/3478123,
	title        = {BreathTrack: Detecting Regular Breathing Phases from Unannotated Acoustic Data Captured by a Smartphone},
	author       = {Islam, Bashima and Rahman, Md Mahbubur and Ahmed, Tousif and Ahmed, Mohsin Yusuf and Hasan, Md Mehedi and Nathan, Viswam and Vatanparvar, Korosh and Nemati, Ebrahim and Kuang, Jilong and Gao, Jun Alex},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478123},
	url          = {https://doi.org/10.1145/3478123},
	issue_date   = {Sept 2021},
	abstract     = {Breathing biomarkers, such as breathing rate, fractional inspiratory time, and inhalation-exhalation ratio, are vital for monitoring the user's health and well-being. Accurate estimation of such biomarkers requires breathing phase detection, i.e., inhalation and exhalation. However, traditional breathing phase monitoring relies on uncomfortable equipment, e.g., chestbands. Smartphone acoustic sensors have shown promising results for passive breathing monitoring during sleep or guided breathing. However, detecting breathing phases using acoustic data can be challenging for various reasons. One of the major obstacles is the complexity of annotating breathing sounds due to inaudible parts in regular breathing and background noises. This paper assesses the potential of using smartphone acoustic sensors for passive unguided breathing phase monitoring in a natural environment. We address the annotation challenges by developing a novel variant of the teacher-student training method for transferring knowledge from an inertial sensor to an acoustic sensor, eliminating the need for manual breathing sound annotation by fusing signal processing with deep learning techniques. We train and evaluate our model on the breathing data collected from 131 subjects, including healthy individuals and respiratory patients. Experimental results show that our model can detect breathing phases with 77.33% accuracy using acoustic sensors. We further present an example use-case of breathing phase-detection by first estimating the biomarkers from the estimated breathing phases and then using these biomarkers for pulmonary patient detection. Using the detected breathing phases, we can estimate fractional inspiratory time with 92.08% accuracy, the inhalation-exhalation ratio with 86.76% accuracy, and the breathing rate with 91.74% accuracy. Moreover, we can distinguish respiratory patients from healthy individuals with up to 76% accuracy. This paper is the first to show the feasibility of detecting regular breathing phases towards passively monitoring respiratory health and well-being using acoustic data captured by a smartphone.},
	articleno    = 124,
	numpages     = 22,
	keywords     = {Teacher-Student Model, Breathing, Smartphone, Audio, Respiratory Diseases}
}
@article{10.1145/3478107,
	title        = {OpiTrack: A Wearable-Based Clinical Opioid Use Tracker with Temporal Convolutional Attention Networks},
	author       = {Gullapalli, Bhanu Teja and Carreiro, Stephanie and Chapman, Brittany P. and Ganesan, Deepak and Sjoquist, Jan and Rahman, Tauhidur},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478107},
	url          = {https://doi.org/10.1145/3478107},
	issue_date   = {Sept 2021},
	abstract     = {Opioid use disorder is a medical condition with major social and economic consequences. While ubiquitous physiological sensing technologies have been widely adopted and extensively used to monitor day-to-day activities and deliver targeted interventions to improve human health, the use of these technologies to detect drug use in natural environments has been largely underexplored. The long-term goal of our work is to develop a mobile technology system that can identify high-risk opioid-related events (i.e., development of tolerance in the setting of prescription opioid use, return-to-use events in the setting of opioid use disorder) and deploy just-in-time interventions to mitigate the risk of overdose morbidity and mortality. In the current paper, we take an initial step by asking a crucial question: Can opioid use be detected using physiological signals obtained from a wrist-mounted sensor? Thirty-six individuals who were admitted to the hospital for an acute painful condition and received opioid analgesics as part of their clinical care were enrolled. Subjects wore a noninvasive wrist sensor during this time (1-14 days) that continuously measured physiological signals (heart rate, skin temperature, accelerometry, electrodermal activity, and interbeat interval). We collected a total of 2070 hours (≈ 86 days) of physiological data and observed a total of 339 opioid administrations. Our results are encouraging and show that using a Channel-Temporal Attention TCN (CTA-TCN) model, we can detect an opioid administration in a time-window with an F1-score of 0.80, a specificity of 0.77, sensitivity of 0.80, and an AUC of 0.77. We also predict the exact moment of administration in this time-window with a normalized mean absolute error of 8.6% and R2 coefficient of 0.85.},
	articleno    = 102,
	numpages     = 29,
	keywords     = {Opioid administration, Depthwise convolutions, Channel and Temporal Attention, Physiological signal, Temporal convolutional network}
}
@article{10.1145/3478089,
	title        = {Model-Based Head Orientation Estimation for Smart Devices},
	author       = {Yang, Qiang and Zheng, Yuanqing},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478089},
	url          = {https://doi.org/10.1145/3478089},
	issue_date   = {Sept 2021},
	abstract     = {Voice interaction is friendly and convenient for users. Smart devices such as Amazon Echo allow users to interact with them by voice commands and become increasingly popular in our daily life. In recent years, research works focus on using the microphone array built in smart devices to localize the user's position, which adds additional context information to voice commands. In contrast, few works explore the user's head orientation, which also contains useful context information. For example, when a user says, "turn on the light", the head orientation could infer which light the user is referring to. Existing model-based works require a large number of microphone arrays to form an array network, while machine learning-based approaches need laborious data collection and training workload. The high deployment/usage cost of these methods is unfriendly to users. In this paper, we propose HOE, a model-based system that enables Head Orientation Estimation for smart devices with only two microphone arrays, which requires a lower training overhead than previous approaches. HOE first estimates the user's head orientation candidates by measuring the voice energy radiation pattern. Then, the voice frequency radiation pattern is leveraged to obtain the final result. Real-world experiments are conducted, and the results show that HOE can achieve a median estimation error of 23 degrees. To the best of our knowledge, HOE is the first model-based attempt to estimate the head orientation by only two microphone arrays without the arduous data training overhead.},
	articleno    = 136,
	numpages     = 24,
	keywords     = {head orientation, smart devices, acoustic sensing}
}
@article{10.1145/3478088,
	title        = {Self-Supervised Learning for Reading Activity Classification},
	author       = {Islam, Md. Rabiul and Sakamoto, Shuji and Yamada, Yoshihiro and Vargo, Andrew W. and Iwata, Motoi and Iwamura, Masakazu and Kise, Koichi},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478088},
	url          = {https://doi.org/10.1145/3478088},
	issue_date   = {Sept 2021},
	abstract     = {Reading analysis can relay information about user's confidence and habits and can be used to construct useful feedback. A lack of labeled data inhibits the effective application of fully-supervised Deep Learning (DL) for automatic reading analysis. We propose a Self-supervised Learning (SSL) method for reading analysis. Previously, SSL has been effective in physical human activity recognition (HAR) tasks, but it has not been applied to cognitive HAR tasks like reading. We first evaluate the proposed method on a four-class classification task on reading detection using electrooculography datasets, followed by an evaluation of a two-class classification task of confidence estimation on multiple-choice questions using eye-tracking datasets. Fully-supervised DL and support vector machines (SVMs) are used as comparisons for the proposed SSL method. The results show that the proposed SSL method is superior to the fully-supervised DL and SVM for both tasks, especially when training data is scarce. This result indicates the proposed method is the superior choice for reading analysis tasks. These results are important for informing the design of automatic reading analysis platforms.},
	articleno    = 105,
	numpages     = 22,
	keywords     = {fully-supervised deep learning, confidence estimation, reading analysis, reading detection, Self-supervised learning}
}
@article{10.1145/3478080,
	title        = {Pushing the Limits of Long Range Wireless Sensing with LoRa},
	author       = {Xie, Binbin and Yin, Yuqing and Xiong, Jie},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478080},
	url          = {https://doi.org/10.1145/3478080},
	issue_date   = {Sept 2021},
	abstract     = {Wireless sensing is an exciting new research area which enables a large variety of applications ranging from coarse-grained daily activity recognition to fine-grained vital sign monitoring. While promising in many aspects, one critical issue is the limited sensing range because weak reflection signals are used for sensing. Recently, LoRa signals are exploited for wireless sensing, moving a big step towards long-range sensing. Although promising, there is still a huge room for improvement. In this work, we qualitatively characterize the relationship between target movements and target-induced signal variations, and propose signal processing methods to enlarge the induced signal variation to achieve a longer sensing range. Experiment results show that the proposed system (1) pushes the contact-free sensing range of human walking from the state-of-the-art 50 m to 120 m; (2) achieves a sensing range of 75 m for fine-grained respiration sensing; and (3) demonstrates human respiration sensing even through seven concrete walls.},
	articleno    = 134,
	numpages     = 21,
	keywords     = {Contact-free, LoRa sensing, Through-wall sensing, Long range sensing}
}
@article{10.1145/3478075,
	title        = {FG-LiquID: A Contact-Less Fine-Grained Liquid Identifier by Pushing the Limits of Millimeter-Wave Sensing},
	author       = {Liang, Yumeng and Zhou, Anfu and Zhang, Huanhuan and Wen, Xinzhe and Ma, Huadong},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478075},
	url          = {https://doi.org/10.1145/3478075},
	issue_date   = {Sept 2021},
	abstract     = {Contact-less liquid identification via wireless sensing has diverse potential applications in our daily life, such as identifying alcohol content in liquids, distinguishing spoiled and fresh milk, and even detecting water contamination. Recent works have verified the feasibility of utilizing mmWave radar to perform coarse-grained material identification, e.g., discriminating liquid and carpet. However, they do not fully exploit the sensing limits of mmWave in terms of fine-grained material classification. In this paper, we propose FG-LiquID, an accurate and robust system for fine-grained liquid identification. To achieve the desired fine granularity, FG-LiquID first focuses on the small but informative region of the mmWave spectrum, so as to extract the most discriminative features of liquids. Then we design a novel neural network, which uncovers and leverages the hidden signal patterns across multiple antennas on mmWave sensors. In this way, FG-LiquID learns to calibrate signals and finally eliminate the adverse effect of location interference caused by minor displacement/rotation of the liquid container, which ensures robust identification towards daily usage scenarios. Extensive experimental results using a custom-build prototype demonstrate that FG-LiquID can accurately distinguish 30 different liquids with an average accuracy of 97%, under 5 different scenarios. More importantly, it can discriminate quite similar liquids, such as liquors with the difference of only 1% alcohol concentration by volume.},
	articleno    = 116,
	numpages     = 27,
	keywords     = {Liquid Identification, 60GHz mmWave, Neural Networks, FMCW Radar, Material Identification, Contactless Sensing, Wireless Sensing}
}
@article{10.1145/3478073,
	title        = {Context-Aware Adaptive Surgery: A Fast and Effective Framework for Adaptative Model Partition},
	author       = {Wang, Hongli and Guo, Bin and Liu, Jiaqi and Liu, Sicong and Wu, Yungang and Yu, Zhiwen},
	year         = 2021,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 3,
	doi          = {10.1145/3478073},
	url          = {https://doi.org/10.1145/3478073},
	issue_date   = {Sept 2021},
	abstract     = {Deep Neural Networks (DNNs) have made massive progress in many fields and deploying DNNs on end devices has become an emerging trend to make intelligence closer to users. However, it is challenging to deploy large-scale and computation-intensive DNNs on resource-constrained end devices due to their small size and lightweight. To this end, model partition, which aims to partition DNNs into multiple parts to realize the collaborative computing of multiple devices, has received extensive research attention. To find the optimal partition, most existing approaches need to run from scratch under given resource constraints. However, they ignore that resources of devices (e.g., storage, battery power), and performance requirements (e.g., inference latency), are often continuously changing, making the optimal partition solution change constantly during processing. Therefore, it is very important to reduce the tuning latency of model partition to realize the real-time adaption under the changing processing context. To address these problems, we propose the Context-aware Adaptive Surgery (CAS) framework to actively perceive the changing processing context, and adaptively find the appropriate partition solution in real-time. Specifically, we construct the partition state graph to comprehensively model different partition solutions of DNNs by import context resources. Then "the neighbor effect" is proposed, which provides the heuristic rule for the search process. When the processing context changes, CAS adopts the runtime search algorithm, Graph-based Adaptive DNN Surgery (GADS), to quickly find the appropriate partition that satisfies resource constraints under the guidance of the neighbor effect. The experimental results show that CAS realizes adaptively rapid tuning of the model partition solutions in 10ms scale even for large DNNs (2.25x to 221.7x search time improvement than the state-of-the-art researches), and the total inference latency still keeps the same level with baselines.},
	articleno    = 131,
	numpages     = 22,
	keywords     = {Edge intelligence, Adaptive model partition, Collaborative model computing, Context perception}
}
@article{10.1145/3463516,
	title        = {Designing Kitchen Technologies for Ageing in Place: A Video Study of Older Adults' Cooking at Home},
	author       = {Kuoppam\"{a}ki, Sanna and Tuncer, Sylvaine and Eriksson, Sara and McMillan, Donald},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463516},
	url          = {https://doi.org/10.1145/3463516},
	issue_date   = {June 2021},
	abstract     = {Assistive technologies can significantly increase older adults' independent living if these technologies are designed to meet their needs and abilities. This study investigates conditions and present possibilities for assistive technology to provide physical and cognitive support to older adults in a specific domestic task, which is cooking a meal at home. The empirical material consists of six video recordings of adults aged 65 and over preparing a meal in their kitchen. The study unpacks the complexity of kitchen tasks, from the physical interactions involved to the temporal and spatial alignment of objects and goals in the kitchen. We focus on a) Physical manipulation, such as chopping, opening packages, and moving objects around the kitchen, b) Organisation and coordination, including switching, synchronising and monitoring cooking tasks, and c) Reorchestration and reorganisation in the form of inserting additional tasks, and rearranging tools and ingredients when adjustments need to be made in the cooking process. The study outlines design principles for operational and organisational interventions to support cooking a meal for independent living. The study concludes with discussing design implications for conversational user interfaces in the kitchen, and the significance of assistive kitchen technologies for ageing in place.},
	articleno    = 69,
	numpages     = 19,
	keywords     = {kitchen technologies, older adults, video study, cooking, human-food interaction}
}
@article{10.1145/3463510,
	title        = {We Hear Your PACE: Passive Acoustic Localization of Multiple Walking Persons},
	author       = {Cai, Chao and Pu, Henglin and Wang, Peng and Chen, Zhe and Luo, Jun},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463510},
	url          = {https://doi.org/10.1145/3463510},
	issue_date   = {June 2021},
	abstract     = {Indoor localization is crucial to enable context-aware applications, but existing solutions mostly require a user to carry a device, so as to actively sense location-discriminating signals. However, many applications do not prefer user involvement due to, e.g., the cumbersome of carrying a device. Therefore, solutions that track user locations passively can be desirable, yet lack of active user involvement has made passive indoor localization very challenging even for a single person. To this end, we propose Passive Acoustic loCalization of multiple walking pErsons (PACE) as a solution for small-scale indoor scenarios: it passively locates users by pinpointing the positions of their footsteps. In particular, PACE leverages both structure-borne and air-borne footstep impact sounds (FIS); it uses structure-borne FIS for range estimations exploiting their acoustic dispersion nature, and it employs air-borne FIS for Angle-of-Arrival (AoA) estimations and person identifications. To combat the low-SNR nature of FIS, PACE innovatively employs domain adversarial adaptation and spectral weighting to ranging/identification and AoA estimations, respectively. We implement a PACE prototype and extensively evaluate its performance in representative environments. The results demonstrate a promising sub-meter localization accuracy with a median error of 30 cm.},
	articleno    = 55,
	numpages     = 24,
	keywords     = {acoustic sensing, ranging, Passive indoor localization, domain adversarial adaptation, angle-of-arrival, user identification}
}
@article{10.1145/3463506,
	title        = {Contrastive Predictive Coding for Human Activity Recognition},
	author       = {Haresamudram, Harish and Essa, Irfan and Pl\"{o}tz, Thomas},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3463506},
	url          = {https://doi.org/10.1145/3463506},
	issue_date   = {June 2021},
	abstract     = {Feature extraction is crucial for human activity recognition (HAR) using body-worn movement sensors. Recently, learned representations have been used successfully, offering promising alternatives to manually engineered features. Our work focuses on effective use of small amounts of labeled data and the opportunistic exploitation of unlabeled data that are straightforward to collect in mobile and ubiquitous computing scenarios. We hypothesize and demonstrate that explicitly considering the temporality of sensor data at representation level plays an important role for effective HAR in challenging scenarios. We introduce the Contrastive Predictive Coding (CPC) framework to human activity recognition, which captures the temporal structure of sensor data streams. Through a range of experimental evaluations on real-life recognition tasks, we demonstrate its effectiveness for improved HAR. CPC-based pre-training is self-supervised, and the resulting learned representations can be integrated into standard activity chains. It leads to significantly improved recognition performance when only small amounts of labeled training data are available, thereby demonstrating the practical value of our approach. Through a series of experiments, we also develop guidelines to help practitioners adapt and modify the framework towards other mobile and ubiquitous computing scenarios.},
	articleno    = 65,
	numpages     = 26,
	keywords     = {human activity recognition, representation learning, contrastive predictive coding}
}
@article{10.1145/3448105,
	title        = {Acoustic-Based Upper Facial Action Recognition for Smart Eyewear},
	author       = {Xie, Wentao and Zhang, Qian and Zhang, Jin},
	year         = 2021,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 2,
	doi          = {10.1145/3448105},
	url          = {https://doi.org/10.1145/3448105},
	issue_date   = {June 2021},
	abstract     = {Smart eyewear (e.g., AR glasses) is considered to be the next big breakthrough for wearable devices. The interaction of state-of-the-art smart eyewear mostly relies on the touchpad which is obtrusive and not user-friendly. In this work, we propose a novel acoustic-based upper facial action (UFA) recognition system that serves as a hands-free interaction mechanism for smart eyewear. The proposed system is a glass-mounted acoustic sensing system with several pairs of commercial speakers and microphones to sense UFAs. There are two main challenges in designing the system. The first challenge is that the system is in a severe multipath environment and the received signal could have large attenuation due to the frequency-selective fading which will degrade the system's performance. To overcome this challenge, we design an Orthogonal Frequency Division Multiplexing (OFDM)-based channel state information (CSI) estimation scheme that is able to measure the phase changes caused by a facial action while mitigating the frequency-selective fading. The second challenge is that because the skin deformation caused by a facial action is tiny, the received signal has very small variations. Thus, it is hard to derive useful information directly from the received signal. To resolve this challenge, we apply a time-frequency analysis to derive the time-frequency domain signal from the CSI. We show that the derived time-frequency domain signal contains distinct patterns for different UFAs. Furthermore, we design a Convolutional Neural Network (CNN) to extract high-level features from the time-frequency patterns and classify the features into six UFAs, namely, cheek-raiser, brow-raiser, brow-lower, wink, blink and neutral. We evaluate the performance of our system through experiments on data collected from 26 subjects. The experimental result shows that our system can recognize the six UFAs with an average F1-score of 0.92.},
	articleno    = 41,
	numpages     = 28,
	keywords     = {facial actions, eyewear, wearables, OFDM, acoustic sensing}
}
@article{10.1145/3448125,
	title        = {AdaSpring: Context-Adaptive and Runtime-Evolutionary Deep Model Compression for Mobile Applications},
	author       = {Liu, Sicong and Guo, Bin and Ma, Ke and Yu, Zhiwen and Du, Junzhao},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448125},
	url          = {https://doi.org/10.1145/3448125},
	issue_date   = {March 2021},
	abstract     = {There are many deep learning (e.g. DNN) powered mobile and wearable applications today continuously and unobtrusively sensing the ambient surroundings to enhance all aspects of human lives. To enable robust and private mobile sensing, DNN tends to be deployed locally on the resource-constrained mobile devices via model compression. The current practice either hand-crafted DNN compression techniques, i.e., for optimizing DNN-relative performance (e.g. parameter size), or on-demand DNN compression methods, i.e., for optimizing hardware-dependent metrics (e.g. latency), cannot be locally online because they require offline retraining to ensure accuracy. Also, none of them have correlated their efforts with runtime adaptive compression to consider the dynamic nature of deployment context of mobile applications. To address those challenges, we present AdaSpring, a context-adaptive and self-evolutionary DNN compression framework. It enables the runtime adaptive DNN compression locally online. Specifically, it presents the ensemble training of a retraining-free and self-evolutionary network to integrate multiple alternative DNN compression configurations (i.e., compressed architectures and weights). It then introduces the runtime search strategy to quickly search for the most suitable compression configurations and evolve the corresponding weights. With evaluation on five tasks across three platforms and a real-world case study, experiment outcomes show that AdaSpring obtains up to 3.1x latency reduction, 4.2x energy efficiency improvement in DNNs, compared to hand-crafted compression techniques, while only incurring ≤ 6.2ms runtime-evolution latency.},
	articleno    = 24,
	numpages     = 22
}
@article{10.1145/3448099,
	title        = {PREFER: Point-of-Interest REcommendation with Efficiency and Privacy-Preservation via Federated Edge LeaRning},
	author       = {Guo, Yeting and Liu, Fang and Cai, Zhiping and Zeng, Hui and Chen, Li and Zhou, Tongqing and Xiao, Nong},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448099},
	url          = {https://doi.org/10.1145/3448099},
	issue_date   = {March 2021},
	abstract     = {Point-of-Interest (POI) recommendation is significant in location-based social networks to help users discover new locations of interest. Previous studies on such recommendation mainly adopted a centralized learning framework where check-in data were uploaded, trained and predicted centrally in the cloud. However, such a framework suffers from privacy risks caused by check-in data exposure and fails to meet real-time recommendation needs when the data volume is huge and communication is blocked in crowded places. In this paper, we propose PREFER, an edge-accelerated federated learning framework for POI recommendation. It decouples the recommendation into two parts. Firstly, to protect privacy, users train local recommendation models and share multi-dimensional user-independent parameters instead of check-in data. Secondly, to improve recommendation efficiency, we aggregate these distributed parameters on edge servers in proximity to users (such as base stations) instead of remote cloud servers. We implement the PREFER prototype and evaluate its performance using two real-world datasets and two POI recommendation models. Extensive experiments demonstrate that PREFER strengthens privacy protection and improves efficiency with little sacrifice to recommendation quality compared to centralized learning. It achieves the best quality and efficiency and is more compatible with increasingly sophisticated POI recommendation models compared to other state-of-the-art privacy-preserving baselines.},
	articleno    = 13,
	numpages     = 25,
	keywords     = {federated learning, edge computing, POI recommendation}
}
@article{10.1145/3448093,
	title        = {Determinants of Longitudinal Adherence in Smartphone-Based Self-Tracking for Chronic Health Conditions: Evidence from Axial Spondyloarthritis},
	author       = {Jones, Simon L. and Hue, William and Kelly, Ryan M. and Barnett, Rosemarie and Henderson, Violet and Sengupta, Raj},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448093},
	url          = {https://doi.org/10.1145/3448093},
	issue_date   = {March 2021},
	abstract     = {The use of interactive mobile and wearable technologies for understanding and managing health conditions is a growing area of interest for patients, health professionals and researchers. Self-tracking technologies such as smartphone apps and wearable devices for measuring symptoms and behaviours generate a wealth of patient-centric data with the potential to support clinical decision making. However, the utility of self-tracking technologies for providing insight into patients' conditions is impacted by poor adherence with data logging. This paper explores factors associated with adherence in smartphone-based tracking, drawing on two studies of patients living with axial spondyloarthritis (axSpA), a chronic rheumatological condition. In Study 1, 184 axSpA patients used the uMotif health tracking smartphone app for a period of up to 593 days. In Study 2, 108 axSpA patients completed a survey about their experience of using self-tracking technologies. We identify six significant correlates of self-tracking adherence, providing insight into the determinants of tracking behaviour. Specifically, our data provides evidence that adherence correlates with the age of the user, the types of tracking devices that are being used (smartphone OS and physical activity tracker), preferences for types of data to record, the timing of interactions with a self-tracking app, and the reported symptom severity of the user. We discuss how these factors may have implications for those designing, deploying or using mobile and wearable tracking technologies to support monitoring and management of chronic diseases.},
	articleno    = 16,
	numpages     = 24,
	keywords     = {chronic conditions, self-report, self-tracking, engagement, personal informatics, adherence, rheumatic disease, smartphones, healthcare, wearable devices}
}
@article{10.1145/3448092,
	title        = {WiPhone: Smartphone-Based Respiration Monitoring Using Ambient Reflected WiFi Signals},
	author       = {Liu, Jinyi and Zeng, Youwei and Gu, Tao and Wang, Leye and Zhang, Daqing},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448092},
	url          = {https://doi.org/10.1145/3448092},
	issue_date   = {March 2021},
	abstract     = {Recent years have witnessed a trend of monitoring human respiration using Channel State Information (CSI) retrieved from commodity WiFi devices. Existing approaches essentially leverage signal propagation in a Line-of-Sight (LoS) setting to achieve good performance. However, in real-life environments, LoS can be easily blocked by furniture, home appliances and walls. This paper presents a novel smartphone-based system named WiPhone, aiming to robustly monitor human respiration in NLoS settings. Since a smartphone is usually carried around by one subject, leveraging directly-reflected CSI signals in LoS becomes infeasible. WiPhone exploits ambient reflected CSI signals in a Non-Line-of-Sight (NLoS) setting to quantify the relationship between CSI signals reflected from the environment and a subject's chest displacement. In this way, WiPhone successfully turns ambient reflected signals which have been previously considered "destructive" into beneficial sensing capability. CSI signals obtained from smartphone are usually very noisy and may scatter over different sub-carriers. We propose a density-based preprocessing method to extract useful CSI amplitude patterns for effective respiration monitoring. We conduct extensive experiments with 8 subjects in a real home environment. WiPhone achieves a respiration rate error of 0.31 bpm (breaths per minute) on average in a range of NLoS settings.},
	articleno    = 23,
	numpages     = 19,
	keywords     = {Ambient Reflection Signal Model, Smartphone, WiFi, Respiration Monitoring, Channel state information}
}
@article{10.1145/3448083,
	title        = {Attend and Discriminate: Beyond the State-of-the-Art for Human Activity Recognition Using Wearable Sensors},
	author       = {Abedin, Alireza and Ehsanpour, Mahsa and Shi, Qinfeng and Rezatofighi, Hamid and Ranasinghe, Damith C.},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448083},
	url          = {https://doi.org/10.1145/3448083},
	issue_date   = {March 2021},
	abstract     = {Wearables are fundamental to improving our understanding of human activities, especially for an increasing number of healthcare applications from rehabilitation to fine-grained gait analysis. Although our collective know-how to solve Human Activity Recognition (HAR) problems with wearables has progressed immensely with end-to-end deep learning paradigms, several fundamental opportunities remain overlooked. We rigorously explore these new opportunities to learn enriched and highly discriminating activity representations. We propose: i) learning to exploit the latent relationships between multi-channel sensor modalities and specific activities; ii) investigating the effectiveness of data-agnostic augmentation for multi-modal sensor data streams to regularize deep HAR models; and iii) incorporating a classification loss criterion to encourage minimal intra-class representation differences whilst maximising inter-class differences to achieve more discriminative features. Our contributions achieves new state-of-the-art performance on four diverse activity recognition problem benchmarks with large margins---with up to 6% relative margin improvement. We extensively validate the contributions from our design concepts through extensive experiments, including activity misalignment measures, ablation studies and insights shared through both quantitative and qualitative studies. The code base and trained network parameters are open-sourced on GitHub https://github.com/AdelaideAuto-IDLab/Attend-And-Discriminate to support further research.},
	articleno    = 1,
	numpages     = 22,
	keywords     = {deep learning, attention, time-series data, center-loss, activity recognition, wearable sensors, data augmentation, cross-channel interaction encoder}
}
@article{10.1145/3448078,
	title        = {Passive Health Monitoring Using Large Scale Mobility Data},
	author       = {Zhang, Yunke and Xu, Fengli and Li, Tong and Kostakos, Vassilis and Hui, Pan and Li, Yong},
	year         = 2021,
	month        = {mar},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 5,
	number       = 1,
	doi          = {10.1145/3448078},
	url          = {https://doi.org/10.1145/3448078},
	issue_date   = {March 2021},
	abstract     = {In this paper, we investigate the feasibility of using mobility patterns and demographic data to predict hospital visits. We collect mobility traces from two thousand users for around two months. We extract 16 mobility features from these passively collected mobility traces and train an XGBoost model to predict users' hospital visits. We demonstrate that the designed mobility features can significantly improve prediction accuracy (p &lt; 0.01, AUC = 0.79). We further analyze how these mobility features affect the prediction results and measure their importance by using Shapley additive explanation values. We discover that users with less mobility activity, less visit diversity, and few sports facilities, bountiful entertainment around their visited locations are more likely to visit hospitals. Moreover, we conduct predictions on the populations with different demographic features, which achieves meaningful and insightful results, i.e. maintaining a high mobility activity is crucial for older people's health, while fast food store more substantially affects younger people's health; visit patterns can indicate females' health, while the neighborhood environment is more indicative of males, etc. These results shed light on how to use and understand large scale mobility data in health monitoring and other health-related applications in practice.},
	articleno    = 49,
	numpages     = 23,
	keywords     = {mobile computing, Health monitoring, human mobility}
}
@article{10.1145/3432702,
	title        = {GAME: Game As a Measurement Environment: Scheme to Evaluate Interfaces and Game Contents Based on Test Theories},
	author       = {Miura, Takahiro and Matsuo, Masaki and Yabu, Ken-ichiro and Katagiri, Atsushi and Sakajiri, Masatsugu and Onishi, Junji and Kurata, Takeshi and Ifukube, Tohru},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432702},
	url          = {https://doi.org/10.1145/3432702},
	issue_date   = {December 2020},
	abstract     = {Although many computer games have become diversified in recent years, much effort and ingenuity are needed to produce games that persons with visual impairments can enjoy. However, game developers face difficulties in adjusting the parameters of these games, including difficulty and interface features, because the control skills and disability conditions of gamers vary. Thus, it is essential to establish a scheme to evaluate game interfaces and stages with individuals trials with a short time. In this study, our purpose is to propose and establish the concept of GAME (Game As a Measurement Environment) that can evaluate game interfaces and stages. This concept aims to divide a game stage into substages and then randomly allocate them to the stage. Then, the generated stage records the play logs and play conditions and then analyzes them logs based on test theories, including classical test theory (CTT) and item response theory (IRT). Based on the analysis of the gathered play logs of side-scroller action games, our concept could clarify the relationship between the stage contents, interface effects, and players skills and disabilities.},
	articleno    = 166,
	numpages     = 30,
	keywords     = {game controllers, item response theory, inclusive game, classical test theory, Usability evaluation}
}
@article{10.1145/3432699,
	title        = {Where You Go Matters: A Study on the Privacy Implications of Continuous Location Tracking},
	author       = {Baron, Benjamin and Musolesi, Mirco},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432699},
	url          = {https://doi.org/10.1145/3432699},
	issue_date   = {December 2020},
	abstract     = {Data gathered from smartphones enables service providers to infer a wide range of personal information about their users, such as their traits, their personality, and their demographics. This personal information can be made available to third parties, such as advertisers, sometimes unbeknownst to the users. Leveraging location information, advertisers can serve ads micro-targeted to users based on the places they visited. Understanding the types of information that can be extracted from location data and implications in terms of user privacy is of critical importance.In this context, we conducted an extensive in-the-wild research study to shed light on the range of personal information that can be inferred from the places visited by users, as well as privacy sensitivity of the personal information. To this end, we developed TrackingAdvisor, a mobile application that continuously collects user location and extracts personal information from it. The app also provides an interface to give feedback about the relevance of the personal information inferred from location data and its corresponding privacy sensitivity. Our findings show that, while some personal information such as social activities is not considered private, other information such as health, religious belief, ethnicity, political opinions, and socio-economic status is considered private by the participants of the study. This study paves the way to the design of privacy-preserving systems that provide contextual recommendations and explanations to help users further protect their privacy by making them aware of the consequences of sharing their personal data.},
	articleno    = 169,
	numpages     = 32,
	keywords     = {Location tracking, personal information inference, self check-in mobile systems}
}
@article{10.1145/3432238,
	title        = {MIFF: Human Mobility Extractions with Cellular Signaling Data under Spatio-Temporal Uncertainty},
	author       = {Song, Yiwei and Liu, Yunhuai and Qiu, Wenqing and Qin, Zhou and Tan, Chang and Yang, Can and Zhang, Desheng},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432238},
	url          = {https://doi.org/10.1145/3432238},
	issue_date   = {December 2020},
	abstract     = {Human Mobility Extraction with cellular Signaling Data (SD) is essential for human mobility understanding, epidemic control, and wireless network planning. SD log the detailed interactions between cellphones and cellular towers, but suffer from a spatio-temporal uncertainty problem due to cellular network tower-level load rebalancing (switching users between towers) and cellphone usage activities. To date, most models focus on utilizing better data like RSSI or GPS, do not directly address uncertainty. To address the SD uncertainty issue, we utilize two insights based on (i) individuals' regular mobility patterns and (ii) common co-movement mobility patterns between cellphone users as suggested by fundamental human mobility nature. Accordingly, we design a Multi-Information Fusion Framework (MIFF) to assist in extracting road-level human mobility based on cell-tower level traces. To evaluate the effectiveness of MIFF, we conduct experiments on one-month SD obtained from a cellular service operator, and SD manually collected by handheld mobile devices in two cities in China. Four transportation modes, namely railways, cars, buses, and bikes are evaluated. Experimental results show that with MIFF, our road-level trajectory extraction accuracy can be improved by 5.0% on Point correct matching index and 68.5% on Geographic Error on average.},
	articleno    = 159,
	numpages     = 19,
	keywords     = {Homogeneous Traces Search, Regular Pattern Exploration, Map Matching, Signaling Data, Multiple Traces Fusion}
}
@article{10.1145/3432237,
	title        = {Your Smart Speaker Can "Hear" Your Heartbeat!},
	author       = {Zhang, Fusang and Wang, Zhi and Jin, Beihong and Xiong, Jie and Zhang, Daqing},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432237},
	url          = {https://doi.org/10.1145/3432237},
	issue_date   = {December 2020},
	abstract     = {Vital sign monitoring is a common practice amongst medical professionals, and plays a key role in patient care and clinical diagnosis. Traditionally, dedicated equipment is employed to monitor these vital signs. For example, electrocardiograms (ECG) with 3-12 electrodes are attached to the target chest for heartbeat monitoring. In the last few years, wireless sensing becomes a hot research topic and wireless signal itself is utilized for sensing purposes without requiring the target to wear any sensors. The contact-free nature of wireless sensing makes it particularly appealing in current COVID-19 pandemic. Recently, promising progress has been achieved and the sensing granularity has been pushed to millimeter level, fine enough to monitor respiration which causes a chest displacement of 5 mm. While a great success with respiration monitoring, it is still very challenging to monitor heartbeat due to the extremely subtle chest displacement (0.1 - 0.5 mm) - smaller than 10% of that caused by respiration. What makes it worse is that the tiny heartbeat-caused chest displacement is buried inside the respiration-caused displacement. In this paper, we show the feasibility of employing the popular smart speakers (e.g., Amazon Echo) to monitor an individual's heartbeats in a contact-free manner. To extract the submillimeter heartbeat motion in the presence of other interference movements, a series of novel signal processing schemes are employed. We successfully prototype the first real-time heartbeat monitoring system using a commodity smart speaker. Experiment results show that the proposed system can monitor a target's heartbeat accurately, achieving a median heart rate estimation error of 0.75 beat per minute (bpm), and a median heartbeat interval estimation error of 13.28 ms (less than 1.8%), outperforming even some popular commodity products available on the market.},
	articleno    = 161,
	numpages     = 24,
	keywords     = {Heart rate, Contactless sensing, Acoustic signal, Vital sign}
}
@article{10.1145/3432204,
	title        = {QwertyRing: Text Entry on Physical Surfaces Using a Ring},
	author       = {Gu, Yizheng and Yu, Chun and Li, Zhipeng and Li, Zhaoheng and Wei, Xiaoying and Shi, Yuanchun},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432204},
	url          = {https://doi.org/10.1145/3432204},
	issue_date   = {December 2020},
	abstract     = {The software keyboard is widely used on digital devices such as smartphones, computers, and tablets. The software keyboard operates via touch, which is efficient, convenient, and familiar to users. However, some emerging technology devices such as AR/VR headsets and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing, a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement Unit) ring. Users wear the ring on the middle phalanx of the index finger and type on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing, users do not focus on monitoring the hand motions. They receive text feedback on a separate screen, e.g., an AR/VR headset or a digital device display, such as a computer monitor. The basic idea of QwertyRing is to detect touch events and predict users' desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing through a five-day user study. Participants achieved a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based techniques [24, 30, 45, 68] and is 86.48% of the speed of typing on a smartphone with an index finger. The results show that QwertyRing enables efficient touch-based text entry on physical surfaces.},
	articleno    = 128,
	numpages     = 29,
	keywords     = {smart ring, touch input, text entry}
}
@article{10.1145/3432199,
	title        = {GAN-Based Style Transformation to Improve Gesture-Recognition Accuracy},
	author       = {Suzuki, Noeru and Watanabe, Yuki and Nakazawa, Atsushi},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432199},
	url          = {https://doi.org/10.1145/3432199},
	issue_date   = {December 2020},
	abstract     = {Gesture recognition and human-activity recognition from multi-channel sensory data are important tasks in wearable and ubiquitous computing. In these tasks, increasing both the number of recognizable activity classes and the recognition accuracy is essential. However, this is usually an ill-posed problem because individual differences in the same gesture class may affect the discrimination of different gesture classes. One promising solution is to use personal classifiers, but this requires personal gesture samples for re-training the classifiers.We propose a method of solving this issue that obtains personal gesture classifiers using few user gesture samples, thus, achieving accurate gesture recognition for an increased number of gesture classes without requiring extensive user calibration. The novelty of our method is introducing a generative adversarial network (GAN)-based style transformer to 'generate' a user's gesture data. The method synthesizes the gesture examples of the target class of a target user by transforming of a) gesture data into another class of the same user (intra-user transformation) or b) gesture data of the same class of another user (inter-user transformation). The synthesized data are then used to train the personal gesture classifier.We conducted comprehensive experiments using 1) different classifiers including SVM and CNN, 2) intra- and inter-user transformations, 3) various data-missing patterns, and 4) two different types of sensory data. Results showed that the proposed method had an increased performance. Specially, the CNN-based classifiers increased in average accuracy from 0.747 to 0.822 in the CheekInput dataset and from 0.856 to 0.899 in the USC-HAD dataset. Moreover, the experimental results with various data-missing conditions revealed a relation between the number of missing gesture classes and the accuracy of the existing and proposed methods, and we were able to clarify several advantages of the proposed method. These results indicate the potential of considerably reducing the number of required training samples of target users.},
	articleno    = 154,
	numpages     = 20,
	keywords     = {deep learning, gesture recognition, human activity recognition (HAR), generative adversarial networks}
}
@article{10.1145/3432195,
	title        = {DeepRange: Acoustic Ranging via Deep Learning},
	author       = {Mao, Wenguang and Sun, Wei and Wang, Mei and Qiu, Lili},
	year         = 2020,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 4,
	doi          = {10.1145/3432195},
	url          = {https://doi.org/10.1145/3432195},
	issue_date   = {December 2020},
	abstract     = {Acoustic ranging is a technique for estimating the distance between two objects using acoustic signals, which plays a critical role in many applications, such as motion tracking, gesture/activity recognition, and indoor localization. Although many ranging algorithms have been developed, their performance still degrades significantly under strong noise, interference and hardware limitations. To improve the robustness of the ranging system, in this paper we develop a Deep learning based Ranging system, called DeepRange. We first develop an effective mechanism to generate synthetic training data that captures noise, speaker/mic distortion, and interference in the signals and remove the need of collecting a large volume of training data. We then design a deep range neural network (DRNet) to estimate distance. Our design is inspired by signal processing that ultra-long convolution kernel sizes help to combat the noise and interference. We further apply an ensemble method to enhance the performance. Moreover, we analyze and visualize the network neurons and filters, and identify a few important findings that can be useful for improving the design of signal processing algorithms. Finally, we implement and evaluate DeepRangeusing 11 smartphones with different brands and models, 4 environments (i.e., a lab, a conference room, a corridor, and a cubic area), and 10 users. Our results show that DRNet significantly outperforms existing ranging algorithms.},
	articleno    = 143,
	numpages     = 23,
	keywords     = {Convolutional Neural Network, Motion Tracking, Ranging, Acoustic Sensing}
}
@article{10.1145/3369837,
	title        = {Intermittent Learning: On-Device Machine Learning on Intermittently Powered System},
	author       = {Lee, Seulki and Islam, Bashima and Luo, Yubo and Nirjon, Shahriar},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369837},
	url          = {https://doi.org/10.1145/3369837},
	issue_date   = {December 2019},
	abstract     = {This paper introduces intermittent learning --- the goal of which is to enable energy harvested computing platforms capable of executing certain classes of machine learning tasks effectively and efficiently. We identify unique challenges to intermittent learning relating to the data and application semantics of machine learning tasks, and to address these challenges, we devise 1) an algorithm that determines a sequence of actions to achieve the desired learning objective under tight energy constraints, and 2) propose three heuristics that help an intermittent learner decide whether to learn or discard training examples at run-time which increases the energy efficiency of the system. We implement and evaluate three intermittent learning applications that learn the 1) air quality, 2) human presence, and 3) vibration using solar, RF, and kinetic energy harvesters, respectively. We demonstrate that the proposed framework improves the energy efficiency of a learner by up to 100% and cuts down the number of learning examples by up to 50% when compared to state-of-the-art intermittent computing systems that do not implement the proposed intermittent learning framework.},
	articleno    = 141,
	numpages     = 30,
	keywords     = {On-device online learning, Semi-supervised learning, Batteryless, Intermittent computing, Unsupervised learning, Energy harvesting}
}
@article{10.1145/3369831,
	title        = {AuraRing: Precise Electromagnetic Finger Tracking},
	author       = {Parizi, Farshid Salemi and Whitmire, Eric and Patel, Shwetak},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369831},
	url          = {https://doi.org/10.1145/3369831},
	issue_date   = {December 2019},
	abstract     = {Wearable computing platforms, such as smartwatches and head-mounted mixed reality displays, demand new input devices for high-fidelity interaction. We present AuraRing, a wearable magnetic tracking system designed for tracking fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the ring. We develop two different approaches to pose reconstruction---a first-principles iterative approach and a closed-form neural network approach. Notably, AuraRing requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.},
	articleno    = 150,
	numpages     = 28,
	keywords     = {Input, Wearable, Additional Key Words and Phrases, mixed reality, finger tracking, Electromagnetic tracking}
}
@article{10.1145/3369829,
	title        = {Personalized Context-Aware Collaborative Online Activity Prediction},
	author       = {Fan, Yali and Tu, Zhen and Li, Yong and Chen, Xiang and Gao, Hui and Zhang, Lin and Su, Li and Jin, Depeng},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369829},
	url          = {https://doi.org/10.1145/3369829},
	issue_date   = {December 2019},
	abstract     = {With the rapid development of Internet services and mobile devices, nowadays, users can connect to online services anytime and anywhere. Naturally, user's online activity behavior is coupled with time and location contexts and highly influenced by them. Therefore, personalized context-aware online activity modelling and prediction is very meaningful and necessary but also very challenging, due to the complicated relationship between users, activities, spatial and temporal contexts and data sparsity issues. To tackle the challenges, we introduce offline check-in data as auxiliary data and build a user-location-time-activity 4D-tensor and a location-time-POI 3D-tensor, aiming to model the relationship between different entities and transfer semantic features of time and location contexts among them. Accordingly, in this paper we propose a transfer learning based collaborative tensor factorization method to achieve personalized context-aware online activity prediction. Based on real-world datasets, we compare the performance of our method with several state-of-the-arts and demonstrate that our method can provide more effective prediction results in the high sparsity scenario. With only 30% of observed time and location contexts, our solution can achieve 40% improvement in predicting user's Top5 activity behavior in new time and location scenarios. Our study is the first step forward for transferring knowledge learned from offline check-in behavior to online activity prediction to provide better personalized context-aware recommendation services for mobile users.},
	articleno    = 132,
	numpages     = 28,
	keywords     = {Context-aware activity prediction, collaborative tensor factorization, transfer learning}
}
@article{10.1145/3369817,
	title        = {Towards the Design of a Ring Sensor-Based MHealth System to Achieve Optimal Motor Function in Stroke Survivors},
	author       = {Kim, Yoojung and Jung, Hee-Tae and Park, Joonwoo and Kim, Yangsoo and Ramasarma, Nathan and Bonato, Paolo and Choe, Eun Kyoung and Lee, Sunghoon Ivan},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369817},
	url          = {https://doi.org/10.1145/3369817},
	issue_date   = {December 2019},
	abstract     = {Maximizing the motor practice in stroke survivors' living environments may significantly improve the functional recovery of their stroke-affected upper-limb. A wearable system that can continuously monitor upper-limb performance has been considered as an effective clinical solution for its potential to provide patient-centered, data-driven feedback to improve the motor dosage. Towards that end, we investigate a system leveraging a pair of finger-worn, ring-type accelerometers capable of monitoring both gross-arm and fine-hand movements that are clinically relevant to the performance of daily activities. In this work, we conduct a mixed-methods study to (1) quantitatively evaluate the efficacy of finger-worn accelerometers in measuring clinically relevant information regarding stroke survivors' upper-limb performance, and (2) qualitatively investigate design requirements for the self-monitoring system, based on data collected from 25 stroke survivors and seven occupational therapists. Our quantitative findings demonstrate strong face and convergent validity of the finger-worn accelerometers, and its responsiveness to changes in motor behavior. Our qualitative findings provide a detailed account of the current rehabilitation process while highlighting several challenges that therapists and stroke survivors face. This study offers promising directions for the design of a self-monitoring system that can encourage the affected limb use during stroke survivors' daily living.},
	articleno    = 138,
	numpages     = 26,
	keywords     = {upper limb, stroke rehabilitation, self-monitoring, stroke survivor, mixed-methods study, hemiparesis, finger-worn accelerometers, Ring accelerometers}
}
@article{10.1145/3369809,
	title        = {Examining Opportunities for Goal-Directed Self-Tracking to Support Chronic Condition Management},
	author       = {Schroeder, Jessica and Karkar, Ravi and Murinova, Natalia and Fogarty, James and Munson, Sean A.},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369809},
	url          = {https://doi.org/10.1145/3369809},
	issue_date   = {December 2019},
	abstract     = {Although self-tracking offers potential for a more complete, accurate, and longer-term understanding of personal health, many people struggle with or fail to achieve their goals for health-related self-tracking. This paper investigates how to address challenges that result from current self-tracking tools leaving a person's goals for their data unstated and lacking explicit support. We examine supporting people and health providers in expressing and pursuing their tracking-related goals via goal-directed self-tracking, a novel method to represent relationships between tracking goals and underlying data. Informed by a reanalysis of data from a prior study of migraine tracking goals, we created a paper prototype to explore whether and how goal-directed self-tracking could address current disconnects between the goals people have for data in their chronic condition management and the tools they use to support such goals. We examined this prototype in interviews with 14 people with migraine and 5 health providers. Our findings indicate the potential for scaffolding goal-directed self-tracking to: 1) elicit different types and hierarchies of management and tracking goals; 2) help people prepare for all stages of self-tracking towards a specific goal; and 3) contribute additional expertise in patient-provider collaboration. Based on our findings, we present implications for the design of tools that explicitly represent and support an individual's specific self-tracking goals.},
	articleno    = 151,
	numpages     = 26,
	keywords     = {Migraine, Chronic Conditions, Health, Patient-Provider Collaboration, Personal Informatics}
}
@article{10.1145/3369806,
	title        = {ReVibe: A Context-Assisted Evening Recall Approach to Improve Self-Report Adherence},
	author       = {Rabbi, Mashfiqui and Li, Katherine and Yan, H. Yanna and Hall, Kelly and Klasnja, Predrag and Murphy, Susan},
	year         = 2019,
	month        = {dec},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 4,
	doi          = {10.1145/3369806},
	url          = {https://doi.org/10.1145/3369806},
	issue_date   = {December 2019},
	abstract     = {Besides passive sensing, ecological momentary assessments (EMAs) are one of the primary methods to collect in-the-moment data in ubiquitous computing and mobile health. While EMAs have the advantage of low recall bias, a disadvantage is that they frequently interrupt the user and thus long-term adherence is generally poor. In this paper, we propose a less-disruptive self-reporting method, "assisted recall," in which in the evening individuals are asked to answer questions concerning a moment from earlier in the day assisted by contextual information such as location, physical activity, and ambient sounds collected around the moment to be recalled. Such contextual information is automatically collected from phone sensor data, so that self-reporting does not require devices other than a smartphone. We hypothesized that providing assistance based on such automatically collected contextual information would increase recall accuracy (i.e., if recall responses for a moment match the EMA responses at the same moment) as compared to no assistance, and we hypothesized that the overall completion rate of evening recalls (assisted or not) would be higher than for in-the-moment EMAs. We conducted a two-week study (N=54) where participants completed recalls and EMAs each day. We found that providing assistance via contextual information increased recall accuracy by 5.6% (p = 0.032) and the overall recall completion rate was on average 27.8% (p &lt; 0.001) higher than that of EMAs.},
	articleno    = 149,
	numpages     = 27,
	keywords     = {ESM, interruption, engagement, real-world study, mobile health, EMA, experience sampling, Context-aware computing, self-report adherence, episodic memory, recall}
}
@article{10.1145/3411841,
	title        = {IMUTube: Automatic Extraction of Virtual on-Body Accelerometry from Video for Human Activity Recognition},
	author       = {Kwon, Hyeokhyen and Tong, Catherine and Haresamudram, Harish and Gao, Yan and Abowd, Gregory D. and Lane, Nicholas D. and Pl\"{o}tz, Thomas},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411841},
	url          = {https://doi.org/10.1145/3411841},
	issue_date   = {September 2020},
	abstract     = {The lack of large-scale, labeled data sets impedes progress in developing robust and generalized predictive models for on-body sensor-based human activity recognition (HAR). Labeled data in human activity recognition is scarce and hard to come by, as sensor data collection is expensive, and the annotation is time-consuming and error-prone. To address this problem, we introduce IMUTube, an automated processing pipeline that integrates existing computer vision and signal processing techniques to convert videos of human activity into virtual streams of IMU data. These virtual IMU streams represent accelerometry at a wide variety of locations on the human body. We show how the virtually-generated IMU data improves the performance of a variety of models on known HAR datasets. Our initial results are very promising, but the greater promise of this work lies in a collective approach by the computer vision, signal processing, and activity recognition communities to extend this work in ways that we outline. This should lead to on-body, sensor-based HAR becoming yet another success story in large-dataset breakthroughs in recognition.},
	articleno    = 87,
	numpages     = 29,
	keywords     = {Data Collection, Machine Learning, Activity Recognition}
}
@article{10.1145/3411825,
	title        = {Guiding Blind Pedestrians in Public Spaces by Understanding Walking Behavior of Nearby Pedestrians},
	author       = {Kayukawa, Seita and Ishihara, Tatsuya and Takagi, Hironobu and Morishima, Shigeo and Asakawa, Chieko},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411825},
	url          = {https://doi.org/10.1145/3411825},
	issue_date   = {September 2020},
	abstract     = {We present a guiding system to help blind people walk in public spaces while making their walking seamless with nearby pedestrians. Blind users carry a rolling suitcase-shaped system that has two RGBD Cameras, an inertial measurement unit (IMU) sensor, and light detection and ranging (LiDAR) sensor. The system senses the behavior of surrounding pedestrians, predicts risks of collisions, and alerts users to help them avoid collisions. It has two modes: the "on-path" mode that helps users avoid collisions without changing their path by adapting their walking speed; and the "off-path" mode that navigates an alternative path to go around pedestrians standing in the way Auditory and tactile modalities have been commonly used for non-visual navigation systems, so we implemented two interfaces to evaluate the effectiveness of each modality for collision avoidance. A user study with 14 blind participants in public spaces revealed that participants could successfully avoid collisions with both modalities. We detail the characteristics of each modality.},
	articleno    = 85,
	numpages     = 22,
	keywords     = {Visual impairments, collision prediction, blind navigation, tactile interface, audio interface, pedestrian avoidance}
}
@article{10.1145/3411818,
	title        = {Handling Missing Sensors in Topology-Aware IoT Applications with Gated Graph Neural Network},
	author       = {Liu, Shengzhong and Yao, Shuochao and Huang, Yifei and Liu, Dongxin and Shao, Huajie and Zhao, Yiran and Li, Jinyang and Wang, Tianshi and Wang, Ruijie and Yang, Chaoqi and Abdelzaher, Tarek},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411818},
	url          = {https://doi.org/10.1145/3411818},
	issue_date   = {September 2020},
	abstract     = {Reliable data collection, transmission, and delivery on Internet of Things (IoT) systems is crucial in order to provide high-quality intelligent services. However, sensor data delivery can be interrupted for various reasons, such as sensor malfunction, network failures, and external attacks. Thus, only data from a partial set of sensors may be available. We call it the missing sensor problem. This problem can lead to severe performance degradation at inference time by neural-network-based recognition models trained on the complete sensor set. This paper enhances the robustness of neural network models to the missing sensor problem by introducing a novel feature reconstruction module, named the graph recovery module, that handles missing sensors directly inside the network. Specifically, we consider topology-aware IoT applications, where sensors are placed on a physically interconnected network. We design a novel neural message passing mechanism that logically mimics physical network topology, based on recent advances in graph neural networks (GNNs). We rely on a spatial locality assumption, where only correlations between physically connected sensors are explicitly explored. When encountering missing sensors, information is passed from available sensors to missing sensors to be used to reconstruct their features. Moreover, at each message passing step, we utilize a gating mechanism inspired by Gated Recurrent Units (GRUs) to automatically control information flow between available sensors and missing sensors. We empirically evaluate the reconstruction performance of the graph recovery module with two representative IoT applications; human activity recognition (HAR) and electroencephalogram (EEG)-based motor-imagery classification, on three public datasets. Two different backbone networks are utilized for the tasks. Our design is shown to effectively maintain model performance, suffering only 7% to 18% accuracy loss when as much as 90% of sensors are removed, compared to a drop of 15% to 47% in the accuracy of competing state-of-the-art algorithms under the same conditions. The accuracy gap is largest when more sensors are missing.},
	articleno    = 90,
	numpages     = 31,
	keywords     = {Graph Neural Networks (GNNs), Internet of Things (IoT), Missing Sensors}
}
@article{10.1145/3411812,
	title        = {Will You Come Back / Check-in Again? Understanding Characteristics Leading to Urban Revisitation and Re-Check-In},
	author       = {Chen, Zhilong and Cao, Hancheng and Wang, Huangdong and Xu, Fengli and Kostakos, Vassilis and Li, Yong},
	year         = 2020,
	month        = {sep},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 3,
	doi          = {10.1145/3411812},
	url          = {https://doi.org/10.1145/3411812},
	issue_date   = {September 2020},
	abstract     = {Recent years have witnessed much work unraveling human mobility patterns through urban visitation and location check-in data. Traditionally, user visitation and check-in have been assumed as the same behavior, yet this fundamental assumption can be questionable and lacks supporting evidence. In this paper, we seek to understand the similarities and differences of visitation and check-in by presenting a large-scale systematic analysis under the specific setting of urban revisitation and re-check-in, which demonstrate people's periodic behaviors and regularities. Leveraging a localization dataset to model urban revisitation and a Foursqaure dataset to delineate re-check-in, we identify features concerning POI visitation patterns, POI background information, user visitation patterns, user preference and users' behavioral characteristics to understand their effects on urban revisitation and re-check-in. We examine the relationship between revisitation/re-check-in rate and the features we identify, highlighting the similarities and differences between urban revisitation and re-check-in. We demonstrate the prediction effectiveness of the identified characteristics utilizing machine learning models, with an overall ROC AUC of 0.92 for urban revisitation and 0.82 for re-check-in, respectively. This study has important research implications, including improved modeling of human mobility and better understanding of human behavior, and sheds light on designing novel ubiquitous computing applications.},
	articleno    = 76,
	numpages     = 27,
	keywords     = {prediction, Revisitation, human mobility, re-check-in}
}
@article{10.1145/3397337,
	title        = {RISC: Resource-Constrained Urban Sensing Task Scheduling Based on Commercial Fleets},
	author       = {Xie, Xiaoyang and Fang, Zhihan and Wang, Yang and Zhang, Fan and Zhang, Desheng},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397337},
	url          = {https://doi.org/10.1145/3397337},
	issue_date   = {June 2020},
	abstract     = {With the trend of vehicles becoming increasingly connected and potentially autonomous, vehicles are being equipped with rich sensing and communication devices. Various vehicular services based on shared real-time sensor data of vehicles from a fleet have been proposed to improve the urban efficiency, e.g., HD-live map, and traffic accident recovery. However, due to the high cost of data uploading (e.g., monthly fees for a cellular network), it would be impractical to make all well-equipped vehicles to upload real-time sensor data constantly. To better utilize these limited uploading resources and achieve an optimal road segment sensing coverage, we present a real-time sensing task scheduling framework, i.e., RISC, for Resource-Constraint modeling for urban sensing by scheduling sensing tasks of commercial vehicles with sensors based on the predictability of vehicles' mobility patterns. In particular, we utilize the commercial vehicles, including taxicabs, buses, and logistics trucks as mobile sensors to sense urban phenomena, e.g., traffic, by using the equipped vehicular sensors, e.g., dash-cam, lidar, automotive radar, etc. We implement RISC on a Chinese city Shenzhen with one-month real-world data from (i) a taxi fleet with 14 thousand vehicles; (ii) a bus fleet with 13 thousand vehicles; (iii) a truck fleet with 4 thousand vehicles. Further, we design an application, i.e., track suspect vehicles (e.g., hit-and-run vehicles), to evaluate the performance of RISC on the urban sensing aspect based on the data from a regular vehicle (i.e., personal car) fleet with 11 thousand vehicles. The evaluation results show that compared to the state-of-the-art solutions, we improved sensing coverage (i.e., the number of road segments covered by sensing vehicles) by 10% on average.},
	articleno    = 62,
	numpages     = 20,
	keywords     = {Mobility Patterns, Vehicle Sensing, Heterogeneous Fleets}
}
@article{10.1145/3397335,
	title        = {MAIL: Multi-Scale Attention-Guided Indoor Localization Using Geomagnetic Sequences},
	author       = {Niu, Qun and He, Tao and Liu, Ning and He, Suining and Luo, Xiaonan and Zhou, Fan},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397335},
	url          = {https://doi.org/10.1145/3397335},
	issue_date   = {June 2020},
	abstract     = {Knowing accurate indoor locations of pedestrians has great social and commercial values, such as pedestrian heatmapping and targeted advertising. Location estimation with sequential inputs (e.g., geomagnetic sequences) has received much attention lately, mainly because they enhance the localization accuracy with temporal correlations. Nevertheless, it is challenging to realize accurate localization with geomagnetic sequences due to environmental factors, such as non-uniform ferromagnetic disturbances. To address this, we propose MAIL, a multi-scale attention-guided indoor localization network, which turns these challenges into favorable advantages. Our key contributions are as follows. First, instead of extracting a single holistic feature from an input sequence directly, we design a scale-based feature extraction unit that takes variational anomalies at different scales into consideration. Second, we propose an attention generation scheme that identifies attention values for different scales. Rather than setting fixed numbers, MAIL learns them adaptively with the input sequence, thus increasing its adaptability and generality. Third, guided by attention values, we fuse multi-scale features by paying more attention to prominent ones and estimate current location with the fused feature. We evaluate the performance of MAIL in three different trial sites. Evaluation results show that MAIL reduces the mean localization error by more than 36% compared with the state-of-the-art competing schemes.},
	articleno    = 54,
	numpages     = 23,
	keywords     = {Attention, Multi-Scale Features, Geomagnetic Indoor Localization}
}
@article{10.1145/3397322,
	title        = {Fine-Grained Air Pollution Inference with Mobile Sensing Systems: A Weather-Related Deep Autoencoder Model},
	author       = {Ma, Rui and Liu, Ning and Xu, Xiangxiang and Wang, Yue and Noh, Hae Young and Zhang, Pei and Zhang, Lin},
	year         = 2020,
	month        = {jun},
	journal      = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 4,
	number       = 2,
	doi          = {10.1145/3397322},
	url          = {https://doi.org/10.1145/3397322},
	issue_date   = {June 2020},
	abstract     = {Air pollution is a global health threat. Except static official air quality stations, mobile sensing systems are deployed for urban air pollution monitoring to achieve larger sensing coverage and greater sampling granularity. However, the data sparsity and irregularity also bring great challenges for pollution map recovery. To address these problems, we propose a deep autoencoder framework based inference algorithm. Under the framework, a partially observed pollution map formed by the irregular samples are input into the model, then an encoder and a decoder work together to recover the entire pollution map. Inside the decoder, we adopt a convolutional long short-term memory (ConvLSTM) model by revealing its physical interpretation with an atmospheric dispersion model, and further present a weather-related ConvLSTM to enable quasi real-time applications.To evaluate our algorithm, a half-year data collection was deployed with a real-world system on a coastal area including the Sino-Singapore Tianjin Eco-city in north China. With the resolution of 500 m x 500 m x 1 h, our offline method is proved to have high robustness against low sampling coverage and accidental sensor errors, obtaining 14.9% performance improvement over existing methods. Our quasi real-time model better captures the spatiotemporal dependencies in the pollution map with unevenly distributed samples than other real-time approaches, obtaining 4.2% error reduction.},
	articleno    = 52,
	numpages     = 21,
	keywords     = {autoencoder, air pollution map, mobile sensing networks, convlstm}
}
