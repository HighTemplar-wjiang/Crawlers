@inproceedings{10.1145/3411763.3441312,
author = {Deng, Jialin and Wang, Yan and Velasco, Carlos and Altarriba Altarriba Bertran, Ferran and Comber, Rob and Obrist, Marianna and Isbister, Katherine and Spence, Charles and 'Floyd' Mueller, Florian},
title = {The Future of Human-Food Interaction},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3441312},
doi = {10.1145/3411763.3441312},
abstract = {There is an increasing interest in food within the HCI discipline, with many interactive prototypes emerging that augment, extend and challenge the various ways people engage with food, ranging from growing plants, cooking ingredients, serving dishes and eating together. Grounding theory is also emerging that in particular draws from embodied interactions, highlighting the need to consider not only instrumental, but also experiential factors specific to human-food interactions. Considering this, we are provided with an opportunity to extend human-food interactions through knowledge gained from designing novel systems emerging through technical advances. This workshop aims to explore the possibility of bringing practitioners, researchers and theorists together to discuss the future of human-food interaction with a particular highlight on the design of experiential aspects of human-food interactions beyond the instrumental. This workshop extends prior community building efforts in this area and hence explicitly invites submissions concerning the empirically-informed knowledge of how technologies can enrich eating experiences. In doing so, people will benefit not only from new technologies around food, but also incorporate the many rich benefits that are associated with eating, especially when eating with others.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {100},
numpages = {6},
keywords = {human-food interaction, Food, eating},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@inproceedings{10.1145/3411763.3458866,
author = {Hudson, Scott},
title = {SIGCHI Lifetime Research Award Talk: The Future Is Not What It Used to Be: What's Changed, What's the Same, and Why the Fun Stuff in Technical HCI is All Ahead of Us},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3458866},
doi = {10.1145/3411763.3458866},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {1},
numpages = {1},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451854,
author = {Han, Jihae},
title = {ChemCraft: A Ludic Approach to Educational Game Design},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451854},
doi = {10.1145/3411763.3451854},
abstract = {We constantly both learn from and play with the world around us. We interact, we experiment, and we are curious; and through the spirit of Zimmerman’s ideas on ‘gaming literacy’, we can see the world as opportunities for play. This project thus presents ChemCraft, an adventure computer game based on chemistry. The game focuses on a ludic approach for educational game design, translating digital game rules from real chemistry rules: gameplay mechanics follow chemical reactions, game objects represent chemical compounds, and game object properties reference real chemistry data values. The game ChemCraft targets the niche field of games for higher education, specifically IB Higher Level Organic Chemistry, to investigate to what extent chemistry systems may be translated into game systems. The project asks not ‘What does a chemistry game look like’ but instead: ‘How can chemistry be a game?’},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {496},
numpages = {5},
keywords = {Educational Games, Human-Computer Interaction, Game Design, User Experience},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451853,
author = {Fukuda, Atsushi},
title = {Onomatangiplay : A Game Experience with a Tangible Auditory User Interface Created by Representing Game Effects with a Real-World Sound Source},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451853},
doi = {10.1145/3411763.3451853},
abstract = {Since the advent of augmented reality, video game representations have been extended beyond the display. Many studies have used real objects to assist in the presentation of visual and haptic information, resulting in video games that are integrated with reality. Researchers often use tangibles UIs as a method to extend visual and haptic information presentation, but they commonly use speakers for auditory information presentation, and tangibles UIs dealing with real objects have few applications except for some music applications. In this study, we developed a device that presents game sound effects using real object sound sources. By assigning a real object sound source to each of the game sound effects, not only can auditory information be presented instead of speakers, but also a tangible UI can be realized where the user can access the video game by touching the sound source. We named this tangible UI as Tangible-Auditory User Interface (t-AUI), referring to GUI, and CLI. The production of t-AUI will enable the application of tangible UI to sound generating devices such as holding down and blocking the output of video game sound effects with hands, and reproducing sound effects with real objects for video game operation.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {494},
numpages = {4},
keywords = {Game interaction, Tangible user interface, Auditory user interface},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451840,
author = {Shi, Yang and Li, Zhaorui and Xu, Lingfei and Cao, Nan},
title = {Understanding the Design Space for Animated Narratives Applied to Illustrations},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451840},
doi = {10.1145/3411763.3451840},
abstract = {Animated illustrations are a genre of graphic design that communicate a specific contextualized message using dynamic visuals. While animated illustrations has been gaining popularity across different applications, exploring them through the storytelling lens has received limited attention. In this work, we introduce a design space for animated narratives applied to illustrations. The design space combines a dimension for object types of animation techniques with one for narrative intents served by such animation techniques. We derived our design space from the analysis of 121 high-quality animated illustrations collected from online sources. To evaluate the effectiveness of our design space, we ran a workshop with 18 participants. The results of our workshop indicated that the design space can be used as a tool that supports ideation and increases creativity for designing expressive animated illustrations.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {347},
numpages = {6},
keywords = {Visual Narratives, Design Space, Animated Illustrations},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451835,
author = {Mishra, Sonali and Haldar, Shefali and Pollack, Ari and Klasnja, Predrag and Pratt, Wanda},
title = {Theory is in the Eye of the Beholder: Exploring Difficulties with Validating Intervention Mechanisms: Theory is in the Eye of the Beholder},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451835},
doi = {10.1145/3411763.3451835},
abstract = {Behavior change researchers frequently base their interventions on theory, targeting specific mechanisms of change to help users achieve their goals. Thus HCI researchers have sought to examine whether their intervention impacts the intended mechanism of change, in addition to evaluating the overall effect of the intervention. Yet an open question remains: how do we know our interventions successfully target the mechanisms we intend? We present results from two validation studies and one user study showing the difficulties of ensuring that behavior change interventions target the mechanisms designers intend. Our findings indicate that experts disagree about what mechanism an intervention targets, that expert consensus on this matter can be hard to achieve, and that end users’ reflections indicate they may follow different mechanisms than those predicted by experts. We recommend that researchers collect data about multiple potential mechanisms that their intervention could operate through, rather than the common single-mechanism approach.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {401},
numpages = {6},
keywords = {intervention validation, health behavior change research, intervention design methods, health behavior change theories},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451828,
author = {Salai, Ana-Maria and Cook, Glenda and Holmquist, Lars Erik},
title = {Situated Buttons: A User Interface to Support Users with Complex Needs and Promote Independent Living},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451828},
doi = {10.1145/3411763.3451828},
abstract = {We present situated buttons, a highly personalized, affordable system to support users with complex needs such as people with learning disabilities and autism, dementia, etc., and promote independent living. The system addresses concerns many users with mental health issues have with accessing assistive services and technologies as a result of their health condition. The solution consists of a wireless push button that via a simple click can launch a video on a device such as a smartphone or a tablet. A trusted person like a family member or carer can appear in the video to illustrate how to complete certain activities of interest to the end-user. The button can be placed anywhere where support is needed, e.g. on a washing machine or at the kitchen table. We conducted workshops and online discussions with social and healthcare organizations, potential end-users (people with learning disabilities and older adults), and their carers. The results show that our solution has the potential to support individuals with complex needs, improve independence, and increase their quality of life, by giving simple access to helpful instructions in their daily life.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {389},
numpages = {7},
keywords = {Disability, Assistive Technology, Smart buttons, Personalization},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451826,
author = {Choi, Jinhan and Oh, Changhoon and Suh, Bongwon and Kim, Nam Wook},
title = {VisLab: Crowdsourcing Visualization Experiments in the Wild},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451826},
doi = {10.1145/3411763.3451826},
abstract = {When creating a visualization to understand and communicate data, we face different design choices. Even though past empirical research provides foundational knowledge for visualization design, practitioners still rely on their hunches to deal with intricate trade-offs in the wild. On the other hand, researchers lack the time and resources to rigorously explore the growing design space through controlled experiments. In this work, we aim to address this two-fold problem by crowdsourcing visualization experiments. We developed VisLab, an online platform in which anyone can design and deploy experiments to evaluate their visualizations. To alleviate the complexity of experiment design and analysis, our platform provides scaffold templates and analytic dashboards. To motivate broad participation in the experiments, the platform enables anonymous participation and provides personalized performance feedback. We present use case scenarios that demonstrate the usability and usefulness of the platform in addressing the different needs of practitioners, researchers, and educators.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {369},
numpages = {7},
keywords = {visualization, cognition, citizen science, crowdsourcing, perception, experiment},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451814,
author = {Zenner, Andr\'{e} and Kriegler, Hannah Maria and Kr\"{u}ger, Antonio},
title = {HaRT - The Virtual Reality Hand Redirection Toolkit},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451814},
doi = {10.1145/3411763.3451814},
abstract = {Past research has proposed various hand redirection techniques for virtual reality (VR). Such techniques modify a user’s hand movements and have been successfully used to enhance haptics and 3D user interfaces. Up to now, however, no unified framework exists that implements previously proposed techniques such as body warping, world warping, and hybrid methods. In this work, we present the Virtual Reality Hand Redirection Toolkit (HaRT), an open-source framework developed for the Unity engine. The toolkit aims to support both novice and expert VR researchers and practitioners in implementing and evaluating hand redirection techniques. It provides implementations of popular redirection algorithms and exposes a modular class hierarchy for easy integration of new approaches. Moreover, simulation, logging, and visualization features allow users of the toolkit to analyze hand redirection setups with minimal technical effort. We present the architecture of the toolkit along with the results of a qualitative expert study.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {387},
numpages = {7},
keywords = {toolkit, reach redirection, hand redirection, redirected touching, haptic retargeting},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451788,
author = {van Renswouw, Loes and Neerhof, Jelle and Vos, Steven and van Wesemael, Pieter and Lallemand, Carine},
title = {Sensation: Sonifying the Urban Running Experience},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451788},
doi = {10.1145/3411763.3451788},
abstract = {Following the need to promote physical activity as a part of a healthy lifestyle, in this study we focus on encouraging more physical activity by improving the experience, with running as an example of a popular outdoor activity. Running in nature is often described as more pleasant and relaxing than running in the city, yet in urban environments it is difficult to integrate true nature in one's running route. To bridge this gap we designed Sensation, a sonified running track that provides sensations of nature using audio feedback. Sensation senses the footsteps of runners and produces synced sounds of footsteps in several nature environments to augment the urban landscape. This way, Sensation aims to enhance the environmental factors that contribute to the positive feelings people experience during a run. We report on insights gathered during our Research-through-Design process, as well as a preliminary user test of this sonified running track.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {405},
numpages = {5},
keywords = {sonification, human-environment interaction, urban park, physical activity, Running experience},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451773,
author = {Jung, Soon-Gyo and Salminen, Joni and Jansen, Bernard},
title = {Persona Analytics: Implementing Mouse-Tracking for an Interactive Persona System},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451773},
doi = {10.1145/3411763.3451773},
abstract = {Observing user interactions with interactive persona systems offers important insights for the design and application of such systems. Using an interactive persona system, user behavior and interaction with personas can be tracked with high precision, addressing the scarcity of behavioral persona user studies. In this research, we introduce and evaluate an implementation of persona analytics based on mouse tracking, which offers researchers new possibilities for conducting persona user studies, especially during times when in-person user studies are challenging to carry out.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {342},
numpages = {8},
keywords = {Persona Analytics, User Studies, Mouse-tracking, Analytics, Personas},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451763,
author = {Salminen, Joni and Nielsen, Lene and Jung, Soon-Gyo and Jansen, Bernard},
title = {Towards a Measurement Scale of Organizational Readiness for Personas},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451763},
doi = {10.1145/3411763.3451763},
abstract = {User studies have found persona application challenging. We argue that a potential reason for the challenges is the organization's readiness to apply personas. This research reports the on-going effort of developing the Persona Readiness Scale, a survey instrument for organizations’ readiness for personas. The scale involves twenty-two items from seven dimensions: Need Readiness, Culture Readiness, Knowledge Readiness, Resource Readiness, Data and Systems Readiness, Capability Readiness, and Goal Readiness. Organizations can apply the current scale to evaluate their persona readiness but using the dimensions for statistical analyses requires further empirical validation.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {384},
numpages = {7},
keywords = {Survey instrument, Psychometrics, Personas, Persona adoption},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451757,
author = {Viswanathan, Sruthi and Legras, Christophe},
title = {UnlockMe: Social Interactions When Co-Located in Online Activities},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451757},
doi = {10.1145/3411763.3451757},
abstract = {All the activities that we do online, by either preference or obligation, deprive us of social interactions especially the impromptu ones. We present UnlockMe, a concept that aims at preserving the social link, the disposition to come across other people serendipitously when engaged in online activities such as purchasing goods, working from home or when viewing media and entertainment. Our concept relies on virtual co-location detection (both synchronous and asynchronous), to allow users to engage with other people with whom they would have been likely to interact when doing the same activities offline in the physical world. We developed and illustrated UnlockMe with six scenarios and low-fidelity prototyping to test it with 10 participants who were isolated due to the coronavirus disease (COVID-19) pandemic. Our findings reveal multimedia recommendations from close social connections to be the best scenario for UnlockMe followed by online shopping and connecting with the local community.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {356},
numpages = {7},
keywords = {Social Connections, Serendipitous Messaging, Virtual Co-location, Network Segregation, Recommendations, Collaborative Decision Making},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451751,
author = {Poretski, Lev and Taabassum, Taamannae and Tang, Anthony},
title = {Immigrant Families' Health-Related Information Behavior on Instant Messaging Platforms: Health-Related Information Exchange in Immigrant Family Groups on Instant Messaging Platforms},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451751},
doi = {10.1145/3411763.3451751},
abstract = {For immigrant families, instant messaging family groups are a common platform for sharing and discussing health-related information. Immigrants often maintain contact with their family abroad and trust information in shared IM family groups more than the information from local authorities and sources. In this study, we aimed to understand health-related information behaviors of immigrant families in their IM family groups. Based on the interviews with 6 participants from immigrant families to Canada, we found that immigrant families’ discourse on IM platforms is motivated by love and care for other family members. The families used local and international sources of information, judged information credibility by its alignment with their pre-existing knowledge, and mostly did not verify information further. The information shared by different users from different sources often contradicted one another. Yet, family members did not discuss the conflicting information due to their desire to avoid tensions.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {439},
numpages = {6},
keywords = {Instant messaging platforms, Information behaviors, immigrant families, Health-related information exchange},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451721,
author = {Song, Byung Cheol and Kim, Dae Ha},
title = {Hidden Emotion Detection Using Multi-Modal Signals},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451721},
doi = {10.1145/3411763.3451721},
abstract = {In order to better understand human emotion, we should not recognize only superficial emotions based on facial images, but also analyze so-called inner emotions by considering biological signals such as electroencephalogram (EEG). Recently, several studies to analyze a person’s inner state by using an image signal and an EEG signal together have been reported. However, there have been no studies dealing with the case where the emotions estimated from the image signal and the EEG signal are different, i.e., emotional mismatch. This paper defines a new task to detect hidden emotions, i.e., emotions in a situation where only the EEG signal is activated without the image signal being activated, and proposes a method to effectively detect the hidden emotions. First, when a subject hides the emotion intentionally, the internal and external emotional characteristics of the subject were analyzed from the viewpoint of multimodal signals. Then, based on the analysis, we designed a method of detecting hidden emotions using convolutional neural networks (CNNs) that exhibit powerful cognitive ability. As a result, this study has upgraded the technology of deeply understanding inner emotions. On the other hand, the hidden emotion dataset and source code that we have built ourselves will be officially released for future emotion recognition research.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {413},
numpages = {7},
keywords = {inner emotion, Convolutional Neural Networks, Hidden emotion detection},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451720,
author = {Seraj, Mazyar},
title = {Learning and Practicing Logic Circuits: Development of a Mobile-Based Learning Prototype},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451720},
doi = {10.1145/3411763.3451720},
abstract = {Nowadays, with the advent of electronic devices in everyday life, mobile devices can be utilized for learning purposes. When designing a mobile-based learning application, a large number of aspects should be taken into account. For the present paper, the following aspects are of special importance: first, it should be considered how to represent information; second, possible interactions between learner and system should be defined; third – and depending on the second aspect – it should be considered how real-time responses can be provided by the system. Moreover, psychological theories as for example the 4C/ID model and findings with respect to blended learning environments should be taken into account. In this paper, a mobile-based learning prototype concerning the learning topic ”logic circuit design” is presented which considers the mentioned aspects to support independent practice. The prototype includes four different representations: (i) code-based (Verilog hardware description language), (ii) graphical-based (gate-level view), (iii) Boolean function, and (iv) truth table for each gate. The proposed learning system divides the learning content into different sections to support independent practice in meaningful steps. Multiple representations are included in order to foster understanding and transfer. The resulting implications for future work are discussed.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {258},
numpages = {7},
keywords = {4C/ID instructional model, logic circuit design, blended learning, multiple representations, mobile-based learning},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451719,
author = {Terashita, Naoyuki and Sugahara, Kyoichi and Terashita, Yosuke},
title = {Turning Your Wind Instrument into a Music Controller: Real-Time Fingering Estimation by Classifying Reflected White Noise},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451719},
doi = {10.1145/3411763.3451719},
abstract = {We propose a novel fingering estimation method that allows a player to use any wind instrument as a music controller by attaching a single microphone and loudspeaker-embedded mouthpiece. The loudspeaker plays white noise continuously while the fingerings are estimated in real-time based on the sound pressure recorded at the end of the instrument. Our method addresses a major problem of conventional music controllers: differences in the tactile feel of keys compared to the player’s own instrument. We demonstrated that the proposed method accurately estimated fingerings on a saxophone with promising performance (a 1.05&nbsp;% misclassification rate), satisfying the low-latency feedback required in the context of musical performance.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {351},
numpages = {7},
keywords = {Wind Instruments, Acoustics, Fingering, Music, Estimation, Latency},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451697,
author = {Buruk, O\dh{}uz 'Oz' and Hamari, Juho},
title = {Towards the Next Generation of Extended Reality Wearables},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451697},
doi = {10.1145/3411763.3451697},
abstract = {Extended reality (XR) systems are among the most prominent interactive environments of today’s entertainment. These systems are often complemented by supportive wearables such as haptic gloves or full–body suits. However, applications are usually limited to tactile feedback and gestural controls while other strong parts of wearables such as the performative, social and interactive features are neglected. To investigate the ways of designing wearables for playful XR environments by drawing upon these strong parts, we conducted five participatory design workshops with 25 participants. Our study resulted in 14 design concepts that were synthesized into three design themes that include 9 sub-themes, namely Virtual Costumes, Modification of Bodily Perception and Social Bioadaptivity. The knowledge created extends the design space of XR wearables and opens new paths for designers and researchers to explore.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {333},
numpages = {7},
keywords = {Wearable, Game Design, Augmented Reality, Participatory Design, Virtual Reality},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451692,
author = {Genova, Margarita and Ghoniem, Nermen and Doherty, Kevin},
title = {‘Hung Up’: Designing for the Mobile App Engagement University Students Desire},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451692},
doi = {10.1145/3411763.3451692},
abstract = {In 2020, many among us have spent more time than ever before with our mobile devices. For many years, technology developers, designers, researchers and ethicists have each debated the impact of technology on how we spend our time and relate to others. Our relationships to and through our devices are increasingly complex; and perhaps for none more so than university students. And yet, there remain many gaps in our knowledge of just how students perceive and desire their engagement with these devices – leading to a lack of real-world design solutions to enable configuration of these vital human-device relationships. This paper presents a design-led inquiry into the role smartphones play in students’ lives; contributing findings from five phases of mixed-methods research conducted as part of a user-centred, iterative design process (n=157), and resulting in a novel scaffolding of the mobile app ecosystem in support of the modes of engagement students desire.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {339},
numpages = {7},
keywords = {Wellbeing, Mobile Applications, Mobile Architectures, Engagement, Mobile Devices, Social Media},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451685,
author = {Wang, Yun and Liu, Ying and Cui, Weiwei and Tang, John and Zhang, Haidong and Walston, Doug and Zhang, Dongmei},
title = {Returning to the Office During the COVID-19 Pandemic Recovery: Early Indicators from China},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451685},
doi = {10.1145/3411763.3451685},
abstract = {The COVID-19 pandemic forced many people to abruptly shift to remote work in early 2020. But as countries progressed through a recovery from the pandemic, as occurred in China beginning in the spring of 2020, companies went through a process of reopening their offices. People worked in a hybrid mode in which they could decide how to divide their time working from home and in the office. In this research, we explored what are the key factors that shaped employees’ decisions. We conducted a survey and interviews with employees in China of a global technology company. The data demonstrated people’s work time arrangements between home and office, their experiences when working from home, and their preferred work mode. Through the interviews, we identified people’s diverse strategies and reasons behind the decisions of where to work during the hybrid work phase.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {417},
numpages = {6},
keywords = {COVID-19, telework, hybrid work, work from home, Remote work},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451679,
author = {Weber, Thomas and Winiker, Christina and Hussmann, Heinrich},
title = {A Closer Look at Machine Learning Code},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451679},
doi = {10.1145/3411763.3451679},
abstract = {Software using Machine Learning algorithms is becoming ever more ubiquitous making it equally important to have good development processes and practices. Whether we can apply insights from software development research remains open though, since it is not yet clear, whether data-driven development has the same requirements as its traditional counterpart. We used eye tracking to investigate whether the code reading behaviour of developers differs between code that uses Machine Learning and code that does not. Our data shows that there are differences in what parts of the code people consider of interest and how they read it. This is a consequence of differences in both syntax and semantics of the code. This reading behaviour already shows that we cannot take existing solutions as universally applicable. In the future, methods that support Machine Learning must iterate on existing knowledge to meet the challenges of data-driven development.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {338},
numpages = {6},
keywords = {machine learning, code reading, eye tracking},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451656,
author = {Sauer, Vanessa and Mertens, Alexander and Heitland, Jens and Nitsch, Verena},
title = {Designing for Trust and Well-Being: Identifying Design Features of Highly Automated Vehicles},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451656},
doi = {10.1145/3411763.3451656},
abstract = {Vehicle automation is one of the major trends in the automotive industry and beyond. In our study, we investigate how future users with different levels of initial trust evaluate design features of level 4 automated vehicles in regards to the features’ ability to create passenger well-being. For this purpose, we identified potential design features from existing automated vehicle concepts and asked experts (n = 15) to rate them regarding their relevance to passenger well-being. In a second step, we conducted a user study (n = 69) to investigate how future users classify those features deemed relevant by the experts. Using the Kano method, the subsample with low initial trust rated 14 of 28 features as relevant, while the subsample with high initial trust rated 20 of 28 features as relevant. Further, the results indicate that the features deemed important for passenger well-being differ depending on the level of initial trust.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {319},
numpages = {7},
keywords = {Trust, Automated driving, Vehicle design, Passenger well-being},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451640,
author = {Ju, Yulan and Zheng, Dingding and Hynds, Danny and Chernyshov, George and Kunze, Kai and Minamizawa, Kouta},
title = {Haptic Empathy: Conveying Emotional Meaning through Vibrotactile Feedback},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451640},
doi = {10.1145/3411763.3451640},
abstract = {Touch plays an essential role in communicating emotions and intensifying interpersonal communication[4]. A lot of research focuses on how to create or improve haptic interfaces looking into challenges and possibilities that the haptic technology can offer[30]. The objective of this research is to investigate whether people can share subjective feelings through simple vibrotactile feedback. In an initial experiment, we used the TECHTILE toolkit[19] to record 28 vibration sample sets for 4 different emotions (joy, anger, sadness, relaxation). We then replayed the vibrations to test how well they could be recognized. The results support the hypothesis that people can use vibration feedback as a medium for expressing specific subjective feelings. It also indicates some universalities in affective vibrotactile stimuli that even strangers with little to no knowledge about the senders could recognize the emotional meanings.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {225},
numpages = {7},
keywords = {emotion encoding, vibration, affective computing, haptic interfaces, emotion recognition, emotion expression},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451634,
author = {Singh, Avinash Kumar and Liu, Jia and Tirado Cortes, Carlos A. and Lin, Chin-Teng},
title = {Virtual Global Landmark: An Augmented Reality Technique to Improve Spatial Navigation Learning},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451634},
doi = {10.1145/3411763.3451634},
abstract = {Navigation is a multifaceted human ability involving complex cognitive functions. It allows the active exploration of unknown environments without becoming lost while enabling us to move efficiently across well-known spaces. However, the increasing reliance on navigation assistance systems reduces surrounding environment processing and decreases spatial knowledge acquisition and thus orienting ability. To prevent such a skill loss induced by current navigation support systems like Google Maps, we propose a novel landmark technique in augmented reality (AR): the virtual global landmark (VGL). This technique seeks to help navigation and promote spatial learning. We conducted a pilot study with five participants to compare the directional arrows with VGL. Our result suggests that the participants learned more about the environment while navigation using VGL than directional arrows without any significant mental workload increase. The results have a substantial impact on the future of our navigation system.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {276},
numpages = {6},
keywords = {Augmented Reality, Navigation Landmark, Mental Workload, Spatial Awareness, Spatial Navigation},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451627,
author = {Xue, Tong and Ali, Abdallah El and Ding, Gangyi and Cesar, Pablo},
title = {Investigating the Relationship between Momentary Emotion Self-Reports and Head and Eye Movements in HMD-Based 360° VR Video Watching},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451627},
doi = {10.1145/3411763.3451627},
abstract = {Inferring emotions from Head Movement (HM) and Eye Movement (EM) data in 360° Virtual Reality (VR) can enable a low-cost means of improving users’ Quality of Experience. Correlations have been shown between retrospective emotions and HM, as well as EM when tested with static 360° images. In this early work, we investigate the relationship between momentary emotion self-reports and HM/EM in HMD-based 360° VR video watching. We draw on HM/EM data from a controlled study (N=32) where participants watched eight 1-minute 360° emotion-inducing video clips, and annotated their valence and arousal levels continuously in real-time. We analyzed HM/EM features across fine-grained emotion labels from video segments with varying lengths (5-60s), and found significant correlations between HM rotation data, as well as some EM features, with valence and arousal ratings. We show that fine-grained emotion labels provide greater insight into how HM/EM relate to emotions during HMD-based 360° VR video watching.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {353},
numpages = {8},
keywords = {eye movement, Emotion, virtual reality, 360° video, head movement},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451620,
author = {Keshavarzi, Mohammad and Hotson, Clayton and Cheng, Chin-Yi and Nourbakhsh, Mehdi and Bergin, Michael and Rahmani Asl, Mohammad},
title = {SketchOpt: Sketch-Based Parametric Model Retrieval for Generative Design},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451620},
doi = {10.1145/3411763.3451620},
abstract = {Developing fully parametric building models for performance-based generative design tasks often requires proficiency in many advanced 3D modeling and visual programming software, limiting its use for many building designers. Moreover, iterations of such models can be time-consuming tasks and sometimes limiting depending on the the design stage, as major changes in the layout design may result in remodeling the entire parametric definition. To address these challenges, we introduce a novel automated generative design system, which takes a basic floor plan sketch as an input and provides a parametric model prepared for multi-objective building optimization as output. In addition, the user-designer can assign various design variables for its desired building elements by using simple annotations in the drawing. We take advantage of a asymmetric convolutional module combined with a parametrizer to allow real-time parametric sketch-retrieval for a performance-based generative workflow. The system would recognize the corresponding element and define variable constraints to prepare for a multi-objective optimization problem. We illustrate the the use case of our proposed system by running a real-time structural optimization form-finding study. Our findings indicate the system can be utilized as a promising generative design tool for novice users.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {297},
numpages = {6},
keywords = {Sketch Retrieval, Multi-objective Optimization, Generative Design},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451611,
author = {Kaul, Oliver Beren and Behrens, Kersten and Rohs, Michael},
title = {Mobile Recognition and Tracking of Objects in the Environment through Augmented Reality and 3D Audio Cues for People with Visual Impairments},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451611},
doi = {10.1145/3411763.3451611},
abstract = {People with visual impairments face challenges in scene and object recognition, especially in unknown environments. We combined the mobile scene detection framework Apple ARKit with MobileNet-v2 and 3D spatial audio to provide an auditory scene description to people with visual impairments. The combination of ARKit and MobileNet allows keeping recognized objects in the scene even if the user turns away from the object. An object can thus serve as an auditory landmark. With a search function, the system can even guide the user to a particular item. The system also provides spatial audio warnings for nearby objects and walls to avoid collisions. We evaluated the implemented app in a preliminary user study. The results show that users can find items without visual feedback using the proposed application. The study also reveals that the range of local object detection through MobileNet-v2 was insufficient, which we aim to overcome using more accurate object detection frameworks in future work (YOLOv5x).},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {394},
numpages = {7},
keywords = {Accessibility, Auditory Scene Description, Collision Warnings, Object Detection, Spatial Audio, Text-to-Speech, Visually Impaired, Mobile Scene Recognition},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451585,
author = {Hung, Min-Wei and Hou, Chi-Ting and Ho, Chieh-Jui and Yuan, Chien Wen (Tina) and Bi, Nanyi and Chen, Shu-Huei and Huang, Ming-Chyi and You, Chuang-Wen},
title = {Exploring the Opportunities and Challenges of Enabling Clinical-Friendly Drug Psychotherapy with Virtual Reality and Biofeedback Technologies},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451585},
doi = {10.1145/3411763.3451585},
abstract = {Drug addiction is a chronic disorder associated with many of the emotional or cognitive problems observed in addicted individuals. As a result, drug-addicted patients often display deficits in their mind-body awareness and easily ignore or fail to identify emotional or environmental cues. This study conducted a needs assessment study to gain guidelines for incorporating virtual reality and biofeedback technologies to assist psychotherapists in raising patients’ awareness and identifying their cues in psychotherapy. Our results identify current difficulties to reinforce mind-body awareness and correct attributions of craving stimuli. Also, we summarized concerns and the potential to promote the effectiveness of psychotherapy when applying virtual reality and biofeedback technologies to induce cravings. We also proposed a preliminary design of technology solutions to incorporate clinical-friendly VR-based craving inducing along with real-time biofeedback for enabling psychotherapists to jointly review the induced craving events based on the collected information.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {278},
numpages = {7},
keywords = {drug addiction, psychotherapy, Virtual reality, biofeedback},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451578,
author = {Lyu, Chengchen and Chen, Hui and Peng, Xiaolan and Xu, Tong and Wang, Hongan},
title = {DailyConnect: A Mobile Aid That Assists the Understanding of Situation-Based Emotions for Children with ASDs},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451578},
doi = {10.1145/3411763.3451578},
abstract = {The understanding of situation-based emotions is of great significance for children, as it is able to assist children to better understand other people's behaviors in certain emotional scenarios and also to understand how their behaviors and actions can affect others. However, children with Autism Spectrum Disorders (ASDs) might have difficulties associating situations with emotions. In this paper, we presented the design of DailyConnect, a visual-based mobile application that supports children with ASDs to recall memories by reviewing photos based on their personal experience, and then taught by their teachers to understand situation-based emotions. There are two types of user modes for parents and teachers respectively in the system. For parents, the system enables them to upload daily-life photos associated with descriptions of contexts and behaviors of the child. For teachers, the system enables them to guide the children to recall what happened by reviewing the uploaded photos, and adopts the Discrete Trial Training (DTT) teaching technique to help the children understand situation-based emotions. Through recalling the memories of the children's real-life experience, the system enables teachers and parents to help children build emotions and situations associations step by step, including the understanding of contexts, emotional cues, emotions recognition, facial expression recognition and learn expected responses. Moreover, the system can provide individual feedback for each child to achieve targeted training after a long-term use: by analyzing of the data recording of the children (if with their consent), the system is able to generate reports on which step of the understanding situation-based emotions needs attention for each child respectively.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {212},
numpages = {6},
keywords = {Understanding of situation-based emotions, Discrete Trial Training, Autism Spectrum Disorders, Mobile application},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451573,
author = {Tseng, Wen-Jie and Huron, Samuel and Lecolinet, Eric and Gugenheimer, Jan},
title = {FingerMapper: Enabling Arm Interaction in Confined Spaces for Virtual Reality through Finger Mappings},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451573},
doi = {10.1145/3411763.3451573},
abstract = {As Virtual Reality (VR) headsets become more mobile, people can interact in public spaces with applications often requiring large arm movements. However, using these open gestures is often uncomfortable and sometimes impossible in confined and public spaces (e.g., commuting in a bus). We present FingerMapper, a mapping technique that maps small and energy-efficient finger motions onto virtual arms so that users have less physical motions while maintaining presence and partially virtual body ownership. FingerMapper works as an alternative function while the environment is not allowed for full arm interaction and enables users to interact inside a small physical, but larger virtual space. We present one example application, FingerSaber that allows the user to perform the large arm swinging movement using FingerMapper.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {194},
numpages = {4},
keywords = {Finger Mapping, Virtual Reality, Confined Space},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451563,
author = {Chen, Yuxin and Yang, Zhuolin and Abbou, Ruben and Lopes, Pedro and Zhao, Ben Y. and Zheng, Haitao},
title = {Demonstrating User Authentication via Electrical Muscle Stimulation},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451563},
doi = {10.1145/3411763.3451563},
abstract = {We propose a novel modality for active biometric authentication: electrical muscle stimulation (EMS). To explore this, we engineered an interactive system, which we call ElectricAuth, that stimulates the user’s forearm muscles with a sequence of electrical impulses (i.e., EMS challenge) and measures the user’s involuntary finger movements (i.e., response to the challenge). ElectricAuth leverages EMS’s intersubject variability, where the same electrical stimulation results in different movements in different users because everybody’s physiology is unique (e.g., differences in bone and muscular structure, skin resistance and composition, etc.). As such, ElectricAuth allows users to login without memorizing passwords or PINs. ElectricAuth’s challenge-response structure makes it secure against data breaches and replay attacks, a major vulnerability facing today’s biometrics such as facial recognition and fingerprints. Furthermore, ElectricAuth never reuses the same challenge twice in authentications – in just one second of stimulation it encodes one of 68M possible challenges.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {191},
numpages = {5},
keywords = {biometric authentication, wearable, electrical muscle stimulation},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451561,
author = {Radu, Iulian and Joy, Tugce and Schneider, Bertrand},
title = {Virtual Makerspaces: Merging AR/VR/MR to Enable Remote Collaborations in Physical Maker Activities},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451561},
doi = {10.1145/3411763.3451561},
abstract = {We present a mixed-reality system for remote collaborations, where collaborators can discuss, explore, create and learn about 3D physical objects. The system combines Hololens augmented reality, 3D Kinect cameras, PC and virtual reality interfaces, into a virtual space that hosts remote collaborators, and physical &amp; virtual objects.When talented people have access to fabrication tools and expertise, incredible inventions can be manifested to solve local and global problems. Because of this, the “maker movement” is a growing phenomenon, manifested through the increased number of maker spaces in many affluent communities. However, many talented individuals lack access to resources in their local communities, and collaboration opportunities with remote experts are wasted due to the limitations of current teleconferencing systems. We present a mixed-reality (MR) system for enabling remote collaborations in the context of maker activities, that allows groups of students and instructors to discuss, explore, create and learn about physical objects. The system combines augmented reality (AR) headsets, 3D cameras, PC and virtual reality (VR) interfaces, into a virtual space that contains multiple remote students, instructors, physical and virtual objects. Remote students can see a real-time 3D scan of the on-site user's physical environment, and the virtual avatars of other students. The system can support learning and exploration by showing virtual overlays on real objects (ex: showing a physical robot's sensor data or internal circuitry) while responding to real-time manipulation of physical objects by the on-site user; and can support design activities by allowing remote and local participants to annotate physical objects with virtual drawings and virtual models. This platform is being developed as an open-source project, and we are currently building applications with the intention to deploy in hybrid makerspace classrooms involving on-site and remote students.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {202},
numpages = {5},
keywords = {mixed reality, maker spaces, collaborative learning, remote education},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451551,
author = {Brooks, Jas and Nagels, Steven and Teng, Shan-Yuan and Wen, Jingxuan and Nith, Romain and Nishida, Jun and Lopes, Pedro},
title = {Demonstrating Trigeminal-Based Interfaces},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451551},
doi = {10.1145/3411763.3451551},
abstract = {We demonstrate two trigeminal-based interfaces. The first provides a temperature illusion that uses low-powered electronics and enables the miniaturization of simple warm and cool sensations. Our illusion relies on the properties of certain scents, such as the coolness of mint or hotness of peppers. These odors trigger not only the olfactory bulb, but also the nose's trigeminal nerve, which has receptors that respond to both temperature and chemicals. The second is a novel type of olfactory device that creates a stereo-smell experience, i.e., directional information about the location of an odor, by rendering the readings of external odor sensors as trigeminal sensations using electrical stimulation of the user's nasal septum. We propose that electrically stimulating the trigeminal nerve is an ideal candidate for stereo-smell rendering. We demonstrate these interfaces by allowing an audience to stimulate an author and receive an explanation of the sensations.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {204},
numpages = {4},
keywords = {olfaction, Trigeminal, electrical stimulation, thermal feedback, intranasal},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451550,
author = {Romat, Hugo and Fender, Andreas Rene and Meier, Manuel and Holz, Christian},
title = {Demonstration of Flashpen: A High-Fidelity and High-Precision Multi-Surface Pen for Virtual Reality},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451550},
doi = {10.1145/3411763.3451550},
abstract = {Pen computing has become popular with tablet and wall-screen computers for digital precision tasks such as writing, annotating, and drawing. Digital pens have been made possible by the developments in input sensing technologies integrated into such screens. Virtual Reality systems, however, largely detect input using cameras, whose update rates are insufficient for capturing pen input with the necessary fidelity. In this demonstration, we showcase a digital pen for VR that accurately digitizes writing and drawing, including small and quick turns. Our prototype Flashpen repurposes an optical flow sensor from gaming mice, which digitizes minute motions at over 8,kHz when dragged across a surface. We demonstrate several use-cases for Flashpen during interaction in VR, including sketching, selecting, annotating, and writing.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {195},
numpages = {4},
keywords = {Graphics input devices, Human computerinteraction (HCI), Human-centered computing, Interaction devices},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451547,
author = {Watanabe, Keisuke and Yamamura, Ryosuke and Kakehi, Yasuaki},
title = {Foamin: A Deformable Sensor for Multimodal Inputs Based on Conductive Foam with a Single Wire},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451547},
doi = {10.1145/3411763.3451547},
abstract = {Soft sensors made of deformable materials, that are capable of sensing touches or gestures, have attracted considerable attention for use in tangible interfaces or soft robotics. However, to achieve multimodal gesture detection with soft sensors, prior studies have combined multiple sensors or utilized complex configurations with multiple wires. To achieve multimodal gesture sensing with a simpler configuration, a novel soft sensor consisting of a conductive foam with a single wire, was proposed in this study. This sensor was named as foamin and utilizes an impedance measurement technique at multiple frequencies called foamin. Additionally, a surface-shielding method was designed for improving the detection performance of the sensor. Several patterns of foamin were implemented to investigate the detection accuracy and three application scenarios based on the sensor were proposed.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {189},
numpages = {4},
keywords = {Soft Material, Deformable Interfaces, Touch Sensing},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451545,
author = {Jing, Allison and May, Kieran William and Naeem, Mahnoor and Lee, Gun and Billinghurst, Mark},
title = {EyemR-Vis: A Mixed Reality System to Visualise Bi-Directional Gaze Behavioural Cues Between Remote Collaborators},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451545},
doi = {10.1145/3411763.3451545},
abstract = {This demonstration shows eyemR-Vis, a 360 panoramic Mixed Reality collaboration system that translates gaze behavioural cues to bi-directional visualisations between a local host (AR) and a remote collaborator (VR). The system is designed to share dynamic gaze behavioural cues as bi-directional spatial virtual visualisations between a local host and a remote collaborator. This enables richer communication of gaze through four visualisation techniques: browse, focus, mutual-gaze, and fixated circle-map. Additionally, our system supports simple bi-directional avatar interaction as well as panoramic video zoom. This makes interaction in the normally constrained remote task space more flexible and relatively natural. By showing visual communication cues that are physically inaccessible in the remote task space through reallocating and visualising the existing ones, our system aims to provide a more engaging and effective remote collaboration experience.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {188},
numpages = {4},
keywords = {Gaze Visualisation, Mixed Reality Remote Collaboration, CSCW, Human-Computer Interaction},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451543,
author = {Al-Sada, Mohammed and Yang, Pin-Chu and Chieh Chiu, Chang and Pradhono Tomo, Tito and Yamen Saraiji, Mhd and Ogata, Tetsuya and Nakajima, Tatsuo},
title = {From Anime To Reality: Embodying An Anime Character As A Humanoid Robot},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451543},
doi = {10.1145/3411763.3451543},
abstract = {Otaku is a Japanese term commonly associated with fans of Japanese animation, comics or video games. Otaku culture has grown to be a global phenomenon with various hobbies and media. Despite its popularity, research efforts to contribute to the otaku culture have been modest. Therefore, we present Hatsuki, which is a humanoid robot that is especially designed to embody anime characters. Hatsuki advances the state of the art as it: 1) realizes aesthetics resembling anime characters, 2) implements 2D anime-like facial expression system, and 3) realizes anime-style behaviors and interactions. We explain Hatsuki's design specifics and its interaction domains as an autonomous robot and as a teleoperated humanoid avatar. We discuss our efforts under each interaction domain, and follow by discussing its potential deployment venues and applications. We highlight opportunities of interplay between otaku culture and interactive systems, potentially enabling highly desirable interactions and familiar system designs to users exposed to otaku culture.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {176},
numpages = {5},
keywords = {Anime, Otaku, Manga, Robot, Media, Humanoid, Culture, Interaction, Design},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451534,
author = {Roumen, Thijs and Kommana, Yannis and Apel, Ingo and Lempert, Conrad and Brand, Markus and Brendel, Erik and Seidel, Laurenz and Rambold, Lukas and Goedecken, Carl and Crenzin, Pascal and Hurdelhey, Ben and Abdullah, Muhammad and Baudisch, Patrick},
title = {Demonstrating Assembler3: 3D Reconstruction of Laser-Cut Models},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451534},
doi = {10.1145/3411763.3451534},
abstract = {We demonstrate&nbsp;Assembler3&nbsp;a software tool that allows users to perform&nbsp;3D&nbsp;parametric manipulations on&nbsp;2D&nbsp;laser&nbsp;cutting plans.&nbsp;Assembler3&nbsp;achieves this by semi-automatically converting 2D&nbsp;laser cutting plans to 3D, where users modify their models using available 3D tools (kyub), before converting them back to 2D. In our user&nbsp;study, this workflow allowed users to&nbsp;modify models&nbsp;10x faster than using the&nbsp;traditional approach of editing 2D cutting plans directly. Assembler3 converts models to 3D in 5 steps: (1)&nbsp;plate detection, (2)&nbsp;joint detection, (3)&nbsp;material thickness detection, (4)&nbsp;joint matching based on hashed joint "signatures", and&nbsp;(5)&nbsp;interactive reconstruction. In our technical evaluation, Assembler3&nbsp;was able to reconstruct 100 of 105 models. Once 3D-reconstructed, we expect users to store and share their models&nbsp;in 3D,&nbsp;which can simplify collaboration and thereby&nbsp;empower&nbsp;the&nbsp;laser cutting community&nbsp;to create&nbsp;models of higher&nbsp;complexity.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {173},
numpages = {4},
keywords = {remixing, laser cutting, reuse, personal fabrication},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3451518,
author = {Li, Tianshuo and Haynes, Mason and Rucker, Bradley E},
title = {Re-Designing A Mobile App with Patients with Discordant Chronic Comorbidities: Usability Study},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451518},
doi = {10.1145/3411763.3451518},
abstract = {Patients with complex conditions and treatment plans often find it challenging to communicate with multiple providers and to prioritize various management tasks. The challenge is even greater for patients with discordant chronic comorbidities (DCCs), a situation where a patient has conditions that have unrelated and/or conflicting treatment plans. We identified possible needs and designed the Apps that addressed those needs (including: goal setting, ease of use, monitoring, and motivation). We then tested this App with patients with DCCs to see if those needs were addressed. We present results from that a usability study that highlight the design preference of patients with DCCs.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {488},
numpages = {5},
keywords = {Mobile Application, User ability study;, Discordant Chronic Comorbidities, Multiple Chronic Conditions, Patients Care and Support},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3450403,
author = {DeVito, Michael Ann and Lustig, Caitlin and Simpson, Ellen and Allison, Kimberley and Chuanromanee, Tya and Spiel, Katta and Ko, Amy and Rode, Jennifer and Dym, Brianna and Muller, Michael and Klaus Scheuerman, Morgan and Marie Walker, Ashley and Brubaker, Jed and Ahmed, Alex},
title = {Queer in HCI: Strengthening the Community of LGBTQIA+ Researchers and Research},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3450403},
doi = {10.1145/3411763.3450403},
abstract = {As Queer Human-Computer Interaction (HCI) becomes an established part of the larger field, both in terms of research on and with queer populations and in terms of employing queering theories and methods, the role of queer researchers has become a timely topic of discussion. However, these discussions have largely centered around member-researcher status and positionality when working with queer populations. Based on insights gathered at multiple ACM events over the past two years, we identified two pressing issues: (1) we need to better support queer people doing HCI research not specific to queer populations, and (2) we need to identify how to best support member-researchers in leading Queer HCI while including collaborators beyond the queer community. This Special Interest Group (SIG) aims to directly address these challenges by convening a broad community of queer researchers and allies, working not only on explicitly-queer topics but across a broad range of HCI topics.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {159},
numpages = {3},
keywords = {Queer HCI, sexual and gender minorities},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3450374,
author = {Muir, Alexander},
title = {Where HCI Meets the Spiritual Path: The Three Yogas of the Bhagavad Gundefinedtundefined},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3450374},
doi = {10.1145/3411763.3450374},
abstract = {HCI practice can be a force for social good, but can it become a spiritual practice? If so, how? These questions are somewhat taboo, and usually discussed quietly at the fringes of the HCI community. This paper is based on the Bhagavad-Gita, and proposes three ways to tie-together a practical HCI career with our spiritual lives. The three approaches are broadly related to humanitarian action (karma yoga), love and devotion (bhakti yoga) and introspective insight (jnana yoga). Each offers a different perspective on our HCI practice, and how the practical challenge of being a researcher can be reframed as part of a spiritual path. It suggests approaches to issues such as emotional burnout and bias-awareness. It is based on teachings given by major Indian teachers.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {38},
numpages = {9},
keywords = {Yoga, Spirituality, Philosophy},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3450363,
author = {Goguey, F\'{e}licien},
title = {Catch Me If You Can: Designing a Disobedient Object to Protest Against GSM Surveillance},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3450363},
doi = {10.1145/3411763.3450363},
abstract = {In this article, I discuss the process of designing an object to protest against a specific surveillance device: the IMSI catcher, a controversial object used to monitor GSM networks. Being widely used in protests, I develop a tactical approach based on obfuscation to be adopted collectively to counteract IMSI catchers. In this case study, (1) I present how can remaking an IMSI catcher allow to re-appropriate the technology and create a basis for designing a disobedient object; (2) I introduce some examples of tactics to defeat surveillance based on obfuscation and the potential of inflatables; (3) I conceptualize a possible design of an object to defeat IMSI catchers and show the types of interactions it might generate in protests.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {32},
numpages = {7},
keywords = {surveillance, protest, critical design, activism, IMSI catcher, GSM network, obfuscation, media art, inflatables},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3445012,
author = {Pfleging, Bastian and Kun, Andrew L. and Shaer, Orit},
title = {Automated Vehicles as a Space for Work &amp; Wellbeing},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3445012},
doi = {10.1145/3411763.3445012},
abstract = {The objective of this CHI course is to provide CHI attendees with an introduction and overview of the rapidly evolving field of automotive user interfaces (AutomotiveUI). The course will focus on UI aspects in the transition towards automated driving. In particular, we will also discuss the opportunities of cars as a new space for non-driving-related activities, such as work, relaxation, and play. For newcomers and experts of other HCI fields, we will present the special properties of this field of HCI and provide an overview of new opportunities, but also general design and evaluation aspects of novel automotive user interfaces.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {140},
numpages = {3},
keywords = {non-driving-related activities, work and wellbeing., Automotive user interfaces, manual and automated driving},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3445006,
author = {Munteanu, Cosmin and Waycott, Jenny and McNaney, Roisin},
title = {Dealing with Ethical Challenges in HCI Fieldwork},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3445006},
doi = {10.1145/3411763.3445006},
abstract = {We are witnessing an increase in fieldwork within the field of HCI, particularly involving marginalized or under-represented populations. This has posed ethical challenges for researchers during such field studies, with "ethical traps" not always identified during planning stages. This is often aggravated by the inconsistent policy guidelines, training, and application of ethical principles. We ground this in our collective experiences with ethically-difficult research, and frame it within common principles that are common across many disciplines and policy guidelines – representative of the instructors’ diverse and international backgrounds.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {143},
numpages = {3},
keywords = {Ethics Guidelines},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3443443,
author = {Wolff, Annika and Knutas, Antti and P\"{a}ssil\"{a}, Anne and Lautala, Jon and Kantola, Lasse and Vainio, Teija},
title = {Designing SciberPunks as Future Personas for More than Human Design},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3443443},
doi = {10.1145/3411763.3443443},
abstract = {In this case study we describe the evolution of a new method for creating future personas, called SciberPunks, for use in sustainable city design scenarios. SciberPunks channel the voice of the environment and have special abilities for feeling and expressing data, such as the ability to taste it, or communicate it through living tattoos on the skin. The aim was to examine how environmental data could act as a bridge between people and nature, to encourage empathy towards ’more-than-human’ perspectives. We engaged 5 participants in activities designed to lead them through a process of engaging with information and data in the process of building their personas. The activities utilised arts-based methods as we were interested in the experiential aspects of engaging with data and how we might foster creative and sensory experiences with it. Activities included drawing, writing and performing and were framed by a single story that took participants on a journey through time: past, present and future. Activities took place online, due to COVID-19. Overall, participants produced 5 characters, including a shaman, a shape-shifter and a fairy, all with special skills for connecting to nature and/or to each other.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {61},
numpages = {8},
keywords = {covid-19, online, arts-based methods, data literacy, sustainability, co-design, environment, datasets},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3443442,
author = {Triantafyllidis, Eleftherios and Li, Zhibin},
title = {The Challenges in Modeling Human Performance in 3D Space with Fitts’ Law},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3443442},
doi = {10.1145/3411763.3443442},
abstract = {With the rapid growth in virtual reality technologies, object interaction is becoming increasingly more immersive, elucidating human perception and leading to promising directions towards evaluating human performance under different settings. This spike in technological growth exponentially increased the need for a human performance metric in 3D space. Fitts’ law is perhaps the most widely used human prediction model in HCI history attempting to capture human movement in lower dimensions. Despite the collective effort towards deriving an advanced extension of a 3D human performance model based on Fitts’ law, a standardized metric is still missing. Moreover, most of the extensions to date assume or limit their findings to certain settings, effectively disregarding important variables that are fundamental to 3D object interaction. In this review, we investigate and analyze the most prominent extensions of Fitts’ law and compare their characteristics pinpointing to potentially important aspects for deriving a higher-dimensional performance model. Lastly, we mention the complexities, frontiers as well as potential challenges that may lay ahead.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {56},
numpages = {9},
keywords = {Interaction Techniques, 3D Manipulation, Models and Metrics, 3D Performance Model, Human Performance, Motor Performance Review, Object Interaction, 3D Pointing, Evaluation Metrics, Virtual Reality, Fitts’ Law, Gestural Input},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3443434,
author = {Chopra, Shaan and Dixon, Emma and Ganesh, Kausalya and Pradhan, Alisha and L. Radnofsky, Mary and Lazar, Amanda},
title = {Designing for and with People with Dementia Using a Human Rights-Based Approach},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3443434},
doi = {10.1145/3411763.3443434},
abstract = {User-centered design is typically framed around meeting the preferences and needs of populations involved in the design process. However, when designing technology for people with disabilities, in particular dementia, there is also a moral imperative to ensure that human rights of this segment of the population are consciously integrated into the process and respectfully included in the product. We introduce a human rights-based user-centered design process which is informed by the United Nations Convention on the Rights of Persons with Disabilities (CRPD). We conducted two editions of a three-day-long design workshop during which undergraduate students and dementia advocates came together to design technology for people with dementia. This case study demonstrates our novel approach to user-centered design that centers human rights through different stages of the workshop and actively involves people with dementia in the design process.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {44},
numpages = {8},
keywords = {Human Rights, Heuristics, CRPD, Dementia, User-Centered Design},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3443432,
author = {Kope\'{c}, Wies\l{}aw and Kalinowski, Krzysztof and Kornacka, Monika and Skorupska, Kinga H. and Paluch, Julia and Jaskulska, Anna and Pochwatko, Grzegorz and Mo\.{z}aryn, Jakub Filip and Kobyli\'{n}ski, Pawe\l{} and Gago, Piotr},
title = {VR Hackathon with Goethe Institute: Lessons Learned from Organizing a Transdisciplinary VR Hackathon},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3443432},
doi = {10.1145/3411763.3443432},
abstract = {In this article we report a case study of a Language Learning Bauhaus VR hackathon with Goethe Institute. It was organized as an educational and research project to tap into the dynamics of transdisciplinary teams challenged with a specific requirement. In our case, it was to build a Bauhaus-themed German Language Learning VR App. We constructed this experiment to simulate how representatives of different disciplines may work together towards a very specific purpose under time pressure. So, each participating team consisted of members of various expert-fields: software development (Unity or Unreal), design, psychology and linguistics. The results of this study cast light on the recommended cycle of design thinking and customer-centered design in VR. Especially in interdisciplinary rapid prototyping conditions, where stakeholders initially do not share competences. They also showcase educational benefits of working in transdisciplinary environments. This study, combined with our previous work on human factors in rapid software development and co-design, including hackathon dynamics, allowed us to formulate recommendations for organizing content creation VR hackathons for specific purposes. We also provide guidelines on how to prepare the participants to work in rapid prototyping VR environments and benefit from such experiences in the long term.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {62},
numpages = {7},
keywords = {teamwork, VR, hackathon, language learning, participatory design},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3411763.3443427,
author = {Terzimehi\'{c}, Naundefineda},
title = {Real-World Presence in the Era of Ubiquitous Computing},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3443427},
doi = {10.1145/3411763.3443427},
abstract = {In the era of everyday ubiquitous computing, all of us have at least once lost track of time, space or our social environment when using our smartphone. In such situations, we might not feel present in the here and now in the physical real world. Presence has been a long term topic of interest in virtual reality research as the feeling of being there, in a technology-mediated virtual world. However, we have yet to understand our feeling of being here and now in the physical real world under the wide availability and easy accessibility of smartphones. In my thesis, I want to examine the concept of real-world presence from a theoretical, users’ and technology development perspective. I aim at understanding current and developing new mobile solutions, with the goal of supporting our presence in the physical real world.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {73},
numpages = {4},
keywords = {Mindfulness, Everyday Computing, Technology Disengagement, Empirical Studies, Literature Review, Presence, Technology Non-Use, Ubiquitous Computing},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

