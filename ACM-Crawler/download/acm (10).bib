@inproceedings{10.1145/3313831.3376621,
author = {Yildirim, Nur and McCann, James and Zimmerman, John},
title = {Digital Fabrication Tools at Work: Probing Professionals' Current Needs and Desired Futures},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376621},
doi = {10.1145/3313831.3376621},
abstract = {Digital fabrication tools have transformed how people work in micro- and small-scale manufacturing settings. While increasing efficiency and precision, these tools raise concerns around user agency and control. This paper describes an exploratory study investigating the felt work experience and desired futures of professionals who use fabrication tools. We conducted co-design workshops with 23 professionals who use 3D printers, laser cutters, and CNC routers. We probed about current practices; machine awareness and autonomy; and user agency. Our findings reveal that current tools are not very professional. They are unreliable and untrustworthy. Participants desired smarter tools that can actively prevent errors and perform self-calibration and self-maintenance. They had few concerns that more intelligence would impact agency. They desired tools that could negotiate trade-offs between time, cost, and quality; and that can operate as super-human shop assistants. We discuss the implications of these findings as opportunities for research that can improve professionals' work experience.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {laser cutting, cnc, digital fabrication, intelligent systems, 3d printing, user experience, future of work, co-design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376616,
author = {Luo, Yuhan and Lee, Bongshin and Choe, Eun Kyoung},
title = {TandemTrack: Shaping Consistent Exercise Experience by Complementing a Mobile App with a Smart Speaker},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376616},
doi = {10.1145/3313831.3376616},
abstract = {Smart speakers such as Amazon Echo present promising opportunities for exploring voice interaction in the domain of in-home exercise tracking. In this work, we examine if and how voice interaction complements and augments a mobile app in promoting consistent exercise. We designed and developed TandemTrack, which combines a mobile app and an Alexa skill to support exercise regimen, data capture, feedback, and reminder. We then conducted a four-week between-subjects study deploying TandemTrack to 22 participants who were instructed to follow a short daily exercise regimen: one group used only the mobile app and the other group used both the app and the skill. We collected rich data on individuals' exercise adherence and performance, and their use of voice and visual interactions, while examining how TandemTrack as a whole influenced their exercise experience. Reflecting on these data, we discuss the benefits and challenges of incorporating voice interaction to assist daily exercise, and implications for designing effective multimodal systems to support self-tracking and promote consistent exercise.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {multimodal interaction, mobile app, smart speaker, self-tracking, field deployment study, exercise},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376609,
author = {Lee, Sung-Chul and Song, Jaeyoon and Ko, Eun-Young and Park, Seongho and Kim, Jihee and Kim, Juho},
title = {SolutionChat: Real-Time Moderator Support for Chat-Based Structured Discussion},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376609},
doi = {10.1145/3313831.3376609},
abstract = {Online chat is an emerging channel for discussing community problems. It is common practice for communities to assign dedicated moderators to maintain a structured discussion and enhance the problem-solving experience. However, due to the synchronous nature of online chat, moderators face a high managerial overhead in tasks like discussion stage management, opinion summarization, and consensus-building support. To assist moderators with facilitating a structured discussion for community problem-solving, we introduce SolutionChat, a system that (1) visualizes discussion stages and featured opinions and (2) recommends contextually appropriate moderator messages. Results from a controlled lab study (n=55, 12 groups) suggest that participants' perceived discussion trackability was significantly higher with SolutionChat than without. Also, moderators provided better summarization with less effort and better managerial support using system-generated messages with SolutionChat than without. With SolutionChat, we envision untrained moderators to effectively facilitate chat-based discussions of important community matters.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {computer mediated communication, moderator, structured discussion, online discussion},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376606,
author = {Tanenbaum, Theresa Jean and Hartoonian, Nazely and Bryan, Jeffrey},
title = {"How Do I Make This Thing Smile?": An Inventory of Expressive Nonverbal Communication in Commercial Social Virtual Reality Platforms},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376606},
doi = {10.1145/3313831.3376606},
abstract = {Despite the proliferation of platforms for social Virtual Reality (VR) communicating emotional expression via an avatar remains a significant design challenge. In order to better understand the design space for expressive Nonverbal Communication (NVC) in social VR we undertook an inventory of the ten most prominent social VR platforms. Our inventory identifies the dominant design strategies for movement, facial control, and gesture in commercial VR applications, and identifies opportunities and challenges for future design and research into social expression in VR. Specifically, we highlight the paucity of interaction paradigms for facial expression and the near nonexistence of meaningful control over ambient aspects of nonverbal communication such as posture, pose, and social status.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {virtual reality, social interactions, nonverbal communication},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376582,
author = {Bahng, Sojung and Kelly, Ryan M. and McCormack, Jon},
title = {Reflexive VR Storytelling Design Beyond Immersion: Facilitating Self-Reflection on Death and Loneliness},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376582},
doi = {10.1145/3313831.3376582},
abstract = {This research examines the reflexive dimensions of cinematic virtual reality (CVR) storytelling. We created Anonymous, an interactive CVR piece that employs a reflexive storytelling method. This method is based on distancing effects and is used to elicit audience awareness and self-reflection about loneliness and death. To understand the audience's experiences, we conducted in-depth interviews to study which design factors and elements prompted reflexive thoughts and feelings. Our findings highlight how the audience experience was impacted by four reflexive dimensions: abstract and minimal aesthetics, everyday materials and textures, the restriction of control, and multiple, disembodied points of view. We use our findings to discuss how these dimensions can inform the design of VR storytelling experiences that provoke self and social reflection.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {virtual reality, immersive storytelling, estrangement, reflexivity, alienation, distancing effect, cinematic vr},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376578,
author = {Voelker, Simon and Hueber, Sebastian and Holz, Christian and Remy, Christian and Marquardt, Nicolai},
title = {GazeConduits: Calibration-Free Cross-Device Collaboration through Gaze and Touch},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376578},
doi = {10.1145/3313831.3376578},
abstract = {We present GazeConduits, a calibration-free ad-hoc mobile interaction concept that enables users to collaboratively interact with tablets, other users, and content in a cross-device setting using gaze and touch input. GazeConduits leverages recently introduced smartphone capabilities to detect facial features and estimate users' gaze directions. To join a collaborative setting, users place one or more tablets onto a shared table and position their phone in the center, which then tracks users present as well as their gaze direction to determine the tablets they look at. We present a series of techniques using GazeConduits for collaborative interaction across mobile devices for content selection and manipulation. Our evaluation with 20 simultaneous tablets on a table shows that GazeConduits can reliably identify which tablet or collaborator a user is looking at.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {touch input, cross-device interaction, gaze input},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376577,
author = {Houben, Maarten and Brankaert, Rens and Bakker, Saskia and Kenning, Gail and Bongers, Inge and Eggen, Berry},
title = {The Role of Everyday Sounds in Advanced Dementia Care},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376577},
doi = {10.1145/3313831.3376577},
abstract = {The representation of sounds derived from everyday life can be beneficial for people with dementia by evoking memories and emotional responses. Despite this potential, integrating sound and sound-based interventions in care facilities has not received much research attention. In this paper, we present the findings from a field study that explored the responses of 19 people with advanced dementia to a selection of everyday sounds presented to them in a care home and the role of these responses in the care environment. To study this, we deployed Vita, a 'pillow-like' sound player, in two dementia care facilities for four weeks, during which observations were recorded. Afterwards, we conducted interviews with caregivers who used Vita in everyday care practice. Our findings reveal how everyday sounds provided by Vita stimulated meaningful conversation, playfulness, and connection between residents and caregivers. Furthermore, we propose design implications for integrating everyday sounds in dementia care.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {soundscapes, dementia, care home, design, everyday sounds},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376575,
author = {Ebert, Nico and Ackermann, Kurt Alexander and Heinrich, Peter},
title = {Does Context in Privacy Communication Really Matter?  A Survey on Consumer Concerns and Preferences},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376575},
doi = {10.1145/3313831.3376575},
abstract = {Privacy policies as a means of communicating with customers still prove ineffective. Researchers have recently suggested that a specific usage context should be considered to make privacy notices more relevant to users. To explore this approach further, we conducted an explorative online survey of privacy concerns and privacy information preferences with 642 participants for two different contexts (loyalty cards and fitness tracking). Our data shows some support for the suggestion that context may be a significant moderator of concerns and preferences. However, the corresponding effects are rather small and limited to specific concerns and information categories. In line with other research, the data supports the known hierarchy of concerns regarding unauthorized secondary use and improper data access, which seem to exceed concerns about erroneous data processing or excessive data collection in both contexts. Furthermore, participants considered information on personal rights and processing purposes more relevant than information on contact persons.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {user preferences, policy, privacy concerns, privacy},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376571,
author = {Mohaddesi, Omid and Sun, Yifan and Azghandi, Rana and Doroudi, Rozhin and Snodgrass, Sam and Ergun, Ozlem and Griffin, Jacqueline and Kaeli, David and Marsella, Stacy and Harteveld, Casper},
title = {Introducing Gamettes: A Playful Approach for Capturing Decision-Making for Informing Behavioral Models},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376571},
doi = {10.1145/3313831.3376571},
abstract = {Agent-based simulations are widely used for modeling human behavior in various contexts. However, such simulations may oversimplify human decision-making. We propose the use of Gamettes to extract rich data on human decision-making and help in improving the human behavioral aspects of models underlying agent-based simulations. We show how Gamettes are designed and provide empirical validation for using Gamettes in an experimental supply chain setting to study human decision-making. Our results show that Gamettes are successful in capturing the expected behaviors and patterns in supply chain decisions, and, thus, we find evidence for the capability of Gamettes to inform behavioral models.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {human behavior, simulation, supply chain, agent-based model, beer game, decision-making, gamette},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376568,
author = {Bennett, Cynthia L. and Rosner, Daniela K. and Taylor, Alex S.},
title = {The Care Work of Access},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376568},
doi = {10.1145/3313831.3376568},
abstract = {Current approaches to AI and Assistive Technology (AT) often foreground task completion over other encounters such as expressions of care. Our paper challenges and complements such task-completion approaches by attending to the care work of access-the continual affective and emotional adjustments that people make by noticing and attending to one another. We explore how this work impacts encounters among people with and without vision impairments who complete tasks together. We find that bound up in attempts to get things done are concerns for one another and how well people are doing together. Reading this work through emerging disability studies and feminist STS scholarship, we account for two important forms of work that give rise to access: (1) mundane attunements and (2) non-innocent authorizations. Together these processes work as sensitizing concepts to help HCI scholars account for the ways that intelligent ATs both produce access while sometimes subverting people with disabilities.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {artificial intelligence, vision impaired, assistance, care, blind, disability, interdependence},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376554,
author = {Blaga, Andreea Dalia and Frutos-Pascual, Maite and Creed, Chris and Williams, Ian},
title = {Too Hot to Handle: An Evaluation of the Effect of Thermal Visual Representation on User Grasping Interaction in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376554},
doi = {10.1145/3313831.3376554},
abstract = {Influence of interaction fidelity and rendering quality on perceived user experience have been largely explored in Virtual Reality (VR). However, differences in interaction choices triggered by these rendering cues have not yet been explored. We present a study analysing the effect of thermal visual cues and contextual information on 50 participants' approach to grasp and move a virtual mug. This study comprises 3 different temperature cues (baseline empty, hot and cold) and 4 contextual representations; all embedded in a VR scenario. We evaluate 2 different hand representations (abstract and human) to assess grasp metrics. Results show temperature cues influenced grasp location, with the mug handle being predominantly grasped with a smaller grasp aperture for the hot condition, while the body and top were preferred for baseline and cold conditions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {grasping metrics, virtual reality, hand tracking, hand interaction},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376548,
author = {Hua, Yiqing and Naaman, Mor and Ristenpart, Thomas},
title = {Characterizing Twitter Users Who Engage in Adversarial Interactions against Political Candidates},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376548},
doi = {10.1145/3313831.3376548},
abstract = {Social media provides a critical communication platform for political figures, but also makes them easy targets for harassment. In this paper, we characterize users who adversarially interact with political figures on Twitter using mixed-method techniques. The analysis is based on a dataset of 400 thousand users' 1.2 million replies to 756 candidates for the U.S. House of Representatives in the two months leading up to the 2018 midterm elections. We show that among moderately active users, adversarial activity is associated with decreased centrality in the social graph and increased attention to candidates from the opposing party. When compared to users who are similarly active, highly adversarial users tend to engage in fewer supportive interactions with their own party's candidates and express negativity in their user profiles. Our results can inform the design of platform moderation mechanisms to support political figures countering online harassment.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {online harassment, political candidates, user behavior, twitter},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376544,
author = {Moorthy, K. L. Bhanu and Kumar, Moneish and Subramanian, Ramanathan and Gandhi, Vineet},
title = {GAZED Gaze-Guided Cinematic Editing of Wide-Angle Monocular Video Recordings},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376544},
doi = {10.1145/3313831.3376544},
abstract = {We present GAZED– eye GAZe-guided EDiting for videos captured by a solitary, static, wide-angle and high-resolution camera. Eye-gaze has been effectively employed in computational applications as a cue to capture interesting scene content; we employ gaze as a proxy to select shots for inclusion in the edited video. Given the original video, scene content and user eye-gaze tracks are combined to generate an edited video comprising cinematically valid actor shots and shot transitions to generate an aesthetic and vivid representation of the original narrative. We model cinematic video editing as an energy minimization problem over shot selection, whose constraints capture cinematographic editing conventions. Gazed scene locations primarily determine the shots constituting the edited video. Effectiveness of GAZED against multiple competing methods is demonstrated via a psychophysical study involving 12 users and twelve performance videos.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {static wide-angle recording, gaze potential, dynamic programming, eye gaze, cinematic video editing, stage performance, shot selection},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376539,
author = {Hauser, Sabrina and Suto, Melinda J. and Holsti, Liisa and Ranger, Manon and MacLean, Karon E.},
title = {Designing and Evaluating Calmer, a Device for Simulating Maternal Skin-to-Skin Holding for Premature Infants},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376539},
doi = {10.1145/3313831.3376539},
abstract = {We describe the design and deployment of Calmer, a technology that simulates key aspects of maternal skin-to-skin holding for prematurely born infants: its inspiration, approach, physical design, and introduction into the Neonatal Intensive Care Unit. Maternal skin-to-skin holding can mitigate neonatal pain during medical procedures by as much as 50%, which can improve weight gain, sleep and later development. However, parents cannot always be present, and some infants are too fragile to be held. Interventions targeting this gap could be perceived as supplanting the mother in this intimate role, exposing her to depression and endangering her maternal bond. Over 10 years, we iteratively developed Calmer and demonstrated infant health benefit in a randomized clinical trial. Here, we report and reflect on pursuing this goal in a socially and technologically complex context: constraints, strategies, features, reception of the device, and surprises, such as leading to mothers feeling channeled rather than replaced.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {automation, premature infants, research through design, parents, neonatal intensive care, pain reduction, nicu},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376537,
author = {Draxler, Fiona and Labrie, Audrey and Schmidt, Albrecht and Chuang, Lewis L.},
title = {Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376537},
doi = {10.1145/3313831.3376537},
abstract = {Augmented Reality (AR) provides a unique opportunity to situate learning content in one's environment. In this work, we investigated how AR could be developed to provide an interactive context-based language learning experience. Specifically, we developed a novel handheld-AR app for learning case grammar by dynamically creating quizzes, based on real-life objects in the learner's surroundings. We compared this to the experience of learning with a non-contextual app that presented the same quizzes with static photographic images. Participants found AR suitable for use in their everyday lives and enjoyed the interactive experience of exploring grammatical relationships in their surroundings. Nonetheless, Bayesian tests provide substantial evidence that the interactive and context-embedded AR app did not improve case grammar skills, vocabulary retention, and usability over the experience with equivalent static images. Based on this, we propose how language learning apps could be designed to combine the benefits of contextual AR and traditional approaches.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {augmented reality, language learning, grammar, self-directed learning, contextual learning},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376532,
author = {Parker, Callum and Tomitsch, Martin and Davies, Nigel and Valkanova, Nina and Kay, Judy},
title = {Foundations for Designing Public Interactive Displays That Provide Value to Users},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376532},
doi = {10.1145/3313831.3376532},
abstract = {Public interactive displays (PID) are a promising technology for providing information and collecting feedback in public spaces. Research on PIDs has shown that, like all public displays, their efficacy is reduced by display blindness. Rather than increase the visual attention-grabbing nature of PIDs, we propose that additional understanding is required around how and when these displays are able to offer value to users. We tackle this through a systematic analysis of PID studies published in the literature, which led to 9 aspects of value across 4 factors: people, location, community, and time. We discuss the identified aspects and their utility for the design of PIDs through a review of our own deployments carried out by 4 different labs across 5 countries. We conclude with a set of recommendations for identifying and optimising the intended value of future PIDs.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {design recommendations, literature review, public interactive displays, deployments, in-the-wild, public displays, value},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376517,
author = {Williford, Blake and Runyon, Matthew and Li, Wayne and Linsey, Julie and Hammond, Tracy},
title = {Exploring the Potential of an Intelligent Tutoring System for Sketching Fundamentals},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376517},
doi = {10.1145/3313831.3376517},
abstract = {Sketching is a practical and useful skill that can benefit communication and problem solving. However, it remains a difficult skill to learn because of low confidence and motivation among students and limited availability for instruction and personalized feedback among teachers. There is an need to improve the educational experience for both groups, and we hypothesized that integrating technology could provide a variety of benefits. We designed and developed an intelligent tutoring system for sketching fundamentals called Sketchtivity, and deployed it in to six existing courses at the high school and university level during the 2017-2018 school year. 268 students used the tool and produced more than 116,000 sketches of basic primitives. We conducted semi-structured interviews with the six teachers who implemented the software, as well as nine students from a course where the tool was used extensively. Using grounded theory, we found ten categories which unveiled the benefits and limitations of integrating an intelligent tutoring system for sketching fundamentals in to existing pedagogy.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {human-computer interaction, intelligent tutoring system, user experience design, sketch recognition, sketching, art education, drawing, design education},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376506,
author = {Schaekermann, Mike and Beaton, Graeme and Sanoubari, Elaheh and Lim, Andrew and Larson, Kate and Law, Edith},
title = {Ambiguity-Aware AI Assistants for Medical Data Analysis},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376506},
doi = {10.1145/3313831.3376506},
abstract = {Artificial intelligence (AI) assistants for clinical decision making show increasing promise in medicine. However, medical assessments can be contentious, leading to expert disagreement. This raises the question of how AI assistants should be designed to handle the classification of ambiguous cases. Our study compared two AI assistants that provide classification labels for medical time series data along with quantitative uncertainty estimates: conventional vs. ambiguity-aware. We simulated our ambiguity-aware AI based on real-world expert discussions to highlight cases likely to lead to expert disagreement, and to present arguments for conflicting classification choices. Our results demonstrate that ambiguity-aware AI can alter expert workflows by significantly increasing the proportion of contentious cases reviewed. We also found that the relevance of AI-provided arguments (selected from guidelines either randomly or by experts) affected experts' accuracy at revising AI-suggested labels. Our work contributes a novel perspective on the design of AI for contentious clinical assessments.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {medical data analysis, ambiguity, artificial intelligence},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376496,
author = {Cockburn, Andy and Lewis, Blaine and Quinn, Philip and Gutwin, Carl},
title = {Framing Effects Influence Interface Feature Decisions},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376496},
doi = {10.1145/3313831.3376496},
abstract = {Studies in psychology have shown that framing effects, where the positive or negative attributes of logically equivalent choices are emphasised, influence people's decisions. When outcomes are uncertain, framing effects also induce patterns of choice reversal, where decisions tend to be risk averse when gains are emphasised and risk seeking when losses are emphasised. Studies of these effects typically use potent framing stimuli, such as the mortality of people suffering from diseases or personal financial standing. We examine whether these effects arise in users' decisions about interface features, which typically have less visceral consequences, using a crowd-sourced study based on snap-to-grid drag-and-drop tasks (n = 842). The study examined several framing conditions: those similar to prior psychological research, and those similar to typical interaction choices (enabling/disabling features). Results indicate that attribute framing strongly influences users' decisions, that these decisions conform to patterns of risk seeking for losses, and that patterns of choice reversal occur.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {risky choice framing, framing effects, interface decisions, attribute framing},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376490,
author = {Neate, Timothy and Roper, Abi and Wilson, Stephanie and Marshall, Jane and Cruice, Madeline},
title = {CreaTable Content and Tangible Interaction in Aphasia},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376490},
doi = {10.1145/3313831.3376490},
abstract = {Multimedia digital content (combining pictures, text and music) is ubiquitous. The process of creating such content using existing tools typically requires complex, language-laden interactions which pose a challenge for users with aphasia (a language impairment following brain injury). Tangible interactions offer a potential means to address this challenge, however, there has been little work exploring their potential for this purpose. In this paper, we present CreaTable a platform that enables us to explore tangible interaction as a means of supporting digital content creation for people with aphasia. We report details of the co-design of CreaTable and findings from a digital creativity workshop. Workshop findings indicated that CreaTable enabled people with aphasia to create something they would not otherwise have been able to. We report how users' aphasia profiles affected their experience, describe tensions in collaborative content creation and provide insight into more accessible content creation using tangibles.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {aphasia, creativity support, accessibility, content creation, tangibles, co-design, creativity, multimedia},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376474,
author = {Hudson, Lorraine and Amponsah, Clement and Bampoe, Josephine Ohenewa and Marshall, Julie and Owusu, Nana Akua Victoria and Hussein, Khalid and Linington, Jess and Banks Gross, Zoe and Stokes, Jane and McNaney, R\'{o}is\'{\i}n},
title = {Co-Designing Digital Tools to Enhance Speech and Language Therapy Training in Ghana},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376474},
doi = {10.1145/3313831.3376474},
abstract = {Ghana has a population of over 27 million people, of which 1 in 15 may have a communication disability. The number of speech and language therapists (SLTs) available to support these people remains remarkably small, presenting a major workforce challenge. As an emerging profession, there remain significant challenges around educating the first generation of SLTs. Ghana, however, has a healthy digital infrastructure which can be taken advantage of. We describe a comprehensive study which aimed to co-design a set of locally appropriate digital tools to enhance SLT training in Ghana. We contribute insights into how digital tools could support social learning and the transition from student to independent practitioner and future clinical supervisor. We offer a set of design recommendations for creating an online Community of Practice to enhance continuing professional development.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {ghana, co-design, mobile learning, speech and language therapy},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376473,
author = {Wang, Yihong and Papangelis, Konstantinos and Saker, Michael and Lykourentzou, Ioanna and Chamberlain, Alan and Khan, Vassilis-Javed},
title = {Crowdsourcing in China: Exploring the Work Experiences of Solo Crowdworkers and Crowdfarm Workers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376473},
doi = {10.1145/3313831.3376473},
abstract = {Recent research highlights the potential of crowdsourcing in China. Yet very few studies explore the workplace context and experiences of Chinese crowdworkers. Those that do, focus mainly on the work experiences of solo crowdworkers but do not deal with issues pertaining to the substantial amount of people working in 'crowdfarms'. This article addresses this gap as one of its primary concerns. Drawing on a study that involves 48 participants, our research explores, compares and contrasts the work experiences of solo crowdworkers to those of crowdfarm workers. Our findings illustrate that the work experiences and context of the solo workers and crowdfarm workers are substantially different, with regards to their motivations, the ways they engage with crowdsourcing, the tasks they work on, and the crowdsourcing platforms they utilize. Overall, our study contributes to furthering the understandings on the work experiences of crowdworkers in China.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {crowdsourcing, reputation management, motivations and attitudes, work life balance, tasks, crowdfarms, crowdworkers, platform satisfaction, work experience},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376471,
author = {Campo Woytuk, Nadia and S\o{}ndergaard, Marie Louise Juul and Ciolfi Felice, Marianela and Balaam, Madeline},
title = {Touching and Being in Touch with the Menstruating Body},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376471},
doi = {10.1145/3313831.3376471},
abstract = {We describe a Research through Design project-Curious Cycles-a collection of objects and interactions which encourage people to be in close contact with their menstruating body. Throughout a full menstrual cycle, five participants used Curious Cycles to look at their bodies in unfamiliar ways and to touch their bodily fluids, specifically, menstrual blood, saliva, and cervical mucus. The act of touching and looking led to the construction of new knowledge about the self and to a nurturing appreciation for the changing body. Yet, participants encountered and reflected upon frictions within themselves, their home, and their social surroundings, which stem from societal stigma and preconceptions about menstruation and bodily fluids. We call for and show how interaction design can engage with technologies that mediate self-touch as a first step towards reconfiguring the way menstruating bodies are treated in society.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {women's health, menstruation, touching, menstrual cycles, research through design, feminist hci},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376469,
author = {Diana, Nicholas and Stamper, John and Koedinger, Ken},
title = {Towards Value-Adaptive Instruction: A Data-Driven Method for Addressing Bias in Argument Evaluation Tasks},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376469},
doi = {10.1145/3313831.3376469},
abstract = {As the media landscape is increasingly populated by less than reputable sources of information, educators have turned to argument evaluation training as a potential solution. Unfortunately, the bias literature suggests that our ability to objectively evaluate an argument is, to a large extent, determined by the relationship between our own beliefs and the beliefs latent in the argument we are evaluating. If the argument supports our worldview, we are much more likely to overlook logical errors. Teachers recognize this need to adapt argument evaluation instruction to the specific beliefs of students. For instance, a teacher might intentionally assign a student an argument that the student disagrees with. Unfortunately, this kind of value-adaptive instruction is infrequent due to its unscalability. We propose a novel method for data-driven value-adaptive instruction in instructional technologies. This method can be used to combat bias in real-world contexts and support human reasoning during media consumption.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {human-computer interaction, civic technology, educational technology, adaptive instruction, civic education},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376468,
author = {Soni, Nikita and Gleaves, Schuyler and Neff, Hannah and Morrison-Smith, Sarah and Esmaeili, Shaghayegh and Mayne, Ian and Bapat, Sayli and Schuman, Carrie and Stofer, Kathryn A. and Anthony, Lisa},
title = {Adults' and Children's Mental Models for Gestural Interactions with Interactive Spherical Displays},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376468},
doi = {10.1145/3313831.3376468},
abstract = {Interactive spherical displays offer numerous opportunities for engagement and education in public settings. Prior work established that users' touch-gesture patterns on spherical displays differ from those on flatscreen tabletops, and speculated that these differences stem from dissimilarity in how users conceptualize interactions with these two form factors. We analyzed think-aloud data collected during a gesture elicitation study to understand adults' and children's (ages 7 to 11) conceptual models of interaction with spherical displays and compared them to conceptual models of interaction with tabletop displays from prior work. Our findings confirm that the form factor strongly influenced users' mental models of interaction with the sphere. For example, participants conceptualized that the spherical display would respond to gestures in a similar way as real-world spherical objects like physical globes. Our work contributes new understanding of how users draw upon the perceived affordances of the sphere as well as prior touchscreen experience during their interactions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {touchscreen gestures, touchscreen displays, mental models, adults, children, interactive spherical displays},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376460,
author = {Smith, Taliesin L. and Moore, Emily B.},
title = {Storytelling to Sensemaking: A Systematic Framework for Designing Auditory Description Display for Interactives},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376460},
doi = {10.1145/3313831.3376460},
abstract = {Auditory description display is verbalized text typically used to describe live, recorded, or graphical displays to support access for people who are blind or visually impaired. Significant prior research has resulted in guidelines for auditory description for non-interactive or minimally interactive contexts. A lack of auditory description for complex interactive environments remains a tremendous barrier to access for people with visual impairments. In this work, we present a systematic design framework for designing auditory description within complex interactive environments. We illustrate how modular descriptions aligned with this framework can result in an interactive storytelling experience constructed through user interactions. This framework has been used in a set of published and widely used interactive science simulations, and in its generalized form could be applied to a variety of contexts.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {non-visual access, interactive information spaces, auditory description display, description design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376454,
author = {Hofman, Jake M. and Goldstein, Daniel G. and Hullman, Jessica},
title = {How Visualizing Inferential Uncertainty Can Mislead Readers About Treatment Effects in Scientific Results},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376454},
doi = {10.1145/3313831.3376454},
abstract = {When presenting visualizations of experimental results, scientists often choose to display either inferential uncertainty (e.g., uncertainty in the estimate of a population mean) or outcome uncertainty (e.g., variation of outcomes around that mean) about their estimates. How does this choice impact readers' beliefs about the size of treatment effects? We investigate this question in two experiments comparing 95% confidence intervals (means and standard errors) to 95% prediction intervals (means and standard deviations). The first experiment finds that participants are willing to pay more for and overestimate the effect of a treatment when shown confidence intervals relative to prediction intervals. The second experiment evaluates how alternative visualizations compare to standard visualizations for different effect sizes. We find that axis rescaling reduces error, but not as well as prediction intervals or animated hypothetical outcome plots (HOPs), and that depicting inferential uncertainty causes participants to underestimate variability in individual outcomes.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {judgment and decision making, confidence intervals, effect sizes, uncertainty visualization, prediction intervals},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376451,
author = {Wu, Tongshuang and Wongsuphasawat, Kanit and Ren, Donghao and Patel, Kayur and DuBois, Chris},
title = {Tempura: Query Analysis with Structural Templates},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376451},
doi = {10.1145/3313831.3376451},
abstract = {Analyzing queries from search engines and intelligent assistants is difficult. A key challenge is organizing queries into interpretable, context-preserving, representative, and flexible groups. We present structural templates, abstract queries that replace tokens with their linguistic feature forms, as a query grouping method. The templates allow analysts to create query groups with structural similarity at different granularities. We introduce Tempura, an interactive tool that lets analysts explore a query dataset with structural templates. Tempura summarizes a query dataset by selecting a representative subset of templates to show the query distribution. The tool also helps analysts navigate the template space by suggesting related templates likely to yield further explorations. Our user study shows that Tempura helps analysts examine the distribution of a query dataset, find labeling errors, and discover model error patterns and outliers.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {query analysis, error analysis, natural language processing},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376450,
author = {Park, Sunjeong and Lim, Youn-kyung},
title = {Investigating User Expectations on the Roles of Family-Shared AI Speakers},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376450},
doi = {10.1145/3313831.3376450},
abstract = {AI assistants that use a voice user interface (VUI), such as AI speakers, have become popular in family homes. However, it is still unclear what roles the AI speaker can support within the family unit. We investigated the roles of an AI speaker as a family-shared technology. By conducting a one-week participatory user study, we discovered that family members' co-ownership toward the AI speaker was the key in the development of its family-oriented roles. Our findings showed seven domains of user expectations on these roles, and we realized that all the expectations can be represented as family cohesion. In addition, privacy awareness was emphasized regarding personal supports. Finally, we discuss a new perspective for AI speaker design and offer two suggestions: 1) leveraging human-likeness to develop its potential roles of supporting the unit of a family and 2) interpreting the home context to seamlessly connect family and personal supporting roles.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {family, participatory design study, AI speaker},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376446,
author = {Frommel, Julian and Sagl, Valentin and Depping, Ansgar E. and Johanson, Colby and Miller, Matthew K. and Mandryk, Regan L.},
title = {Recognizing Affiliation: Using Behavioural Traces to Predict the Quality of Social Interactions in Online Games},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376446},
doi = {10.1145/3313831.3376446},
abstract = {Online social interactions in multiplayer games can be supportive and positive or toxic and harmful; however, few methods can easily assess interpersonal interaction quality in games. We use behavioural traces to predict affiliation between dyadic strangers, facilitated through their social interactions in an online gaming setting. We collected audio, video, in-game, and self-report data from 23 dyads, extracted 75 features, trained Random Forest and Support Vector Machine models, and evaluated their performance predicting binary (high/low) as well as continuous affiliation toward a partner. The models can predict both binary and continuous affiliation with up to 79.1% accuracy (F1) and 20.1% explained variance (R2) on unseen data, with features based on verbal communication demonstrating the highest potential. Our findings can inform the design of multiplayer games and game communities, and guide the development of systems for matchmaking and mitigating toxic behaviour in online games.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–16},
numpages = {16},
keywords = {cooperative games, prediction, recognition, affiliation, evaluation, machine learning, bonding, social interaction},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376442,
author = {Drosos, Ian and Barik, Titus and Guo, Philip J. and DeLine, Robert and Gulwani, Sumit},
title = {Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376442},
doi = {10.1145/3313831.3376442},
abstract = {Data wrangling is a difficult and time-consuming activity in computational notebooks, and existing wrangling tools do not fit the exploratory workflow for data scientists in these environments. We propose a unified interaction model based on programming-by-example that generates readable code for a variety of useful data transformations, implemented as a Jupyter notebook extension called Wrex. User study results demonstrate that data scientists are significantly more effective and efficient at data wrangling with Wrex over manual programming. Qualitative participant feedback indicates that Wrex was useful and reduced barriers in having to recall or look up the usage of various data transform functions. The synthesized code allowed data scientists to verify the intended data transformation, increased their trust and confidence in Wrex, and fit seamlessly within their cell-based notebook workflows. This work suggests that presenting readable code to professional data scientists is an indispensable component of offering data wrangling tools in notebooks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {data science, program synthesis, computational notebooks},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376438,
author = {Sidenmark, Ludwig and Clarke, Christopher and Zhang, Xuesong and Phu, Jenny and Gellersen, Hans},
title = {Outline Pursuits: Gaze-Assisted Selection of Occluded Objects in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376438},
doi = {10.1145/3313831.3376438},
abstract = {In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {smooth pursuits, occlusion, eye tracking, virtual reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376436,
author = {Chen, Zhutian and Tong, Wai and Wang, Qianwen and Bach, Benjamin and Qu, Huamin},
title = {Augmenting Static Visualizations with PapARVis Designer},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376436},
doi = {10.1145/3313831.3376436},
abstract = {This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality.Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workflow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wall-sized visualizations. A user study shows high user satisfaction of our environment and confirms that participants can create augmented visualizations in an average of 4.63 minutes.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {data visualization authoring, augmented static visualization, visualization in augmented reality},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376423,
author = {Pfau, Johannes and Smeddinck, Jan David and Malaka, Rainer},
title = {Enemy Within: Long-Term Motivation Effects of Deep Player Behavior Models for Dynamic Difficulty Adjustment},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376423},
doi = {10.1145/3313831.3376423},
abstract = {Balancing games and producing content that remains interesting and challenging is a main cost factor in the design and maintenance of games. Dynamic difficulty adjustments (DDA) can successfully tune challenge levels to player abilities, but when implemented with classic heuristic parameter tuning (HPT) often turns out to be very noticeable, e.g. as "rubber-banding". Deep learning techniques can be employed for deep player behavior modeling (DPBM), enabling more complex adaptivity, but effects over frequent and longer-lasting game engagements, as well as how it compares to HPT has not been empirically investigated. We present a situated study of the effects of DDA via DPBM as compared to HPT on intrinsic motivation, perceived challenge and player motivation in a real-world MMORPG. The results indicate that DPBM can lead to significant improvements in intrinsic motivation and players prefer game experience episodes featuring DPBM over experience episodes with classic difficulty management.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {games, MMORPGs, neural networks, deep learning, player modeling, dynamic difficulty adjustment},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376401,
author = {Kepplinger, Daniel and Wallner, G\"{u}nter and Kriglstein, Simone and Lankes, Michael},
title = {See, Feel, Move: Player Behaviour Analysis through Combined Visualization of Gaze, Emotions, and Movement},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376401},
doi = {10.1145/3313831.3376401},
abstract = {Playtesting of games often relies on a mixed-methods approach to obtain more holistic insights about and, in turn, improve the player experience. However, triangulating the different data sources and visualizing them in an integrated manner such that they contextualize each other still proves challenging. Despite its potential value for gauging player behaviour, this area of research continues to be underexplored. In this paper, we propose a visualization approach that combines commonly tracked movement data with - from a visualization perspective rarely considered - gaze behaviour and emotional responses. We evaluated our approach through a qualitative expert study with five professional game developers. Our results show that both the individual visualization of gaze, emotions, and movement but especially their combination are valuable to understand and form hypotheses about player behaviour. At the same time, our results stress that careful attention needs to be paid to ensure that the visualization remains legible and does not obfuscate information.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {visual game analytics, playtesting, information visualization, emotions, gaze, movement analysis},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376393,
author = {Wong, Pui Chung and Zhu, Kening and Yang, Xing-Dong and Fu, Hongbo},
title = {Exploring Eyes-Free Bezel-Initiated Swipe on Round Smartwatches},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376393},
doi = {10.1145/3313831.3376393},
abstract = {Bezel-based gestures expand the interaction space of touch-screen devices (e.g., smartphones and smartwatches). Existing works have mainly focused on bezel-initiated swipe (BIS) on square screens. To investigate the usability of BIS on round smartwatches, we design six different circular bezel layouts, by dividing the bezel into 6, 8, 12, 16, 24, and 32 segments. We evaluate the user performance of BIS on these layouts in an eyes-free situation. The results show that the performance of BIS is highly orientation dependent, and varies significantly among users. Using the Support-Vector-Machine (SVM) model significantly increases the accuracy on 6-, 8-, 12-, and 16-segment layouts. We then compare the performance of personal and general SVM models, and find that personal models significantly improve the accuracy for 8-, 12-, 16-, and 24-segment layouts. Lastly, we discuss the potential smartwatch applications enabled by the BIS.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {round smartwatches, bezel, bezel-initiated gestures, eyes-free},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376387,
author = {Zhang, Hechuan and Chen, Zhiyong and Guo, Shihui and Lin, Juncong and Shi, Yating and Liu, Xiangyang and Ma, Yong},
title = {Sensock: 3D Foot Reconstruction with Flexible Sensors},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376387},
doi = {10.1145/3313831.3376387},
abstract = {Capturing 3D foot models is important for applications such as manufacturing customized shoes and creating clubfoot orthotics. In this paper, we propose a novel prototype, Sensock, to offer a fully wearable solution for the task of 3D foot reconstruction. The prototype consists of four soft stretchable sensors, made from silk fibroin yarn. We identify four characteristic foot girths based on the existing knowledge of foot anatomy, and measure their lengths with the resistance value of the stretchable sensors. A learning-based model is trained offline and maps the foot girths to the corresponding 3D foot shapes. We compare our method with existing solutions using red-green-blue (RGB) or RGBD (RGB-depth) cameras, and show the advantages of our method in terms of both efficiency and accuracy. In the user experiment, we find that the relative error of Sensock is lower than 0.55%. It performs consistently across different trials and is considered comfortable and suitable for long-term wearing.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {3d reconstruction, flexible sensors, foot modeling},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376385,
author = {Bitton, Ron and Boymgold, Kobi and Puzis, Rami and Shabtai, Asaf},
title = {Evaluating the Information Security Awareness of Smartphone Users},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376385},
doi = {10.1145/3313831.3376385},
abstract = {Information security awareness (ISA) is a practice focused on the set of skills which help a user successfully mitigate social engineering (SE) attacks. Evaluating the ISA of users is crucial, since early identification of users who are more vulnerable to SE attacks improves system security. Previous studies for evaluating the ISA of smartphone users rely on subjective data sources (questionnaires) and do not address the differences between classes of SE attacks. This paper presents a framework for evaluating the ISA of smartphone users for specific attack classes. In addition to questionnaires, we utilize objective data sources: a mobile agent, a network traffic monitor, and cybersecurity challenges. We evaluated the framework by conducting a long-term user study involving 162 users. The results show that: the self-reported behavior of users differs significantly from their actual behavior and the ISA level derived from the actual behavior of users is highly correlated with their ability to mitigate SE attacks.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {social engineering, mobile devices, information security awareness, human factors},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376376,
author = {Valencia, Stephanie and Pavel, Amy and Santa Maria, Jared and Yu, Seunga (Gloria) and Bigham, Jeffrey P. and Admoni, Henny},
title = {Conversational Agency in Augmentative and Alternative Communication},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376376},
doi = {10.1145/3313831.3376376},
abstract = {Augmented communicators (ACs) use augmentative and alternative communication (AAC) technologies to speak. Prior work in AAC research has looked to improve efficiency and expressivity of AAC via device improvements and user training. However, ACs also face constraints in communication beyond their device and individual abilities such as when they can speak, what they can say, and who they can address. In this work, we recast and broaden this prior work using conversational agency as a new frame to study AC communication. We investigate AC conversational agency with a study examining different conversational tasks between four triads of expert ACs, their close conversation partners (paid aide or parent), and a third party (experimenter). We define metrics to analyze AAC conversational agency quantitatively and qualitatively. We conclude with implications for future research to enable ACs to easily exercise conversational agency.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {agency, aac, accessibility, cerebral palsy, conversation},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376373,
author = {Adams, Alexander T. and Mandel, Ilan and Shats, Anna and Robin, Alina and Choudhury, Tanzeem},
title = {PuffPacket: A Platform for Unobtrusively Tracking the Fine-Grained Consumption Patterns of E-Cigarette Users},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376373},
doi = {10.1145/3313831.3376373},
abstract = {The proliferation of e-cigarettes and portable vaporizers presents new opportunities for accurately and unobtrusively tracking e-cigarette use. PuffPacket is a hardware and soft-ware research platform that leverages the technology built into vaporizers, e-cigarettes and other electronic drug delivery devices to ubiquitously track their usage. The system piggybacks on the signals these devices use to directly measure and track the nicotine consumed by users. PuffPacket augments e-cigarettes with Bluetooth to calculate the frequency, intensity, and duration of each inhalation. This information is augmented with smartphone-based location and activity information to help identify potential contextual triggers. Puff-Packet is generalizable to a wide variety of electronic nicotine,THC, and other drug delivery devices currently on the mar-ket. The hardware and software for PuffPacket is open-source so it can be expanded upon and leveraged for mobile health tracking research.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {e-cigarettes, ends, topology, mobile sensing, addiction, health, research platform, sud},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376370,
author = {Honary, Mahsa and Bell, Beth and Clinch, Sarah and Vega, Julio and Kroll, Leo and Sefi, Aaron and McNaney, Roisin},
title = {Shaping the Design of Smartphone-Based Interventions for Self-Harm},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376370},
doi = {10.1145/3313831.3376370},
abstract = {Self-harm is a prevalent issue amongst young people, yet it is thought around 40% will never seek professional help due to stigma surrounding it. It is generally a way of coping with emotional distress and can have a range of triggers which are highly heterogeneous to the individual. In a move towards enhancing the accessibility of personalized interventions for self-harm, we undertook a three-stage study. We first conducted interviews with 4 counsellors in self-harm to understand how they clinically respond to self-harm triggers. We then ran a survey with 37 young people, to explore perceptions of mobile sensing, and current and future uses for smartphone-based interventions. Finally, we ran a workshop with 11 young people to further explore how a context-aware self-management application might be used to support them. We contribute an in-depth understanding of how triggers for self-harm might be identified and subsequently predicted and prevented using mobile-sensing technology.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {self-harm, mental health, mobile sensing, co-design, situation-aware app, non-suicidal self-injury, trust, intervention},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376368,
author = {Meena, Yogesh Kumar and Seunarine, Krishna and Sahoo, Deepak Ranjan and Robinson, Simon and Pearson, Jennifer and Zhang, Chi and Carnie, Matt and Pockett, Adam and Prescott, Andrew and Thomas, Suzanne K. and Lee, Harrison Ka Hin and Jones, Matt},
title = {PV-Tiles: Towards Closely-Coupled Photovoltaic and Digital Materials for Useful, Beautiful and Sustainable Interactive Surfaces},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376368},
doi = {10.1145/3313831.3376368},
abstract = {The interactive, digital future with its seductive vision of Internet-of-Things connected sensors, actuators and displays comes at a high cost in terms of both energy demands and the clutter it brings to the physical world. But what if such devices were made of materials that enabled them to self-power their interactive features? And, what if those materials were directly used to build aesthetically pleasing environments and objects that met practical physical needs as well as digital ones? In this paper we introduce PV-Tiles ? a novel material that closely couples photovoltaic energy harvesting and light sensing materials with digital interface components. We consider potential contexts, use-cases and light gestures surfaced through co-creation workshops; and, present initial technological designs and prototypes. The work opens a new set of opportunities and collaborations between HCI and material science, stimulating technical and design pointers to accommodate and exploit the material's properties.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {connected home, sustainability, self-powered devices, internet of things, interaction design},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376355,
author = {Porfirio, David and Saupp\'{e}, Allison and Albarghouthi, Aws and Mutlu, Bilge},
title = {Transforming Robot Programs Based on Social Context},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376355},
doi = {10.1145/3313831.3376355},
abstract = {Social robots have varied effectiveness when interacting with humans in different interaction contexts. A robot programmed to escort individuals to a different location, for instance, may behave more appropriately in a crowded airport than a quiet library, or vice versa. To address these issues, we exploit ideas from program synthesis and propose an approach to transforming the structure of hand-crafted interaction programs that uses user-scored execution traces as input, in which end users score their paths through the interaction based on their experience. Additionally, our approach guarantees that transformations to a program will not violate task and social expectations that must be maintained across contexts. We evaluated our approach by adapting a robot program to both real-world and simulated contexts and found evidence that making informed edits to the robot's program improves user experience.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {interaction adaptation, human-robot interaction, program repair, model checking},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376353,
author = {Siu, Alexa F. and Sinclair, Mike and Kovacs, Robert and Ofek, Eyal and Holz, Christian and Cutrell, Edward},
title = {Virtual Reality Without Vision: A Haptic and Auditory White Cane to Navigate Complex Virtual Worlds},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376353},
doi = {10.1145/3313831.3376353},
abstract = {Current Virtual Reality (VR) technologies focus on rendering visuospatial effects, and thus are inaccessible for blind or low vision users. We examine the use of a novel white cane controller that enables navigation without vision of large virtual environments with complex architecture, such as winding paths and occluding walls and doors. The cane controller employs a lightweight three-axis brake mechanism to provide large-scale shape of virtual objects. The multiple degrees-of-freedom enables users to adapt the controller to their preferred techniques and grip. In addition, surface textures are rendered with a voice coil actuator based on contact vibrations; and spatialized audio is determined based on the progression of sound through the geometry around the user. We design a scavenger hunt game that demonstrates how our device enables blind users to navigate a complex virtual environment. Seven out of eight users were able to successfully navigate the virtual room (6x6m) to locate targets while avoiding collisions. We conclude with design consideration on creating immersive non-visual VR experiences based on user preferences for cane techniques, and cane material properties.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {blindness, white cane, virtual reality, visual impairments, haptic feedback, mobility, 3d audio, auditory feedback},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376309,
author = {Kornfield, Rachel and Zhang, Renwen and Nicholas, Jennifer and Schueller, Stephen M. and Cambo, Scott A. and Mohr, David C. and Reddy, Madhu},
title = {"Energy is a Finite Resource": Designing Technology to Support Individuals across Fluctuating Symptoms of Depression},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376309},
doi = {10.1145/3313831.3376309},
abstract = {While the HCI field increasingly examines how digital tools can support individuals in managing mental health conditions, it remains unclear how these tools can accommodate these conditions' temporal aspects. Based on weekly interviews with five individuals with depression, conducted over six weeks, this study identifies design opportunities and challenges related to extending technology-based support across fluctuating symptoms. Our findings suggest that participants perceive events and contexts in daily life to have marked impact on their symptoms. Results also illustrate that ebbs and flows in symptoms profoundly affect how individuals practice depression self-management. While digital tools often aim to reach individuals while they feel depressed, we suggest they should also engage individuals when they are less symptomatic, leveraging their energy and motivation to build habits, establish plans and goals, and generate and organize content to prepare for symptom onset.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–17},
numpages = {17},
keywords = {depression, personalization, temporality, tailoring, mental health, digital interventions, motivation},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376303,
author = {Strandholt, Patrick L. and Dogaru, Oana A. and Nilsson, Niels C. and Nordahl, Rolf and Serafin, Stefania},
title = {Knock on Wood: Combining Redirected Touching and Physical Props for Tool-Based Interaction in Virtual Reality},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376303},
doi = {10.1145/3313831.3376303},
abstract = {When physical props serve as proxies for virtual tools used to manipulate the virtual environment, it is challenging to provide appropriate haptic feedback. Redirected tool-mediated manipulation addresses this challenge by distorting the mapping between physical and virtual tools to provide a sensation of manipulating the virtual environment, when the physical tool comes into contact with another physical prop. For example, a virtual hammer's position can be offset to ensure that physical impacts accompany each strike of a virtual nail. We demonstrate the idea by showing that it can be used to create sensations of impact and resistance when driving a virtual nail into a surface, when tightening a virtual screw, and when sawing through a virtual plank. The results of a user study indicate that the proposed approach is perceived as more realistic than interaction with a single physical prop or controller and no notable detriments to precision were observed.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {virtual reality, redirected touching, passive haptics},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376296,
author = {Henderson, Jay and Malacria, Sylvain and Nancel, Mathieu and Lank, Edward},
title = {Investigating the Necessity of Delay in Marking Menu Invocation},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376296},
doi = {10.1145/3313831.3376296},
abstract = {Delayed display of menu items is a core design component of marking menus, arguably to prevent visual distraction and foster the use of mark mode. We investigate these assumptions, by contrasting the original marking menu design with immediately-displayed marking menus. In three controlled experiments, we fail to reveal obvious and systematic performance or usability advantages to using delay and mark mode. Only in very constrained settings – after significant training and only two items to learn – did traditional marking menus show a time improvement of about 260~ms. Otherwise, we found an overall decrease in performance with delay, whether participants exhibited practiced or unpracticed behaviour. Our final study failed to demonstrate that an immediately-displayed menu interface is more visually disrupting than a delayed menu. These findings inform the costs and benefits of incorporating delay in marking menus, and motivate guidelines for situations in which its use is desirable.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {marking menu, delay},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376295,
author = {Gadiraju, Vinitha and Muehlbradt, Annika and Kane, Shaun K.},
title = {BrailleBlocks: Computational Braille Toys for Collaborative Learning},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376295},
doi = {10.1145/3313831.3376295},
abstract = {Braille literacy has fallen in recent years, and many blind children now grow up without learning Braille. However, learning Braille can increase employment chances and improve literacy skills. We introduce BrailleBlocks, a system to help visually impaired children learn and practice Braille alongside a sighted parent. BrailleBlocks comprises a set of tangible blocks and pegs, each block representing a Braille cell, and an associated application with games. The system automatically tracks and recognizes the blocks so that parents can follow along even if they cannot read Braille. We conducted a user study to test BrailleBlocks with five families, with five parents and six visually impaired children. The contributions of this work are a novel approach to Braille education toys, observations of how visually impaired children and sighted parents used this system together, their insights on current issues with Braille educational tools, and actionable feedback for future Braille-based learning tools.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {collaboration, blind, visually impaired, children, accessibility, education, braille},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376294,
author = {Mustafa, Maryam and Batool, Amna and Fatima, Beenish and Nawaz, Fareeda and Toyama, Kentaro and Raza, Agha Ali},
title = {Patriarchy, Maternal Health and Spiritual Healing: Designing Maternal Health Interventions in Pakistan},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376294},
doi = {10.1145/3313831.3376294},
abstract = {We examine the opportunities and challenges in designing for maternal health in low-income, low-resource communities in patriarchal and religious contexts. Pakistan faces a crisis in maternal health with a maternal mortality ratio of 178 deaths per 100,000 live births, as compared to the developed-country average of just 12 deaths per 100,000. Through a 6-month long qualitative, empirical study we examine the prevalent beliefs and practices around maternal health in Pakistan, the access women have to health-care, the existing religious practices that influence them and the agency they exert in their own health-care decision making. We reveal the rampant misinformation among mothers and health workers, house-hold power dynamics that impact maternal health and the deep link between maternal health and religious beliefs. We also show how current maternal health care interventions fit poorly into this context and discuss alternate design recommendations for meeting the maternal health needs of these women.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {maternal health, hci4d, patriarchy},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3313831.3376266,
author = {Kurzhals, Kuno and G\"{o}bel, Fabian and Angerbauer, Katrin and Sedlmair, Michael and Raubal, Martin},
title = {A View on the Viewer: Gaze-Adaptive Captions for Videos},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376266},
doi = {10.1145/3313831.3376266},
abstract = {Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {multimedia, subtitles, gaze input, eye tracking, gaze-responsive display, video captions},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

