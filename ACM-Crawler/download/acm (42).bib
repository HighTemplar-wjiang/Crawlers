@inproceedings{10.1145/3411764.3445786,
author = {Kim, Hyunyoung and Everitt, Aluna and Tejada, Carlos and Zhong, Mengyu and Ashbrook, Daniel},
title = {MorpheesPlug: A Toolkit for Prototyping Shape-Changing Interfaces},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445786},
doi = {10.1145/3411764.3445786},
abstract = {Toolkits for shape-changing interfaces (SCIs) enable designers and researchers to easily explore the broad design space of SCIs. However, despite their utility, existing approaches are often limited in the number of shape-change features they can express. This paper introduces MorpheesPlug , a toolkit for creating SCIs that covers seven of the eleven shape-change features identified in the literature. MorpheesPlug is comprised of (1) a set of six standardized widgets that express the shape-change features with user-definable parameters; (2) software for 3D-modeling the widgets to create 3D-printable pneumatic SCIs; and (3) a hardware platform to control the widgets. To evaluate MorpheesPlug we carried out ten open-ended interviews with novice and expert designers who were asked to design a SCI using our software. Participants highlighted the ease of use and expressivity of the MorpheesPlug.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {101},
numpages = {13},
keywords = {Shape-changing Interfaces, Toolkit design},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445772,
author = {Goguey, Alix and Gutwin, Carl and Chen, Zhe and Suwanaposee, Pang and Cockburn, Andy},
title = {Interaction Pace and User Preferences},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445772},
doi = {10.1145/3411764.3445772},
abstract = {The overall pace of interaction combines the user’s pace and the system’s pace, and a pace mismatch could impair user preferences (e.g., animations or timeouts that are too fast or slow for the user). Motivated by studies of speech rate convergence, we conducted an experiment to examine whether user preferences for system pace are correlated with user pace. Subjects first completed a series of trials to determine their user pace. They then completed a series of hierarchical drag-and-drop trials in which folders automatically expanded when the cursor hovered for longer than a controlled timeout. Results showed that preferences for timeout values correlated with user pace – slow-paced users preferred long timeouts, and fast-paced users preferred short timeouts. Results indicate potential benefits in moving away from fixed or customisable settings for system pace. Instead, systems could improve preferences by automatically adapting their pace to converge towards that of the user.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {195},
numpages = {14},
keywords = {timeouts, Interaction pace convergence, user preferences.},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445771,
author = {Zhang, Renwen and E. Ringland, Kathryn and Paan, Melina and C. Mohr, David and Reddy, Madhu},
title = {Designing for Emotional Well-Being: Integrating Persuasion and Customization into Mental Health Technologies},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445771},
doi = {10.1145/3411764.3445771},
abstract = {A growing body of work has emphasized the need for customizability and flexibility in mobile health technologies to increase support user autonomy. However, customization may be burdensome for people with motivational and cognitive challenges, such as those with mental illnesses, and the optimal level and type of customizability are unclear. Based on 32 interviews with people who experience symptoms of depression and anxiety, we examine how individuals use and customize mental health apps to manage their symptoms. Our findings suggest that participants’ engagement with the apps is affected by their level of energy and motivation, depending on the severity of symptoms. Customization is deemed desirable when the required user effort does not exceed users’ mental and motivational capacity and when ample resources are available. We discuss how customizable systems can increase autonomy without overburdening users in the context of mental health.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {542},
numpages = {13},
keywords = {behavior change, mobile apps, customization, persuasion, mental health, ethics, mHealth},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445763,
author = {Ding, Xianghua (Sharon) and Wei, Shuhan and Gui, Xinning and Gu, Ning and Zhang, Peng},
title = {Data Engagement Reconsidered: A Study of Automatic Stress Tracking Technology in Use},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445763},
doi = {10.1145/3411764.3445763},
abstract = {In today’s fast-paced world, stress has become a growing health concern. While more automatic stress tracking technologies have recently become available on wearable or mobile devices, there is still a limited understanding of how they are actually used in everyday life. This paper presents an empirical study of automatic stress-tracking technologies in use in China, based on semi-structured interviews with 17 users. The study highlights three challenges of stress-tracking data engagement that prevent effective technology usage: the lack of immediate awareness, the lack of pre-required knowledge, and the lack of corresponding communal support. Drawing on the stress-tracking practices uncovered in the study, we bring these issues to the fore, and unpack assumptions embedded in related works on self-tracking and how data engagement is approached. We end by calling for a reconsideration of data engagement as part of self-tracking practices with technologies rather than simply looking at the user interface.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {535},
numpages = {13},
keywords = {self-tracking, stress-tracking, knowing},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445758,
author = {Ali, Abdullah and Ringel Morris, Meredith and O. Wobbrock, Jacob},
title = {“I Am Iron Man”: Priming Improves the Learnability and Memorability of User-Elicited Gestures},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445758},
doi = {10.1145/3411764.3445758},
abstract = {Priming is used as a way of increasing the diversity of proposals in end-user elicitation studies, but priming has not been investigated thoroughly in this context. We conduct a distributed end-user elicitation study with 167 participants, which had three priming groups: a no-priming control group, sci-fi priming, and a creative mindset group. We evaluated the gestures proposed by these groups in a distributed learnability and memorability study with 18 participants. We found that the user-elicited gestures from the sci-fi group were significantly faster to learn, requiring an average of 1.22 viewings to learn compared to 1.60 viewings required to learn the control gestures, and 1.56 viewings to learn the gestures elicited from the creative mindset group. In addition, both primed gesture groups had higher memorability with 80% of the sci-fi-primed gestures and 73% of the creative mindset group gestures were recalled correctly after one week without practice compared to 43% of the control group gestures.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {359},
numpages = {14},
keywords = {crowdsourcing, Mechanical Turk, memorability, end-user identification, learnability, Distributed Interaction Design, end-user elicitation study},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445753,
author = {D. Molina, Maria and Sundar, S. Shyam and Rony, Md Main Uddin and Hassan, Naeemul and Le, Thai and Lee, Dongwon},
title = {Does Clickbait Actually Attract More Clicks? Three Clickbait Studies You Must Read},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445753},
doi = {10.1145/3411764.3445753},
abstract = {Studies show that users do not reliably click more often on headlines classified as clickbait by automated classifiers. Is this because the linguistic criteria (e.g., use of lists or questions) emphasized by the classifiers are not psychologically relevant in attracting interest, or because their classifications are confounded by other unknown factors associated with assumptions of the classifiers? We address these possibilities with three studies—a quasi-experiment using headlines classified as clickbait by three machine-learning models (Study 1), a controlled experiment varying the headline of an identical news story to contain only one clickbait characteristic (Study 2), and a computational analysis of four classifiers using real-world sharing data (Study 3). Studies 1 and 2 revealed that clickbait did not generate more curiosity than non-clickbait. Study 3 revealed that while some headlines generate more engagement, the detectors agreed on a classification only 47% of the time, raising fundamental questions about their validity.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {234},
numpages = {19},
keywords = {machine learning, content perception, engagement, clickbait},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445745,
author = {Dylan, Thomas and Durrant, Abigail and \c{C}er\c{c}i, Sena and Lawson, Shaun and Vines, John},
title = {Lanterns: Configuring a Digital Resource to Inspire Preschool Children's Free Play Outdoors},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445745},
doi = {10.1145/3411764.3445745},
abstract = {Previous HCI research has highlighted opportunities for digital technologies to support outdoor play amongst children. However, the tendency has been to focus on older children and forms of play that are structured and rule-based. We report on a Research-through-Design (RtD) inquiry, grounded in an Embodied Interactional approach, that investigated configurations of off-the-shelf Internet of Things (IoT) tool-kits to inspire new forms of free play outdoors for preschool children. We designed the Lanterns, a tangible interactive resource that is made using household materials and guided by a template, and which explores new possibilities to inspire social play and embodied interaction outdoors. Based on observations of the Lanterns being used by preschool children and Early Years Practitioners outdoors, we identify qualities of free play promoted by the Lanterns outdoors, such as enchantment, improvisation, anticipation and choice. We discuss our findings by defining three sensitising concepts to support future design research in this space: Choosing the Way; Improvising through Movement; Anticipating a Response.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {635},
numpages = {15},
keywords = {iot, preschool, free play, tool-kits, interaction design, pervasive play, children, tangibles, outdoor play},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445738,
author = {M. Faas, Stefanie and Kraus, Johannes and Schoenhals, Alexander and Baumann, Martin},
title = {Calibrating Pedestrians' Trust in Automated Vehicles: Does an Intent Display in an External HMI Support Trust Calibration and Safe Crossing Behavior?},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445738},
doi = {10.1145/3411764.3445738},
abstract = {Policymakers recommend that automated vehicles (AVs) display their automated driving status using an external human-machine interface (eHMI). However, previous studies suggest that a status eHMI is associated with overtrust, which might be overcome by an additional yielding intent message. We conducted a video-based laboratory study (N = 67) to investigate pedestrians’ trust and crossing behavior in repeated encounters with AVs. In a 2x2 between-subjects design, we investigated (1) the occurrence of a malfunction (AV failing to yield) and (2) system transparency (status eHMI vs. status+intent eHMI). Results show that during initial encounters, trust gradually increases and crossing onset time decreases. After a malfunction, trust declines but recovers quickly. In the status eHMI group, trust was reduced more, and participants showed 7.3 times higher odds of colliding with the AV as compared to the status+intent group. We conclude that a status eHMI can cause pedestrians to overtrust AVs and advocate additional intent messages.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {157},
numpages = {17},
keywords = {automated vehicles, external human-machine interface, pedestrians, malfunction, trust in automation, Self-driving vehicles, transparency},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445735,
author = {Hong, Matthew K. and Fourney, Adam and DeBellis, Derek and Amershi, Saleema},
title = {Planning for Natural Language Failures with the AI Playbook},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445735},
doi = {10.1145/3411764.3445735},
abstract = {Prototyping AI user experiences is challenging due in part to probabilistic AI models making it difficult to anticipate, test, and mitigate AI failures before deployment. In this work, we set out to support practitioners with early AI prototyping, with a focus on natural language (NL)-based technologies. Our interviews with 12 NL practitioners from a large technology company revealed that, in addition to challenges prototyping AI, prototyping was often not happening at all or focused only on idealized scenarios due to a lack of tools and tight timelines. These findings informed our design of the AI Playbook, an interactive and low-cost tool we developed to encourage proactive and systematic consideration of AI errors before deployment. Our evaluation of the AI Playbook demonstrates its potential to 1) encourage product teams to prioritize both ideal and failure scenarios, 2) standardize the articulation of AI failures from a user experience perspective, and 3) act as a boundary object between user experience designers, data scientists, and engineers.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {386},
numpages = {11},
keywords = {prototyping, Human-AI interaction, natural language technologies, AI failures},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445732,
author = {Garza, Jorge and Merrill, Devon J. and Swanson, Steven},
title = {Appliancizer: Transforming Web Pages into Electronic Devices},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445732},
doi = {10.1145/3411764.3445732},
abstract = {Prototyping electronic devices that meet today’s consumer standards is a time-consuming task that requires multi-domain expertise. Consumers expect electronic devices to have visually appealing interfaces with both tactile and screen-based interfaces. Appliancizer, our interactive computational design tool, exploits the similarities between graphical and tangible interfaces, allowing web pages to be rapidly transformed into physical electronic devices. Using a novel technique we call essential interface mapping, our tool converts graphical user interface elements (e.g., an HTML button) into tangible interface components (e.g., a physical button) without changing the application source code. Appliancizer automatically generates the PCB and low-level code from web-based prototypes and HTML mock-ups. This makes the prototyping of mixed graphical-tangible interactions as easy as modifying a web page and allows designers to leverage the well-developed ecosystem of web technologies. We demonstrate how our technique simplifies and accelerates prototyping by developing two devices with Appliancizer.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {415},
numpages = {13},
keywords = {human-computer interaction, synthesis, HTML, electronic design automation, Web applications, PCB layout},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445723,
author = {Harrington, Christina and Dillahunt, Tawanna R},
title = {Eliciting Tech Futures Among Black Young Adults: A Case Study of Remote Speculative Co-Design},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445723},
doi = {10.1145/3411764.3445723},
abstract = {The question of who gets to contribute to design futures and technology innovation has become a topic of conversation across HCI, CSCW, and other computing communities. This conversation has grave implications for communities that often find themselves an afterthought in technology design, and who coincidentally could benefit most from technological interventions in response to societal oppression. To explore this topic, we examined “futuring” through co-designed speculative design fictions as methods to envision utopian and dystopian futures. In a case study, we examined technology’s role in the imagined futures of youth participants of a Chicago summer design program. We highlight emerging themes and contribute an analysis of remote co-design through an Afrofuturism lens. Our analysis shows that concepts of utopian futures and technologies to support those futures are still heavily laden with dystopian realities of racism and poverty. We discuss ways that speculative design fictions and futuring can serve to address inclusivity in concept generation for new technologies, and we provide recommendations for conducting design techniques remotely with historically excluded populations.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {397},
numpages = {15},
keywords = {co-design, participatory design fictions, speculative design, Afrofuturism, design workshops},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445721,
author = {Truong, Anh and Chi, Peggy and Salesin, David and Essa, Irfan and Agrawala, Maneesh},
title = {Automatic Generation of Two-Level Hierarchical Tutorials from Instructional Makeup Videos},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445721},
doi = {10.1145/3411764.3445721},
abstract = {We present a multi-modal approach for automatically generating hierarchical tutorials from instructional makeup videos. Our approach is inspired by prior research in cognitive psychology, which suggests that people mentally segment procedural tasks into event hierarchies, where coarse-grained events focus on objects while fine-grained events focus on actions. In the instructional makeup domain, we find that objects correspond to facial parts while fine-grained steps correspond to actions on those facial parts. Given an input instructional makeup video, we apply a set of heuristics that combine computer vision techniques with transcript text analysis to automatically identify the fine-level action steps and group these steps by facial part to form the coarse-level events. We provide a voice-enabled, mixed-media UI to visualize the resulting hierarchy and allow users to efficiently navigate the tutorial (e.g., skip ahead, return to previous steps) at their own pace. Users can navigate the hierarchy at both the facial-part and action-step levels using click-based interactions and voice commands. We demonstrate the effectiveness of segmentation algorithms and the resulting mixed-media UI on a variety of input makeup videos. A user study shows that users prefer following instructional makeup videos in our mixed-media format to the standard video UI and that they find our format much easier to navigate.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {108},
numpages = {16},
keywords = {Video Segmentation., Video Navigation, Video Tutorials},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445717,
author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
title = {Does the Whole Exceed Its Parts? The Effect of AI Explanations on Complementary Team Performance},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445717},
doi = {10.1145/3411764.3445717},
abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {81},
numpages = {16},
keywords = {Explainable AI, Human-AI teams, Augmented intelligence},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445714,
author = {Zhang, Enhao and Banovic, Nikola},
title = {Method for Exploring Generative Adversarial Networks (GANs) via Automatically Generated Image Galleries},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445714},
doi = {10.1145/3411764.3445714},
abstract = {Generative Adversarial Networks (GANs) can automatically generate quality images from learned model parameters. However, it remains challenging to explore and objectively assess the quality of all possible images generated using a GAN. Currently, model creators evaluate their GANs via tedious visual examination of generated images sampled from narrow prior probability distributions on model parameters. Here, we introduce an interactive method to explore and sample quality images from GANs. Our first two user studies showed that participants can use the tool to explore a GAN and select quality images. Our third user study showed that images sampled from a posterior probability distribution using a Markov Chain Monte Carlo (MCMC) method on parameters of images collected in our first study resulted in on average higher quality and more diverse images than existing baselines. Our work enables principled qualitative GAN exploration and evaluation.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {76},
numpages = {15},
keywords = {Interactive model exploration, qualitative model validation.},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445708,
author = {Aneja, Deepali and Hoegen, Rens and McDuff, Daniel and Czerwinski, Mary},
title = {Understanding Conversational and Expressive Style in a Multimodal Embodied Conversational Agent},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445708},
doi = {10.1145/3411764.3445708},
abstract = {Embodied conversational agents have changed the ways we can interact with machines. However, these systems often do not meet users’ expectations. A limitation is that the agents are monotonic in behavior and do not adapt to an interlocutor. We present SIVA (a Socially Intelligent Virtual Agent), an expressive, embodied conversational agent that can recognize human behavior during open-ended conversations and automatically align its responses to the conversational and expressive style of the other party. SIVA leverages multimodal inputs to produce rich and perceptually valid responses (lip syncing and facial expressions) during the conversation. We conducted a user study (N=30) in which participants rated SIVA as being more empathetic and believable than the control (agent without style matching). Based on almost 10 hours of interaction, participants who preferred interpersonal involvement evaluated SIVA as significantly more animate than the participants who valued consideration and independence.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {102},
numpages = {10},
keywords = {multi-modality, Embodied agents, social dialogue, facial expressive style, social behavior, emotional expressions, conversational style},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445701,
author = {MacArthur, Cayley and Grinberg, Arielle and Harley, Daniel and Hancock, Mark},
title = {You’re Making Me Sick: A Systematic Review of How Virtual Reality Research Considers Gender &amp; Cybersickness},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445701},
doi = {10.1145/3411764.3445701},
abstract = {While multiple studies suggest that female-identified participants are more likely to experience cybersickness in virtual reality (VR), our systematic review of 71 eligible VR publications (59 studies and 12 surveys) pertaining to gender and cybersickness reveals a number of confounding factors in study design (e.g., a variety of technical specifications, tasks, content), a lack of demographic data, and a bias in participant recruitment. Our review shows an ongoing need within VR research to more consistently include and report on women’s experiences in VR to better understand the gendered possibility of cybersickness. Based on the gaps identified in our systematic review, we contribute study design recommendations for future work, arguing that gender considerations are necessary at every stage of VR study design, even when the study is not ‘about’ gender.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {401},
numpages = {15},
keywords = {systematic review, simulator sickness, virtual reality, virtual environments, cybersickness, sex, gender},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445692,
author = {Nisser, Martin and Liao, Christina Chen and Chai, Yuchen and Adhikari, Aradhana and Hodges, Steve and Mueller, Stefanie},
title = {LaserFactory: A Laser Cutter-Based Electromechanical Assembly and Fabrication Platform to Make Functional Devices &amp; Robots},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445692},
doi = {10.1145/3411764.3445692},
abstract = {LaserFactory is an integrated fabrication process that augments a commercially available fabrication machine to support the manufacture of fully functioning devices without human intervention. In addition to creating 2D and 3D mechanical structures, LaserFactory creates conductive circuit traces with arbitrary geometries, picks-and-places electronic and electromechanical components, and solders them in place. To enable this functionality, we make four contributions. First, we build a hardware add-on to the laser cutter head that can deposit silver circuit traces and assemble components. Second, we develop a new method to cure dispensed silver using a CO2 laser. Third, we build a motion-based signaling method that allows our system to be readily integrated with commercial laser cutters. Finally, we provide a design and visualization tool for making functional devices with LaserFactory. Having described the LaserFactory system, we demonstrate how it is used to fabricate devices such as a fully functioning quadcopter and a sensor-equipped wristband. Our evaluation shows that LaserFactory can assemble a variety of differently sized components (up to 65g), that these can be connected by narrow traces (down to 0.75mm) that become highly conductive after laser soldering (3.2Ω/m), and that our acceleration-based sensing scheme works reliably (to 99.5% accuracy).},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {663},
numpages = {15},
keywords = {printed electronics, robotics, Human-computer interaction, personal fabrication, rapid prototyping},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445689,
author = {Alexandrovsky, Dmitry and Friehs, Maximilian Achim and Grittner, Jendrik and Putze, Susanne and Birk, Max V. and Malaka, Rainer and Mandryk, Regan L},
title = {Serious Snacking: A Survival Analysis of How Snacking Mechanics Affect Attrition in a Mobile Serious Game},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445689},
doi = {10.1145/3411764.3445689},
abstract = {Many serious games are most effective when played regularly; however, little is known about how individual game elements support player adherence over time. This work draws on evidence from existing frameworks and game design theories as well as from the design of casual games to investigate how individual game mechanics affect player attrition in a serious game. We implemented a math-learning game in which we could individually layer various game mechanics, and over the course of 3 weeks, 99 participants played one of six versions: Baseline, Rewards, Novelty, Completion, Waiting, or Blocking. We compared the game versions by analyzing the players’ performance as well as behaviour. Using survival analysis, we identified that the addition of Completion and Blocking mechanics facilitated the strongest sustained engagement. These findings are congruent with existing theories of player experience and promote the development of guidelines on designing for sustained engagement in serious games.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {113},
numpages = {18},
keywords = {blocking, adherence, missions, design, rewards, game mechanics, waiting, novelty, Game mechanics, snacking.},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445670,
author = {Devakumar, Anjali and Modh, Jay and Saket, Bahador and Baumer, Eric P. S. and De Choudhury, Munmun},
title = {A Review on Strategies for Data Collection, Reflection, and Communication in Eating Disorder Apps},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445670},
doi = {10.1145/3411764.3445670},
abstract = {Eating disorders (EDs) constitute a mental illness with the highest mortality. Today, mobile health apps provide promising means to ED patients for managing their condition. Apps enable users to monitor their eating habits, thoughts, and feelings, and offer analytic insights for behavior change. However, not only have scholars critiqued the clinical validity of these apps, their underlying design principles are not well understood. Through a review of 34 ED apps, we uncovered 11 different data types ED apps collect, and 9 strategies they employ to support collection and reflection. Drawing upon personal health informatics and visualization frameworks, we found that most apps did not adhere to best practices on what and how data should be collected from and reflected to users, or how data-driven insights should be communicated. Our review offers suggestions for improving the design of ED apps such that they can be useful and meaningful in ED recovery.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {547},
numpages = {19},
keywords = {reflection, self-tracking, apps, eating disorder},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445664,
author = {K. Miller, Matthew and Johannes Dechant, Martin and L. Mandryk, Regan},
title = {Meeting You, Seeing Me: The Role of Social Anxiety, Visual Feedback, and Interface Layout in a Get-to-Know-You Task via Video Chat.},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445664},
doi = {10.1145/3411764.3445664},
abstract = {The growing number of video chat users includes socially anxious people, but it is not known how video chat interfaces affect their interpersonal interactions. In our first study, we use a get-to-know-you task to show that when video feedback of oneself is disabled, higher social anxiety is associated with more public self-awareness, use of 2nd person pronouns, and experienced anxiety. Higher social anxiety was linked to discussing more topics, but discussing more topics only elicited higher self-disclosure and trust when social anxiety was low. In our second study, we assess these same effects using a presentation layout video chat interface and observe no effects of social anxiety on public self-awareness, 2nd person pronoun use, or number of topics discussed; no effect of feedback on experienced anxiety; and no link between number of topics and self-disclosure. Video chat adopters and designers should consider how feedback and interface layout affect conversations.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {339},
numpages = {14},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445661,
author = {Raju, Dani Kalarikalayil and Seunarine, Krishna and Reitmaier, Thomas and Thomas, Gethin and Meena, Yogesh Kumar and Zhang, Chi and Pockett, Adam and Pearson, Jennifer and Robinson, Simon and Carnie, Matt and Sahoo, Deepak Ranjan and Jones, Matt},
title = {PV-Pix: Slum Community Co-Design of Self-Powered Deformable Smart Messaging Materials},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445661},
doi = {10.1145/3411764.3445661},
abstract = {Working with emergent users in two of Mumbai’s slums, we explored the value and uses of photovoltaic (PV) self-powering digital materials. Through a series of co-design workshops, a diary study and responses by artists and craftspeople, we developed the PV-Pix concept for inter-home connections. Each PV-Pix element consists of a deformable energy harvesting material that, when actuated by a person in one home, changes its physical state both there and in a connected home. To explore the concept we considered two forms of PV-Pix: one uses rigid materials and the other flexible ones. We deployed two low-fidelity prototypes, each constructed of a grid of one PV-Pix type, in four slum homes over a four week period to further understand the usability and uses of the materials, eliciting interesting inter-family communication practices. Encouraged by these results we report on a first-step towards working prototypes and demonstrate the technical viability of the approach.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {304},
numpages = {14},
keywords = {interaction design, connected home, Self-powered devices, Internet of Things, sustainability},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445658,
author = {Raptis, George E. and Katsini, Christina and Cen, Andrew Jian-lan and Arachchilage, Nalin Asanka Gamagedara and Nacke, Lennart E.},
title = {Better, Funner, Stronger: A Gameful Approach to Nudge People into Making Less Predictable Graphical Password Choices},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445658},
doi = {10.1145/3411764.3445658},
abstract = {Graphical user authentication (GUA) is a common alternative to text-based user authentication, where people are required to draw graphical passwords on background images. Such schemes are theoretically considered remarkably secure because they offer a large password space. However, people tend to create their passwords on salient image areas introducing high password predictability. Aiming to help people use the password space more effectively, we propose a gameful password creation process. In this paper, we present GamePass, a gamified mechanism that integrates the GUA password creation process. We provide the first evidence that it is possible to nudge people towards better password choices by gamifying the process. GamePass randomly guides participants’ attention to areas other than the salient areas of authentication images, makes the password creation process more fun, and people are more engaged. Gamifying the password creation process enables users to interact better and make less predictable graphical password choices instead of being forced to use a strict password policy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {112},
numpages = {17},
keywords = {graphical passwords, behavior change, gamification, games and play},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445636,
author = {Foong, Eureka and Gerber, Elizabeth},
title = {Understanding Gender Differences in Pricing Strategies in Online Labor Marketplaces},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445636},
doi = {10.1145/3411764.3445636},
abstract = {The growing online gig economy provides ways for women to participate in a flexible, remote workforce and close the offline gender pay and participation gap. While women in online labor marketplaces earn about as much overall as men, women set lower bill rates suggesting gender differences in pricing strategies. In this study, we surveyed 392 freelancers in the United States (US) on the popular marketplace platform, Upwork, to understand strategies used to set hourly bill rates. We did not find gender differences in pricing strategies that were significantly related to bill rate. Instead, we found that other factors, such as full-time freelancer status and level of self-esteem, may help explain gender differences in bill rates. To better support equity and fairness in the growing gig economy, CHI researchers must identify, assess, and address the complex interaction between societal conditions in online labor markets.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {135},
numpages = {16},
keywords = {online work, pricing, survey, Gender, online labor marketplaces, bill rate, freelancing, gig economy},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445628,
author = {Tsaknaki, Vasiliki and Cotton, Kelsey and Karpashevich, Pavel and Sanches, Pedro},
title = {“Feeling the Sensor Feeling You”: A Soma Design Exploration on Sensing Non-Habitual Breathing},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445628},
doi = {10.1145/3411764.3445628},
abstract = {Though seemingly straightforward and habitual, breathing is a complex bodily function. Problematising the space of designing for breathing as a non-habitual act pertaining to different bodies or situations, we conducted a soma design exploration together with a classical singer. Reflecting on how sensors could capture the impact and somatic experience of being sensed led us to develop a new sensing mechanism using shape-change technologies integrated in the Breathing Shell: a wearable that evokes a reciprocal experience of “feeling the sensor feeling you” when breathing. We contribute with two design implications: 1) Enabling reflections of the somatic impact of being sensed in tandem with the type of data captured, 2) creating a tactile impact of the sensor data on the body. Both implications aim to deepen one’s understanding of how the whole soma relates to or with biosensors and ultimately leading to designing for symbiotic experiences between biosensors and bodies.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {266},
numpages = {16},
keywords = {actuation, shape-change, autobiographical design, non-habitual, soma design, breathing, sensing},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445627,
author = {W\"{o}hler, Leslie and Zembaty, Martin and Castillo, Susana and Magnor, Marcus},
title = {Towards Understanding Perceptual Differences between Genuine and Face-Swapped Videos},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445627},
doi = {10.1145/3411764.3445627},
abstract = {In this paper, we report on perceptual experiments indicating that there are distinct and quantitatively measurable differences in the way we visually perceive genuine versus face-swapped videos. Recent progress in deep learning has made face-swapping techniques a powerful tool for creative purposes, but also a means for unethical forgeries. Currently, it remains unclear why people are misled, and which indicators they use to recognize potential manipulations. Here, we conduct three perceptual experiments focusing on a wide range of aspects: the conspicuousness of artifacts, the viewing behavior using eye tracking, the recognition accuracy for different video lengths, and the assessment of emotions. Our experiments show that responses differ distinctly when watching manipulated as opposed to original faces, from which we derive perceptual cues to recognize face swaps. By investigating physiologically measurable signals, our findings yield valuable insights that may also be useful for advanced algorithmic detection.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {240},
numpages = {13},
keywords = {face swapping, eye tracking, human perception, video manipulation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445623,
author = {Fruchard, Bruno and Strohmeier, Paul and Bennewitz, Roland and Steimle, J\"{u}rgen},
title = {Squish This: Force Input on Soft Surfacesfor Visual Targeting Tasks},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445623},
doi = {10.1145/3411764.3445623},
abstract = {Today’s typical input device is flat, rigid and made of glass. However, advances in sensing technology and interaction design suggest thinking about input on other surface, including soft materials. While touching rigid and soft materials might feel similar, they clearly feel different when pressure is applied to them. Yet, to date, studies only investigated force input on rigid surfaces. We present a first systematic evaluation of the effects of compliance on force input. Results of a visual targeting task for three levels of softness indicate that high force levels appear more demanding for soft surfaces, but that performance is otherwise similar. Performance remained very high (∼ 5% for 20 force levels) regardless of the compliance, suggesting force input was underestimated so far. We infer implications for the design of force input on soft surfaces and conclude that interaction models used on rigid surfaces might be used on soft surfaces.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {219},
numpages = {9},
keywords = {User Study, Soft Surfaces, Pressure Input, Force Input},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445622,
author = {Petersen Matjeka, Louise and Hobye, Mads and Larsen, Henrik Svarrer},
title = {Restraints as a Mechanic for Bodily Play},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445622},
doi = {10.1145/3411764.3445622},
abstract = {This paper presents restraints - directly imposed restrictions on players' bodily movements, as a mechanic for bodily play in HCI. While this is a familiar mechanic in non-digital movement-based games, its potential in designing bodily play experiences in HCI has been scarcely explored. Three types of restraints observed in non-digital movement-based games, are explored here: fixating body parts,&nbsp;excluding body parts and&nbsp;depriving/manipulating bodily senses. Then, we investigate the experiential dynamics of restraints as a bodily play mechanic bridging a phenomenological perspective on bodily movement with theories on play. These investigations form the theoretical framework for the subsequent analysis of five digital body game examples. Building on this analysis and theoretical framework, we formulate five design strategies for implementing restraints as a mechanic for bodily play in HCI. We propose restraints as a generative resource for researchers and designers interested in understanding and designing bodily play experiences in HCI.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {441},
numpages = {14},
keywords = {Phenomenology, Bodily Play, Bodily Play Experiences, Body Games, Game Design, Interaction Design, Movement-Based Games, Restraints},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445621,
author = {Streli, Paul and Holz, Christian},
title = {CapContact: Super-Resolution Contact Areas from Capacitive Touchscreens},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445621},
doi = {10.1145/3411764.3445621},
abstract = {Touch input is dominantly detected using mutual-capacitance sensing, which measures the proximity of close-by objects that change the electric field between the sensor lines. The exponential drop-off in intensities with growing distance enables software to detect touch events, but does not reveal true contact areas. In this paper, we introduce CapContact, a novel method to precisely infer the contact area between the user’s finger and the surface from a single capacitive image. At 8 \texttimes{} super-resolution, our convolutional neural network generates refined touch masks from 16-bit capacitive images as input, which can even discriminate adjacent touches that are not distinguishable with existing methods. We trained and evaluated our method using supervised learning on data from 10 participants who performed touch gestures. Our capture apparatus integrates optical touch sensing to obtain ground-truth contact through high-resolution frustrated total internal reflection. We compare our method with a baseline using bicubic upsampling as well as the ground truth from FTIR images. We separately evaluate our method’s performance in discriminating adjacent touches. CapContact successfully separated closely adjacent touch contacts in 494 of 570 cases (87%) compared to the baseline’s 43 of 570 cases (8%). Importantly, we demonstrate that our method accurately performs even at half of the sensing resolution at twice the grid-line pitch across the same surface area, challenging the current industry-wide standard of a ∼ 4&nbsp;mm sensing pitch. We conclude this paper with implications for capacitive touch sensing in general and for touch-input accuracy in particular.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {289},
numpages = {14},
keywords = {Capacitive sensing, Generative adversarial networks;, Touch input, Accuracy, Contact area, Super-resolution},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445616,
author = {Tahaei, Mohammad and Vaniea, Kami and Beznosov, Konstantin (Kosta) and Wolters, Maria K},
title = {Security Notifications in Static Analysis Tools: Developers’ Attitudes, Comprehension, and Ability to Act on Them},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445616},
doi = {10.1145/3411764.3445616},
abstract = {Static analysis tools (SATs) have the potential to assist developers in finding and fixing vulnerabilities in the early stages of software development, requiring them to be able to understand and act on tools’ notifications. To understand how helpful such SAT guidance is to developers, we ran an online experiment (N=132) where participants were shown four vulnerable code samples (SQL injection, hard-coded credentials, encryption, and logging sensitive data) along with SAT guidance, and asked to indicate the appropriate fix. Participants had a positive attitude towards both SAT notifications and particularly liked the example solutions and vulnerable code. Seeing SAT notifications also led to more detailed open-ended answers and slightly improved code correction answers. Still, most SAT (SpotBugs 67%, SonarQube 86%) and Control (96%) participants answered at least one code-correction question incorrectly. Prior software development experience, perceived vulnerability severity, and answer confidence all positively impacted answer accuracy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {691},
numpages = {17},
keywords = {software developers, usable security, static analysis tools, security notifications},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445615,
author = {Samrose, Samiha and McDuff, Daniel and Sim, Robert and Suh, Jina and Rowan, Kael and Hernandez, Javier and Rintel, Sean and Moynihan, Kevin and Czerwinski, Mary},
title = {MeetingCoach: An Intelligent Dashboard for Supporting Effective &amp; Inclusive Meetings},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445615},
doi = {10.1145/3411764.3445615},
abstract = {Video-conferencing is essential for many companies, but its limitations in conveying social cues can lead to ineffective meetings. We present MeetingCoach, an intelligent post-meeting feedback dashboard that summarizes contextual and behavioral meeting information. Through an exploratory survey&nbsp;(N=120), we identified important signals&nbsp;(e.g., turn taking, sentiment) and used these insights to create a wireframe dashboard. The design was evaluated with in situ participants&nbsp;(N=16) who helped identify the components they would prefer in a post-meeting dashboard. After recording video-conferencing meetings of eight teams over four weeks, we developed an AI system to quantify the meeting features and created personalized dashboards for each participant. Through interviews and surveys&nbsp;(N=23), we found that reviewing the dashboard helped improve attendees’ awareness of meeting dynamics, with implications for improved effectiveness and inclusivity. Based on our findings, we provide suggestions for future feedback system designs of video-conferencing meetings.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {252},
numpages = {13},
keywords = {meeting, group, sensing, feedback, video-conferencing},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445613,
author = {Muthukumarana, Sachith and Messerschmidt, Moritz Alexander and Matthies, Denys J.C. and Steimle, J\"{u}rgen and Scholl, Philipp M. and Nanayakkara, Suranga},
title = {ClothTiles: A Prototyping Platform to Fabricate Customized Actuators on Clothing Using 3D Printing and Shape-Memory Alloys},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445613},
doi = {10.1145/3411764.3445613},
abstract = {Emerging research has demonstrated the viability of on-textile actuation mechanisms, however, an easily customizable and versatile on-cloth actuation mechanism is yet to be explored. In this paper, we present ClothTiles along with its rapid fabrication technique that enables actuation of clothes. ClothTiles leverage flexible 3D-printing and Shape-Memory Alloys (SMAs) alongside new parametric actuation designs. We validate the concept of fabric actuation using a base element, and then systematically explore methods of aggregating, scaling, and orienting prospects for extended actuation in garments. A user study demonstrated that our technique enables multiple actuation types applied across a variety of clothes. Users identified both aesthetic and functional applications of ClothTiles. We conclude with a number of insights for the Do-It-Yourself community on how to employ 3D-printing with SMAs to enable actuation on clothes.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {510},
numpages = {12},
keywords = {Textile, Do-It-Yourself, Smart Textiles, Clothing, Shape-memory Alloy, Actuation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445610,
author = {Mathur, Arunesh and Kshirsagar, Mihir and Mayer, Jonathan},
title = {What Makes a Dark Pattern... Dark? Design Attributes, Normative Considerations, and Measurement Methods},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445610},
doi = {10.1145/3411764.3445610},
abstract = {There is a rapidly growing literature on dark patterns, user interface designs—typically related to shopping or privacy—that researchers deem problematic. Recent work has been predominantly descriptive, documenting and categorizing objectionable user interfaces. These contributions have been invaluable in highlighting specific designs for researchers and policymakers. But the current literature lacks a conceptual foundation: What makes a user interface a dark pattern? Why are certain designs problematic for users or society? We review recent work on dark patterns and demonstrate that the literature does not reflect a singular concern or consistent definition, but rather, a set of thematically related considerations. Drawing from scholarship in psychology, economics, ethics, philosophy, and law, we articulate a set of normative perspectives for analyzing dark patterns and their effects on individuals and society. We then show how future research on dark patterns can go beyond subjective criticism of user interface designs and apply empirical methods grounded in normative perspectives.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {360},
numpages = {18},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445598,
author = {Koshy, Vinay and Park, Joon Sung Sung and Cheng, Ti-Chung and Karahalios, Karrie},
title = {“We Just Use What They Give Us”: Understanding Passenger User Perspectives in Smart Homes},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445598},
doi = {10.1145/3411764.3445598},
abstract = {With a plethora of off-the-shelf smart home devices available commercially, people are increasingly taking a do-it-yourself approach to configuring their smart homes. While this allows for customization, users responsible for smart home configuration often end up with more control over the devices than other household members. This separates those who introduce new functionality to the smart home (pilot users) from those who do not (passenger users). To investigate the prevalence and impact of pilot-passenger user relationships, we conducted a Mechanical Turk survey and a series of one-hour interviews. Our results suggest that pilot-passenger relationships are common in multi-user households and shape how people form habits around devices. We find from interview data that smart homes reflect the values of their pilot users, making it harder for passenger users to incorporate their devices into daily life. We conclude the paper with design recommendations to improve passenger and pilot user experience.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {41},
numpages = {14},
keywords = {smart homes, passenger users, internet of things, mixed-methods study, user experience, domestication theory, pilot users},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445592,
author = {M\'{a}rquez Segura, Elena and Rogers, Katja and Martin-Niedecken, Anna Lisa and Niedecken, Stephan and Vidal, Laia Turmo},
title = {Exploring the Design Space of Immersive Social Fitness Games: The ImSoFit Games Model},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445592},
doi = {10.1145/3411764.3445592},
abstract = {The design space of social exergames remains narrow despite the many benefits of playing and exercising together. Towards opening this design space, we followed a Research through Design (RtD) approach focused on exergames that can be fun and immersive social training experiences. Through embodied sketching activities with designers and 10 pairs of players, we explored future games for the ExerCube, an immersive exergame platform. Our work contributes with forms of intermediate-level knowledge: a design space model (the Immersive Social Fitness—ImSoFit—Games model); and a novel design vocabulary including new bodily orientations in co-located physical interaction. We illustrate their use and value scrutinizing three of our games and applying three analytical lenses to 1) understand how design choices impact how players move together; 2) evaluate design expectations and analyze players’ behavior in relation to design choices; and 3) potentially extend the design space of immersive co-located social fitness games.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {442},
numpages = {14},
keywords = {social immersion, design space, exergame, fitness game, multiplayer, mixed reality, embodied sketching},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445590,
author = {To, Alexandra and Carey, Hillary and Kaufman, Geoff and Hammer, Jessica},
title = {Reducing Uncertainty and Offering Comfort: Designing Technology for Coping with Interpersonal Racism},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445590},
doi = {10.1145/3411764.3445590},
abstract = {Ranging from subtle to overt, unintentional to systemic, navigating racism is additional everyday work for many people. Yet the needs of people who experience racism have been overlooked as a fertile ground for better technology. Through a series of workshops we call Foundational Fiction, we engaged BIPOC (Black, Indigenous, People of Color) in participatory design to identify qualities of technology that can support people coping before, during, and after a racist interaction. Participants developed storyboards for digital tools that offer advice, predict consequences, identify racist remarks and intervene, educate both targets and perpetrators about interpersonal and systemic racism, and more. In the paper we present our workshop method utilizing interactive fiction, participants’ design concepts, prevalent themes (reducing uncertainty and offering comfort), and we provide critical analysis of the complexity of technology in these contexts. This work identifies specific opportunities for exploring anti-racist social tools.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {398},
numpages = {17},
keywords = {microaggressions, uncertainty, participatory design, racism, design workshops, interactive fiction},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445587,
author = {Kirchner, Susanne and Schroeder, Jessica and Fogarty, James and Munson, Sean A.},
title = {“They Don’t Always Think about That”: Translational Needs in the Design of Personal Health Informatics Applications},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445587},
doi = {10.1145/3411764.3445587},
abstract = {Personal health informatics continues to grow in both research and practice, revealing many challenges of designing applications that address people’s needs in their health, everyday lives, and collaborations with clinicians. Research suggests strategies to address such challenges, but has struggled to translate these strategies into design practice. This study examines translation of insights from personal health informatics research into resources to support designers. Informed by a review of relevant literature, we present our development of a prototype set of design cards intended to support designers in re-thinking potential assumptions about personal health informatics. We examined our design cards in semi-structured interviews, first with 12 student designers and then with 12 health-focused professional designers and researchers. Our results and discussion reveal tensions and barriers designers encounter, the potential for translational resources to inform the design of health-related technologies, and a need to support designers in addressing challenges of knowledge, advocacy, and evidence in designing for health.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {210},
numpages = {16},
keywords = {Health Design, Translational Research, Personal Health Informatics},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445575,
author = {Currano, Rebecca and Park, So Yeon and Moore, Dylan James and Lyons, Kent and Sirkin, David},
title = {Little Road Driving HUD: Heads-Up Display Complexity Influences Drivers’ Perceptions of Automated Vehicles},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445575},
doi = {10.1145/3411764.3445575},
abstract = {Modern vehicles are using AI and increasingly sophisticated sensor suites to improve Advanced Driving Assistance Systems (ADAS) and support automated driving capabilities. Heads-Up-Displays (HUDs) provide an opportunity to visually inform drivers about vehicle perception and interpretation of the driving environment. One approach to HUD design may be to reveal to drivers the vehicle’s full contextual understanding, though it is not clear if the benefits of additional information outweigh the drawbacks of added complexity, or if this balance holds across drivers. We designed and tested an Augmented Reality (AR) HUD in an online study (N = 298), focusing on the influence of HUD visualizations on drivers’ situation awareness and perceptions. Participants viewed two driving scenes with one of three HUD conditions. Results were nuanced: situation awareness declined with increasing driving context complexity, and contrary to expectation, also declined with the presence of a HUD compared to no HUD. Significant differences were found by varying HUD complexity, which led us to explore different characterizations of complexity, including counts of scene items, item categories, and illuminated pixels. Our analysis finds that driving style interacts with driving context and HUD complexity, warranting further study.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {511},
numpages = {15},
keywords = {heads-up-display, situation awareness, augmented reality, vehicle interface, interaction design, user interface},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445563,
author = {Pakdamanian, Erfan and Sheng, Shili and Baee, Sonia and Heo, Seongkook and Kraus, Sarit and Feng, Lu},
title = {DeepTake: Prediction of Driver Takeover Behavior Using Multimodal Data},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445563},
doi = {10.1145/3411764.3445563},
abstract = {Automated vehicles promise a future where drivers can engage in non-driving tasks without hands on the steering wheels for a prolonged period. Nevertheless, automated vehicles may still need to occasionally hand the control back to drivers due to technology limitations and legal requirements. While some systems determine the need for driver takeover using driver context and road condition to initiate a takeover request, studies show that the driver may not react to it. We present DeepTake, a novel deep neural network-based framework that predicts multiple aspects of takeover behavior to ensure that the driver is able to safely take over the control when engaged in non-driving tasks. Using features from vehicle data, driver biometrics, and subjective measurements, DeepTake predicts the driver’s intention, time, and quality of takeover. We evaluate DeepTake performance using multiple evaluation metrics. Results show that DeepTake reliably predicts the takeover intention, time, and quality, with an accuracy of 96%, 93%, and 83%, respectively. Results also indicate that DeepTake outperforms previous state-of-the-art methods on predicting driver takeover time and quality. Our findings have implications for the algorithm development of driver monitoring and state detection.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {103},
numpages = {14},
keywords = {Takeover behavior, Multimodal data, Automated driving, Deep neural networks, Human-automation interaction},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445561,
author = {Park, Soomi and Healey, Patrick G. T. and Kaniadakis, Antonios},
title = {Should Robots Blush?},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445561},
doi = {10.1145/3411764.3445561},
abstract = {Social interaction is the most complex challenge in daily life. Inevitably, social robots will encounter interactions that are outside their competence. This raises a basic design question: how can robots fail gracefully in social interaction? The characteristic human response to social failure is embarrassment. Usefully, embarrassment signals both recognition of a problem and typically enlists sympathy and assistance to resolve it. This could enhance robot acceptability and provides an opportunity for interactive learning. Using a speculative design approach we explore how, when and why robots might communicate embarrassment. A series of specially developed cultural probes, scenario development and low-fidelity prototyping exercises suggest that: embarrassment is relevant for managing a diverse range of social scenarios, impacts on both humanoid and non-humanoid robot design, and highlights the critical importance of understanding interactional context. We conclude that embarrassment is fundamental to competent social functioning and provides a potentially fertile area for interaction design.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {717},
numpages = {14},
keywords = {Speculative Design, Human-Robot Interactions, Embarrassment, Design Workshop, Symbolic Interactionism, Cultural Probes, Affective Robotics},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445557,
author = {Robertson, Ronald E and Olteanu, Alexandra and Diaz, Fernando and Shokouhi, Milad and Bailey, Peter},
title = {“I Can’t Reply with That”: Characterizing Problematic Email Reply Suggestions},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445557},
doi = {10.1145/3411764.3445557},
abstract = {In email interfaces, providing users with reply suggestions may simplify or accelerate correspondence. While the “success” of such systems is typically quantified using the number of suggestions selected by users, this ignores the impact of social context, which can change how suggestions are perceived. To address this, we developed a mixed-methods framework involving qualitative interviews and crowdsourced experiments to characterize problematic email reply suggestions. Our interviews revealed issues with over-positive, dissonant, cultural, and gender-assuming replies, as well as contextual politeness. In our experiments, crowdworkers assessed email scenarios that we generated and systematically controlled, showing that contextual factors like social ties and the presence of salutations impacts users’ perceptions of email correspondence. These assessments created a novel dataset of human-authored corrections for problematic email replies. Our study highlights the social complexity of providing suggestions for email correspondence, raising issues that may apply to all social messaging systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {724},
numpages = {18},
keywords = {smart compose, algorithm auditing, smart reply, AI-assisted writing, email, CMC, AI-MC},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445540,
author = {Carstensdottir, Elin and Kleinman, Erica and Williams, Ryan and Seif El-Nasr, Magy Seif},
title = {”Naked and on Fire”: Examining Player Agency Experiences in Narrative-Focused Gameplay},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445540},
doi = {10.1145/3411764.3445540},
abstract = {Player agency is central to interactive narrative and games. While previous work focuses on analyzing player perception of agency through various lenses and phenomena, like meaningful choice and expectations, it is largely theoretical. Few user studies within games explore how players reason about and judge their own agency within interactive narratives. We present an interview study where participants rated their agency experiences within narrative-focused games and described their reasoning. The analysis suggests that agency perception depends on multiple factors beyond meaningful choice, such as social investment and genre-conventions. Participants described varying preferences and value judgements for different factors, indicating that individual differences have a deep impact on agency perception in narrative-focused gameplay. We discuss the implications of these cognitive variables on design, how they can be leveraged with other factors, and how our findings can help future work enhance and measure player agency, within interactive narrative and beyond.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {121},
numpages = {13},
keywords = {user experience, player experience, interactive narrative, storytelling, video games, player agency},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445537,
author = {Li, Nianlong and Zhang, Zhengquan and Liu, Can and Yang, Zengyao and Fu, Yinan and Tian, Feng and Han, Teng and Fan, Mingming},
title = {VMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445537},
doi = {10.1145/3411764.3445537},
abstract = {Interacting with out of reach or occluded VR objects can be cumbersome. Although users can change their position and orientation, such as via teleporting, to help observe and select, doing so frequently may cause loss of spatial orientation or motion sickness. We present vMirror, an interactive widget leveraging reflection of mirrors to observe and select distant or occluded objects. We first designed interaction techniques for placing mirrors and interacting with objects through mirrors. We then conducted a formative study to explore a semi-automated mirror placement method with manual adjustments. Next, we conducted a target-selection experiment to measure the effect of the mirror’s orientation on users’ performance. Results showed that vMirror can be as efficient as direct target selection for most mirror orientations. We further compared vMirror with teleport technique in a virtual treasure hunt game and measured participants’ task performance and subjective experiences. Finally, we discuss vMirorr user experience and present future directions.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {132},
numpages = {11},
keywords = {out of reach, Virtual mirror, occlusion, Virtual Reality, VR, DOF, vMirror, target selection, raycasting},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445528,
author = {Liebers, Jonathan and Abdelaziz, Mark and Mecke, Lukas and Saad, Alia and Auda, Jonas and Gruenefeld, Uwe and Alt, Florian and Schneegass, Stefan},
title = {Understanding User Identification in Virtual Reality Through Behavioral Biometrics and the Effect of Body Normalization},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445528},
doi = {10.1145/3411764.3445528},
abstract = {Virtual Reality (VR) is becoming increasingly popular both in the entertainment and professional domains. Behavioral biometrics have recently been investigated as a means to continuously and implicitly identify users in VR. Applications in VR can specifically benefit from this, for example, to adapt virtual environments and user interfaces as well as to authenticate users. In this work, we conduct a lab study (N = 16) to explore how accurately users can be identified during two task-driven scenarios based on their spatial movement. We show that an identification accuracy of up to 90% is possible across sessions recorded on different days. Moreover, we investigate the role of users’ physiology in behavioral biometrics by virtually altering and normalizing their body proportions. We find that body normalization in general increases the identification rate, in some cases by up to 38%; hence, it improves the performance of identification systems.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {517},
numpages = {11},
keywords = {virtual reality, usable security, task-driven biometrics, identification},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445508,
author = {Zheng, Rebecca and Fern\'{a}ndez Camporro, Marina and Romat, Hugo and Henry Riche, Nathalie and Bach, Benjamin and Chevalier, Fanny and Hinckley, Ken and Marquardt, Nicolai},
title = {Sketchnote Components, Design Space Dimensions, and Strategies for Effective Visual Note Taking},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445508},
doi = {10.1145/3411764.3445508},
abstract = {Sketchnoting is a form of visual note taking where people listen to, synthesize, and visualize ideas from a talk or other event using a combination of pictures, diagrams, and text. Little is known about the design space of this kind of visual note taking. With an eye towards informing the implementation of digital equivalents of sketchnoting, inking, and note taking, we introduce a classification of sketchnote styles and techniques, with a qualitative analysis of 103 sketchnotes, and situated in context with six semi-structured follow up interviews. Our findings distill core sketchnote components (content, layout, structuring elements, and visual styling) and dimensions of the sketchnote design space, classifying levels of conciseness, illustration, structure, personification, cohesion, and craftsmanship. We unpack strategies to address particular note taking challenges, for example dealing with constraints of live drawings, and discuss relevance for future digital inking tools, such as recomposition, styling, and design suggestions.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {466},
numpages = {15},
keywords = {taxonomy, visual note taking, digital ink, digital sketching, graphical styles, sketchnotes, classification, sketching},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445487,
author = {Xue, Tong and El Ali, Abdallah and Zhang, Tianyi and Ding, Gangyi and Cesar, Pablo},
title = {RCEA-360VR: Real-Time, Continuous Emotion Annotation in 360° VR Videos for Collecting Precise Viewport-Dependent Ground Truth Labels},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445487},
doi = {10.1145/3411764.3445487},
abstract = {Precise emotion ground truth labels for 360° virtual reality (VR) video watching are essential for fine-grained predictions under varying viewing behavior. However, current annotation techniques either rely on post-stimulus discrete self-reports, or real-time, continuous emotion annotations (RCEA) but only for desktop/mobile settings. We present RCEA for 360° VR videos (RCEA-360VR), where we evaluate in a controlled study (N=32) the usability of two peripheral visualization techniques: HaloLight and DotSize. We furthermore develop a method that considers head movements when fusing labels. Using physiological, behavioral, and subjective measures, we show that (1) both techniques do not increase users’ workload, sickness, nor break presence (2) our continuous valence and arousal annotations are consistent with discrete within-VR and original stimuli ratings (3) users exhibit high similarity in viewing behavior, where fused ratings perfectly align with intended labels. Our work contributes usable and effective techniques for collecting fine-grained viewport-dependent emotion labels in 360° VR.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {513},
numpages = {15},
keywords = {continuous, viewport-dependent, Emotion, virtual reality, real-time, 360° video, ground truth, labels, annotation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445469,
author = {Hong, Freddie and Myant, Connor and Boyle, David E},
title = {Thermoformed Circuit Boards: Fabrication of Highly Conductive Freeform 3D Printed Circuit Boards with Heat Bending},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445469},
doi = {10.1145/3411764.3445469},
abstract = {Fabricating 3D printed electronics using desktop printers has become more accessible with recent developments in conductive thermoplastic filaments. Because of their high resistance and difficulties in printing traces in vertical directions, most applications are restricted to capacitive sensing. In this paper, we introduce Thermoformed Circuit Board (TCB), a novel approach that employs the thermoformability of the 3D printed plastics to construct various double-sided, rigid and highly conductive freeform circuit boards that can withstand high current applications through copper electroplating. To illustrate the capability of the TCB, we showcase a range of examples with various shapes, electrical characteristics and interaction mechanisms. We also demonstrate a new design tool extension to an existing CAD environment that allows users to parametrically draw the substrate and conductive trace, and export 3D printable files. TCB is an inexpensive and highly accessible fabrication technique intended to broaden HCI researcher participation.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {669},
numpages = {10},
keywords = {conductive filament, 3D printed electronics, hybrid additive manufacturing},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445465,
author = {Reinhardt, Daniel and Borchard, Johannes and Hurtienne, J\"{o}rn},
title = {Visual Interactive Privacy Policy: The Better Choice?},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445465},
doi = {10.1145/3411764.3445465},
abstract = {Online privacy policies should enable users to make informed decisions. Current text policies, however, lack usability: users often miss crucial information and consent to them without reading. Visual representation formats may increase comprehension, but are rarely used in practice. In an iterative design process we gathered qualitative feedback on typical policy contents and on existing and newly designed representation formats. We developed design guidelines and a Visual Interactive Privacy Policy based on the Privacy Policy Nutrition Label enriched with control options and further interactive elements. In an empirical evaluation, both visual representations received higher ratings of attractiveness, stimulation, novelty and transparency compared to a standard policy long text. Interactivity improved time spent with the policy. There were no effects on conversion rate, perceived control or perceived trust, efficiency and perspicuity. More research is needed, especially with regard to the cost-benefit ratio of visual privacy policies.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {66},
numpages = {12},
keywords = {Privacy Policy Nutrition Label, Privacy Policy, Privacy},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445460,
author = {Saquib, Nazmus and Kazi, Rubaiat Habib and Wei, Li-yi and Mark, Gloria and Roy, Deb},
title = {Constructing Embodied Algebra by Sketching},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445460},
doi = {10.1145/3411764.3445460},
abstract = {Mathematical models and expressions traditionally evolved as symbolic representations, with cognitively arbitrary rules of symbol manipulation. The embodied mathematics philosophy posits that abstract math concepts are layers of metaphors grounded in our intuitive arithmetic capabilities, such as categorizing objects and part-whole analysis. We introduce a design framework that facilitates the construction and exploration of embodied representations for algebraic expressions, using interactions inspired by innate arithmetic capabilities. We instantiated our design in a sketch interface that enables construction of visually interpretable compositions that are directly mappable to algebraic expressions and explorable through a ladder of abstraction [47]. The emphasis is on bottom-up construction, with the user sketching pictures while the system generates corresponding algebra. We present diverse examples created by our prototype system. A coverage of the US Common Core curriculum and playtesting studies with children point to the future direction and potential for a sketch-based design paradigm for mathematics.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {428},
numpages = {16},
keywords = {sketching, embodied cognition, embodied math, math modeling},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445458,
author = {Subramanian, Krishna and Maas, Johannes and Borchers, Jan and Hollan, James},
title = {From Detectables to Inspectables: Understanding Qualitative Analysis of Audiovisual Data},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445458},
doi = {10.1145/3411764.3445458},
abstract = {Audiovisual recordings of user studies and interviews provide important data in qualitative HCI research. Even when a textual transcription is available, researchers frequently turn to these recordings due to their rich information content. However, the temporal, unstructured nature of audiovisual recordings makes them less efficient to work with than text. Through interviews and a survey, we explored how HCI researchers work with audiovisual recordings. We investigated researchers’ transcription and annotation practice, their overall analysis workflow, and the prevalence of direct analysis of audiovisual recordings. We found that a key task was locating and analyzing inspectables, interesting segments in recordings. Since locating inspectables can be time consuming, participants look for detectables, visual or auditory cues that indicate the presence of an inspectable. Based on our findings, we discuss the potential for automation in locating detectables in qualitative audiovisual analysis.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {518},
numpages = {10},
keywords = {information behavior, researchers, Video analysis, qualitative research},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445442,
author = {Kim, Daehwa and Park, Keunwoo and Lee, Geehyuk},
title = {AtaTouch: Robust Finger Pinch Detection for a VR Controller Using RF Return Loss},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445442},
doi = {10.1145/3411764.3445442},
abstract = {Handheld controllers are an essential part of VR systems. Modern sensing techniques enable them to track users’ finger movements to support natural interaction using hands. The sensing techniques, however, often fail to precisely determine whether two fingertips touch each other, which is important for the robust detection of a pinch gesture. To address this problem, we propose AtaTouch, which is a novel, robust sensing technique for detecting the closure of a finger pinch. It utilizes a change in the coupled impedance of an antenna and human fingers when the thumb and finger form a loop. We implemented a prototype controller in which AtaTouch detects the finger pinch of the grabbing hand. A user test with the prototype showed a finger-touch detection accuracy of 96.4%. Another user test with the scenarios of moving virtual blocks demonstrated low object-drop rate (2.75%) and false-pinch rate (4.40%). The results and feedback from the participants support the robustness and sensitivity of AtaTouch.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {11},
numpages = {9},
keywords = {Antenna, Virtual Reality (VR), Touch segmentation, RF Sensing, Hand gesture},
location = {Yokohama, Japan},
series = {CHI '21}
}

