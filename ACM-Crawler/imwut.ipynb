{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doi</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.1145/3314388</td>\n",
       "      <td>Sayma Akther, Nazir Saleheen, Shahin Alan Sami...</td>\n",
       "      <td>mORAL: An mHealth Model for Inferring Oral Hyg...</td>\n",
       "      <td>We address the open problem of reliably detect...</td>\n",
       "      <td>Volume 3 Issue 1, March 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1145/3314389</td>\n",
       "      <td>Ling Chen, Yifang Ding, Dandan Lyu, Xiaoze Liu...</td>\n",
       "      <td>Deep Multi-Task Learning Based Urban Air Quali...</td>\n",
       "      <td>Obtaining comprehensive air quality informatio...</td>\n",
       "      <td>Volume 3 Issue 1, March 2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              doi  \\\n",
       "0           0  10.1145/3314388   \n",
       "1           1  10.1145/3314389   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Sayma Akther, Nazir Saleheen, Shahin Alan Sami...   \n",
       "1  Ling Chen, Yifang Ding, Dandan Lyu, Xiaoze Liu...   \n",
       "\n",
       "                                               title  \\\n",
       "0  mORAL: An mHealth Model for Inferring Oral Hyg...   \n",
       "1  Deep Multi-Task Learning Based Urban Air Quali...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We address the open problem of reliably detect...   \n",
       "1  Obtaining comprehensive air quality informatio...   \n",
       "\n",
       "                          issue  \n",
       "0  Volume 3 Issue 1, March 2019  \n",
       "1  Volume 3 Issue 1, March 2019  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurations.\n",
    "debug = False\n",
    "homeurl = \"https://dl.acm.org/citation.cfm?id=J1566&picked=prox\"\n",
    "re_issue = re.compile(\".*\\n(?P<issue>.*)\\n.*\")\n",
    "re_toc = re.compile(\"Table of Contents\")\n",
    "re_paper = re.compile(\"https://doi.org/(?P<doi>.*)\")\n",
    "re_title = re.compile(\".*citation.cfm\\?id=(?P<id>)\\d+\")\n",
    "\n",
    "# Load data if exported.\n",
    "df = pd.read_csv(\"./IMWUT.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a webdriver instance.\n",
    "driver = webdriver.Chrome(\"./chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 issues found.\n"
     ]
    }
   ],
   "source": [
    "# Navigate to the home page.\n",
    "driver.get(homeurl)\n",
    "\n",
    "# Request TOC.\n",
    "buttons = driver.find_elements_by_tag_name(\"button\")\n",
    "for button in buttons:\n",
    "    inner_html = button.get_attribute(\"innerHTML\")\n",
    "    if re_toc.search(inner_html):\n",
    "        button.click()\n",
    "\n",
    "# Wait until TOC loaded.\n",
    "try:\n",
    "    toc = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//div[@id='cf_layoutareaprox']/descendant::a\"))\n",
    "    )\n",
    "except TimeoutException as e:\n",
    "    print(\"{} time out.\".format(homeurl))\n",
    "\n",
    "# Get issue urls.\n",
    "issue_urls = driver.find_elements_by_xpath(\"//div[@id='cf_layoutareaprox']/descendant::a\")\n",
    "issue_hrefs = [url.get_attribute(\"href\") for url in issue_urls]\n",
    "print(\"{} issues found.\".format(len(issue_hrefs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume 3 Issue 1, March 2019\n",
      "34 papers extracted.\n",
      "34 34 34 34\n",
      "Volume 2 Issue 4, December 2018\n",
      "50 papers extracted.\n",
      "84 84 84 84\n",
      "Volume 2 Issue 3, September 2018\n",
      "64 papers extracted.\n",
      "148 148 148 148\n",
      "Volume 2 Issue 2, June 2018\n",
      "32 papers extracted.\n",
      "180 180 180 180\n",
      "Volume 2 Issue 1, March 2018\n",
      "56 papers extracted.\n",
      "236 236 236 236\n",
      "Volume 1 Issue 4, December 2017\n",
      "55 papers extracted.\n",
      "291 291 291 291\n",
      "Volume 1 Issue 3, September 2017\n",
      "90 papers extracted.\n",
      "381 381 381 381\n",
      "Volume 1 Issue 2, June 2017\n",
      "29 papers extracted.\n",
      "410 410 410 410\n",
      "Volume 1 Issue 1, March 2017\n",
      "Message: no such element: Unable to locate element: {\"method\":\"tag name\",\"selector\":\"a\"}\n",
      "  (Session info: chrome=73.0.3683.86)\n",
      "  (Driver info: chromedriver=73.0.3683.68 (47787ec04b6e38e22703e856e101e840b65afe72),platform=Mac OS X 10.14.4 x86_64)\n",
      "\n",
      "3 papers extracted.\n",
      "413 413 413 413\n"
     ]
    }
   ],
   "source": [
    "# Iterating issues to get the titles and abstracts.\n",
    "issue_list = []\n",
    "doi_list = []\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "author_list = []\n",
    "\n",
    "for issue_href in issue_hrefs:\n",
    "    # Navigate to the issue page.\n",
    "    driver.get(issue_href)\n",
    "    \n",
    "    # Request TOC.\n",
    "    buttons = driver.find_elements_by_tag_name(\"button\")\n",
    "    for button in buttons:\n",
    "        inner_html = button.get_attribute(\"innerHTML\")\n",
    "        if re_toc.search(inner_html):\n",
    "            button.click()\n",
    "    \n",
    "    # Wait until TOC loaded.\n",
    "    try:\n",
    "        toc = WebDriverWait(driver, 60).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//div[@id='cf_layoutareaprox']/descendant::p\"))\n",
    "        )\n",
    "    except TimeoutException as e:\n",
    "        print(\"{} time out.\".format(issue_href))\n",
    "        continue\n",
    "    \n",
    "    # Get issue name.\n",
    "    heading = driver.find_element_by_xpath(\"//h1[text()[contains(., 'Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies')]]/..\")\n",
    "    re_result = re_issue.match(heading.get_attribute(\"innerHTML\")[10:-10])\n",
    "    issue_name = re_result.group(\"issue\")\n",
    "    print(issue_name)\n",
    "    \n",
    "    # Get title, doi, and abstact for current issue.\n",
    "    tr_elements = driver.find_elements_by_xpath(\"//div[@id='cf_layoutareaprox']/descendant::table/tbody/tr\")\n",
    "    idx_tr = 0\n",
    "    counter = 0\n",
    "    while idx_tr < len(tr_elements):\n",
    "\n",
    "        # Start search when hitting a title.\n",
    "        tr = tr_elements[idx_tr]\n",
    "        try:\n",
    "            href = tr.find_element_by_tag_name(\"a\").get_attribute(\"href\")\n",
    "            re_result = re_title.match(href)\n",
    "            if re_result:\n",
    "                # Hit a title.\n",
    "                title = tr.text\n",
    "                if debug:\n",
    "                    print(title)\n",
    "                idx_tr += 1\n",
    "                tr = tr_elements[idx_tr]\n",
    "\n",
    "                # Authors.\n",
    "                authors = tr.text\n",
    "                if debug:\n",
    "                    print(authors)\n",
    "                idx_tr += 1\n",
    "                tr = tr_elements[idx_tr]\n",
    "\n",
    "                # Article number, skip.\n",
    "                if not tr.text.startswith(\"Article No\"):\n",
    "                    continue\n",
    "                idx_tr += 1\n",
    "                tr = tr_elements[idx_tr]\n",
    "\n",
    "                # doi.\n",
    "                doi = tr.find_element_by_tag_name(\"a\").text\n",
    "                if debug:\n",
    "                    print(doi)\n",
    "                idx_tr += 1\n",
    "                tr = tr_elements[idx_tr]\n",
    "\n",
    "                # Full text (pdf), skip.\n",
    "                idx_tr += 1\n",
    "                tr = tr_elements[idx_tr]\n",
    "\n",
    "                # Abstract.\n",
    "                abstract = \"\\n\".join([p.get_attribute(\"innerHTML\") for p in tr.find_elements_by_tag_name(\"p\")])\n",
    "\n",
    "                # Add to database.\n",
    "                author_list.append(authors)\n",
    "                issue_list.append(issue_name)\n",
    "                doi_list.append(doi)\n",
    "                title_list.append(title)\n",
    "                abstract_list.append(abstract)\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "        except NoSuchElementException as e:\n",
    "            print(e)\n",
    "\n",
    "        idx_tr += 1\n",
    "\n",
    "    print(\"{} papers extracted.\".format(counter))\n",
    "    print(\"{} {} {} {}\".format(len(doi_list), len(author_list), len(title_list), len(abstract_list)))\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From dataframe and export.\n",
    "df = pd.DataFrame({\"doi\": doi_list,\n",
    "                   \"authors\": author_list,\n",
    "                   \"title\": title_list,\n",
    "                   \"abstract\": abstract_list,\n",
    "                   \"issue\": issue_list})\n",
    "df.to_csv(\"IMWUT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yong Li              12\n",
       "Vassilis Kostakos    11\n",
       "Daqing Zhang          8\n",
       "Cecilia Mascolo       8\n",
       "Uichin Lee            7\n",
       "Jie Xiong             7\n",
       "Jorge Goncalves       7\n",
       "Tao Gu                7\n",
       "Wei Wang              7\n",
       "Niels van Berkel      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Author stat.\n",
    "author_papers = {}\n",
    "\n",
    "for authors in df.authors:\n",
    "    names = authors.split(\", \")\n",
    "    for name in names:\n",
    "        if name in author_papers:\n",
    "            author_papers[name] += 1\n",
    "        else:\n",
    "            author_papers[name] = 1\n",
    "            \n",
    "ser_author_papers = pd.Series(author_papers)\n",
    "ser_author_papers = ser_author_papers.sort_values(ascending=False)\n",
    "ser_author_papers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user        544\n",
       "datum       511\n",
       "system      501\n",
       "device      361\n",
       "study       293\n",
       "accuracy    228\n",
       "method      223\n",
       "model       223\n",
       "approach    221\n",
       "paper       220\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count.\n",
    "import string\n",
    "import nltk\n",
    "from pattern.text.en import singularize, pluralize, lemma\n",
    "\n",
    "from collections import Counter\n",
    "combined_text = \" \".join(df.abstract).lower()\n",
    "\n",
    "# Preprocessing.\n",
    "# Remove punctuation.\n",
    "combined_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "# Remove prepositions, conjunctions, etc. \n",
    "tokens = nltk.word_tokenize(combined_text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "black_list = [\"%\", \"result\",]\n",
    "white_list = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "filtered = list(filter(lambda x: x[1] in white_list, tagged))\n",
    "filtered = [token[0] for token in filtered]\n",
    "filtered = list(filter(lambda x: x not in black_list, filtered))\n",
    "filtered = [singularize(word) for word in filtered]\n",
    "\n",
    "wordcount = Counter(filtered)\n",
    "ser_wordcount = pd.Series(wordcount)\n",
    "ser_wordcount = ser_wordcount.sort_values(ascending=False)\n",
    "ser_wordcount.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datum           33\n",
       "sensor          32\n",
       "system          29\n",
       "device          28\n",
       "recognition     25\n",
       "activity        24\n",
       "interaction     19\n",
       "smartphone      19\n",
       "detection       14\n",
       "localization    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count.\n",
    "import string\n",
    "import nltk\n",
    "from pattern.text.en import singularize, pluralize, lemma\n",
    "\n",
    "from collections import Counter\n",
    "combined_text = \" \".join(df.title).lower()\n",
    "\n",
    "# Preprocessing.\n",
    "# Remove punctuation.\n",
    "combined_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "# Remove prepositions, conjunctions, etc. \n",
    "tokens = nltk.word_tokenize(combined_text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "black_list = [\"%\", \"result\",]\n",
    "white_list = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "filtered = list(filter(lambda x: x[1] in white_list, tagged))\n",
    "filtered = [token[0] for token in filtered]\n",
    "filtered = list(filter(lambda x: x not in black_list, filtered))\n",
    "filtered = [singularize(word) for word in filtered]\n",
    "\n",
    "wordcount = Counter(filtered)\n",
    "ser_wordcount = pd.Series(wordcount)\n",
    "ser_wordcount = ser_wordcount.sort_values(ascending=False)\n",
    "ser_wordcount.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context\n",
      "and data                   2\n",
      "clinical data              1\n",
      "coarse-grained data        1\n",
      "cohort data                1\n",
      "dyadic data                1\n",
      "gps data                   1\n",
      "improves data              1\n",
      "intimate data              1\n",
      "mobility data              1\n",
      "on data                    1\n",
      "personal data              1\n",
      "privacy-preserving data    1\n",
      "sensing data               1\n",
      "spatio-temporal data       1\n",
      "urban data                 1\n",
      "use data                   1\n",
      "dtype: int64\n",
      "context\n",
      "activity sensor            1\n",
      "algorithmic sensor         1\n",
      "ear-mounted sensor         1\n",
      "inertial sensor            1\n",
      "magnetic sensor            1\n",
      "mobile sensor              2\n",
      "motion sensor              1\n",
      "of sensor                  1\n",
      "physiological sensor       1\n",
      "quality sensor             1\n",
      "respiration sensor         1\n",
      "smartphone-based sensor    1\n",
      "soft sensor                1\n",
      "unaided sensor             1\n",
      "vibration sensor           2\n",
      "wearable sensor            5\n",
      "dtype: int64\n",
      "context\n",
      "hmd device                     1\n",
      "mobile device                  5\n",
      "portable device                1\n",
      "power-as-needed device         1\n",
      "resource-constrained device    1\n",
      "self-reporting device          1\n",
      "touchscreen device             1\n",
      "wearable device                5\n",
      "wi-fi device                   3\n",
      "wrist-worn device              2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"data\", \"sensor\", \"system\", \"device\", \"recognition\", \"activity\", \"interaction\", \"smartphone\", \"detection\", \"localization\"]\n",
    "keyword_context_list = []\n",
    "keyword_list = []\n",
    "\n",
    "for keyword in keywords:\n",
    "    for title in df.title.tolist():\n",
    "        title_words = singularize(title).lower().split()\n",
    "        # Search keywods.\n",
    "        try:\n",
    "            index_keyword_list = [i for i, x in enumerate(title_words) if x == keyword]\n",
    "            for index_keyword in index_keyword_list:\n",
    "                index_keyword = title_words.index(keyword)\n",
    "                if index_keyword > 0:\n",
    "                    context_phrase = \" \".join([title_words[index_keyword-1], keyword])\n",
    "                    keyword_list.append(keyword)\n",
    "                    keyword_context_list.append(context_phrase)\n",
    "        except ValueError as e:\n",
    "            pass\n",
    "        \n",
    "df_keyword_context = pd.DataFrame({\"keyword\": keyword_list,\n",
    "                                   \"context\": keyword_context_list})\n",
    "ser_count = df_keyword_context.groupby(df_keyword_context.columns.tolist(),as_index=False).size()\n",
    "print(ser_count.loc[\"data\"])\n",
    "print(ser_count.loc[\"sensor\"])\n",
    "print(ser_count.loc[\"device\"])\n",
    "\n",
    "\n",
    "# keyword_context_counter = Counter(keyword_context_list)\n",
    "# ser_keyword_context = pd.Series(keyword_context_counter).sort_index()\n",
    "# ser_keyword_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"word\", \"word\"].index(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
